{"2024-10-21T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2410.14234v2","updated":"2024-10-21T06:31:50Z","published":"2024-10-18T07:31:32Z","title":"Optimal, Non-pipelined Reduce-scatter and Allreduce Algorithms","summary":"  The reduce-scatter collective operation in which $p$ processors in a network\nof processors collectively reduce $p$ input vectors into a result vector that\nis partitioned over the processors is important both in its own right and as\nbuilding block for other collective operations. We present a surprisingly\nsimple, but non-trivial algorithm for solving this problem optimally in\n$\\lceil\\log_2 p\\rceil$ communication rounds with each process sending,\nreceiving and reducing exactly $p-1$ blocks of vector elements. We combine this\nwith a similarly simple allgather algorithm to get a likewise optimal algorithm\nfor the allreduce collective operation where the result vector is replicated on\nall processors. The communication pattern is a simple, $\\lceil\\log_2\np\\rceil$-regular, circulant graph also used elsewhere. The algorithms assume\nthe binary reduction operator to be commutative and we discuss this assumption.\nThe algorithms can readily be implemented and used for the collective\noperations MPI_Reduce_scatter_block, MPI_Reduce_scatter and MPI_Allreduce as\nspecified in the MPI standard. The communication pattern can likewise be used\nfor all-to-all communication.\n","authors":["Jesper Larsson Träff"],"pdf_url":"https://arxiv.org/pdf/2410.14234v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16141v1","updated":"2024-10-21T16:08:20Z","published":"2024-10-21T16:08:20Z","title":"AdChain: Decentralized Header Bidding","summary":"  Due to the involvement of multiple intermediaries without trusted parties,\nlack of proper regulations, and a complicated supply chain, ad impression\ndiscrepancy affects online advertising. This issue causes up to $82 billion\nannual revenue loss for honest parties. The loss can be significantly reduced\nwith a precise and trusted decentralized mechanism. This paper presents\nAdChain, a decentralized, distributed, and verifiable solution that detects and\nminimizes online advertisement impression discrepancies. AdChain establishes\ntrust by employing multiple independent agents to receive and record log-level\ndata, along with a consensus protocol to validate each ad data. AdChain is\nscalable, efficient, and compatible with the current infrastructure. Our\nexperimental evaluation, using over half a million ad data points, identifies\nsystem parameters that achieve 98% accuracy, reducing the ad discrepancy rate\nfrom 20% to 2%. Our cost analysis shows that active nodes on AdChain can\ngenerate profits comparable to miners on major blockchain networks like\nBitcoin.\n","authors":["Behkish Nassirzadeh","Albert Heinle","Stefanos Leonardos","Anwar Hasan","Vijay Ganesh"],"pdf_url":"https://arxiv.org/pdf/2410.16141v1.pdf","comment":"Being published at MARBLE 2024 (The 5th International Conference on\n  Mathematical Research for Blockchain Economy)"},{"id":"http://arxiv.org/abs/2406.12727v2","updated":"2024-10-21T15:41:18Z","published":"2024-06-18T15:47:30Z","title":"Massively Parallel Ruling Set Made Deterministic","summary":"  We study the deterministic complexity of the $2$-Ruling Set problem in the\nmodel of Massively Parallel Computation (MPC) with linear and strongly\nsublinear local memory.\n  Linear MPC: We present a constant-round deterministic algorithm for the\n$2$-Ruling Set problem that matches the randomized round complexity recently\nsettled by Cambus, Kuhn, Pai, and Uitto [DISC'23], and improves upon the\ndeterministic $O(\\log \\log n)$-round algorithm by Pai and Pemmaraju [PODC'22].\nOur main ingredient is a simpler analysis of CKPU's algorithm based solely on\nbounded independence, which makes its efficient derandomization possible.\n  Sublinear MPC: We present a deterministic algorithm that computes a\n$2$-Ruling Set in $\\tilde O(\\sqrt{\\log n})$ rounds deterministically. Notably,\nthis is the first deterministic ruling set algorithm with sublogarithmic round\ncomplexity, improving on the $O(\\log \\Delta + \\log \\log^* n)$-round complexity\nthat stems from the deterministic MIS algorithm of Czumaj, Davies, and Parter\n[TALG'21]. Our result is based on a simple and fast randomness-efficient\nconstruction that achieves the same sparsification as that of the randomized\n$\\tilde O(\\sqrt{\\log n})$-round LOCAL algorithm by Kothapalli and Pemmaraju\n[FSTTCS'12].\n","authors":["Jeff Giliberti","Zahra Parsaeian"],"pdf_url":"https://arxiv.org/pdf/2406.12727v2.pdf","comment":"Accepted at DISC'24"},{"id":"http://arxiv.org/abs/2410.16110v1","updated":"2024-10-21T15:38:38Z","published":"2024-10-21T15:38:38Z","title":"DUMBO: Making durable read-only transactions fly on hardware\n  transactional memory","summary":"  Despite the recent improvements in supporting Persistent Hardware\nTransactions (PHTs) on emerging persistent memories (PM), the poor performance\nof Read-Only (RO) transactions remains largely overlooked. We propose DUMBO, a\nnew design for PHT that eliminates the two most crucial bottlenecks that hinder\nRO transactions in state-of-the-art PHT. At its core, DUMBO exploits advanced\ninstructions that some contemporary HTMs provide to suspend (and resume)\ntransactional access tracking. Our experimental evaluation with an IBM POWER9\nsystem using the TPC-C benchmark shows that DUMBO can outperform the state of\nthe art designs for persistent hardware (SPHT) and software memory transactions\n(Pisces), by up to 4.0x.\n","authors":["João Barreto","Daniel Castro","Paolo Romano","Alexandro Baldassin"],"pdf_url":"https://arxiv.org/pdf/2410.16110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16093v1","updated":"2024-10-21T15:16:00Z","published":"2024-10-21T15:16:00Z","title":"Final Report for CHESS: Cloud, High-Performance Computing, and Edge for\n  Science and Security","summary":"  Automating the theory-experiment cycle requires effective distributed\nworkflows that utilize a computing continuum spanning lab instruments, edge\nsensors, computing resources at multiple facilities, data sets distributed\nacross multiple information sources, and potentially cloud. Unfortunately, the\nobvious methods for constructing continuum platforms, orchestrating workflow\ntasks, and curating datasets over time fail to achieve scientific requirements\nfor performance, energy, security, and reliability. Furthermore, achieving the\nbest use of continuum resources depends upon the efficient composition and\nexecution of workflow tasks, i.e., combinations of numerical solvers, data\nanalytics, and machine learning. Pacific Northwest National Laboratory's LDRD\n\"Cloud, High-Performance Computing (HPC), and Edge for Science and Security\"\n(CHESS) has developed a set of interrelated capabilities for enabling\ndistributed scientific workflows and curating datasets. This report describes\nthe results and successes of CHESS from the perspective of open science.\n","authors":["Nathan Tallent","Jan Strube","Luanzheng Guo","Hyungro Lee","Jesun Firoz","Sayan Ghosh","Bo Fang","Oceane Bel","Steven Spurgeon","Sarah Akers","Christina Doty","Erol Cromwell"],"pdf_url":"https://arxiv.org/pdf/2410.16093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16026v1","updated":"2024-10-21T13:59:27Z","published":"2024-10-21T13:59:27Z","title":"HyperDrive: Scheduling Serverless Functions in the Edge-Cloud-Space 3D\n  Continuum","summary":"  The number of Low Earth Orbit~(LEO) satellites has grown enormously in the\npast years. Their abundance and low orbits allow for low latency communication\nwith a satellite almost anywhere on Earth, and high-speed inter-satellite laser\nlinks~(ISLs) enable a quick exchange of large amounts of data among satellites.\nAs the computational capabilities of LEO satellites grow, they are becoming\neligible as general-purpose compute nodes. In the 3D continuum, which combines\nCloud and Edge nodes on Earth and satellites in space into a seamless computing\nfabric, workloads can be executed on any of the aforementioned compute nodes,\ndepending on where it is most beneficial. However, scheduling on LEO satellites\nmoving at approx. 27,000 km/h requires picking the satellite with the lowest\nlatency to all data sources (ground and, possibly, earth observation\nsatellites). Dissipating heat from onboard hardware is challenging when facing\nthe sun and workloads must not drain the satellite's batteries. These factors\nmake meeting SLOs more challenging than in the Edge-Cloud continuum, i.e., on\nEarth alone. We present HyperDrive, an SLO-aware scheduler for serverless\nfunctions specifically designed for the 3D continuum. It places functions on\nCloud, Edge, or Space compute nodes, based on their availability and ability to\nmeet the SLO requirements of the workflow. We evaluate HyperDrive using a\nwildfire disaster response use case with high Earth Observation data processing\nrequirements and stringent SLOs, showing that it enables the design and\nexecution of such next-generation 3D scenarios with 71% lower network latency\nthan the best baseline scheduler.\n","authors":["Thomas Pusztai","Cynthia Marcelino","Stefan Nastic"],"pdf_url":"https://arxiv.org/pdf/2410.16026v1.pdf","comment":"2024 IEEE/ACM Symposium on Edge Computing(SEC)"},{"id":"http://arxiv.org/abs/2311.04875v2","updated":"2024-10-21T13:48:26Z","published":"2023-11-08T18:22:42Z","title":"Fusionize++: Improving Serverless Application Performance Using Dynamic\n  Task Inlining and Infrastructure Optimization","summary":"  The Function-as-a-Service (FaaS) execution model increases developer\nproductivity by removing operational concerns such as managing hardware or\nsoftware runtimes. Developers, however, still need to partition their\napplications into FaaS functions, which is error-prone and complex:\nEncapsulating only the smallest logical unit of an application as a FaaS\nfunction maximizes flexibility and reusability. Yet, it also leads to\ninvocation overheads, additional cold starts, and may increase cost due to\ndouble billing during synchronous invocations. Conversely, deploying an entire\napplication as a single FaaS function avoids these overheads but decreases\nflexibility. In this paper we present Fusionize, a framework that automates\noptimizing for this trade-off by automatically fusing application code into an\noptimized multi-function composition. Developers only need to write\nfine-grained application code following the serverless model, while Fusionize\nautomatically fuses different parts of the application into FaaS functions,\nmanages their interactions, and configures the underlying infrastructure. At\nruntime, it monitors application performance and adapts it to minimize\nrequest-response latency and costs. Real-world use cases show that Fusionize\ncan improve the deployment artifacts of the application, reducing both median\nrequest-response latency and cost of an example IoT application by more than\n35%.\n","authors":["Trever Schirmer","Joel Scheuner","Tobias Pfandzelter","David Bermbach"],"pdf_url":"https://arxiv.org/pdf/2311.04875v2.pdf","comment":"Author copy of article accepted in IEEE Transactions on Cloud\n  Computing with DOI 10.1109/TCC.2024.3451108"},{"id":"http://arxiv.org/abs/2410.15758v1","updated":"2024-10-21T08:18:52Z","published":"2024-10-21T08:18:52Z","title":"Digital Product Passport Management with Decentralised Identifiers and\n  Verifiable Credentials","summary":"  Digital product passports (DPP) have been proposed in the European Ecodesign\nfor Sustainable Products Regulation (ESPR) as a means to keep and provide\nproduct information that facilitates product reusage, reparation, and\nrecycling. Thus, DPPs should provide a positive effect on the environmental\nimpact of future manufactured products, preventing waste and promoting a\ncircular economy (CE) model. ESPR settles a set of requirements in collecting\nand administering product-related data. Decentralised identifiers (DID) and\nverifiable credentials (VC) are two self-sovereign-identity-related elements\nthat may help in that DPP management since they introduce a decentralised\nadministration of identity that may enhance the overall scalability of the\nresulting system, improving also its reliability. This paper analyses the ESPR\nrequirements and describes how they may be achieved using DIDs and VCs,\nassessing their performance in some scenarios.\n","authors":["Ismael Illán García","Francesc D. Muñoz-Escoí","Jordi Arjona Aroca","F. Javier Fernández-Bravo Peñuela"],"pdf_url":"https://arxiv.org/pdf/2410.15758v1.pdf","comment":"22 pages, 8 images"},{"id":"http://arxiv.org/abs/2410.15681v1","updated":"2024-10-21T06:43:04Z","published":"2024-10-21T06:43:04Z","title":"Federated Learning with MMD-based Early Stopping for Adaptive GNSS\n  Interference Classification","summary":"  Federated learning (FL) enables multiple devices to collaboratively train a\nglobal model while maintaining data on local servers. Each device trains the\nmodel on its local server and shares only the model updates (i.e., gradient\nweights) during the aggregation step. A significant challenge in FL is managing\nthe feature distribution of novel, unbalanced data across devices. In this\npaper, we propose an FL approach using few-shot learning and aggregation of the\nmodel weights on a global server. We introduce a dynamic early stopping method\nto balance out-of-distribution classes based on representation learning,\nspecifically utilizing the maximum mean discrepancy of feature embeddings\nbetween local and global models. An exemplary application of FL is\norchestrating machine learning models along highways for interference\nclassification based on snapshots from global navigation satellite system\n(GNSS) receivers. Extensive experiments on four GNSS datasets from two\nreal-world highways and controlled environments demonstrate that our FL method\nsurpasses state-of-the-art techniques in adapting to both novel interference\nclasses and multipath scenarios.\n","authors":["Nishant S. Gaikwad","Lucas Heublein","Nisha L. Raichur","Tobias Feigl","Christopher Mutschler","Felix Ott"],"pdf_url":"https://arxiv.org/pdf/2410.15681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15626v1","updated":"2024-10-21T04:10:54Z","published":"2024-10-21T04:10:54Z","title":"Hybrid Quantum-HPC Solutions for Max-Cut: Bridging Classical and Quantum\n  Algorithms","summary":"  This research explores the integration of the Quantum Approximate\nOptimization Algorithm (QAOA) into Hybrid Quantum-HPC systems for solving the\nMax-Cut problem, comparing its performance with classical algorithms like\nbrute-force search and greedy heuristics. We develop a theoretical model to\nanalyze the time complexity, scalability, and communication overhead in hybrid\nsystems. Using simulations, we evaluate QAOA's performance on small-scale\nMax-Cut instances, benchmarking its runtime, solution accuracy, and resource\nutilization. The study also investigates the scalability of QAOA with\nincreasing problem size, offering insights into its potential advantages over\nclassical methods for large-scale combinatorial optimization problems, with\nimplications for future Quantum computing applications in HPC environments.\n","authors":["Ishan Patwardhan","Akhil Akkapelli"],"pdf_url":"https://arxiv.org/pdf/2410.15626v1.pdf","comment":"Submitted to IEEE PuneCon"},{"id":"http://arxiv.org/abs/2410.15625v1","updated":"2024-10-21T04:08:37Z","published":"2024-10-21T04:08:37Z","title":"Improving Parallel Program Performance Through DSL-Driven Code\n  Generation with LLM Optimizers","summary":"  Mapping computations to processors and assigning data to memory are critical\nfor maximizing performance in parallel programming. These mapping decisions are\nmanaged through the development of specialized low-level system code, called\nmappers, crafted by performance engineers. Each mapper is tailored to a\nspecific application and optimized for the underlying machine architecture, a\nprocess that requires days of refinement and tuning from an expert. Despite\nadvances in system research, automating mapper generation remains a challenge\ndue to the complexity of making millions of decisions to find the optimal\nsolution and generate the solution as code. We introduce an approach that\nleverages recent advances in LLM-based optimizers for mapper design. In under\nten minutes, our method automatically discovers mappers that surpass human\nexpert designs in scientific applications by up to 1.34X speedup. For parallel\nmatrix multiplication algorithms, our mapper achieves up to 1.31X of the\nexpert-designed solution. To achieve this, we simplify the complexity of\nlow-level code generation by introducing a domain-specific language (DSL) that\nabstracts the low-level system programming details and defines a structured\nsearch space for LLMs to explore. To maximize the application performance, we\nuse an LLM optimizer to improve an agentic system that generates the mapper\ncode. As a result, this approach significantly reduces the workload for\nperformance engineers while achieving substantial performance gains across\ndiverse applications. Finally, our results demonstrate the effectiveness of\nLLM-based optimization in system design and suggest its potential for\naddressing other complex system challenges.\n","authors":["Anjiang Wei","Allen Nie","Thiago S. F. X. Teixeira","Rohan Yadav","Wonchan Lee","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2410.15625v1.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.09202v2","updated":"2024-10-21T02:35:08Z","published":"2024-09-13T21:31:45Z","title":"WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions","summary":"  This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.\n","authors":["Rui Li","Devesh Tiwari","Gene Cooperman"],"pdf_url":"https://arxiv.org/pdf/2409.09202v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.08934v2","updated":"2024-10-21T20:57:53Z","published":"2024-10-11T16:00:21Z","title":"The Effect of Personalization in FedProx: A Fine-grained Analysis on\n  Statistical Accuracy and Communication Efficiency","summary":"  FedProx is a simple yet effective federated learning method that enables\nmodel personalization via regularization. Despite remarkable success in\npractice, a rigorous analysis of how such a regularization provably improves\nthe statistical accuracy of each client's local model hasn't been fully\nestablished. Setting the regularization strength heuristically presents a risk,\nas an inappropriate choice may even degrade accuracy. This work fills in the\ngap by analyzing the effect of regularization on statistical accuracy, thereby\nproviding a theoretical guideline for setting the regularization strength for\nachieving personalization. We prove that by adaptively choosing the\nregularization strength under different statistical heterogeneity, FedProx can\nconsistently outperform pure local training and achieve a nearly\nminimax-optimal statistical rate. In addition, to shed light on resource\nallocation, we design an algorithm, provably showing that stronger\npersonalization reduces communication complexity without increasing the\ncomputation cost overhead. Finally, our theory is validated on both synthetic\nand real-world datasets and its generalizability is verified in a non-convex\nsetting.\n","authors":["Xin Yu","Zelin He","Ying Sun","Lingzhou Xue","Runze Li"],"pdf_url":"https://arxiv.org/pdf/2410.08934v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16569v1","updated":"2024-10-21T22:58:03Z","published":"2024-10-21T22:58:03Z","title":"Streamlining Cloud-Native Application Development and Deployment with\n  Robust Encapsulation","summary":"  Current Serverless abstractions (e.g., FaaS) poorly support non-functional\nrequirements (e.g., QoS and constraints), are provider-dependent, and are\nincompatible with other cloud abstractions (e.g., databases). As a result,\napplication developers have to undergo numerous rounds of development and\nmanual deployment refinements to finally achieve their desired quality and\nefficiency. In this paper, we present Object-as-a-Service (OaaS) -- a novel\nserverless paradigm that borrows the object-oriented programming concepts to\nencapsulate business logic, data, and non-functional requirements into a single\ndeployment package, thereby streamlining provider-agnostic cloud-native\napplication development. We also propose a declarative interface for the\nnon-functional requirements of applications that relieves developers from\ndaunting refinements to meet their desired QoS and deployment constraint\ntargets. We realized the OaaS paradigm through a platform called Oparaca and\nevaluated it against various real-world applications and scenarios. The\nevaluation results demonstrate that Oparaca can enhance application performance\nby 60X and improve reliability by 50X through latency, throughput, and\navailability enforcement -- all with remarkably less development and deployment\ntime and effort.\n","authors":["Pawissanutt Lertpongrujikorn","Hai Duc Nguyen","Mohsen Amini Salehi"],"pdf_url":"https://arxiv.org/pdf/2410.16569v1.pdf","comment":"Accepted at ACM Symposium of Cloud Computing (SoCC '24)"},{"id":"http://arxiv.org/abs/2410.16487v1","updated":"2024-10-21T20:27:19Z","published":"2024-10-21T20:27:19Z","title":"Adventures with Grace Hopper AI Super Chip and the National Research\n  Platform","summary":"  The National Science Foundation (NSF) funded National Research Platform (NRP)\nis a hyper-converged cluster of nationally and globally interconnected\nheterogeneous computing resources. The dominant computing environment of the\nNRP is the x86 64 instruction set architecture (ISA), often with graphics\nprocessing units (GPUs). Researchers across the nation leverage containers and\nKubernetes to execute high-throughput computing (HTC) workloads across the\nheterogeneous cyberinfrastructure with minimal friction and maximum\nflexibility. As part of the NSF-funded GP-ENGINE project, we stood up the first\nserver with an NVIDIA Grace Hopper AI Chip (GH200), an alternative ARM ISA, for\nthe NRP. This presents challenges, as containers must be specifically built for\nARM versus x86 64. Herein, we describe the challenges encountered, as well as\nour resulting solutions and some relevant performance benchmarks. We\nspecifically compare the GH200 to A100 for computer vision workloads, within\ncompute nodes in the NRP.\n","authors":["J. Alex Hurt","Grant J. Scott","Derek Weitzel","Huijun Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.16487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16398v1","updated":"2024-10-21T18:09:22Z","published":"2024-10-21T18:09:22Z","title":"Federated Communication-Efficient Multi-Objective Optimization","summary":"  We study a federated version of multi-objective optimization (MOO), where a\nsingle model is trained to optimize multiple objective functions. MOO has been\nextensively studied in the centralized setting but is less explored in\nfederated or distributed settings. We propose FedCMOO, a novel\ncommunication-efficient federated multi-objective optimization (FMOO) algorithm\nthat improves the error convergence performance of the model compared to\nexisting approaches. Unlike prior works, the communication cost of FedCMOO does\nnot scale with the number of objectives, as each client sends a single\naggregated gradient, obtained using randomized SVD (singular value\ndecomposition), to the central server. We provide a convergence analysis of the\nproposed method for smooth non-convex objective functions under milder\nassumptions than in prior work. In addition, we introduce a variant of FedCMOO\nthat allows users to specify a preference over the objectives in terms of a\ndesired ratio of the final objective values. Through extensive experiments, we\ndemonstrate the superiority of our proposed method over baseline approaches.\n","authors":["Baris Askin","Pranay Sharma","Gauri Joshi","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2410.16398v1.pdf","comment":null}],"Hardware Architecture":[{"id":"http://arxiv.org/abs/2410.14234v2","updated":"2024-10-21T06:31:50Z","published":"2024-10-18T07:31:32Z","title":"Optimal, Non-pipelined Reduce-scatter and Allreduce Algorithms","summary":"  The reduce-scatter collective operation in which $p$ processors in a network\nof processors collectively reduce $p$ input vectors into a result vector that\nis partitioned over the processors is important both in its own right and as\nbuilding block for other collective operations. We present a surprisingly\nsimple, but non-trivial algorithm for solving this problem optimally in\n$\\lceil\\log_2 p\\rceil$ communication rounds with each process sending,\nreceiving and reducing exactly $p-1$ blocks of vector elements. We combine this\nwith a similarly simple allgather algorithm to get a likewise optimal algorithm\nfor the allreduce collective operation where the result vector is replicated on\nall processors. The communication pattern is a simple, $\\lceil\\log_2\np\\rceil$-regular, circulant graph also used elsewhere. The algorithms assume\nthe binary reduction operator to be commutative and we discuss this assumption.\nThe algorithms can readily be implemented and used for the collective\noperations MPI_Reduce_scatter_block, MPI_Reduce_scatter and MPI_Allreduce as\nspecified in the MPI standard. The communication pattern can likewise be used\nfor all-to-all communication.\n","authors":["Jesper Larsson Träff"],"pdf_url":"https://arxiv.org/pdf/2410.14234v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16141v1","updated":"2024-10-21T16:08:20Z","published":"2024-10-21T16:08:20Z","title":"AdChain: Decentralized Header Bidding","summary":"  Due to the involvement of multiple intermediaries without trusted parties,\nlack of proper regulations, and a complicated supply chain, ad impression\ndiscrepancy affects online advertising. This issue causes up to $82 billion\nannual revenue loss for honest parties. The loss can be significantly reduced\nwith a precise and trusted decentralized mechanism. This paper presents\nAdChain, a decentralized, distributed, and verifiable solution that detects and\nminimizes online advertisement impression discrepancies. AdChain establishes\ntrust by employing multiple independent agents to receive and record log-level\ndata, along with a consensus protocol to validate each ad data. AdChain is\nscalable, efficient, and compatible with the current infrastructure. Our\nexperimental evaluation, using over half a million ad data points, identifies\nsystem parameters that achieve 98% accuracy, reducing the ad discrepancy rate\nfrom 20% to 2%. Our cost analysis shows that active nodes on AdChain can\ngenerate profits comparable to miners on major blockchain networks like\nBitcoin.\n","authors":["Behkish Nassirzadeh","Albert Heinle","Stefanos Leonardos","Anwar Hasan","Vijay Ganesh"],"pdf_url":"https://arxiv.org/pdf/2410.16141v1.pdf","comment":"Being published at MARBLE 2024 (The 5th International Conference on\n  Mathematical Research for Blockchain Economy)"},{"id":"http://arxiv.org/abs/2406.12727v2","updated":"2024-10-21T15:41:18Z","published":"2024-06-18T15:47:30Z","title":"Massively Parallel Ruling Set Made Deterministic","summary":"  We study the deterministic complexity of the $2$-Ruling Set problem in the\nmodel of Massively Parallel Computation (MPC) with linear and strongly\nsublinear local memory.\n  Linear MPC: We present a constant-round deterministic algorithm for the\n$2$-Ruling Set problem that matches the randomized round complexity recently\nsettled by Cambus, Kuhn, Pai, and Uitto [DISC'23], and improves upon the\ndeterministic $O(\\log \\log n)$-round algorithm by Pai and Pemmaraju [PODC'22].\nOur main ingredient is a simpler analysis of CKPU's algorithm based solely on\nbounded independence, which makes its efficient derandomization possible.\n  Sublinear MPC: We present a deterministic algorithm that computes a\n$2$-Ruling Set in $\\tilde O(\\sqrt{\\log n})$ rounds deterministically. Notably,\nthis is the first deterministic ruling set algorithm with sublogarithmic round\ncomplexity, improving on the $O(\\log \\Delta + \\log \\log^* n)$-round complexity\nthat stems from the deterministic MIS algorithm of Czumaj, Davies, and Parter\n[TALG'21]. Our result is based on a simple and fast randomness-efficient\nconstruction that achieves the same sparsification as that of the randomized\n$\\tilde O(\\sqrt{\\log n})$-round LOCAL algorithm by Kothapalli and Pemmaraju\n[FSTTCS'12].\n","authors":["Jeff Giliberti","Zahra Parsaeian"],"pdf_url":"https://arxiv.org/pdf/2406.12727v2.pdf","comment":"Accepted at DISC'24"},{"id":"http://arxiv.org/abs/2410.16110v1","updated":"2024-10-21T15:38:38Z","published":"2024-10-21T15:38:38Z","title":"DUMBO: Making durable read-only transactions fly on hardware\n  transactional memory","summary":"  Despite the recent improvements in supporting Persistent Hardware\nTransactions (PHTs) on emerging persistent memories (PM), the poor performance\nof Read-Only (RO) transactions remains largely overlooked. We propose DUMBO, a\nnew design for PHT that eliminates the two most crucial bottlenecks that hinder\nRO transactions in state-of-the-art PHT. At its core, DUMBO exploits advanced\ninstructions that some contemporary HTMs provide to suspend (and resume)\ntransactional access tracking. Our experimental evaluation with an IBM POWER9\nsystem using the TPC-C benchmark shows that DUMBO can outperform the state of\nthe art designs for persistent hardware (SPHT) and software memory transactions\n(Pisces), by up to 4.0x.\n","authors":["João Barreto","Daniel Castro","Paolo Romano","Alexandro Baldassin"],"pdf_url":"https://arxiv.org/pdf/2410.16110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16093v1","updated":"2024-10-21T15:16:00Z","published":"2024-10-21T15:16:00Z","title":"Final Report for CHESS: Cloud, High-Performance Computing, and Edge for\n  Science and Security","summary":"  Automating the theory-experiment cycle requires effective distributed\nworkflows that utilize a computing continuum spanning lab instruments, edge\nsensors, computing resources at multiple facilities, data sets distributed\nacross multiple information sources, and potentially cloud. Unfortunately, the\nobvious methods for constructing continuum platforms, orchestrating workflow\ntasks, and curating datasets over time fail to achieve scientific requirements\nfor performance, energy, security, and reliability. Furthermore, achieving the\nbest use of continuum resources depends upon the efficient composition and\nexecution of workflow tasks, i.e., combinations of numerical solvers, data\nanalytics, and machine learning. Pacific Northwest National Laboratory's LDRD\n\"Cloud, High-Performance Computing (HPC), and Edge for Science and Security\"\n(CHESS) has developed a set of interrelated capabilities for enabling\ndistributed scientific workflows and curating datasets. This report describes\nthe results and successes of CHESS from the perspective of open science.\n","authors":["Nathan Tallent","Jan Strube","Luanzheng Guo","Hyungro Lee","Jesun Firoz","Sayan Ghosh","Bo Fang","Oceane Bel","Steven Spurgeon","Sarah Akers","Christina Doty","Erol Cromwell"],"pdf_url":"https://arxiv.org/pdf/2410.16093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16026v1","updated":"2024-10-21T13:59:27Z","published":"2024-10-21T13:59:27Z","title":"HyperDrive: Scheduling Serverless Functions in the Edge-Cloud-Space 3D\n  Continuum","summary":"  The number of Low Earth Orbit~(LEO) satellites has grown enormously in the\npast years. Their abundance and low orbits allow for low latency communication\nwith a satellite almost anywhere on Earth, and high-speed inter-satellite laser\nlinks~(ISLs) enable a quick exchange of large amounts of data among satellites.\nAs the computational capabilities of LEO satellites grow, they are becoming\neligible as general-purpose compute nodes. In the 3D continuum, which combines\nCloud and Edge nodes on Earth and satellites in space into a seamless computing\nfabric, workloads can be executed on any of the aforementioned compute nodes,\ndepending on where it is most beneficial. However, scheduling on LEO satellites\nmoving at approx. 27,000 km/h requires picking the satellite with the lowest\nlatency to all data sources (ground and, possibly, earth observation\nsatellites). Dissipating heat from onboard hardware is challenging when facing\nthe sun and workloads must not drain the satellite's batteries. These factors\nmake meeting SLOs more challenging than in the Edge-Cloud continuum, i.e., on\nEarth alone. We present HyperDrive, an SLO-aware scheduler for serverless\nfunctions specifically designed for the 3D continuum. It places functions on\nCloud, Edge, or Space compute nodes, based on their availability and ability to\nmeet the SLO requirements of the workflow. We evaluate HyperDrive using a\nwildfire disaster response use case with high Earth Observation data processing\nrequirements and stringent SLOs, showing that it enables the design and\nexecution of such next-generation 3D scenarios with 71% lower network latency\nthan the best baseline scheduler.\n","authors":["Thomas Pusztai","Cynthia Marcelino","Stefan Nastic"],"pdf_url":"https://arxiv.org/pdf/2410.16026v1.pdf","comment":"2024 IEEE/ACM Symposium on Edge Computing(SEC)"},{"id":"http://arxiv.org/abs/2311.04875v2","updated":"2024-10-21T13:48:26Z","published":"2023-11-08T18:22:42Z","title":"Fusionize++: Improving Serverless Application Performance Using Dynamic\n  Task Inlining and Infrastructure Optimization","summary":"  The Function-as-a-Service (FaaS) execution model increases developer\nproductivity by removing operational concerns such as managing hardware or\nsoftware runtimes. Developers, however, still need to partition their\napplications into FaaS functions, which is error-prone and complex:\nEncapsulating only the smallest logical unit of an application as a FaaS\nfunction maximizes flexibility and reusability. Yet, it also leads to\ninvocation overheads, additional cold starts, and may increase cost due to\ndouble billing during synchronous invocations. Conversely, deploying an entire\napplication as a single FaaS function avoids these overheads but decreases\nflexibility. In this paper we present Fusionize, a framework that automates\noptimizing for this trade-off by automatically fusing application code into an\noptimized multi-function composition. Developers only need to write\nfine-grained application code following the serverless model, while Fusionize\nautomatically fuses different parts of the application into FaaS functions,\nmanages their interactions, and configures the underlying infrastructure. At\nruntime, it monitors application performance and adapts it to minimize\nrequest-response latency and costs. Real-world use cases show that Fusionize\ncan improve the deployment artifacts of the application, reducing both median\nrequest-response latency and cost of an example IoT application by more than\n35%.\n","authors":["Trever Schirmer","Joel Scheuner","Tobias Pfandzelter","David Bermbach"],"pdf_url":"https://arxiv.org/pdf/2311.04875v2.pdf","comment":"Author copy of article accepted in IEEE Transactions on Cloud\n  Computing with DOI 10.1109/TCC.2024.3451108"},{"id":"http://arxiv.org/abs/2410.15758v1","updated":"2024-10-21T08:18:52Z","published":"2024-10-21T08:18:52Z","title":"Digital Product Passport Management with Decentralised Identifiers and\n  Verifiable Credentials","summary":"  Digital product passports (DPP) have been proposed in the European Ecodesign\nfor Sustainable Products Regulation (ESPR) as a means to keep and provide\nproduct information that facilitates product reusage, reparation, and\nrecycling. Thus, DPPs should provide a positive effect on the environmental\nimpact of future manufactured products, preventing waste and promoting a\ncircular economy (CE) model. ESPR settles a set of requirements in collecting\nand administering product-related data. Decentralised identifiers (DID) and\nverifiable credentials (VC) are two self-sovereign-identity-related elements\nthat may help in that DPP management since they introduce a decentralised\nadministration of identity that may enhance the overall scalability of the\nresulting system, improving also its reliability. This paper analyses the ESPR\nrequirements and describes how they may be achieved using DIDs and VCs,\nassessing their performance in some scenarios.\n","authors":["Ismael Illán García","Francesc D. Muñoz-Escoí","Jordi Arjona Aroca","F. Javier Fernández-Bravo Peñuela"],"pdf_url":"https://arxiv.org/pdf/2410.15758v1.pdf","comment":"22 pages, 8 images"},{"id":"http://arxiv.org/abs/2410.15681v1","updated":"2024-10-21T06:43:04Z","published":"2024-10-21T06:43:04Z","title":"Federated Learning with MMD-based Early Stopping for Adaptive GNSS\n  Interference Classification","summary":"  Federated learning (FL) enables multiple devices to collaboratively train a\nglobal model while maintaining data on local servers. Each device trains the\nmodel on its local server and shares only the model updates (i.e., gradient\nweights) during the aggregation step. A significant challenge in FL is managing\nthe feature distribution of novel, unbalanced data across devices. In this\npaper, we propose an FL approach using few-shot learning and aggregation of the\nmodel weights on a global server. We introduce a dynamic early stopping method\nto balance out-of-distribution classes based on representation learning,\nspecifically utilizing the maximum mean discrepancy of feature embeddings\nbetween local and global models. An exemplary application of FL is\norchestrating machine learning models along highways for interference\nclassification based on snapshots from global navigation satellite system\n(GNSS) receivers. Extensive experiments on four GNSS datasets from two\nreal-world highways and controlled environments demonstrate that our FL method\nsurpasses state-of-the-art techniques in adapting to both novel interference\nclasses and multipath scenarios.\n","authors":["Nishant S. Gaikwad","Lucas Heublein","Nisha L. Raichur","Tobias Feigl","Christopher Mutschler","Felix Ott"],"pdf_url":"https://arxiv.org/pdf/2410.15681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15626v1","updated":"2024-10-21T04:10:54Z","published":"2024-10-21T04:10:54Z","title":"Hybrid Quantum-HPC Solutions for Max-Cut: Bridging Classical and Quantum\n  Algorithms","summary":"  This research explores the integration of the Quantum Approximate\nOptimization Algorithm (QAOA) into Hybrid Quantum-HPC systems for solving the\nMax-Cut problem, comparing its performance with classical algorithms like\nbrute-force search and greedy heuristics. We develop a theoretical model to\nanalyze the time complexity, scalability, and communication overhead in hybrid\nsystems. Using simulations, we evaluate QAOA's performance on small-scale\nMax-Cut instances, benchmarking its runtime, solution accuracy, and resource\nutilization. The study also investigates the scalability of QAOA with\nincreasing problem size, offering insights into its potential advantages over\nclassical methods for large-scale combinatorial optimization problems, with\nimplications for future Quantum computing applications in HPC environments.\n","authors":["Ishan Patwardhan","Akhil Akkapelli"],"pdf_url":"https://arxiv.org/pdf/2410.15626v1.pdf","comment":"Submitted to IEEE PuneCon"},{"id":"http://arxiv.org/abs/2410.15625v1","updated":"2024-10-21T04:08:37Z","published":"2024-10-21T04:08:37Z","title":"Improving Parallel Program Performance Through DSL-Driven Code\n  Generation with LLM Optimizers","summary":"  Mapping computations to processors and assigning data to memory are critical\nfor maximizing performance in parallel programming. These mapping decisions are\nmanaged through the development of specialized low-level system code, called\nmappers, crafted by performance engineers. Each mapper is tailored to a\nspecific application and optimized for the underlying machine architecture, a\nprocess that requires days of refinement and tuning from an expert. Despite\nadvances in system research, automating mapper generation remains a challenge\ndue to the complexity of making millions of decisions to find the optimal\nsolution and generate the solution as code. We introduce an approach that\nleverages recent advances in LLM-based optimizers for mapper design. In under\nten minutes, our method automatically discovers mappers that surpass human\nexpert designs in scientific applications by up to 1.34X speedup. For parallel\nmatrix multiplication algorithms, our mapper achieves up to 1.31X of the\nexpert-designed solution. To achieve this, we simplify the complexity of\nlow-level code generation by introducing a domain-specific language (DSL) that\nabstracts the low-level system programming details and defines a structured\nsearch space for LLMs to explore. To maximize the application performance, we\nuse an LLM optimizer to improve an agentic system that generates the mapper\ncode. As a result, this approach significantly reduces the workload for\nperformance engineers while achieving substantial performance gains across\ndiverse applications. Finally, our results demonstrate the effectiveness of\nLLM-based optimization in system design and suggest its potential for\naddressing other complex system challenges.\n","authors":["Anjiang Wei","Allen Nie","Thiago S. F. X. Teixeira","Rohan Yadav","Wonchan Lee","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2410.15625v1.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.09202v2","updated":"2024-10-21T02:35:08Z","published":"2024-09-13T21:31:45Z","title":"WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions","summary":"  This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.\n","authors":["Rui Li","Devesh Tiwari","Gene Cooperman"],"pdf_url":"https://arxiv.org/pdf/2409.09202v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.08934v2","updated":"2024-10-21T20:57:53Z","published":"2024-10-11T16:00:21Z","title":"The Effect of Personalization in FedProx: A Fine-grained Analysis on\n  Statistical Accuracy and Communication Efficiency","summary":"  FedProx is a simple yet effective federated learning method that enables\nmodel personalization via regularization. Despite remarkable success in\npractice, a rigorous analysis of how such a regularization provably improves\nthe statistical accuracy of each client's local model hasn't been fully\nestablished. Setting the regularization strength heuristically presents a risk,\nas an inappropriate choice may even degrade accuracy. This work fills in the\ngap by analyzing the effect of regularization on statistical accuracy, thereby\nproviding a theoretical guideline for setting the regularization strength for\nachieving personalization. We prove that by adaptively choosing the\nregularization strength under different statistical heterogeneity, FedProx can\nconsistently outperform pure local training and achieve a nearly\nminimax-optimal statistical rate. In addition, to shed light on resource\nallocation, we design an algorithm, provably showing that stronger\npersonalization reduces communication complexity without increasing the\ncomputation cost overhead. Finally, our theory is validated on both synthetic\nand real-world datasets and its generalizability is verified in a non-convex\nsetting.\n","authors":["Xin Yu","Zelin He","Ying Sun","Lingzhou Xue","Runze Li"],"pdf_url":"https://arxiv.org/pdf/2410.08934v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16569v1","updated":"2024-10-21T22:58:03Z","published":"2024-10-21T22:58:03Z","title":"Streamlining Cloud-Native Application Development and Deployment with\n  Robust Encapsulation","summary":"  Current Serverless abstractions (e.g., FaaS) poorly support non-functional\nrequirements (e.g., QoS and constraints), are provider-dependent, and are\nincompatible with other cloud abstractions (e.g., databases). As a result,\napplication developers have to undergo numerous rounds of development and\nmanual deployment refinements to finally achieve their desired quality and\nefficiency. In this paper, we present Object-as-a-Service (OaaS) -- a novel\nserverless paradigm that borrows the object-oriented programming concepts to\nencapsulate business logic, data, and non-functional requirements into a single\ndeployment package, thereby streamlining provider-agnostic cloud-native\napplication development. We also propose a declarative interface for the\nnon-functional requirements of applications that relieves developers from\ndaunting refinements to meet their desired QoS and deployment constraint\ntargets. We realized the OaaS paradigm through a platform called Oparaca and\nevaluated it against various real-world applications and scenarios. The\nevaluation results demonstrate that Oparaca can enhance application performance\nby 60X and improve reliability by 50X through latency, throughput, and\navailability enforcement -- all with remarkably less development and deployment\ntime and effort.\n","authors":["Pawissanutt Lertpongrujikorn","Hai Duc Nguyen","Mohsen Amini Salehi"],"pdf_url":"https://arxiv.org/pdf/2410.16569v1.pdf","comment":"Accepted at ACM Symposium of Cloud Computing (SoCC '24)"},{"id":"http://arxiv.org/abs/2410.16487v1","updated":"2024-10-21T20:27:19Z","published":"2024-10-21T20:27:19Z","title":"Adventures with Grace Hopper AI Super Chip and the National Research\n  Platform","summary":"  The National Science Foundation (NSF) funded National Research Platform (NRP)\nis a hyper-converged cluster of nationally and globally interconnected\nheterogeneous computing resources. The dominant computing environment of the\nNRP is the x86 64 instruction set architecture (ISA), often with graphics\nprocessing units (GPUs). Researchers across the nation leverage containers and\nKubernetes to execute high-throughput computing (HTC) workloads across the\nheterogeneous cyberinfrastructure with minimal friction and maximum\nflexibility. As part of the NSF-funded GP-ENGINE project, we stood up the first\nserver with an NVIDIA Grace Hopper AI Chip (GH200), an alternative ARM ISA, for\nthe NRP. This presents challenges, as containers must be specifically built for\nARM versus x86 64. Herein, we describe the challenges encountered, as well as\nour resulting solutions and some relevant performance benchmarks. We\nspecifically compare the GH200 to A100 for computer vision workloads, within\ncompute nodes in the NRP.\n","authors":["J. Alex Hurt","Grant J. Scott","Derek Weitzel","Huijun Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.16487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16398v1","updated":"2024-10-21T18:09:22Z","published":"2024-10-21T18:09:22Z","title":"Federated Communication-Efficient Multi-Objective Optimization","summary":"  We study a federated version of multi-objective optimization (MOO), where a\nsingle model is trained to optimize multiple objective functions. MOO has been\nextensively studied in the centralized setting but is less explored in\nfederated or distributed settings. We propose FedCMOO, a novel\ncommunication-efficient federated multi-objective optimization (FMOO) algorithm\nthat improves the error convergence performance of the model compared to\nexisting approaches. Unlike prior works, the communication cost of FedCMOO does\nnot scale with the number of objectives, as each client sends a single\naggregated gradient, obtained using randomized SVD (singular value\ndecomposition), to the central server. We provide a convergence analysis of the\nproposed method for smooth non-convex objective functions under milder\nassumptions than in prior work. In addition, we introduce a variant of FedCMOO\nthat allows users to specify a preference over the objectives in terms of a\ndesired ratio of the final objective values. Through extensive experiments, we\ndemonstrate the superiority of our proposed method over baseline approaches.\n","authors":["Baris Askin","Pranay Sharma","Gauri Joshi","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2410.16398v1.pdf","comment":null}],"Databases":[{"id":"http://arxiv.org/abs/2410.10270v3","updated":"2024-10-21T08:13:45Z","published":"2024-10-14T08:21:25Z","title":"QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis","summary":"  Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.\n","authors":["Abhijit Manatkar","Ashlesha Akella","Parthivi Gupta","Krishnasuri Narayanam"],"pdf_url":"https://arxiv.org/pdf/2410.10270v3.pdf","comment":"Accepted for EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.14066v2","updated":"2024-10-21T07:50:28Z","published":"2024-10-17T22:28:07Z","title":"Lightweight Correlation-Aware Table Compression","summary":"  The growing adoption of data lakes for managing relational data necessitates\nefficient, open storage formats that provide high scan performance and\ncompetitive compression ratios. While existing formats achieve fast scans\nthrough lightweight encoding techniques, they have reached a plateau in terms\nof minimizing storage footprint. Recently, correlation-aware compression\nschemes have been shown to reduce file sizes further. Yet, current approaches\neither incur significant scan overheads or require manual specification of\ncorrelations, limiting their practicability. We present $\\texttt{Virtual}$, a\nframework that integrates seamlessly with existing open formats to\nautomatically leverage data correlations, achieving substantial compression\ngains while having minimal scan performance overhead. Experiments on data-gov\ndatasets show that $\\texttt{Virtual}$ reduces file sizes by up to 40% compared\nto Apache Parquet.\n","authors":["Mihail Stoian","Alexander van Renen","Jan Kobiolka","Ping-Lin Kuo","Josif Grabocka","Andreas Kipf"],"pdf_url":"https://arxiv.org/pdf/2410.14066v2.pdf","comment":"Third Table Representation Learning Workshop (TRL @ NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.06062v3","updated":"2024-10-21T09:13:48Z","published":"2024-10-08T14:09:12Z","title":"LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs","summary":"  We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.\n","authors":["Vincent Emonet","Jerven Bolleman","Severine Duvaud","Tarcisio Mendes de Farias","Ana Claudia Sima"],"pdf_url":"https://arxiv.org/pdf/2410.06062v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16165v1","updated":"2024-10-21T16:31:23Z","published":"2024-10-21T16:31:23Z","title":"From Tokens to Materials: Leveraging Language Models for Scientific\n  Discovery","summary":"  Exploring the predictive capabilities of language models in material science\nis an ongoing interest. This study investigates the application of language\nmodel embeddings to enhance material property prediction in materials science.\nBy evaluating various contextual embedding methods and pre-trained models,\nincluding Bidirectional Encoder Representations from Transformers (BERT) and\nGenerative Pre-trained Transformers (GPT), we demonstrate that domain-specific\nmodels, particularly MatBERT significantly outperform general-purpose models in\nextracting implicit knowledge from compound names and material properties. Our\nfindings reveal that information-dense embeddings from the third layer of\nMatBERT, combined with a context-averaging approach, offer the most effective\nmethod for capturing material-property relationships from the scientific\nliterature. We also identify a crucial \"tokenizer effect,\" highlighting the\nimportance of specialized text processing techniques that preserve complete\ncompound names while maintaining consistent token counts. These insights\nunderscore the value of domain-specific training and tokenization in materials\nscience applications and offer a promising pathway for accelerating the\ndiscovery and development of new materials through AI-driven approaches.\n","authors":["Yuwei Wan","Tong Xie","Nan Wu","Wenjie Zhang","Chunyu Kit","Bram Hoex"],"pdf_url":"https://arxiv.org/pdf/2410.16165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16120v1","updated":"2024-10-21T15:47:58Z","published":"2024-10-21T15:47:58Z","title":"Learning SQL from within: integrating database exercises into the\n  database itself","summary":"  SQL adventure builder (SQLab) is an open-source framework for creating SQL\ngames that are embedded within the very database they query. Students' answers\nare evaluated using query fingerprinting, a novel technique that allows for\nbetter feedback than traditional SQL online judge systems. Fingerprints act as\ntokens that are used to unlock messages encrypted in an isolated auxiliary\ntable. These messages may include hints, answer keys, examples, explanations,\nor narrative elements. They can also contain the problem statement of the next\ntask, which turns them into nodes in a virtual DAG with queries as edges. This\nmakes it possible to design a coherent adventure with a storyline of arbitrary\ncomplexity.\n  This paper describes the theoretical underpinnings of SQLab's query\nfingerprinting model, its implementation challenges, and its potential to\nimprove SQL education through game-based learning. The underlying concepts are\nfully cross-vendor, and support for SQLite, PostgreSQL and MySQL is already\navailable. As a proof of concept, two games, 30 exercises and one mock exam\nwere tested over a three-year period with about 300 students.\n","authors":["Aristide Grange"],"pdf_url":"https://arxiv.org/pdf/2410.16120v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2410.15831v1","updated":"2024-10-21T09:48:34Z","published":"2024-10-21T09:48:34Z","title":"Rethinking State Management in Actor Systems for Cloud-Native\n  Applications","summary":"  The actor model has gained increasing popularity. However, it lacks support\nfor complex state management tasks, such as enforcing foreign key constraints\nand ensuring data replication consistency across actors. These are crucial\nproperties in partitioned application designs, such as microservices. To fill\nthis gap, we start by analyzing the key impediments in state-of-the-art actor\nsystems. We find it difficult for developers to express complex data\nrelationships across actors and reason about the impact of state updates on\nperformance due to opaque state management abstractions.\n  To solve this conundrum, we develop SmSa, a novel data management layer for\nactor systems, allowing developers to declare data dependencies that cut across\nactors, including foreign keys, data replications, and other dependencies. SmSa\ncan transparently enforce the declared dependencies, reducing the burden on\ndevelopers. Furthermore, SmSa employs novel logging and concurrency control\nalgorithms to support transactional maintenance of data dependencies.\n  We demonstrate SmSa can support core data management tasks where dependencies\nacross components appear frequently without jeopardizing application logic\nexpressiveness and performance. Our experiments show SmSa significantly reduces\nthe logging overhead and leads to increased concurrency level, improving by up\nto 2X the performance of state-of-the-art deterministic scheduling approaches.\nAs a result, SmSa will make it easier to design and implement highly\npartitioned and distributed applications.\n","authors":["Yijian Liu","Rodrigo Laigner","Yongluan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.15831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15753v1","updated":"2024-10-21T08:11:47Z","published":"2024-10-21T08:11:47Z","title":"Natural Language Querying System Through Entity Enrichment","summary":"  This paper focuses on a domain expert querying system over databases. It\npresents a solution designed for a French enterprise interested in offering a\nnatural language interface for its clients. The approach, based on entity\nenrichment, aims at translating natural language queries into database queries.\nIn this paper, the database is treated through a logical paradigm, suggesting\nthe adaptability of our approach to different database models. The good\nprecision of our method is shown through some preliminary experiments.\n","authors":["Joshua Amavi","Mirian Halfeld Ferrari","Nicolas Hiot"],"pdf_url":"https://arxiv.org/pdf/2410.15753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06134v2","updated":"2024-10-21T05:36:42Z","published":"2024-08-12T13:23:49Z","title":"Learned Indexes with Distribution Smoothing via Virtual Points","summary":"  Recent research on learned indexes has created a new perspective for indexes\nas models that map keys to their respective storage locations. These learned\nindexes are created to approximate the cumulative distribution function of the\nkey set, where using only a single model may have limited accuracy. To overcome\nthis limitation, a typical method is to use multiple models, arranged in a\nhierarchical manner, where the query performance depends on two aspects: (i)\ntraversal time to find the correct model and (ii) search time to find the key\nin the selected model. Such a method may cause some key space regions that are\ndifficult to model to be placed at deeper levels in the hierarchy. To address\nthis issue, we propose an alternative method that modifies the key space as\nopposed to any structural or model modifications. This is achieved through\nmaking the key set more learnable (i.e., smoothing the distribution) by\ninserting virtual points. Furthermore, we develop an algorithm named CSV to\nintegrate our virtual point insertion method into existing learned indexes,\nreducing both their traversal and search time. We implement CSV on\nstate-of-the-art learned indexes and evaluate them on real-world datasets.\nExtensive experimental results show significant query performance improvement\nfor the keys in deeper levels of the index structures at a low storage cost.\n","authors":["Kasun Amarasinghe","Farhana Choudhury","Jianzhong Qi","James Bailey"],"pdf_url":"https://arxiv.org/pdf/2408.06134v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15547v1","updated":"2024-10-21T00:29:40Z","published":"2024-10-21T00:29:40Z","title":"Data Cleaning Using Large Language Models","summary":"  Data cleaning is a crucial yet challenging task in data analysis, often\nrequiring significant manual effort. To automate data cleaning, previous\nsystems have relied on statistical rules derived from erroneous data, resulting\nin low accuracy and recall. This work introduces Cocoon, a novel data cleaning\nsystem that leverages large language models for rules based on semantic\nunderstanding and combines them with statistical error detection. However, data\ncleaning is still too complex a task for current LLMs to handle in one shot. To\naddress this, we introduce Cocoon, which decomposes complex cleaning tasks into\nmanageable components in a workflow that mimics human cleaning processes. Our\nexperiments show that Cocoon outperforms state-of-the-art data cleaning systems\non standard benchmarks.\n","authors":["Shuo Zhang","Zezhou Huang","Eugene Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16501v1","updated":"2024-10-21T20:45:32Z","published":"2024-10-21T20:45:32Z","title":"The Cost of Representation by Subset Repairs","summary":"  Datasets may include errors, and specifically violations of integrity\nconstraints, for various reasons. Standard techniques for ``minimal-cost''\ndatabase repairing resolve these violations by aiming for minimum change in the\ndata, and in the process, may sway representations of different\nsub-populations. For instance, the repair may end up deleting more females than\nmales, or more tuples from a certain age group or race, due to varying levels\nof inconsistency in different sub-populations. Such repaired data can mislead\nconsumers when used for analytics, and can lead to biased decisions for\ndownstream machine learning tasks. We study the ``cost of representation'' in\nsubset repairs for functional dependencies. In simple terms, we target the\nquestion of how many additional tuples have to be deleted if we want to satisfy\nnot only the integrity constraints but also representation constraints for\ngiven sub-populations. We study the complexity of this problem and compare it\nwith the complexity of optimal subset repairs without representations. While\nthe problem is NP-hard in general, we give polynomial-time algorithms for\nspecial cases, and efficient heuristics for general cases. We perform a suite\nof experiments that show the effectiveness of our algorithms in computing or\napproximating the cost of representation.\n","authors":["Yuxi Liu","Fangzhu Shen","Kushagra Ghosh","Amir Gilad","Benny Kimelfeld","Sudeepa Roy"],"pdf_url":"https://arxiv.org/pdf/2410.16501v1.pdf","comment":"full version, to appear at VLDB25"}],"Performance":[{"id":"http://arxiv.org/abs/2312.10497v3","updated":"2024-10-21T13:48:27Z","published":"2023-12-16T16:54:47Z","title":"Asymptotic Optimality of the Speed-Aware Join-the-Shortest-Queue in the\n  Halfin-Whitt Regime for Heterogeneous Systems","summary":"  The Join-the-Shortest-Queue (JSQ) load balancing scheme is known to minimise\nthe average response time of jobs in homogeneous systems with identical\nservers. However, for {\\em heterogeneous} systems with servers having different\nprocessing speeds, finding an optimal load balancing scheme remains an open\nproblem for finite system sizes. Recently, for systems with heterogeneous\nservers, a variant of the JSQ scheme, called the {\\em\nSpeed-Aware-Join-the-Shortest-Queue (SA-JSQ)} scheme, has been shown to achieve\nasymptotic optimality in the fluid-scaling regime where the number of servers\n$n$ tends to infinity but the normalised the arrival rate of jobs remains\nconstant. {In this paper, we show that the SA-JSQ scheme is also asymptotically\noptimal for heterogeneous systems in the {\\em Halfin-Whitt} traffic regime\nwhere the normalised arrival rate scales as $1-O(1/\\sqrt{n})$.} Our analysis\nbegins by establishing that an appropriately scaled and centered version of the\nMarkov process describing system dynamics weakly converges to a two-dimensional\nreflected {\\em Ornstein-Uhlenbeck (OU) process}. We then show using {\\em\nStein's method} that the stationary distribution of the underlying Markov\nprocess converges to that of the OU process as the system size increases by\nestablishing the validity of interchange of limits. {Finally, through coupling\nwith a suitably constructed system, we show that SA-JSQ asymptotically\nminimises the diffusion-scaled total number of jobs and the diffusion-scaled\nnumber of waiting jobs in the steady-state in the Halfin-Whitt regime among all\npolicies which dispatch jobs based on queue lengths and server speeds.}\n","authors":["Sanidhay Bhambay","Burak Büke","Arpan Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2312.10497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16093v1","updated":"2024-10-21T15:16:00Z","published":"2024-10-21T15:16:00Z","title":"Final Report for CHESS: Cloud, High-Performance Computing, and Edge for\n  Science and Security","summary":"  Automating the theory-experiment cycle requires effective distributed\nworkflows that utilize a computing continuum spanning lab instruments, edge\nsensors, computing resources at multiple facilities, data sets distributed\nacross multiple information sources, and potentially cloud. Unfortunately, the\nobvious methods for constructing continuum platforms, orchestrating workflow\ntasks, and curating datasets over time fail to achieve scientific requirements\nfor performance, energy, security, and reliability. Furthermore, achieving the\nbest use of continuum resources depends upon the efficient composition and\nexecution of workflow tasks, i.e., combinations of numerical solvers, data\nanalytics, and machine learning. Pacific Northwest National Laboratory's LDRD\n\"Cloud, High-Performance Computing (HPC), and Edge for Science and Security\"\n(CHESS) has developed a set of interrelated capabilities for enabling\ndistributed scientific workflows and curating datasets. This report describes\nthe results and successes of CHESS from the perspective of open science.\n","authors":["Nathan Tallent","Jan Strube","Luanzheng Guo","Hyungro Lee","Jesun Firoz","Sayan Ghosh","Bo Fang","Oceane Bel","Steven Spurgeon","Sarah Akers","Christina Doty","Erol Cromwell"],"pdf_url":"https://arxiv.org/pdf/2410.16093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15853v1","updated":"2024-10-21T10:29:04Z","published":"2024-10-21T10:29:04Z","title":"ADS Performance Revisited","summary":"  Real-time measurements are important for in-depth control of manufacturing\nprocesses, which, for modern AI methods, need integration with high-level\nlanguages. In our last SSP paper we investigated the performance of a Python\nand a Java-JNA based approach to integrate the Beckhoff ADS protocol for\nreal-time edge communication into an Industry 4.0 platform. There, we have\nshown that while Java outperforms Python, both solutions do not meet the\ndesired goal of 1-20kHz depending on the task. However, we are are still\nlacking an explanation for this result as well as an analysis of alternatives.\nFor the first topic, we show in this paper that 1) exchanging Java-JNA with\nJava-JNI in this setting does not further improve the performance 2) a C++\nprogram realizing the same behavior in a more direct integration does not\nperform better and 3) profiling shows that the majority of the execution is\nspend in ADS. For the second topic, we show that alternative uses of the ADS\nlibrary allow for better performance.\n","authors":["Alexander Weber","Holger Eichelberger","Jobst Hildebrand"],"pdf_url":"https://arxiv.org/pdf/2410.15853v1.pdf","comment":"Three pages about ADS integration into Java"},{"id":"http://arxiv.org/abs/2410.15813v1","updated":"2024-10-21T09:28:08Z","published":"2024-10-21T09:28:08Z","title":"Industry 4.0 Connectors -- A Performance Experiment with Modbus/TCP","summary":"  For Industry 4.0 applications, communication protocols and data formats even\nfor legacy devices are fundamental. In this paper, we focus on the Modbus/TCP\nprotocol, which is, e.g., used in energy metering. Allowing Industry 4.0\napplications to include data from such protocols without need for programming\nwould increase flexibility and, in turn, improve development efficiency. As one\nparticular approach, we discuss the automated generation of Modbus/TCP\nconnectors for our Open Source oktoflow platform and compare the performance of\nhandcrafted as well as generated connectors in different settings, including\nindustrial energy metering devices.\n","authors":["Christian Nikolajew","Holger Eichelberger"],"pdf_url":"https://arxiv.org/pdf/2410.15813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15621v1","updated":"2024-10-21T03:51:54Z","published":"2024-10-21T03:51:54Z","title":"DRIM-ANN: An Approximate Nearest Neighbor Search Engine based on\n  Commercial DRAM-PIMs","summary":"  Approximate Nearest Neighbor Search (ANNS), which enables efficient semantic\nsimilarity search in large datasets, has become a fundamental component of\ncritical applications such as information retrieval and retrieval-augmented\ngeneration (RAG). However, ANNS is a well-known I/O-intensive algorithm with a\nlow compute-to-I/O ratio, often requiring massive storage due to the large\nvolume of high-dimensional data. This leads to I/O bottlenecks on CPUs and\nmemory limitations on GPUs. DRAM-based Processing-in-Memory (DRAM-PIM)\narchitecture, which offers high bandwidth, large-capacity memory, and the\nability to perform efficient computation in or near the data, presents a\npromising solution for ANNS. In this work, we investigate the use of commercial\nDRAM-PIM for ANNS for the first time and propose DRIM-ANN, an optimized ANNS\nengine based on DRAM-PIMs from UPMEM. Notably, given that the target DRAM-PIM\nexhibits an even lower compute-to-I/O ratio than basic ANNS, we leverage lookup\ntables (LUTs) to replace more multiplications with I/O operations. We then\nsystematically tune ANNS to search optimized configurations with lower\ncomputational load, aligning the compute-to-I/O ratio of ANNS with that of\nDRAM-PIMs while maintaining accuracy constraints. Building on this tuned ANNS\nalgorithm, we further explore implementation optimizations to fully utilize the\ntwo thousand parallel processing units with private local memory in DRAM-PIMs.\nTo address the load imbalance caused by ANNS requests distributed across\ndifferent clusters of large datasets, we propose a load-balancing strategy that\ncombines static data layout optimization with dynamic runtime request\nscheduling. Experimental results on representative datasets show that DRIM-ANN\nachieves an average performance speedup of 2.92x compared to a 32-thread CPU\ncounterpart.\n","authors":["Mingkai Chen","Tianhua Han","Cheng Liu","Shengwen Liang","Kuai Yu","Lei Dai","Ziming Yuan","Ying Wang","Lei Zhang","Huawei Li","Xiaowei Li"],"pdf_url":"https://arxiv.org/pdf/2410.15621v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2410.14445v2","updated":"2024-10-21T01:45:47Z","published":"2024-10-18T13:04:35Z","title":"Toward Generalizing Visual Brain Decoding to Unseen Subjects","summary":"  Visual brain decoding aims to decode visual information from human brain\nactivities. Despite the great progress, one critical limitation of current\nbrain decoding research lies in the lack of generalization capability to unseen\nsubjects. Prior works typically focus on decoding brain activity of individuals\nbased on the observation that different subjects exhibit different brain\nactivities, while it remains unclear whether brain decoding can be generalized\nto unseen subjects. This study aims to answer this question. We first\nconsolidate an image-fMRI dataset consisting of stimulus-image and\nfMRI-response pairs, involving 177 subjects in the movie-viewing task of the\nHuman Connectome Project (HCP). This dataset allows us to investigate the brain\ndecoding performance with the increase of participants. We then present a\nlearning paradigm that applies uniform processing across all subjects, instead\nof employing different network heads or tokenizers for individuals as in\nprevious methods, which can accommodate a large number of subjects to explore\nthe generalization capability across different subjects. A series of\nexperiments are conducted and we have the following findings. First, the\nnetwork exhibits clear generalization capabilities with the increase of\ntraining subjects. Second, the generalization capability is common to popular\nnetwork architectures (MLP, CNN and Transformer). Third, the generalization\nperformance is affected by the similarity between subjects. Our findings reveal\nthe inherent similarities in brain activities across individuals. With the\nemerging of larger and more comprehensive datasets, it is possible to train a\nbrain decoding foundation model in the future. Codes and models can be found at\nhttps://github.com/Xiangtaokong/TGBD.\n","authors":["Xiangtao Kong","Kexin Huang","Ping Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10270v3","updated":"2024-10-21T08:13:45Z","published":"2024-10-14T08:21:25Z","title":"QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis","summary":"  Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.\n","authors":["Abhijit Manatkar","Ashlesha Akella","Parthivi Gupta","Krishnasuri Narayanam"],"pdf_url":"https://arxiv.org/pdf/2410.10270v3.pdf","comment":"Accepted for EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.10284v3","updated":"2024-10-21T05:22:13Z","published":"2024-10-14T08:36:06Z","title":"Trust or Bust: Ensuring Trustworthiness in Autonomous Weapon Systems","summary":"  The integration of Autonomous Weapon Systems (AWS) into military operations\npresents both significant opportunities and challenges. This paper explores the\nmultifaceted nature of trust in AWS, emphasising the necessity of establishing\nreliable and transparent systems to mitigate risks associated with bias,\noperational failures, and accountability. Despite advancements in Artificial\nIntelligence (AI), the trustworthiness of these systems, especially in\nhigh-stakes military applications, remains a critical issue. Through a\nsystematic review of existing literature, this research identifies gaps in the\nunderstanding of trust dynamics during the development and deployment phases of\nAWS. It advocates for a collaborative approach that includes technologists,\nethicists, and military strategists to address these ongoing challenges. The\nfindings underscore the importance of Human-Machine teaming and enhancing\nsystem intelligibility to ensure accountability and adherence to International\nHumanitarian Law. Ultimately, this paper aims to contribute to the ongoing\ndiscourse on the ethical implications of AWS and the imperative for trustworthy\nAI in defense contexts.\n","authors":["Kasper Cools","Clara Maathuis"],"pdf_url":"https://arxiv.org/pdf/2410.10284v3.pdf","comment":"Accepted as a workshop paper at MILCOM 2024, 8 pages"},{"id":"http://arxiv.org/abs/2410.16270v1","updated":"2024-10-21T17:59:50Z","published":"2024-10-21T17:59:50Z","title":"Reflection-Bench: probing AI intelligence with reflection","summary":"  The ability to adapt beliefs or behaviors in response to unexpected outcomes,\nreflection, is fundamental to intelligent systems' interaction with the world.\nFrom a cognitive science perspective, this serves as a core principle of\nintelligence applicable to both human and AI systems. To address the debate on\nthe intelligence of large language models (LLMs), we propose Reflection-Bench,\na comprehensive benchmark comprising 7 tasks spanning core cognitive functions\ncrucial for reflection, including perception, memory, belief updating,\ndecision-making, prediction, counterfactual thinking, and meta-reflection. We\nevaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude\n3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory\nreflection ability. We discuss the underlying causes of these results and\nsuggest potential avenues for future research. In conclusion, Reflection-Bench\noffers both evaluation tools and inspiration for developing AI capable of\nreliably interacting with the environment. Our data and code are available at\nhttps://github.com/YabYum/ReflectionBench.\n","authors":["Lingyu Li","Yixu Wang","Haiquan Zhao","Shuqi Kong","Yan Teng","Chunbo Li","Yingchun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16270v1.pdf","comment":"11 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.08472v2","updated":"2024-10-21T17:59:13Z","published":"2024-06-12T17:56:31Z","title":"RILe: Reinforced Imitation Learning","summary":"  Reinforcement Learning has achieved significant success in generating complex\nbehavior but often requires extensive reward function engineering. Adversarial\nvariants of Imitation Learning and Inverse Reinforcement Learning offer an\nalternative by learning policies from expert demonstrations via a\ndiscriminator. However, these methods struggle in complex tasks where randomly\nsampling expert-like behaviors is challenging. This limitation stems from their\nreliance on policy-agnostic discriminators, which provide insufficient guidance\nfor agent improvement, especially as task complexity increases and expert\nbehavior becomes more distinct. We introduce RILe (Reinforced Imitation\nLearning environment), a novel trainer-student system that learns a dynamic\nreward function based on the student's performance and alignment with expert\ndemonstrations. In RILe, the student learns an action policy while the trainer,\nusing reinforcement learning, continuously updates itself via the\ndiscriminator's feedback to optimize the alignment between the student and the\nexpert. The trainer optimizes for long-term cumulative rewards from the\ndiscriminator, enabling it to provide nuanced feedback that accounts for the\ncomplexity of the task and the student's current capabilities. This approach\nallows for greater exploration of agent actions by providing graduated feedback\nrather than binary expert/non-expert classifications. By reducing dependence on\npolicy-agnostic discriminators, RILe enables better performance in complex\nsettings where traditional methods falter, outperforming existing methods by 2x\nin complex simulated robot-locomotion tasks.\n","authors":["Mert Albaba","Sammy Christen","Thomas Langarek","Christoph Gebhardt","Otmar Hilliges","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2406.08472v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16267v1","updated":"2024-10-21T17:59:11Z","published":"2024-10-21T17:59:11Z","title":"xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video\n  Even in VLMs","summary":"  We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html\n","authors":["Michael S. Ryoo","Honglu Zhou","Shrikant Kendre","Can Qin","Le Xue","Manli Shu","Silvio Savarese","Ran Xu","Caiming Xiong","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2410.16267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16266v1","updated":"2024-10-21T17:59:09Z","published":"2024-10-21T17:59:09Z","title":"3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with\n  View-consistent 2D Diffusion Priors","summary":"  Novel-view synthesis aims to generate novel views of a scene from multiple\ninput images or videos, and recent advancements like 3D Gaussian splatting\n(3DGS) have achieved notable success in producing photorealistic renderings\nwith efficient pipelines. However, generating high-quality novel views under\nchallenging settings, such as sparse input views, remains difficult due to\ninsufficient information in under-sampled areas, often resulting in noticeable\nartifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing\nthe representation quality of 3DGS representations. We leverage 2D video\ndiffusion priors to address the challenging 3D view consistency problem,\nreformulating it as achieving temporal consistency within a video generation\nprocess. 3DGS-Enhancer restores view-consistent latent features of rendered\nnovel views and integrates them with the input views through a spatial-temporal\ndecoder. The enhanced views are then used to fine-tune the initial 3DGS model,\nsignificantly improving its rendering performance. Extensive experiments on\nlarge-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields\nsuperior reconstruction performance and high-fidelity rendering results\ncompared to state-of-the-art methods. The project webpage is\nhttps://xiliu8006.github.io/3DGS-Enhancer-project .\n","authors":["Xi Liu","Chaoyi Zhou","Siyu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.16266v1.pdf","comment":"Accepted by NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.16256v1","updated":"2024-10-21T17:56:51Z","published":"2024-10-21T17:56:51Z","title":"CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and\n  Evolution","summary":"  Efficient and accurate evaluation is crucial for the continuous improvement\nof large language models (LLMs). Among various assessment methods, subjective\nevaluation has garnered significant attention due to its superior alignment\nwith real-world usage scenarios and human preferences. However, human-based\nevaluations are costly and lack reproducibility, making precise automated\nevaluators (judgers) vital in this process. In this report, we introduce\n\\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM.\nCompassJudger-1 is a general-purpose LLM that demonstrates remarkable\nversatility. It is capable of: 1. Performing unitary scoring and two-model\ncomparisons as a reward model; 2. Conducting evaluations according to specified\nformats; 3. Generating critiques; 4. Executing diverse tasks like a general\nLLM. To assess the evaluation capabilities of different judge models under a\nunified setting, we have also established \\textbf{JudgerBench}, a new benchmark\nthat encompasses various subjective evaluation tasks and covers a wide range of\ntopics. CompassJudger-1 offers a comprehensive solution for various evaluation\ntasks while maintaining the flexibility to adapt to diverse requirements. Both\nCompassJudger and JudgerBench are released and available to the research\ncommunity athttps://github.com/open-compass/CompassJudger. We believe that by\nopen-sourcing these tools, we can foster collaboration and accelerate progress\nin LLM evaluation methodologies.\n","authors":["Maosong Cao","Alexander Lam","Haodong Duan","Hongwei Liu","Songyang Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16256v1.pdf","comment":"Technical Report, Code and Models:\n  https://github.com/open-compass/CompassJudger"},{"id":"http://arxiv.org/abs/2410.16239v1","updated":"2024-10-21T17:42:41Z","published":"2024-10-21T17:42:41Z","title":"MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays,\n  ECGs, and Diagnostic Report","summary":"  In this paper, we introduce a novel Multi-Modal Contrastive Pre-training\nFramework that synergistically combines X-rays, electrocardiograms (ECGs), and\nradiology/cardiology reports. Our approach leverages transformers to encode\nthese diverse modalities into a unified representation space, aiming to enhance\ndiagnostic accuracy and facilitate comprehensive patient assessments. We\nutilize LoRA-Peft to significantly reduce trainable parameters in the LLM and\nincorporate recent linear attention dropping strategy in the Vision\nTransformer(ViT) for smoother attention. Furthermore, we provide novel\nmultimodal attention explanations and retrieval for our model. To the best of\nour knowledge, we are the first to propose an integrated model that combines\nX-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing\ncontrastive loss, MoRE effectively aligns modality-specific features into a\ncoherent embedding, which supports various downstream tasks such as zero-shot\nclassification and multimodal retrieval. Employing our proposed methodology, we\nachieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and\nPtbXl downstream datasets, surpassing existing multimodal approaches. Our\nproposed framework shows significant improvements in capturing intricate\ninter-modal relationships and its robustness in medical diagnosis that\nestablishes a framework for future research in multimodal learning in the\nhealthcare sector.\n","authors":["Samrajya Thapa","Koushik Howlader","Subhankar Bhattacharjee","Wei le"],"pdf_url":"https://arxiv.org/pdf/2410.16239v1.pdf","comment":"10 pages, 5 figures, 9 tables. Supplementary detail in Appendix. Code\n  made available in Github for reproducibility"},{"id":"http://arxiv.org/abs/2410.16232v1","updated":"2024-10-21T17:39:49Z","published":"2024-10-21T17:39:49Z","title":"Sketch2Code: Evaluating Vision-Language Models for Interactive Web\n  Design Prototyping","summary":"  Sketches are a natural and accessible medium for UI designers to\nconceptualize early-stage ideas. However, existing research on UI/UX automation\noften requires high-fidelity inputs like Figma designs or detailed screenshots,\nlimiting accessibility and impeding efficient design iteration. To bridge this\ngap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art\nVision Language Models (VLMs) on automating the conversion of rudimentary\nsketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code\nsupports interactive agent evaluation that mimics real-world design workflows,\nwhere a VLM-based agent iteratively refines its generations by communicating\nwith a simulated user, either passively receiving feedback instructions or\nproactively asking clarification questions. We comprehensively analyze ten\ncommercial and open-source models, showing that Sketch2Code is challenging for\nexisting VLMs; even the most capable models struggle to accurately interpret\nsketches and formulate effective questions that lead to steady improvement.\nNevertheless, a user study with UI/UX experts reveals a significant preference\nfor proactive question-asking over passive feedback reception, highlighting the\nneed to develop more effective paradigms for multi-turn conversational agents.\n","authors":["Ryan Li","Yanzhe Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16232v1.pdf","comment":"preprint, 9 pages"},{"id":"http://arxiv.org/abs/2408.17355v2","updated":"2024-10-21T17:27:00Z","published":"2024-08-30T15:39:34Z","title":"Bidirectional Decoding: Improving Action Chunking via Closed-Loop\n  Resampling","summary":"  Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. Yet, its reported effects on the learned policy are\ninconsistent: some studies find it crucial for achieving strong results, while\nothers observe decreased performance. In this paper, we first dissect how\naction chunking impacts the divergence between a learner and a demonstrator. We\nfind that action chunking allows the learner to better capture the temporal\ndependencies in demonstrations but at the cost of reduced reactivity in\nstochastic environments. To address this tradeoff, we propose Bidirectional\nDecoding (BID), a test-time inference algorithm that bridges action chunking\nwith closed-loop operations. BID samples multiple predictions at each time step\nand searches for the optimal one based on two criteria: (i) backward coherence,\nwhich favors samples that align with previous decisions; (ii) forward contrast,\nwhich seeks samples of high likelihood for future plans. By coupling decisions\nwithin and across action chunks, BID promotes consistency over time while\nmaintaining reactivity to unexpected changes. Experimental results show that\nBID boosts the performance of two state-of-the-art generative policies across\nseven simulation benchmarks and two real-world tasks. Code and videos are\navailable at https://bid-robot.github.io.\n","authors":["Yuejiang Liu","Jubayer Ibn Hamid","Annie Xie","Yoonho Lee","Maximilian Du","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2408.17355v2.pdf","comment":"Project website: https://bid-robot.github.io/"},{"id":"http://arxiv.org/abs/2410.16215v1","updated":"2024-10-21T17:16:13Z","published":"2024-10-21T17:16:13Z","title":"Pre-training Distillation for Large Language Models: A Design Space\n  Exploration","summary":"  Knowledge distillation (KD) aims to transfer knowledge from a large teacher\nmodel to a smaller student model. Previous work applying KD in the field of\nlarge language models (LLMs) typically focused on the post-training phase,\nwhere the student LLM learns directly from instructions and corresponding\nresponses generated by the teacher model. In this paper, we extend KD to the\npre-training phase of LLMs, named pre-training distillation (PD). We first\nconduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a\n1.9B parameter student LLM, validating the effectiveness of PD. Considering the\nkey impact factors of distillation, we systematically explore the design space\nof pre-training distillation across four aspects: logits processing, loss\nselection, scaling law, and offline or online logits. We conduct extensive\nexperiments to explore the design space of pre-training distillation and find\nbetter configurations and interesting conclusions, such as larger student LLMs\ngenerally benefiting more from pre-training distillation, while a larger\nteacher LLM does not necessarily guarantee better results. We hope our\nexploration of the design space will inform future practices in pre-training\ndistillation.\n","authors":["Hao Peng","Xin Lv","Yushi Bai","Zijun Yao","Jiajie Zhang","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2410.16215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16212v1","updated":"2024-10-21T17:12:06Z","published":"2024-10-21T17:12:06Z","title":"Comprehensive benchmarking of large language models for RNA secondary\n  structure prediction","summary":"  Inspired by the success of large language models (LLM) for DNA and proteins,\nseveral LLM for RNA have been developed recently. RNA-LLM uses large datasets\nof RNA sequences to learn, in a self-supervised way, how to represent each RNA\nbase with a semantically rich numerical vector. This is done under the\nhypothesis that obtaining high-quality RNA representations can enhance\ndata-costly downstream tasks. Among them, predicting the secondary structure is\na fundamental task for uncovering RNA functional mechanisms. In this work we\npresent a comprehensive experimental analysis of several pre-trained RNA-LLM,\ncomparing them for the RNA secondary structure prediction task in an unified\ndeep learning framework. The RNA-LLM were assessed with increasing\ngeneralization difficulty on benchmark datasets. Results showed that two LLM\nclearly outperform the other models, and revealed significant challenges for\ngeneralization in low-homology scenarios.\n","authors":["L. I. Zablocki","L. A. Bugnon","M. Gerard","L. Di Persia","G. Stegmayer","D. H. Milone"],"pdf_url":"https://arxiv.org/pdf/2410.16212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16208v1","updated":"2024-10-21T17:11:21Z","published":"2024-10-21T17:11:21Z","title":"Compute-Constrained Data Selection","summary":"  Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. These experiments show the\nvalidity of this model in real-world experiments. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective.\n","authors":["Junjie Oscar Yin","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.16208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19323v2","updated":"2024-10-21T17:05:15Z","published":"2024-05-29T17:54:22Z","title":"Are Large Language Models Chameleons? An Attempt to Simulate Social\n  Surveys","summary":"  Can large language models (LLMs) simulate social surveys? To answer this\nquestion, we conducted millions of simulations in which LLMs were asked to\nanswer subjective questions. A comparison of different LLM responses with the\nEuropean Social Survey (ESS) data suggests that the effect of prompts on bias\nand variability is fundamental, highlighting major cultural, age, and gender\nbiases. We further discussed statistical methods for measuring the difference\nbetween LLM answers and survey data and proposed a novel measure inspired by\nJaccard similarity, as LLM-generated responses are likely to have a smaller\nvariance. Our experiments also reveal that it is important to analyze the\nrobustness and variability of prompts before using LLMs to simulate social\nsurveys, as their imitation abilities are approximate at best.\n","authors":["Mingmeng Geng","Sihong He","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2405.19323v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.16198v1","updated":"2024-10-21T17:00:06Z","published":"2024-10-21T17:00:06Z","title":"Improve Vision Language Model Chain-of-thought Reasoning","summary":"  Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial\nfor improving interpretability and trustworthiness. However, current training\nrecipes lack robust CoT reasoning data, relying on datasets dominated by short\nannotations with minimal rationales. In this work, we show that training VLM on\nshort answers does not generalize well to reasoning tasks that require more\ndetailed responses. To address this, we propose a two-fold approach. First, we\ndistill rationales from GPT-4o model to enrich the training data and fine-tune\nVLMs, boosting their CoT performance. Second, we apply reinforcement learning\nto further calibrate reasoning quality. Specifically, we construct positive\n(correct) and negative (incorrect) pairs of model-generated reasoning chains,\nby comparing their predictions with annotated short answers. Using this\npairwise data, we apply the Direct Preference Optimization algorithm to refine\nthe model's reasoning abilities. Our experiments demonstrate significant\nimprovements in CoT reasoning on benchmark datasets and better generalization\nto direct answer prediction as well. This work emphasizes the importance of\nincorporating detailed rationales in training and leveraging reinforcement\nlearning to strengthen the reasoning capabilities of VLMs.\n","authors":["Ruohong Zhang","Bowen Zhang","Yanghao Li","Haotian Zhang","Zhiqing Sun","Zhe Gan","Yinfei Yang","Ruoming Pang","Yiming Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16198v1.pdf","comment":"10 pages + appendix"},{"id":"http://arxiv.org/abs/2410.16196v1","updated":"2024-10-21T16:59:25Z","published":"2024-10-21T16:59:25Z","title":"Information for Conversation Generation: Proposals Utilising Knowledge\n  Graphs","summary":"  LLMs are frequently used tools for conversational generation. Without\nadditional information LLMs can generate lower quality responses due to lacking\nrelevant content and hallucinations, as well as the perception of poor\nemotional capability, and an inability to maintain a consistent character.\nKnowledge graphs are commonly used forms of external knowledge and may provide\nsolutions to these challenges. This paper introduces three proposals, utilizing\nknowledge graphs to enhance LLM generation. Firstly, dynamic knowledge graph\nembeddings and recommendation could allow for the integration of new\ninformation and the selection of relevant knowledge for response generation.\nSecondly, storing entities with emotional values as additional features may\nprovide knowledge that is better emotionally aligned with the user input.\nThirdly, integrating character information through narrative bubbles would\nmaintain character consistency, as well as introducing a structure that would\nreadily incorporate new information.\n","authors":["Alex Clay","Ernesto Jiménez-Ruiz"],"pdf_url":"https://arxiv.org/pdf/2410.16196v1.pdf","comment":"7 pages with citations, 1 figure, accepted to the ISWC 2024 Special\n  Session"},{"id":"http://arxiv.org/abs/2406.13791v3","updated":"2024-10-21T16:55:31Z","published":"2024-06-19T19:35:14Z","title":"IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards\n  for Better Well-Being","summary":"  Sustainable Development Goals (SDGs) give the UN a road map for development\nwith Agenda 2030 as a target. SDG3 \"Good Health and Well-Being\" ensures healthy\nlives and promotes well-being for all ages. Digital technologies can support\nSDG3. Burnout and even depression could be reduced by encouraging better\npreventive health. Due to the lack of patient knowledge and focus to take care\nof their health, it is necessary to help patients before it is too late. New\ntrends such as positive psychology and mindfulness are highly encouraged in the\nUSA. Digital Twins (DTs) can help with the continuous monitoring of emotion\nusing physiological signals (e.g., collected via wearables). DTs facilitate\nmonitoring and provide constant health insight to improve quality of life and\nwell-being with better personalization. Healthcare DTs challenges are\nstandardizing data formats, communication protocols, and data exchange\nmechanisms. As an example, ISO has the ISO/IEC JTC 1/SC 41 Internet of Things\n(IoT) and DTs Working Group, with standards such as \"ISO/IEC 21823-3:2021 IoT -\nInteroperability for IoT Systems - Part 3 Semantic interoperability\", \"ISO/IEC\nCD 30178 - IoT - Data format, value and coding\". To achieve those data\nintegration and knowledge challenges, we designed the Mental Health Knowledge\nGraph (ontology and dataset) to boost mental health. As an example, explicit\nknowledge is described such as chocolate contains magnesium which is\nrecommended for depression. The Knowledge Graph (KG) acquires knowledge from\nontology-based mental health projects classified within the LOV4IoT ontology\ncatalog (Emotion, Depression, and Mental Health). Furthermore, the KG is mapped\nto standards when possible. Standards from ETSI SmartM2M can be used such as\nSAREF4EHAW to represent medical devices and sensors, but also ITU/WHO, ISO,\nW3C, NIST, and IEEE standards relevant to mental health can be considered.\n","authors":["Amelie Gyrard","Seyedali Mohammadi","Manas Gaur","Antonio Kung"],"pdf_url":"https://arxiv.org/pdf/2406.13791v3.pdf","comment":"20 pages, Book chapter, Smart Technologies for Achieving Good Health\n  and Well-Being: Towards Sustainable Development Goal, Taylor & Francis"},{"id":"http://arxiv.org/abs/2409.18169v3","updated":"2024-10-21T16:51:22Z","published":"2024-09-26T17:55:22Z","title":"Harmful Fine-tuning Attacks and Defenses for Large Language Models: A\n  Survey","summary":"  Recent research demonstrates that the nascent fine-tuning-as-a-service\nbusiness model exposes serious safety concerns -- fine-tuning over a few\nharmful data uploaded by the users can compromise the safety alignment of the\nmodel. The attack, known as harmful fine-tuning, has raised a broad research\ninterest among the community. However, as the attack is still new, \\textbf{we\nobserve from our miserable submission experience that there are general\nmisunderstandings within the research community.} We in this paper aim to clear\nsome common concerns for the attack setting, and formally establish the\nresearch problem. Specifically, we first present the threat model of the\nproblem, and introduce the harmful fine-tuning attack and its variants. Then we\nsystematically survey the existing literature on attacks/defenses/mechanical\nanalysis of the problem. Finally, we outline future research directions that\nmight contribute to the development of the field. Additionally, we present a\nlist of questions of interest, which might be useful to refer to when reviewers\nin the peer review process question the realism of the\nexperiment/attack/defense setting. A curated list of relevant papers is\nmaintained and made accessible at:\n\\url{https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers}.\n","authors":["Tiansheng Huang","Sihao Hu","Fatih Ilhan","Selim Furkan Tekin","Ling Liu"],"pdf_url":"https://arxiv.org/pdf/2409.18169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16170v1","updated":"2024-10-21T16:35:58Z","published":"2024-10-21T16:35:58Z","title":"Learning How to Vote With Principles: Axiomatic Insights Into the\n  Collective Decisions of Neural Networks","summary":"  Can neural networks be applied in voting theory, while satisfying the need\nfor transparency in collective decisions? We propose axiomatic deep voting: a\nframework to build and evaluate neural networks that aggregate preferences,\nusing the well-established axiomatic method of voting theory. Our findings are:\n(1) Neural networks, despite being highly accurate, often fail to align with\nthe core axioms of voting rules, revealing a disconnect between mimicking\noutcomes and reasoning. (2) Training with axiom-specific data does not enhance\nalignment with those axioms. (3) By solely optimizing axiom satisfaction,\nneural networks can synthesize new voting rules that often surpass and\nsubstantially differ from existing ones. This offers insights for both fields:\nFor AI, important concepts like bias and value-alignment are studied in a\nmathematically rigorous way; for voting theory, new areas of the space of\nvoting rules are explored.\n","authors":["Levin Hornischer","Zoi Terzopoulou"],"pdf_url":"https://arxiv.org/pdf/2410.16170v1.pdf","comment":"15 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2405.16195v2","updated":"2024-10-21T16:32:24Z","published":"2024-05-25T11:57:43Z","title":"Adaptive $Q$-Network: On-the-fly Target Selection for Deep Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) is well known for being highly sensitive to\nhyperparameters, requiring practitioners substantial efforts to optimize them\nfor the problem at hand. This also limits the applicability of RL in real-world\nscenarios. In recent years, the field of automated Reinforcement Learning\n(AutoRL) has grown in popularity by trying to address this issue. However,\nthese approaches typically hinge on additional samples to select\nwell-performing hyperparameters, hindering sample-efficiency and practicality.\nFurthermore, most AutoRL methods are heavily based on already existing AutoML\nmethods, which were originally developed neglecting the additional challenges\ninherent to RL due to its non-stationarities. In this work, we propose a new\napproach for AutoRL, called Adaptive $Q$-Network (AdaQN), that is tailored to\nRL to take into account the non-stationarity of the optimization procedure\nwithout requiring additional samples. AdaQN learns several $Q$-functions, each\none trained with different hyperparameters, which are updated online using the\n$Q$-function with the smallest approximation error as a shared target. Our\nselection scheme simultaneously handles different hyperparameters while coping\nwith the non-stationarity induced by the RL optimization procedure and being\northogonal to any critic-based RL algorithm. We demonstrate that AdaQN is\ntheoretically sound and empirically validate it in MuJoCo control problems and\nAtari $2600$ games, showing benefits in sample-efficiency, overall performance,\nrobustness to stochasticity and training stability.\n","authors":["Théo Vincent","Fabian Wahren","Jan Peters","Boris Belousov","Carlo D'Eramo"],"pdf_url":"https://arxiv.org/pdf/2405.16195v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.16164v1","updated":"2024-10-21T16:31:16Z","published":"2024-10-21T16:31:16Z","title":"GenAI Assisting Medical Training","summary":"  Medical procedures such as venipuncture and cannulation are essential for\nnurses and require precise skills. Learning this skill, in turn, is a challenge\nfor educators due to the number of teachers per class and the complexity of the\ntask. The study aims to help students with skill acquisition and alleviate the\neducator's workload by integrating generative AI methods to provide real-time\nfeedback on medical procedures such as venipuncture and cannulation.\n","authors":["Stefan Fritsch","Matthias Tschoepe","Vitor Fortes Rey","Lars Krupp","Agnes Gruenerbl","Eloise Monger","Sarah Travenna"],"pdf_url":"https://arxiv.org/pdf/2410.16164v1.pdf","comment":"2 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.16152v1","updated":"2024-10-21T16:19:34Z","published":"2024-10-21T16:19:34Z","title":"Warped Diffusion: Solving Video Inverse Problems with Image Diffusion\n  Models","summary":"  Using image models naively for solving inverse video problems often suffers\nfrom flickering, texture-sticking, and temporal inconsistency in generated\nvideos. To tackle these problems, in this paper, we view frames as continuous\nfunctions in the 2D space, and videos as a sequence of continuous warping\ntransformations between different frames. This perspective allows us to train\nfunction space diffusion models only on images and utilize them to solve\ntemporally correlated inverse problems. The function space diffusion models\nneed to be equivariant with respect to the underlying spatial transformations.\nTo ensure temporal consistency, we introduce a simple post-hoc test-time\nguidance towards (self)-equivariant solutions. Our method allows us to deploy\nstate-of-the-art latent diffusion models such as Stable Diffusion XL to solve\nvideo inverse problems. We demonstrate the effectiveness of our method for\nvideo inpainting and $8\\times$ video super-resolution, outperforming existing\ntechniques based on noise transformations. We provide generated video results:\nhttps://giannisdaras.github.io/warped\\_diffusion.github.io/.\n","authors":["Giannis Daras","Weili Nie","Karsten Kreis","Alex Dimakis","Morteza Mardani","Nikola Borislavov Kovachki","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2410.16152v1.pdf","comment":"Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.18406v2","updated":"2024-10-21T16:18:37Z","published":"2024-05-28T17:46:36Z","title":"RACCooN: A Versatile Instructional Video Editing Framework with\n  Auto-Generated Narratives","summary":"  Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. (3) RACCooN also plans to imagine new objects in a\ngiven video, so users simply prompt the model to receive a detailed video\nediting plan for complex video editing. The proposed framework demonstrates\nimpressive versatile capabilities in video-to-paragraph generation, video\ncontent editing, and can be incorporated into other SoTA video generative\nmodels for further enhancement.\n","authors":["Jaehong Yoon","Shoubin Yu","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2405.18406v2.pdf","comment":"The first two authors contribute equally. Project Page:\n  https://raccoon-mllm-gen.github.io/"},{"id":"http://arxiv.org/abs/2410.16151v1","updated":"2024-10-21T16:18:31Z","published":"2024-10-21T16:18:31Z","title":"Small Contributions, Small Networks: Efficient Neural Network Pruning\n  Based on Relative Importance","summary":"  Recent advancements have scaled neural networks to unprecedented sizes,\nachieving remarkable performance across a wide range of tasks. However,\ndeploying these large-scale models on resource-constrained devices poses\nsignificant challenges due to substantial storage and computational\nrequirements. Neural network pruning has emerged as an effective technique to\nmitigate these limitations by reducing model size and complexity. In this\npaper, we introduce an intuitive and interpretable pruning method based on\nactivation statistics, rooted in information theory and statistical analysis.\nOur approach leverages the statistical properties of neuron activations to\nidentify and remove weights with minimal contributions to neuron outputs.\nSpecifically, we build a distribution of weight contributions across the\ndataset and utilize its parameters to guide the pruning process. Furthermore,\nwe propose a Pruning-aware Training strategy that incorporates an additional\nregularization term to enhance the effectiveness of our pruning method.\nExtensive experiments on multiple datasets and network architectures\ndemonstrate that our method consistently outperforms several baseline and\nstate-of-the-art pruning techniques.\n","authors":["Mostafa Hussien","Mahmoud Afifi","Kim Khoa Nguyen","Mohamed Cheriet"],"pdf_url":"https://arxiv.org/pdf/2410.16151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16148v1","updated":"2024-10-21T16:17:22Z","published":"2024-10-21T16:17:22Z","title":"PODTILE: Facilitating Podcast Episode Browsing with Auto-generated\n  Chapters","summary":"  Listeners of long-form talk-audio content, such as podcast episodes, often\nfind it challenging to understand the overall structure and locate relevant\nsections. A practical solution is to divide episodes into\nchapters--semantically coherent segments labeled with titles and timestamps.\nSince most episodes on our platform at Spotify currently lack creator-provided\nchapters, automating the creation of chapters is essential. Scaling the\nchapterization of podcast episodes presents unique challenges. First, episodes\ntend to be less structured than written texts, featuring spontaneous\ndiscussions with nuanced transitions. Second, the transcripts are usually\nlengthy, averaging about 16,000 tokens, which necessitates efficient processing\nthat can preserve context. To address these challenges, we introduce PODTILE, a\nfine-tuned encoder-decoder transformer to segment conversational data. The\nmodel simultaneously generates chapter transitions and titles for the input\ntranscript. To preserve context, each input text is augmented with global\ncontext, including the episode's title, description, and previous chapter\ntitles. In our intrinsic evaluation, PODTILE achieved an 11% improvement in\nROUGE score over the strongest baseline. Additionally, we provide insights into\nthe practical benefits of auto-generated chapters for listeners navigating\nepisode content. Our findings indicate that auto-generated chapters serve as a\nuseful tool for engaging with less popular podcasts. Finally, we present\nempirical evidence that using chapter titles can enhance effectiveness of\nsparse retrieval in search tasks.\n","authors":["Azin Ghazimatin","Ekaterina Garmash","Gustavo Penha","Kristen Sheets","Martin Achenbach","Oguz Semerci","Remi Galvez","Marcus Tannenberg","Sahitya Mantravadi","Divya Narayanan","Ofeliya Kalaydzhyan","Douglas Cole","Ben Carterette","Ann Clifton","Paul N. Bennett","Claudia Hauff","Mounia Lalmas"],"pdf_url":"https://arxiv.org/pdf/2410.16148v1.pdf","comment":"9 pages, 4 figures, CIKM industry track 2024"},{"id":"http://arxiv.org/abs/2410.16136v1","updated":"2024-10-21T16:01:39Z","published":"2024-10-21T16:01:39Z","title":"Modeling dynamic neural activity by combining naturalistic video stimuli\n  and stimulus-independent latent factors","summary":"  Understanding how the brain processes dynamic natural stimuli remains a\nfundamental challenge in neuroscience. Current dynamic neural encoding models\neither take stimuli as input but ignore shared variability in neural responses,\nor they model this variability by deriving latent embeddings from neural\nresponses or behavior while ignoring the visual input. To address this gap, we\npropose a probabilistic model that incorporates video inputs along with\nstimulus-independent latent factors to capture variability in neuronal\nresponses, predicting a joint distribution for the entire population. After\ntraining and testing our model on mouse V1 neuronal responses, we found that it\noutperforms video-only models in terms of log-likelihood and achieves further\nimprovements when conditioned on responses from other neurons. Furthermore, we\nfind that the learned latent factors strongly correlate with mouse behavior,\nalthough the model was trained without behavior data.\n","authors":["Finn Schmidt","Suhas Shrinivasan","Polina Turishcheva","Fabian H. Sinz"],"pdf_url":"https://arxiv.org/pdf/2410.16136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16135v1","updated":"2024-10-21T16:00:04Z","published":"2024-10-21T16:00:04Z","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference\n  on GPUs","summary":"  To date, 2:4 sparsity has stood as the only sparse pattern that can be\naccelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often\npossesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios,\nmeaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity,\ndo not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity\nis promising in addressing these limitations of 2:4 sparsity. However,\nregarding accuracy, the effects of V:N:M sparsity on broader Transformer\nmodels, such as vision Transformers and large language models (LLMs), are\nlargely unexamined. Moreover, Some specific issues related to V:N:M sparsity,\nsuch as how to select appropriate V and M values, remain unresolved. In this\nstudy, we thoroughly investigate the application of V:N:M sparsity in vision\nmodels and LLMs across multiple tasks, from pertaining to downstream tasks. We\npropose three key approaches to enhance the applicability and accuracy of\nV:N:M-sparse Transformers, including heuristic V and M selection,\nV:N:M-specific channel permutation, and three-staged LoRA training techniques.\nExperimental results show that, with our methods, the DeiT-small achieves\nlossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy\neven at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5\nsparsity performs comparably or better than training-free 2:4 sparse\nalternatives on downstream tasks. More importantly, V:N:M-sparse Transformers\noffer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity.\nOverall, our exploration largely facilitates the V:N:M sparsity to act as a\ntruly effective acceleration solution for Transformers in cost-sensitive\ninference scenarios.\n","authors":["Kang Zhao","Tao Yuan","Han Bao","Zhenfeng Su","Chang Gao","Zhaofeng Sun","Zichen Liang","Liping Jing","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14134v2","updated":"2024-10-21T15:59:18Z","published":"2024-08-26T09:29:56Z","title":"Exploring the Potential of Large Language Models for Heterophilic Graphs","summary":"  Large language models (LLMs) have presented significant opportunities to\nenhance various machine learning applications, including graph neural networks\n(GNNs). By leveraging the vast open-world knowledge within LLMs, we can more\neffectively interpret and utilize textual data to better characterize\nheterophilic graphs, where neighboring nodes often have different labels.\nHowever, existing approaches for heterophilic graphs overlook the rich textual\ndata associated with nodes, which could unlock deeper insights into their\nheterophilic contexts. In this work, we explore the potential of LLMs for\nmodeling heterophilic graphs and propose a novel two-stage framework:\nLLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first\nstage, we fine-tune the LLM to better identify homophilic and heterophilic\nedges based on the textual content of their nodes. In the second stage, we\nadaptively manage message propagation in GNNs for different edge types based on\nnode features, structures, and heterophilic or homophilic characteristics. To\ncope with the computational demands when deploying LLMs in practical scenarios,\nwe further explore model distillation techniques to fine-tune smaller, more\nefficient models that maintain competitive performance. Extensive experiments\nvalidate the effectiveness of our framework, demonstrating the feasibility of\nusing LLMs to enhance node classification on heterophilic graphs.\n","authors":["Yuxia Wu","Shujie Li","Yuan Fang","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.14134v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.13761v2","updated":"2024-10-21T15:59:18Z","published":"2024-09-16T18:46:24Z","title":"Do Large Language Models Need a Content Delivery Network?","summary":"  As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.\n","authors":["Yihua Cheng","Kuntai Du","Jiayi Yao","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.13761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13502v2","updated":"2024-10-21T15:58:30Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to problems that\nare more complex than the ones on which they have been trained. Empirical\ninvestigations of such questions are impeded by two major flaws of current\nevaluations: (i) much of the evaluation data is contaminated, in the sense that\nit has already been seen during training, and (ii) benchmark datasets do not\ncapture how problem proofs may be arbitrarily complex in various ways. As a\nstep towards addressing these issues, we present a framework for evaluating\nLLMs on problems with arbitrarily complex arithmetic proofs, called MathGAP.\nMathGAP generates problems that follow fixed proof specifications -- along with\nchain-of-thought reasoning annotations -- enabling systematic studies on\ngeneralization with respect to arithmetic proof complexity. We apply MathGAP to\nanalyze how in-context learning interacts with generalization to problems that\nhave more complex proofs. We find that among the models tested, most show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for GPT-4o. Surprisingly, providing in-context examples from\nthe same distribution as the test set is not always beneficial for performance.\nIn particular, zero-shot prompting as well as demonstrating a diverse range of\nexamples that are less complex than the test data sometimes yield similar or\nhigher accuracies.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schölkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.00299v4","updated":"2024-10-21T15:56:23Z","published":"2024-06-29T03:37:29Z","title":"Human-Agent Joint Learning for Efficient Robot Manipulation Skill\n  Acquisition","summary":"  Employing a teleoperation system for gathering demonstrations offers the\npotential for more efficient learning of robot manipulation. However,\nteleoperating a robot arm equipped with a dexterous hand or gripper, via a\nteleoperation system presents inherent challenges due to the task's high\ndimensionality, complexity of motion, and differences between physiological\nstructures. In this study, we introduce a novel system for joint learning\nbetween human operators and robots, that enables human operators to share\ncontrol of a robot end-effector with a learned assistive agent, simplifies the\ndata collection process, and facilitates simultaneous human demonstration\ncollection and robot manipulation training. As data accumulates, the assistive\nagent gradually learns. Consequently, less human effort and attention are\nrequired, enhancing the efficiency of the data collection process. It also\nallows the human operator to adjust the control ratio to achieve a trade-off\nbetween manual and automated control. We conducted experiments in both\nsimulated environments and physical real-world settings. Through user studies\nand quantitative evaluations, it is evident that the proposed system could\nenhance data collection efficiency and reduce the need for human adaptation\nwhile ensuring the collected data is of sufficient quality for downstream\ntasks. \\textit{For more details, please refer to our webpage\nhttps://norweig1an.github.io/HAJL.github.io/.\n","authors":["Shengcheng Luo","Quanquan Peng","Jun Lv","Kaiwen Hong","Katherine Rose Driggs-Campbell","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2407.00299v4.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16132v1","updated":"2024-10-21T15:56:17Z","published":"2024-10-21T15:56:17Z","title":"A Data-driven Crowd Simulation Framework Integrating Physics-informed\n  Machine Learning with Navigation Potential Fields","summary":"  Traditional rule-based physical models are limited by their reliance on\nsingular physical formulas and parameters, making it difficult to effectively\ntackle the intricate tasks associated with crowd simulation. Recent research\nhas introduced deep learning methods to tackle these issues, but most current\napproaches focus primarily on generating pedestrian trajectories, often lacking\ninterpretability and failing to provide real-time dynamic simulations.To\naddress the aforementioned issues, we propose a novel data-driven crowd\nsimulation framework that integrates Physics-informed Machine Learning (PIML)\nwith navigation potential fields. Our approach leverages the strengths of both\nphysical models and PIML. Specifically, we design an innovative\nPhysics-informed Spatio-temporal Graph Convolutional Network (PI-STGCN) as a\ndata-driven module to predict pedestrian movement trends based on crowd\nspatio-temporal data. Additionally, we construct a physical model of navigation\npotential fields based on flow field theory to guide pedestrian movements,\nthereby reinforcing physical constraints during the simulation. In our\nframework, navigation potential fields are dynamically computed and updated\nbased on the movement trends predicted by the PI-STGCN, while the updated crowd\ndynamics, guided by these fields, subsequently feed back into the PI-STGCN.\nComparative experiments on two publicly available large-scale real-world\ndatasets across five scenes demonstrate that our proposed framework outperforms\nexisting rule-based methods in accuracy and fidelity. The similarity between\nsimulated and actual pedestrian trajectories increases by 10.8%, while the\naverage error is reduced by 4%. Moreover, our framework exhibits greater\nadaptability and better interpretability compared to methods that rely solely\non deep learning for trajectory generation.\n","authors":["Runkang Guo","Bin Chen","Qi Zhang","Yong Zhao","Xiao Wang","Zhengqiu Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.16132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16128v1","updated":"2024-10-21T15:55:04Z","published":"2024-10-21T15:55:04Z","title":"SMART: Self-learning Meta-strategy Agent for Reasoning Tasks","summary":"  Tasks requiring deductive reasoning, especially those involving multiple\nsteps, often demand adaptive strategies such as intermediate generation of\nrationales or programs, as no single approach is universally optimal. While\nLanguage Models (LMs) can enhance their outputs through iterative\nself-refinement and strategy adjustments, they frequently fail to apply the\nmost effective strategy in their first attempt. This inefficiency raises the\nquestion: Can LMs learn to select the optimal strategy in the first attempt,\nwithout a need for refinement? To address this challenge, we introduce SMART\n(Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that\nenables LMs to autonomously learn and select the most effective strategies for\nvarious reasoning tasks. We model the strategy selection process as a Markov\nDecision Process and leverage reinforcement learning-driven continuous\nself-improvement to allow the model to find the suitable strategy to solve a\ngiven task. Unlike traditional self-refinement methods that rely on multiple\ninference passes or external feedback, SMART allows an LM to internalize the\noutcomes of its own reasoning processes and adjust its strategy accordingly,\naiming for correct solutions on the first attempt. Our experiments across\nvarious reasoning datasets and with different model architectures demonstrate\nthat SMART significantly enhances the ability of models to choose optimal\nstrategies without external guidance (+15 points on the GSM8K dataset). By\nachieving higher accuracy with a single inference pass, SMART not only improves\nperformance but also reduces computational costs for refinement-based\nstrategies, paving the way for more efficient and intelligent reasoning in LMs.\n","authors":["Rongxing Liu","Kumar Shridhar","Manish Prajapat","Patrick Xia","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.16128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16119v1","updated":"2024-10-21T15:47:03Z","published":"2024-10-21T15:47:03Z","title":"SeaDAG: Semi-autoregressive Diffusion for Conditional Directed Acyclic\n  Graph Generation","summary":"  We introduce SeaDAG, a semi-autoregressive diffusion model for conditional\ngeneration of Directed Acyclic Graphs (DAGs). Considering their inherent\nlayer-wise structure, we simulate layer-wise autoregressive generation by\ndesigning different denoising speed for different layers. Unlike conventional\nautoregressive generation that lacks a global graph structure view, our method\nmaintains a complete graph structure at each diffusion step, enabling\noperations such as property control that require the full graph structure.\nLeveraging this capability, we evaluate the DAG properties during training by\nemploying a graph property decoder. We explicitly train the model to learn\ngraph conditioning with a condition loss, which enhances the diffusion model's\ncapacity to generate graphs that are both realistic and aligned with specified\nproperties. We evaluate our method on two representative conditional DAG\ngeneration tasks: (1) circuit generation from truth tables, where precise DAG\nstructures are crucial for realizing circuit functionality, and (2) molecule\ngeneration based on quantum properties. Our approach demonstrates promising\nresults, generating high-quality and realistic DAGs that closely align with\ngiven conditions.\n","authors":["Xinyi Zhou","Xing Li","Yingzhao Lian","Yiwen Wang","Lei Chen","Mingxuan Yuan","Jianye Hao","Guangyong Chen","Pheng Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2410.16119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16116v1","updated":"2024-10-21T15:42:47Z","published":"2024-10-21T15:42:47Z","title":"Multimodal Flare Forecasting with Deep Learning","summary":"  Solar flare forecasting mainly relies on photospheric magnetograms and\nassociated physical features to predict forthcoming flares. However, it is\nbelieved that flare initiation mechanisms often originate in the chromosphere\nand the lower corona. In this study, we employ deep learning as a purely\ndata-driven approach to compare the predictive capabilities of chromospheric\nand coronal UV and EUV emissions across different wavelengths with those of\nphotospheric line-of-sight magnetograms. Our findings indicate that individual\nEUV wavelengths can provide discriminatory power comparable or better to that\nof line-of-sight magnetograms. Moreover, we identify simple multimodal neural\nnetwork architectures that consistently outperform single-input models, showing\ncomplementarity between the flare precursors that can be extracted from the\ndistinct layers of the solar atmosphere. To mitigate potential biases from\nknown misattributions in Active Region flare catalogs, our models are trained\nand evaluated using full-disk images and a comprehensive flare event catalog at\nthe full-disk level. We introduce a deep-learning architecture suited for\nextracting temporal features from full-disk videos.\n","authors":["Grégoire Francisco","Sabrina Guastavino","Teresa Barata","João Fernandes","Dario Del Moro"],"pdf_url":"https://arxiv.org/pdf/2410.16116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16105v1","updated":"2024-10-21T15:34:33Z","published":"2024-10-21T15:34:33Z","title":"Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep\n  Learning","summary":"  Deep neural networks (DNNs) suffer from the spectral bias, wherein DNNs\ntypically exhibit a tendency to prioritize the learning of lower-frequency\ncomponents of a function, struggling to capture its high-frequency features.\nThis paper is to address this issue. Notice that a function having only low\nfrequency components may be well-represented by a shallow neural network (SNN),\na network having only a few layers. By observing that composition of low\nfrequency functions can effectively approximate a high-frequency function, we\npropose to learn a function containing high-frequency components by composing\nseveral SNNs, each of which learns certain low-frequency information from the\ngiven data. We implement the proposed idea by exploiting the multi-grade deep\nlearning (MGDL) model, a recently introduced model that trains a DNN\nincrementally, grade by grade, a current grade learning from the residue of the\nprevious grade only an SNN composed with the SNNs trained in the preceding\ngrades as features. We apply MGDL to synthetic, manifold, colored images, and\nMNIST datasets, all characterized by presence of high-frequency features. Our\nstudy reveals that MGDL excels at representing functions containing\nhigh-frequency information. Specifically, the neural networks learned in each\ngrade adeptly capture some low-frequency information, allowing their\ncompositions with SNNs learned in the previous grades effectively representing\nthe high-frequency features. Our experimental results underscore the efficacy\nof MGDL in addressing the spectral bias inherent in DNNs. By leveraging MGDL,\nwe offer insights into overcoming spectral bias limitation of DNNs, thereby\nenhancing the performance and applicability of deep learning models in tasks\nrequiring the representation of high-frequency information. This study confirms\nthat the proposed method offers a promising solution to address the spectral\nbias of DNNs.\n","authors":["Ronglong Fang","Yuesheng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.16105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06955v3","updated":"2024-10-21T15:26:26Z","published":"2024-02-10T13:51:09Z","title":"Feature Mapping in Physics-Informed Neural Networks (PINNs)","summary":"  In this paper, the training dynamics of PINNs with a feature mapping layer\nvia the limiting Conjugate Kernel and Neural Tangent Kernel is investigated,\nshedding light on the convergence of PINNs; Although the commonly used\nFourier-based feature mapping has achieved great success, we show its\ninadequacy in some physics scenarios. Via these two scopes, we propose\nconditionally positive definite Radial Basis Function as a better alternative.\nLastly, we explore the feature mapping numerically in wide neural networks. Our\nempirical results reveal the efficacy of our method in diverse forward and\ninverse problem sets. Composing feature functions is found to be a practical\nway to address the expressivity and generalisability trade-off, viz., tuning\nthe bandwidth of the kernels and the surjectivity of the feature mapping\nfunction. This simple technique can be implemented for coordinate inputs and\nbenefits the broader PINNs research.\n","authors":["Chengxi Zeng","Tilo Burghardt","Alberto M Gambaruto"],"pdf_url":"https://arxiv.org/pdf/2402.06955v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14485v8","updated":"2024-10-21T15:24:04Z","published":"2024-06-20T16:48:14Z","title":"Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)","summary":"  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n","authors":["Nick Bryan-Kinns","Corey Ford","Shuoyang Zheng","Helen Kennedy","Alan Chamberlain","Makayla Lewis","Drew Hemment","Zijin Li","Qiong Wu","Lanxi Xiao","Gus Xia","Jeba Rezwana","Michael Clemens","Gabriel Vigliensoni"],"pdf_url":"https://arxiv.org/pdf/2406.14485v8.pdf","comment":"Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)"},{"id":"http://arxiv.org/abs/2410.13203v2","updated":"2024-10-21T15:21:56Z","published":"2024-10-17T04:10:36Z","title":"TabSeq: A Framework for Deep Learning on Tabular Data via Sequential\n  Ordering","summary":"  Effective analysis of tabular data still poses a significant problem in deep\nlearning, mainly because features in tabular datasets are often heterogeneous\nand have different levels of relevance. This work introduces TabSeq, a novel\nframework for the sequential ordering of features, addressing the vital\nnecessity to optimize the learning process. Features are not always equally\ninformative, and for certain deep learning models, their random arrangement can\nhinder the model's learning capacity. Finding the optimum sequence order for\nsuch features could improve the deep learning models' learning process. The\nnovel feature ordering technique we provide in this work is based on clustering\nand incorporates both local ordering and global ordering. It is designed to be\nused with a multi-head attention mechanism in a denoising autoencoder network.\nOur framework uses clustering to align comparable features and improve data\norganization. Multi-head attention focuses on essential characteristics,\nwhereas the denoising autoencoder highlights important aspects by rebuilding\nfrom distorted inputs. This method improves the capability to learn from\ntabular data while lowering redundancy. Our research, demonstrating improved\nperformance through appropriate feature sequence rearrangement using raw\nantibody microarray and two other real-world biomedical datasets, validates the\nimpact of feature ordering. These results demonstrate that feature ordering can\nbe a viable approach to improved deep learning of tabular data.\n","authors":["Al Zadid Sultan Bin Habib","Kesheng Wang","Mary-Anne Hartley","Gianfranco Doretto","Donald A. Adjeroh"],"pdf_url":"https://arxiv.org/pdf/2410.13203v2.pdf","comment":"This paper has been accepted for presentation at the 27th\n  International Conference on Pattern Recognition (ICPR 2024) in Kolkata, India"},{"id":"http://arxiv.org/abs/2410.16091v1","updated":"2024-10-21T15:13:17Z","published":"2024-10-21T15:13:17Z","title":"Neural Quantum Propagators for Driven-Dissipative Quantum Dynamics","summary":"  Describing the dynamics of strong-laser driven open quantum systems is a very\nchallenging task that requires the solution of highly involved equations of\nmotion. While machine learning techniques are being applied with some success\nto simulate the time evolution of individual quantum states, their use to\napproximate time-dependent operators (that can evolve various states) remains\nlargely unexplored. In this work, we develop driven neural quantum propagators\n(NQP), a universal neural network framework that solves driven-dissipative\nquantum dynamics by approximating propagators rather than wavefunctions or\ndensity matrices. NQP can handle arbitrary initial quantum states, adapt to\nvarious external fields, and simulate long-time dynamics, even when trained on\nfar shorter time windows. Furthermore, by appropriately configuring the\nexternal fields, our trained NQP can be transferred to systems governed by\ndifferent Hamiltonians. We demonstrate the effectiveness of our approach by\nstudying the spin-boson and the three-state transition Gamma models.\n","authors":["Jiaji Zhang","Carlos L. Benavides-Riveros","Lipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16091v1.pdf","comment":"7 pages, comment are welcome!"},{"id":"http://arxiv.org/abs/2410.16089v1","updated":"2024-10-21T15:12:37Z","published":"2024-10-21T15:12:37Z","title":"Multi-Sensor Fusion for UAV Classification Based on Feature Maps of\n  Image and Radar Data","summary":"  The unique cost, flexibility, speed, and efficiency of modern UAVs make them\nan attractive choice in many applications in contemporary society. This,\nhowever, causes an ever-increasing number of reported malicious or accidental\nincidents, rendering the need for the development of UAV detection and\nclassification mechanisms essential. We propose a methodology for developing a\nsystem that fuses already processed multi-sensor data into a new Deep Neural\nNetwork to increase its classification accuracy towards UAV detection. The DNN\nmodel fuses high-level features extracted from individual object detection and\nclassification models associated with thermal, optronic, and radar data.\nAdditionally, emphasis is given to the model's Convolutional Neural Network\n(CNN) based architecture that combines the features of the three sensor\nmodalities by stacking the extracted image features of the thermal and optronic\nsensor achieving higher classification accuracy than each sensor alone.\n","authors":["Nikos Sakellariou","Antonios Lalas","Konstantinos Votis","Dimitrios Tzovaras"],"pdf_url":"https://arxiv.org/pdf/2410.16089v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16088v1","updated":"2024-10-21T15:12:20Z","published":"2024-10-21T15:12:20Z","title":"Fine-Tuning LLMs for Reliable Medical Question-Answering Services","summary":"  We present an advanced approach to medical question-answering (QA) services,\nusing fine-tuned Large Language Models (LLMs) to improve the accuracy and\nreliability of healthcare information. Our study focuses on optimizing models\nlike LLaMA-2 and Mistral, which have shown great promise in delivering precise,\nreliable medical answers. By leveraging comprehensive datasets, we applied\nfine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model\nperformance through a combination of decomposed model weights, varied learning\nrates for low-rank matrices, and rank stabilization, leading to improved\nefficiency. ReRAG, which integrates retrieval on demand and question rewriting,\nfurther refines the accuracy of the responses. This approach enables healthcare\nproviders to access fast, dependable information, aiding in more efficient\ndecision-making and fostering greater patient trust. Our work highlights the\npotential of fine-tuned LLMs to significantly improve the quality and\naccessibility of medical information services, ultimately contributing to\nbetter healthcare outcomes for all.\n","authors":["Ali Anaissi","Ali Braytee","Junaid Akram"],"pdf_url":"https://arxiv.org/pdf/2410.16088v1.pdf","comment":"8 pages, 10 figures, accepted and to be published in the proceedings\n  of 2024 IEEE International Conference on Data Mining Workshops (ICDMW)"},{"id":"http://arxiv.org/abs/2410.16083v1","updated":"2024-10-21T15:02:30Z","published":"2024-10-21T15:02:30Z","title":"Critical Example Mining for Vehicle Trajectory Prediction using\n  Flow-based Generative Models","summary":"  Precise trajectory prediction in complex driving scenarios is essential for\nautonomous vehicles. In practice, different driving scenarios present varying\nlevels of difficulty for trajectory prediction models. However, most existing\nresearch focuses on the average precision of prediction results, while ignoring\nthe underlying distribution of the input scenarios. This paper proposes a\ncritical example mining method that utilizes a data-driven approach to estimate\nthe rareness of the trajectories. By combining the rareness estimation of\nobservations with whole trajectories, the proposed method effectively\nidentifies a subset of data that is relatively hard to predict BEFORE feeding\nthem to a specific prediction model. The experimental results show that the\nmined subset has higher prediction error when applied to different downstream\nprediction models, which reaches +108.1% error (greater than two times compared\nto the average on dataset) when mining 5% samples. Further analysis indicates\nthat the mined critical examples include uncommon cases such as sudden brake\nand cancelled lane-change, which helps to better understand and improve the\nperformance of prediction models.\n","authors":["Zhezhang Ding","Huijing Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16083v1.pdf","comment":"8 pages,6 figures"},{"id":"http://arxiv.org/abs/2410.16070v1","updated":"2024-10-21T14:48:35Z","published":"2024-10-21T14:48:35Z","title":"On-Device LLMs for SMEs: Challenges and Opportunities","summary":"  This paper presents a systematic review of the infrastructure requirements\nfor deploying Large Language Models (LLMs) on-device within the context of\nsmall and medium-sized enterprises (SMEs), focusing on both hardware and\nsoftware perspectives. From the hardware viewpoint, we discuss the utilization\nof processing units like GPUs and TPUs, efficient memory and storage solutions,\nand strategies for effective deployment, addressing the challenges of limited\ncomputational resources typical in SME settings. From the software perspective,\nwe explore framework compatibility, operating system optimization, and the use\nof specialized libraries tailored for resource-constrained environments. The\nreview is structured to first identify the unique challenges faced by SMEs in\ndeploying LLMs on-device, followed by an exploration of the opportunities that\nboth hardware innovations and software adaptations offer to overcome these\nobstacles. Such a structured review provides practical insights, contributing\nsignificantly to the community by enhancing the technological resilience of\nSMEs in integrating LLMs.\n","authors":["Jeremy Stephen Gabriel Yee Zhi Wen","Pai Chet Ng","Zhengkui Wang","Ian McLoughlin","Aik Beng Ng","Simon See"],"pdf_url":"https://arxiv.org/pdf/2410.16070v1.pdf","comment":"9 pages, 1 figure. The work is supported by the SIT-NVIDIA Joint AI\n  Centre"},{"id":"http://arxiv.org/abs/2410.16063v1","updated":"2024-10-21T14:44:08Z","published":"2024-10-21T14:44:08Z","title":"Integrated Image-Text Based on Semi-supervised Learning for Small Sample\n  Instance Segmentation","summary":"  Small sample instance segmentation is a very challenging task, and many\nexisting methods follow the training strategy of meta-learning which pre-train\nmodels on support set and fine-tune on query set. The pre-training phase, which\nis highly task related, requires a significant amount of additional training\ntime and the selection of datasets with close proximity to ensure\neffectiveness. The article proposes a novel small sample instance segmentation\nsolution from the perspective of maximizing the utilization of existing\ninformation without increasing annotation burden and training costs. The\nproposed method designs two modules to address the problems encountered in\nsmall sample instance segmentation. First, it helps the model fully utilize\nunlabeled data by learning to generate pseudo labels, increasing the number of\navailable samples. Second, by integrating the features of text and image, more\naccurate classification results can be obtained. These two modules are suitable\nfor box-free and box-dependent frameworks. In the way, the proposed method not\nonly improves the performance of small sample instance segmentation, but also\ngreatly reduce reliance on pre-training. We have conducted experiments in three\ndatasets from different scenes: on land, underwater and under microscope. As\nevidenced by our experiments, integrated image-text corrects the confidence of\nclassification, and pseudo labels help the model obtain preciser masks. All the\nresults demonstrate the effectiveness and superiority of our method.\n","authors":["Ruting Chi","Zhiyi Huang","Yuexing Han"],"pdf_url":"https://arxiv.org/pdf/2410.16063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09940v2","updated":"2024-10-21T14:36:35Z","published":"2024-10-13T17:51:21Z","title":"Generalized Group Data Attribution","summary":"  Data Attribution (DA) methods quantify the influence of individual training\ndata points on model outputs and have broad applications such as\nexplainability, data selection, and noisy label identification. However,\nexisting DA methods are often computationally intensive, limiting their\napplicability to large-scale machine learning models. To address this\nchallenge, we introduce the Generalized Group Data Attribution (GGDA)\nframework, which computationally simplifies DA by attributing to groups of\ntraining points instead of individual ones. GGDA is a general framework that\nsubsumes existing attribution methods and can be applied to new DA techniques\nas they emerge. It allows users to optimize the trade-off between efficiency\nand fidelity based on their needs. Our empirical results demonstrate that GGDA\napplied to popular DA methods such as Influence Functions, TracIn, and TRAK\nresults in upto 10x-50x speedups over standard DA methods while gracefully\ntrading off attribution fidelity. For downstream applications such as dataset\npruning and noisy label identification, we demonstrate that GGDA significantly\nimproves computational efficiency and maintains effectiveness, enabling\npractical applications in large-scale machine learning scenarios that were\npreviously infeasible.\n","authors":["Dan Ley","Suraj Srinivas","Shichang Zhang","Gili Rusak","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2410.09940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04684v2","updated":"2024-10-21T14:21:59Z","published":"2023-12-07T20:36:10Z","title":"Latent Skill Discovery for Chain-of-Thought Reasoning","summary":"  Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks.\n","authors":["Zifan Xu","Haozhu Wang","Dmitriy Bespalov","Xuan Wang","Peter Stone","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2312.04684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09478v2","updated":"2024-10-21T14:15:30Z","published":"2024-09-14T16:39:17Z","title":"From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter\n  Lesion Segmentation in PET/CT Imaging","summary":"  Automated lesion segmentation in PET/CT scans is crucial for improving\nclinical workflows and advancing cancer diagnostics. However, the task is\nchallenging due to physiological variability, different tracers used in PET\nimaging, and diverse imaging protocols across medical centers. To address this,\nthe autoPET series was created to challenge researchers to develop algorithms\nthat generalize across diverse PET/CT environments. This paper presents our\nsolution for the autoPET III challenge, targeting multitracer, multicenter\ngeneralization using the nnU-Net framework with the ResEncL architecture. Key\ntechniques include misalignment data augmentation and multi-modal pretraining\nacross CT, MR, and PET datasets to provide an initial anatomical understanding.\nWe incorporate organ supervision as a multitask approach, enabling the model to\ndistinguish between physiological uptake and tracer-specific patterns, which is\nparticularly beneficial in cases where no lesions are present. Compared to the\ndefault nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL\n(65.31) our model significantly improved performance with a Dice score of\n68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative\n(FNvol: 10.35) volumes. These results underscore the effectiveness of combining\nadvanced network design, augmentation, pretraining, and multitask learning for\nPET/CT lesion segmentation. After evaluation on the test set, our approach was\nawarded the first place in the model-centric category (Team LesionTracer). Code\nis publicly available at https://github.com/MIC-DKFZ/autopet-3-submission.\n","authors":["Maximilian Rokuss","Balint Kovacs","Yannick Kirchhoff","Shuhan Xiao","Constantin Ulrich","Klaus H. Maier-Hein","Fabian Isensee"],"pdf_url":"https://arxiv.org/pdf/2409.09478v2.pdf","comment":"Winning method of the autoPET III challenge (model-centric) - Team\n  LesionTracer"},{"id":"http://arxiv.org/abs/2410.16032v1","updated":"2024-10-21T14:06:53Z","published":"2024-10-21T14:06:53Z","title":"TimeMixer++: A General Time Series Pattern Machine for Universal\n  Predictive Analysis","summary":"  Time series analysis plays a critical role in numerous applications,\nsupporting tasks such as forecasting, classification, anomaly detection, and\nimputation. In this work, we present the time series pattern machine (TSPM), a\nmodel designed to excel in a broad range of time series tasks through powerful\nrepresentation and pattern extraction capabilities. Traditional time series\nmodels often struggle to capture universal patterns, limiting their\neffectiveness across diverse tasks. To address this, we define multiple scales\nin the time domain and various resolutions in the frequency domain, employing\nvarious mixing strategies to extract intricate, task-adaptive time series\npatterns. Specifically, we introduce a general-purpose TSPM that processes\nmulti-scale time series using (1) multi-resolution time imaging (MRTI), (2)\ntime image decomposition (TID), (3) multi-scale mixing (MCM), and (4)\nmulti-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI\ntransforms multi-scale time series into multi-resolution time images, capturing\npatterns across both temporal and frequency domains. TID leverages dual-axis\nattention to extract seasonal and trend patterns, while MCM hierarchically\naggregates these patterns across scales. MRM adaptively integrates all\nrepresentations across resolutions. This method achieves state-of-the-art\nperformance across 8 time series analytical tasks, consistently surpassing both\ngeneral-purpose and task-specific models. Our work marks a promising step\ntoward the next generation of TSPMs, paving the way for further advancements in\ntime series analysis.\n","authors":["Shiyu Wang","Jiawei Li","Xiaoming Shi","Zhou Ye","Baichuan Mo","Wenze Lin","Shengtong Ju","Zhixuan Chu","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2410.16032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16024v1","updated":"2024-10-21T13:58:38Z","published":"2024-10-21T13:58:38Z","title":"A New Approach to Solving SMAC Task: Generating Decision Tree Code from\n  Large Language Models","summary":"  StarCraft Multi-Agent Challenge (SMAC) is one of the most commonly used\nexperimental environments in multi-agent reinforcement learning (MARL), where\nthe specific task is to control a set number of allied units to defeat enemy\nforces. Traditional MARL algorithms often require interacting with the\nenvironment for up to 1 million steps to train a model, and the resulting\npolicies are typically non-interpretable with weak transferability. In this\npaper, we propose a novel approach to solving SMAC tasks called LLM-SMAC. In\nour framework, agents leverage large language models (LLMs) to generate\ndecision tree code by providing task descriptions. The model is further\nself-reflection using feedback from the rewards provided by the environment. We\nconduct experiments in the SMAC and demonstrate that our method can produce\nhigh-quality, interpretable decision trees with minimal environmental\nexploration. Moreover, these models exhibit strong transferability,\nsuccessfully applying to similar SMAC environments without modification. We\nbelieve this approach offers a new direction for solving decision-making tasks\nin the future.\n","authors":["Yue Deng","Weiyu Ma","Yuxin Fan","Yin Zhang","Haifeng Zhang","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04202v6","updated":"2024-10-21T13:47:44Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v6.pdf","comment":"Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA)\n  https://ojs.aaai.org/index.php/AIES/article/view/31736"},{"id":"http://arxiv.org/abs/2410.16012v1","updated":"2024-10-21T13:43:02Z","published":"2024-10-21T13:43:02Z","title":"Massimo: Public Queue Monitoring and Management using Mass-Spring Model","summary":"  An efficient system of a queue control and regulation in public spaces is\nvery important in order to avoid the traffic jams and to improve the customer\nsatisfaction. This article offers a detailed road map based on a merger of\nintelligent systems and creating an efficient systems of queues in public\nplaces. Through the utilization of different technologies i.e. computer vision,\nmachine learning algorithms, deep learning our system provide accurate\ninformation about the place is crowded or not and the necessary efforts to be\ntaken.\n","authors":["Abhijeet Kumar","Unnati Singh","Rajdeep Chatterjee","Tathagata Bandyopadhyay"],"pdf_url":"https://arxiv.org/pdf/2410.16012v1.pdf","comment":"8 pages, 6 figures, 3 algorithms, 3 tables"},{"id":"http://arxiv.org/abs/2410.16011v1","updated":"2024-10-21T13:42:19Z","published":"2024-10-21T13:42:19Z","title":"CA*: Addressing Evaluation Pitfalls in Computation-Aware Latency for\n  Simultaneous Speech Translation","summary":"  Simultaneous speech translation (SimulST) systems must balance translation\nquality with response time, making latency measurement crucial for evaluating\ntheir real-world performance. However, there has been a longstanding belief\nthat current metrics yield unrealistically high latency measurements in\nunsegmented streaming settings. In this paper, we investigate this phenomenon,\nrevealing its root cause in a fundamental misconception underlying existing\nlatency evaluation approaches. We demonstrate that this issue affects not only\nstreaming but also segment-level latency evaluation across different metrics.\nFurthermore, we propose a modification to correctly measure computation-aware\nlatency for SimulST systems, addressing the limitations present in existing\nmetrics.\n","authors":["Xi Xu","Wenda Xu","Siqi Ouyang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.16011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16008v1","updated":"2024-10-21T13:41:27Z","published":"2024-10-21T13:41:27Z","title":"Resilient Temporal GCN for Smart Grid State Estimation Under Topology\n  Inaccuracies","summary":"  State Estimation is a crucial task in power systems. Graph Neural Networks\nhave demonstrated significant potential in state estimation for power systems\nby effectively analyzing measurement data and capturing the complex\ninteractions and interrelations among the measurements through the system's\ngraph structure. However, the information about the system's graph structure\nmay be inaccurate due to noise, attack or lack of accurate information about\nthe topology of the system. This paper studies these scenarios under topology\nuncertainties and evaluates the impact of the topology uncertainties on the\nperformance of a Temporal Graph Convolutional Network (TGCN) for state\nestimation in power systems. In order to make the model resilient to topology\nuncertainties, modifications in the TGCN model are proposed to incorporate a\nknowledge graph, generated based on the measurement data. This knowledge graph\nsupports the assumed uncertain system graph. Two variations of the TGCN\narchitecture are introduced to integrate the knowledge graph, and their\nperformances are evaluated and compared to demonstrate improved resilience\nagainst topology uncertainties. The evaluation results indicate that while the\ntwo proposed architecture show different performance, they both improve the\nperformance of the TGCN state estimation under topology uncertainties.\n","authors":["Seyed Hamed Haghshenas","Mia Naeini"],"pdf_url":"https://arxiv.org/pdf/2410.16008v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.16007v1","updated":"2024-10-21T13:41:15Z","published":"2024-10-21T13:41:15Z","title":"Are Language Model Logits Calibrated?","summary":"  Some information is factual (e.g., \"Paris is in France\"), whereas other\ninformation is probabilistic (e.g., \"the coin flip will be a [Heads/Tails].\").\nWe believe that good Language Models (LMs) should understand and reflect this\nnuance. Our work investigates this by testing if LMs' output probabilities are\ncalibrated to their textual contexts. We define model \"calibration\" as the\ndegree to which the output probabilities of candidate tokens are aligned with\nthe relative likelihood that should be inferred from the given context. For\nexample, if the context concerns two equally likely options (e.g., heads or\ntails for a fair coin), the output probabilities should reflect this. Likewise,\ncontext that concerns non-uniformly likely events (e.g., rolling a six with a\ndie) should also be appropriately captured with proportionate output\nprobabilities. We find that even in simple settings the best LMs (1) are poorly\ncalibrated, and (2) have systematic biases (e.g., preferred colors and\nsensitivities to word orderings). For example, gpt-4o-mini often picks the\nfirst of two options presented in the prompt regardless of the options' implied\nlikelihood, whereas Llama-3.1-8B picks the second. Our other consistent finding\nis mode-collapse: Instruction-tuned models often over-allocate probability mass\non a single option. These systematic biases introduce non-intuitive model\nbehavior, making models harder for users to understand.\n","authors":["Charles Lovering","Michael Krumdick","Viet Dac Lai","Nilesh Kumar","Varshini Reddy","Rik Koncel-Kedziorski","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2410.16007v1.pdf","comment":"10 pages (main), 24 pages (appendix), under review"},{"id":"http://arxiv.org/abs/2406.19705v5","updated":"2024-10-21T13:38:48Z","published":"2024-06-28T07:36:31Z","title":"DISCO: Efficient Diffusion Solver for Large-Scale Combinatorial\n  Optimization Problems","summary":"  Combinatorial Optimization (CO) problems are fundamentally important in\nnumerous real-world applications across diverse industries, characterized by\nentailing enormous solution space and demanding time-sensitive response.\nDespite recent advancements in neural solvers, their limited expressiveness\nstruggles to capture the multi-modal nature of CO landscapes. While some\nresearch has shifted towards diffusion models, these models still sample\nsolutions indiscriminately from the entire NP-complete solution space with\ntime-consuming denoising processes, which limit their practicality for large\nproblem scales. We propose DISCO, an efficient DIffusion Solver for large-scale\nCombinatorial Optimization problems that excels in both solution quality and\ninference speed. DISCO's efficacy is twofold: First, it enhances solution\nquality by constraining the sampling space to a more meaningful domain guided\nby solution residues, while preserving the multi-modal properties of the output\ndistributions. Second, it accelerates the denoising process through an\nanalytically solvable approach, enabling solution sampling with minimal\nreverse-time steps and significantly reducing inference time. DISCO delivers\nstrong performance on large-scale Traveling Salesman Problems and challenging\nMaximal Independent Set benchmarks, with inference time up to 5.28 times faster\nthan other diffusion alternatives. By incorporating a divide-and-conquer\nstrategy, DISCO can well generalize to solve unseen-scale problem instances,\neven surpassing models specifically trained for those scales.\n","authors":["Kexiong Yu","Hang Zhao","Yuhang Huang","Renjiao Yi","Kai Xu","Chenyang Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.19705v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15998v1","updated":"2024-10-21T13:29:08Z","published":"2024-10-21T13:29:08Z","title":"1024m at SMM4H 2024: Tasks 3, 5 & 6 -- Ensembles of Transformers and\n  Large Language Models for Medical Text Classification","summary":"  Social media is a great source of data for users reporting information and\nregarding their health and how various things have had an effect on them. This\npaper presents various approaches using Transformers and Large Language Models\nand their ensembles, their performance along with advantages and drawbacks for\nvarious tasks of SMM4H'24 - Classifying texts on impact of nature and outdoor\nspaces on the author's mental health (Task 3), Binary classification of tweets\nreporting their children's health disorders like Asthma, Autism, ADHD and\nSpeech disorder (task 5), Binary classification of users self-reporting their\nage (task 6).\n","authors":["Ram Mohan Rao Kadiyala","M. V. P. Chandra Sekhara Rao"],"pdf_url":"https://arxiv.org/pdf/2410.15998v1.pdf","comment":"short paper , acl 2024"},{"id":"http://arxiv.org/abs/2404.10425v2","updated":"2024-10-21T13:28:20Z","published":"2024-04-16T09:43:58Z","title":"Optimizing BioTac Simulation for Realistic Tactile Perception","summary":"  Tactile sensing presents a promising opportunity for enhancing the\ninteraction capabilities of today's robots. BioTac is a commonly used tactile\nsensor that enables robots to perceive and respond to physical tactile stimuli.\nHowever, the sensor's non-linearity poses challenges in simulating its\nbehavior. In this paper, we first investigate a BioTac simulation that uses\ntemperature, force, and contact point positions to predict the sensor outputs.\nWe show that training with BioTac temperature readings does not yield accurate\nsensor output predictions during deployment. Consequently, we tested three\nalternative models, i.e., an XGBoost regressor, a neural network, and a\ntransformer encoder. We train these models without temperature readings and\nprovide a detailed investigation of the window size of the input vectors. We\ndemonstrate that we achieve statistically significant improvements over the\nbaseline network. Furthermore, our results reveal that the XGBoost regressor\nand transformer outperform traditional feed-forward neural networks in this\ntask. We make all our code and results available online on\nhttps://github.com/wzaielamri/Optimizing_BioTac_Simulation.\n","authors":["Wadhah Zai El Amri","Nicolás Navarro-Guerrero"],"pdf_url":"https://arxiv.org/pdf/2404.10425v2.pdf","comment":"12 pages (including appendix), Accepted at the International Joint\n  Conference on Neural Network (IJCNN) 2024, Yokohama, Japan. \\c{opyright} 2024\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for all other uses, in any current or future media... (We refer\n  to IEEE Copyrights)"},{"id":"http://arxiv.org/abs/2410.15990v1","updated":"2024-10-21T13:20:15Z","published":"2024-10-21T13:20:15Z","title":"Augmenting Legal Decision Support Systems with LLM-based NLI for\n  Analyzing Social Media Evidence","summary":"  This paper presents our system description and error analysis of our entry\nfor NLLP 2024 shared task on Legal Natural Language Inference (L-NLI)\n\\citep{hagag2024legallenssharedtask2024}. The task required classifying these\nrelationships as entailed, contradicted, or neutral, indicating any association\nbetween the review and the complaint. Our system emerged as the winning\nsubmission, significantly outperforming other entries with a substantial margin\nand demonstrating the effectiveness of our approach in legal text analysis. We\nprovide a detailed analysis of the strengths and limitations of each model and\napproach tested, along with a thorough error analysis and suggestions for\nfuture improvements. This paper aims to contribute to the growing field of\nlegal NLP by offering insights into advanced techniques for natural language\ninference in legal contexts, making it accessible to both experts and newcomers\nin the field.\n","authors":["Ram Mohan Rao Kadiyala","Siddartha Pullakhandam","Kanwal Mehreen","Subhasya Tippareddy","Ashay Srivastava"],"pdf_url":"https://arxiv.org/pdf/2410.15990v1.pdf","comment":"8 pages , accepted to emnlp 2024"},{"id":"http://arxiv.org/abs/2410.15987v1","updated":"2024-10-21T13:16:58Z","published":"2024-10-21T13:16:58Z","title":"Analyzing Closed-loop Training Techniques for Realistic Traffic Agent\n  Models in Autonomous Highway Driving Simulations","summary":"  Simulation plays a crucial role in the rapid development and safe deployment\nof autonomous vehicles. Realistic traffic agent models are indispensable for\nbridging the gap between simulation and the real world. Many existing\napproaches for imitating human behavior are based on learning from\ndemonstration. However, these approaches are often constrained by focusing on\nindividual training strategies. Therefore, to foster a broader understanding of\nrealistic traffic agent modeling, in this paper, we provide an extensive\ncomparative analysis of different training principles, with a focus on\nclosed-loop methods for highway driving simulation. We experimentally compare\n(i) open-loop vs. closed-loop multi-agent training, (ii) adversarial vs.\ndeterministic supervised training, (iii) the impact of reinforcement losses,\nand (iv) the impact of training alongside log-replayed agents to identify\nsuitable training techniques for realistic agent modeling. Furthermore, we\nidentify promising combinations of different closed-loop training methods.\n","authors":["Matthias Bitzer","Reinis Cimurs","Benjamin Coors","Johannes Goth","Sebastian Ziesche","Philipp Geiger","Maximilian Naumann"],"pdf_url":"https://arxiv.org/pdf/2410.15987v1.pdf","comment":"15 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.11786v2","updated":"2024-10-21T13:11:44Z","published":"2024-10-15T17:05:25Z","title":"Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.\n","authors":["Tsz Ting Chung","Leyang Cui","Lemao Liu","Xinting Huang","Shuming Shi","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.11786v2.pdf","comment":"14 pages, 5 figures, 10 tables, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15978v1","updated":"2024-10-21T13:05:33Z","published":"2024-10-21T13:05:33Z","title":"PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs","summary":"  The growing volume of academic publications poses significant challenges for\nresearchers conducting timely and accurate Systematic Literature Reviews,\nparticularly in fast-evolving fields like artificial intelligence. This growth\nof academic literature also makes it increasingly difficult for lay people to\naccess scientific knowledge effectively, meaning academic literature is often\nmisrepresented in the popular press and, more broadly, in society. Traditional\nSLR methods are labor-intensive and error-prone, and they struggle to keep up\nwith the rapid pace of new research. To address these issues, we developed\n\\textit{PROMPTHEUS}: an AI-driven pipeline solution that automates the SLR\nprocess using Large Language Models. We aimed to enhance efficiency by reducing\nthe manual workload while maintaining the precision and coherence required for\ncomprehensive literature synthesis. PROMPTHEUS automates key stages of the SLR\nprocess, including systematic search, data extraction, topic modeling using\nBERTopic, and summarization with transformer models. Evaluations conducted\nacross five research domains demonstrate that PROMPTHEUS reduces review time,\nachieves high precision, and provides coherent topic organization, offering a\nscalable and effective solution for conducting literature reviews in an\nincreasingly crowded research landscape. In addition, such tools may reduce the\nincreasing mistrust in science by making summarization more accessible to\nlaypeople.\n  The code for this project can be found on the GitHub repository at\nhttps://github.com/joaopftorres/PROMPTHEUS.git\n","authors":["João Pedro Fernandes Torres","Catherine Muligan","Joaquim Jorge","Catarina Moreira"],"pdf_url":"https://arxiv.org/pdf/2410.15978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15977v1","updated":"2024-10-21T13:04:44Z","published":"2024-10-21T13:04:44Z","title":"Enabling Energy-Efficient Deployment of Large Language Models on\n  Memristor Crossbar: A Synergy of Large and Small","summary":"  Large language models (LLMs) have garnered substantial attention due to their\npromising applications in diverse domains. Nevertheless, the increasing size of\nLLMs comes with a significant surge in the computational requirements for\ntraining and deployment. Memristor crossbars have emerged as a promising\nsolution, which demonstrated a small footprint and remarkably high energy\nefficiency in computer vision (CV) models. Memristors possess higher density\ncompared to conventional memory technologies, making them highly suitable for\neffectively managing the extreme model size associated with LLMs. However,\ndeploying LLMs on memristor crossbars faces three major challenges. Firstly,\nthe size of LLMs increases rapidly, already surpassing the capabilities of\nstate-of-the-art memristor chips. Secondly, LLMs often incorporate multi-head\nattention blocks, which involve non-weight stationary multiplications that\ntraditional memristor crossbars cannot support. Third, while memristor\ncrossbars excel at performing linear operations, they are not capable of\nexecuting complex nonlinear operations in LLM such as softmax and layer\nnormalization. To address these challenges, we present a novel architecture for\nthe memristor crossbar that enables the deployment of state-of-the-art LLM on a\nsingle chip or package, eliminating the energy and time inefficiencies\nassociated with off-chip communication. Our testing on BERT_Large showed\nnegligible accuracy loss. Compared to traditional memristor crossbars, our\narchitecture achieves enhancements of up to 39X in area overhead and 18X in\nenergy consumption. Compared to modern TPU/GPU systems, our architecture\ndemonstrates at least a 68X reduction in the area-delay product and a\nsignificant 69% energy consumption reduction.\n","authors":["Zhehui Wang","Tao Luo","Cheng Liu","Weichen Liu","Rick Siow Mong Goh","Weng-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.15977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15974v1","updated":"2024-10-21T13:00:09Z","published":"2024-10-21T13:00:09Z","title":"Large Language Models for Cross-lingual Emotion Detection","summary":"  This paper presents a detailed system description of our entry for the WASSA\n2024 Task 2, focused on cross-lingual emotion detection. We utilized a\ncombination of large language models (LLMs) and their ensembles to effectively\nunderstand and categorize emotions across different languages. Our approach not\nonly outperformed other submissions with a large margin, but also demonstrated\nthe strength of integrating multiple models to enhance performance.\nAdditionally, We conducted a thorough comparison of the benefits and\nlimitations of each model used. An error analysis is included along with\nsuggested areas for future improvement. This paper aims to offer a clear and\ncomprehensive understanding of advanced techniques in emotion detection, making\nit accessible even to those new to the field.\n","authors":["Ram Mohan Rao Kadiyala"],"pdf_url":"https://arxiv.org/pdf/2410.15974v1.pdf","comment":"6 pages , accepted to acl 2024"},{"id":"http://arxiv.org/abs/2410.15973v1","updated":"2024-10-21T12:59:58Z","published":"2024-10-21T12:59:58Z","title":"Karush-Kuhn-Tucker Condition-Trained Neural Networks (KKT Nets)","summary":"  This paper presents a novel approach to solving convex optimization problems\nby leveraging the fact that, under certain regularity conditions, any set of\nprimal or dual variables satisfying the Karush-Kuhn-Tucker (KKT) conditions is\nnecessary and sufficient for optimality. Similar to Theory-Trained Neural\nNetworks (TTNNs), the parameters of the convex optimization problem are input\nto the neural network, and the expected outputs are the optimal primal and dual\nvariables. A choice for the loss function in this case is a loss, which we\nrefer to as the KKT Loss, that measures how well the network's outputs satisfy\nthe KKT conditions. We demonstrate the effectiveness of this approach using a\nlinear program as an example. For this problem, we observe that minimizing the\nKKT Loss alone outperforms training the network with a weighted sum of the KKT\nLoss and a Data Loss (the mean-squared error between the ground truth optimal\nsolutions and the network's output). Moreover, minimizing only the Data Loss\nyields inferior results compared to those obtained by minimizing the KKT Loss.\nWhile the approach is promising, the obtained primal and dual solutions are not\nsufficiently close to the ground truth optimal solutions. In the future, we aim\nto develop improved models to obtain solutions closer to the ground truth and\nextend the approach to other problem classes.\n","authors":["Shreya Arvind","Rishabh Pomaje","Rajshekhar V Bhat"],"pdf_url":"https://arxiv.org/pdf/2410.15973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14038v3","updated":"2024-10-21T12:54:33Z","published":"2024-09-21T06:49:34Z","title":"OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching","summary":"  Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.14038v3.pdf","comment":"5 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2405.17991v2","updated":"2024-10-21T12:53:21Z","published":"2024-05-28T09:23:14Z","title":"VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections","summary":"  Large language models (LLMs) have recently emerged as powerful tools for\ntackling many language-processing tasks. Despite their success, training and\nfine-tuning these models is still far too computationally and memory intensive.\nIn this paper, we identify and characterise the important components needed for\neffective model convergence using gradient descent. In doing so we find that\nthe intermediate activations used to implement backpropagation can be\nexcessively compressed without incurring any degradation in performance. This\nresult leads us to a cheap and memory-efficient algorithm for both fine-tuning\nand pre-training LLMs. The proposed algorithm simply divides the tokens up into\nsmaller sub-tokens before projecting them onto a fixed 1-dimensional subspace\nduring the forward pass. These features are then coarsely reconstructed during\nthe backward pass to implement the update rules. We confirm the effectiveness\nof our algorithm as being complimentary to many state-of-the-art PEFT methods\non the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for\nfine-tuning LLaMA and show competitive performance against other\nmemory-efficient pre-training methods on the large-scale C4 dataset.\n","authors":["Roy Miles","Pradyumna Reddy","Ismail Elezi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2405.17991v2.pdf","comment":"NeurIPS 2024. Code available at https://github.com/roymiles/VeLoRA"},{"id":"http://arxiv.org/abs/2410.15966v1","updated":"2024-10-21T12:52:03Z","published":"2024-10-21T12:52:03Z","title":"Self-Explained Keywords Empower Large Language Models for Code\n  Generation","summary":"  Large language models (LLMs) have achieved impressive performance in code\ngeneration. However, due to the long-tail distribution of LLMs' training data,\nlow-frequency terms are typically underrepresented in the training process.\nConsequently, LLMs often misunderstand or overlook problem-specific,\nlow-frequency keywords during code generation, compromising the accuracy of the\ngenerated code. To address this, we propose a novel technique named\nSEK(\\textbf{S}elf-\\textbf{E}xplained \\textbf{K}eywords), which empowers an LLM\nfor better code generation by extracting and explaining the key terms in the\nproblem description with the LLM itself and ranking them based on frequency.\nComprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+),\nand APPS, with five representative LLMs, show that SEK can significantly\nimprove LLMs in code generation, yielding substantial and consistent gains. For\ninstance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\\% to\n93.3\\% on the Humaneval benchmark. Further analysis confirms that SEK enables\nthe LLMs to shift their attention from low-frequency keywords to their\ncorresponding high-frequency counterparts.\n","authors":["Lishui Fan","Mouxiang Chen","Zhongxin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15962v1","updated":"2024-10-21T12:47:57Z","published":"2024-10-21T12:47:57Z","title":"Systematic Exploration of Dialogue Summarization Approaches for\n  Reproducibility, Comparative Assessment, and Methodological Innovations for\n  Advancing Natural Language Processing in Abstractive Summarization","summary":"  Reproducibility in scientific research, particularly within the realm of\nnatural language processing (NLP), is essential for validating and verifying\nthe robustness of experimental findings. This paper delves into the\nreproduction and evaluation of dialogue summarization models, focusing\nspecifically on the discrepancies observed between original studies and our\nreproduction efforts. Dialogue summarization is a critical aspect of NLP,\naiming to condense conversational content into concise and informative\nsummaries, thus aiding in efficient information retrieval and decision-making\nprocesses. Our research involved a thorough examination of several dialogue\nsummarization models using the AMI (Augmented Multi-party Interaction) dataset.\nThe models assessed include Hierarchical Memory Networks (HMNet) and various\nversions of Pointer-Generator Networks (PGN), namely PGN(DKE), PGN(DRD),\nPGN(DTS), and PGN(DALL). The primary objective was to evaluate the\ninformativeness and quality of the summaries generated by these models through\nhuman assessment, a method that introduces subjectivity and variability in the\nevaluation process. The analysis began with Dataset 1, where the sample\nstandard deviation of 0.656 indicated a moderate dispersion of data points\naround the mean.\n","authors":["Yugandhar Reddy Gogireddy","Jithendra Reddy Gogireddy"],"pdf_url":"https://arxiv.org/pdf/2410.15962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15960v1","updated":"2024-10-21T12:45:10Z","published":"2024-10-21T12:45:10Z","title":"AI-Driven Innovations in Modern Cloud Computing","summary":"  The world has witnessed rapid technological transformation, past couple of\ndecades and with Advent of Cloud computing the landscape evolved exponentially\nleading to efficient and scalable application development. Now, the past couple\nof years the digital ecosystem has brought in numerous innovations with\nintegration of Artificial Intelligence commonly known as AI. This paper\nexplores how AI and cloud computing intersect to deliver transformative\ncapabilities for modernizing applications by providing services and\ninfrastructure. Harnessing the combined potential of both AI & Cloud\ntechnologies, technology providers can now exploit intelligent resource\nmanagement, predictive analytics, automated deployment & scaling with enhanced\nsecurity leading to offering innovative solutions to their customers.\nFurthermore, by leveraging such technologies of cloud & AI businesses can reap\nrich rewards in the form of reducing operational costs and improving service\ndelivery. This paper further addresses challenges associated such as data\nprivacy concerns and how it can be mitigated with robust AI governance\nframeworks.\n","authors":["Animesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.15960v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.15956v1","updated":"2024-10-21T12:34:17Z","published":"2024-10-21T12:34:17Z","title":"Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs","summary":"  Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.\n","authors":["Yanzhu Guo","Simone Conia","Zelin Zhou","Min Li","Saloni Potdar","Henry Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.15956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15954v1","updated":"2024-10-21T12:34:02Z","published":"2024-10-21T12:34:02Z","title":"TS-ACL: A Time Series Analytic Continual Learning Framework for\n  Privacy-Preserving and Class-Incremental Pattern Recognition","summary":"  Class-incremental Learning (CIL) in Time Series Classification (TSC) aims to\nincrementally train models using the streaming time series data that arrives\ncontinuously. The main problem in this scenario is catastrophic forgetting,\ni.e., training models with new samples inevitably leads to the forgetting of\npreviously learned knowledge. Among existing methods, the replay-based methods\nachieve satisfactory performance but compromise privacy, while exemplar-free\nmethods protect privacy but suffer from low accuracy. However, more critically,\nowing to their reliance on gradient-based update techniques, these existing\nmethods fundamentally cannot solve the catastrophic forgetting problem. In TSC\nscenarios with continuously arriving data and temporally shifting\ndistributions, these methods become even less practical. In this paper, we\npropose a Time Series Analytic Continual Learning framework, called TS-ACL.\nInspired by analytical learning, TS-ACL transforms neural network updates into\ngradient-free linear regression problems, thereby fundamentally mitigating\ncatastrophic forgetting. Specifically, employing a pre-trained and frozen\nfeature extraction encoder, TS-ACL only needs to update its analytic classifier\nrecursively in a lightweight manner that is highly suitable for real-time\napplications and large-scale data processing. Additionally, we theoretically\ndemonstrate that the model obtained recursively through the TS-ACL is exactly\nequivalent to a model trained on the complete dataset in a centralized manner,\nthereby establishing the property of absolute knowledge memory. Extensive\nexperiments validate the superior performance of our TS-ACL.\n","authors":["Kejia Fan","Jiaxu Li","Songning Lai","Linpu Lv","Anfeng Liu","Jianheng Tang","Houbing Herbert Song","Huiping Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.15954v1.pdf","comment":"11 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.15952v1","updated":"2024-10-21T12:32:39Z","published":"2024-10-21T12:32:39Z","title":"User-centric evaluation of explainability of AI with and for humans: a\n  comprehensive empirical study","summary":"  This study is located in the Human-Centered Artificial Intelligence (HCAI)\nand focuses on the results of a user-centered assessment of commonly used\neXplainable Artificial Intelligence (XAI) algorithms, specifically\ninvestigating how humans understand and interact with the explanations provided\nby these algorithms. To achieve this, we employed a multi-disciplinary approach\nthat included state-of-the-art research methods from social sciences to measure\nthe comprehensibility of explanations generated by a state-of-the-art lachine\nlearning model, specifically the Gradient Boosting Classifier (XGBClassifier).\nWe conducted an extensive empirical user study involving interviews with 39\nparticipants from three different groups, each with varying expertise in data\nscience, data visualization, and domain-specific knowledge related to the\ndataset used for training the machine learning model. Participants were asked a\nseries of questions to assess their understanding of the model's explanations.\nTo ensure replicability, we built the model using a publicly available dataset\nfrom the UC Irvine Machine Learning Repository, focusing on edible and\nnon-edible mushrooms. Our findings reveal limitations in existing XAI methods\nand confirm the need for new design principles and evaluation techniques that\naddress the specific information needs and user perspectives of different\nclasses of AI stakeholders. We believe that the results of our research and the\ncross-disciplinary methodology we developed can be successfully adapted to\nvarious data types and user profiles, thus promoting dialogue and address\nopportunities in HCAI research. To support this, we are making the data\nresulting from our study publicly available.\n","authors":["Szymon Bobek","Paloma Korycińska","Monika Krakowska","Maciej Mozolewski","Dorota Rak","Magdalena Zych","Magdalena Wójcik","Grzegorz J. Nalepa"],"pdf_url":"https://arxiv.org/pdf/2410.15952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15951v1","updated":"2024-10-21T12:32:17Z","published":"2024-10-21T12:32:17Z","title":"Redefining Finance: The Influence of Artificial Intelligence (AI) and\n  Machine Learning (ML)","summary":"  With rapid transformation of technologies, the fusion of Artificial\nIntelligence (AI) and Machine Learning (ML) in finance is disrupting the entire\necosystem and operations which were followed for decades. The current landscape\nis where decisions are increasingly data-driven by financial institutions with\nan appetite for automation while mitigating risks. The segments of financial\ninstitutions which are getting heavily influenced are retail banking, wealth\nmanagement, corporate banking & payment ecosystem. The solution ranges from\nonboarding the customers all the way fraud detection & prevention to enhancing\nthe customer services. Financial Institutes are leap frogging with integration\nof Artificial Intelligence and Machine Learning in mainstream applications and\nenhancing operational efficiency through advanced predictive analytics,\nextending personalized customer experiences, and automation to minimize risk\nwith fraud detection techniques. However, with Adoption of AI & ML, it is\nimperative that the financial institute also needs to address ethical and\nregulatory challenges, by putting in place robust governance frameworks and\nresponsible AI practices.\n","authors":["Animesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.15951v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.15947v1","updated":"2024-10-21T12:26:53Z","published":"2024-10-21T12:26:53Z","title":"AI-Driven Approaches for Glaucoma Detection -- A Comprehensive Review","summary":"  The diagnosis of glaucoma plays a critical role in the management and\ntreatment of this vision-threatening disease. Glaucoma is a group of eye\ndiseases that cause blindness by damaging the optic nerve at the back of the\neye. Often called \"silent thief of sight\", it exhibits no symptoms during the\nearly stages. Therefore, early detection is crucial to prevent vision loss.\nWith the rise of Artificial Intelligence (AI), particularly Deep Learning (DL)\ntechniques, Computer-Aided Diagnosis (CADx) systems have emerged as promising\ntools to assist clinicians in accurately diagnosing glaucoma early. This paper\naims to provide a comprehensive overview of AI techniques utilized in CADx\nsystems for glaucoma diagnosis. Through a detailed analysis of current\nliterature, we identify key gaps and challenges in these systems, emphasizing\nthe need for improved safety, reliability, interpretability, and\nexplainability. By identifying research gaps, we aim to advance the field of\nCADx systems especially for the early diagnosis of glaucoma, in order to\nprevent any potential loss of vision.\n","authors":["Yuki Hagiwara","Octavia-Andreaa Ciora","Maureen Monnet","Gino Lancho","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2410.15947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15944v1","updated":"2024-10-21T12:21:49Z","published":"2024-10-21T12:21:49Z","title":"Developing Retrieval Augmented Generation (RAG) based LLM Systems from\n  PDFs: An Experience Report","summary":"  This paper presents an experience report on the development of Retrieval\nAugmented Generation (RAG) systems using PDF documents as the primary data\nsource. The RAG architecture combines generative capabilities of Large Language\nModels (LLMs) with the precision of information retrieval. This approach has\nthe potential to redefine how we interact with and augment both structured and\nunstructured knowledge in generative models to enhance transparency, accuracy,\nand contextuality of responses. The paper details the end-to-end pipeline, from\ndata collection, preprocessing, to retrieval indexing and response generation,\nhighlighting technical challenges and practical solutions. We aim to offer\ninsights to researchers and practitioners developing similar systems using two\ndistinct approaches: OpenAI's Assistant API with GPT Series and Llama's\nopen-source models. The practical implications of this research lie in\nenhancing the reliability of generative AI systems in various sectors where\ndomain-specific knowledge and real-time information retrieval is important. The\nPython code used in this work is also available at:\nhttps://github.com/GPT-Laboratory/RAG-LLM-Development-Guidebook-from-PDFs.\n","authors":["Ayman Asad Khan","Md Toufique Hasan","Kai Kristian Kemell","Jussi Rasku","Pekka Abrahamsson"],"pdf_url":"https://arxiv.org/pdf/2410.15944v1.pdf","comment":"36 pages, 8 figures, 2 tables, and python code snippets"},{"id":"http://arxiv.org/abs/2309.06223v3","updated":"2024-10-21T12:11:52Z","published":"2023-09-12T13:42:20Z","title":"Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack\n  Surfaces in DNN Executables","summary":"  Bit-flip attacks (BFAs) can manipulate deep neural networks (DNNs). For\nhigh-level DNN models running on deep learning (DL) frameworks like PyTorch,\nextensive BFAs have been used to flip bits in model weights and shown\neffective. Defenses have also been proposed to guard model weights. However,\nDNNs are increasingly compiled into DNN executables by DL compilers to leverage\nhardware primitives. These executables manifest distinct computation paradigms;\nexisting research fails to accurately capture and expose the BFA surfaces on\nDNN executables.\n  To this end, we launch the first systematic study of BFAs on DNN executables.\nPrior BFAs are limited to attacking model weights and assume a strong whitebox\nattacker with full knowledge of victim model weights, which is unrealistic as\nweights are often confidential. In contrast, we find that BFAs on DNN\nexecutables can achieve high effectiveness by exploiting the model structure\n(usually stored in the executable code), which only requires knowing the (often\npublic) model structure. Importantly, such structure-based BFAs are pervasive,\ntransferable, and more severe in DNN executables. They also slip past existing\ndefenses.\n  To demonstrate the new attack surfaces, we assume a weak and more realistic\nattacker with no knowledge of victim model weights. We design an automated tool\nto identify vulnerable bits in victim executables with high confidence (70% vs.\nbaseline 2%). We show on DDR4 DRAM that only 1.4 flips on average are needed to\nfully downgrade the accuracy of victim models, including quantized ones which\ncould require 23x more flips previously, to random guesses. We comprehensively\nevaluate 16 DNN executables, covering large-scale models trained on\ncommonly-used datasets compiled by the two most popular DL compilers. Our\nfinding calls for incorporating security mechanisms in future DNN compilation\ntoolchains.\n","authors":["Yanzuo Chen","Zhibo Liu","Yuanyuan Yuan","Sihang Hu","Tianxiang Li","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2309.06223v3.pdf","comment":"Accepted by NDSS 2025"},{"id":"http://arxiv.org/abs/2410.15930v1","updated":"2024-10-21T11:59:14Z","published":"2024-10-21T11:59:14Z","title":"Centrality-aware Product Retrieval and Ranking","summary":"  This paper addresses the challenge of improving user experience on e-commerce\nplatforms by enhancing product ranking relevant to users' search queries.\nAmbiguity and complexity of user queries often lead to a mismatch between the\nuser's intent and retrieved product titles or documents. Recent approaches have\nproposed the use of Transformer-based models, which need millions of annotated\nquery-title pairs during the pre-training stage, and this data often does not\ntake user intent into account. To tackle this, we curate samples from existing\ndatasets at eBay, manually annotated with buyer-centric relevance scores and\ncentrality scores, which reflect how well the product title matches the users'\nintent. We introduce a User-intent Centrality Optimization (UCO) approach for\nexisting models, which optimises for the user intent in semantic product\nsearch. To that end, we propose a dual-loss based optimisation to handle hard\nnegatives, i.e., product titles that are semantically relevant but do not\nreflect the user's intent. Our contributions include curating challenging\nevaluation sets and implementing UCO, resulting in significant product ranking\nefficiency improvements observed for different evaluation metrics. Our work\naims to ensure that the most buyer-centric titles for a query are ranked\nhigher, thereby, enhancing the user experience on e-commerce platforms.\n","authors":["Hadeel Saadany","Swapnil Bhosale","Samarth Agrawal","Diptesh Kanojia","Constantin Orasan","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15930v1.pdf","comment":"EMNLP 2024: Industry track"},{"id":"http://arxiv.org/abs/2410.15927v1","updated":"2024-10-21T11:55:06Z","published":"2024-10-21T11:55:06Z","title":"GReFEL: Geometry-Aware Reliable Facial Expression Learning under Bias\n  and Imbalanced Data Distribution","summary":"  Reliable facial expression learning (FEL) involves the effective learning of\ndistinctive facial expression characteristics for more reliable, unbiased and\naccurate predictions in real-life settings. However, current systems struggle\nwith FEL tasks because of the variance in people's facial expressions due to\ntheir unique facial structures, movements, tones, and demographics. Biased and\nimbalanced datasets compound this challenge, leading to wrong and biased\nprediction labels. To tackle these, we introduce GReFEL, leveraging Vision\nTransformers and a facial geometry-aware anchor-based reliability balancing\nmodule to combat imbalanced data distributions, bias, and uncertainty in facial\nexpression learning. Integrating local and global data with anchors that learn\ndifferent facial data points and structural features, our approach adjusts\nbiased and mislabeled emotions caused by intra-class disparity, inter-class\nsimilarity, and scale sensitivity, resulting in comprehensive, accurate, and\nreliable facial expression predictions. Our model outperforms current\nstate-of-the-art methodologies, as demonstrated by extensive experiments on\nvarious datasets.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Karlo Serbetar","Dong Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2410.15927v1.pdf","comment":"ACCV 2024. Extended version of ARBEx (arXiv:2305.01486)"},{"id":"http://arxiv.org/abs/2402.08958v2","updated":"2024-10-21T11:49:53Z","published":"2024-02-14T05:58:43Z","title":"Towards Next-Level Post-Training Quantization of Hyper-Scale\n  Transformers","summary":"  With the increasing complexity of generative AI models, post-training\nquantization (PTQ) has emerged as a promising solution for deploying\nhyper-scale models on edge devices such as mobile and TVs. Existing PTQ\nschemes, however, consume considerable time and resources, which could be a\nbottleneck in real situations where frequent model updates and multiple\nhyperparameter tunings are required. As a cost-effective alternative,\nlearning-free PTQ schemes have been proposed. However, the performance is\nsomewhat limited because they cannot consider the inter-layer dependency within\nthe attention module, which is a significant feature of Transformers. In this\npaper, we thus propose a novel PTQ algorithm that balances accuracy and\nefficiency. The key idea of the proposed algorithm called aespa is to perform\nquantization layer-wise for efficiency while targeting attention-wise\nreconstruction to consider the cross-layer dependency. Through extensive\nexperiments on various language models and complexity analysis, we demonstrate\nthat aespa is accurate and efficient in quantizing Transformer models.\n","authors":["Junhan Kim","Chungman Lee","Eulrang Cho","Kyungphil Park","Ho-young Kim","Joonyoung Kim","Yongkweon Jeon"],"pdf_url":"https://arxiv.org/pdf/2402.08958v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15912v1","updated":"2024-10-21T11:35:33Z","published":"2024-10-21T11:35:33Z","title":"Bench4Merge: A Comprehensive Benchmark for Merging in Realistic Dense\n  Traffic with Micro-Interactive Vehicles","summary":"  While the capabilities of autonomous driving have advanced rapidly, merging\ninto dense traffic remains a significant challenge, many motion planning\nmethods for this scenario have been proposed but it is hard to evaluate them.\nMost existing closed-loop simulators rely on rule-based controls for other\nvehicles, which results in a lack of diversity and randomness, thus failing to\naccurately assess the motion planning capabilities in highly interactive\nscenarios. Moreover, traditional evaluation metrics are insufficient for\ncomprehensively evaluating the performance of merging in dense traffic. In\nresponse, we proposed a closed-loop evaluation benchmark for assessing motion\nplanning capabilities in merging scenarios. Our approach involves other\nvehicles trained in large scale datasets with micro-behavioral characteristics\nthat significantly enhance the complexity and diversity. Additionally, we have\nrestructured the evaluation mechanism by leveraging large language models to\nassess each autonomous vehicle merging onto the main road. Extensive\nexperiments have demonstrated the advanced nature of this evaluation benchmark.\nThrough this benchmark, we have obtained an evaluation of existing methods and\nidentified common issues. The environment and vehicle motion planning models we\nhave designed can be accessed at\nhttps://anonymous.4open.science/r/Bench4Merge-EB5D\n","authors":["Zhengming Wang","Junli Wang","Pengfei Li","Zhaohan Li","Peng Li","Yilun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15912v1.pdf","comment":"6 pages, 7 figures, IEEE international conference on robotics and\n  automation"},{"id":"http://arxiv.org/abs/2403.17633v4","updated":"2024-10-21T11:34:27Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v4.pdf","comment":"Accepted for IEEE RA-L 2024"},{"id":"http://arxiv.org/abs/2410.15910v1","updated":"2024-10-21T11:33:14Z","published":"2024-10-21T11:33:14Z","title":"Diverse Policies Recovering via Pointwise Mutual Information Weighted\n  Imitation Learning","summary":"  Recovering a spectrum of diverse policies from a set of expert trajectories\nis an important research topic in imitation learning. After determining a\nlatent style for a trajectory, previous diverse policies recovering methods\nusually employ a vanilla behavioral cloning learning objective conditioned on\nthe latent style, treating each state-action pair in the trajectory with equal\nimportance. Based on an observation that in many scenarios, behavioral styles\nare often highly relevant with only a subset of state-action pairs, this paper\npresents a new principled method in diverse polices recovery. In particular,\nafter inferring or assigning a latent style for a trajectory, we enhance the\nvanilla behavioral cloning by incorporating a weighting mechanism based on\npointwise mutual information. This additional weighting reflects the\nsignificance of each state-action pair's contribution to learning the style,\nthus allowing our method to focus on state-action pairs most representative of\nthat style. We provide theoretical justifications for our new objective, and\nextensive empirical evaluations confirm the effectiveness of our method in\nrecovering diverse policies from expert data.\n","authors":["Hanlin Yang","Jian Yao","Weiming Liu","Qing Wang","Hanmin Qin","Hansheng Kong","Kirk Tang","Jiechao Xiong","Chao Yu","Kai Li","Junliang Xing","Hongwu Chen","Juchao Zhuo","Qiang Fu","Yang Wei","Haobo Fu"],"pdf_url":"https://arxiv.org/pdf/2410.15910v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.15538v2","updated":"2024-10-21T11:32:57Z","published":"2024-08-28T05:11:16Z","title":"TrafficGamer: Reliable and Flexible Traffic Simulation for\n  Safety-Critical Scenarios with Game-Theoretic Oracles","summary":"  While modern Autonomous Vehicle (AV) systems can develop reliable driving\npolicies under regular traffic conditions, they frequently struggle with\nsafety-critical traffic scenarios. This difficulty primarily arises from the\nrarity of such scenarios in driving datasets and the complexities associated\nwith predictive modeling among multiple vehicles. To support the testing and\nrefinement of AV policies, simulating safety-critical traffic events is an\nessential challenge to be addressed. In this work, we introduce TrafficGamer,\nwhich facilitates game-theoretic traffic simulation by viewing common road\ndriving as a multi-agent game. In evaluating the empirical performance across\nvarious real-world datasets, TrafficGamer ensures both fidelity and\nexploitability of the simulated scenarios, guaranteeing that they not only\nstatically align with real-world traffic distribution but also efficiently\ncapture equilibriums for representing safety-critical scenarios involving\nmultiple agents. Additionally, the results demonstrate that TrafficGamer\nexhibits highly flexible simulation across various contexts. Specifically, we\ndemonstrate that the generated scenarios can dynamically adapt to equilibriums\nof varying tightness by configuring risk-sensitive constraints during\noptimization. To the best of our knowledge, TrafficGamer is the first simulator\ncapable of generating diverse traffic scenarios involving multiple agents. We\nhave provided a demo webpage for the project at\nhttps://qiaoguanren.github.io/trafficgamer-demo/.\n","authors":["Guanren Qiao","Guorui Quan","Jiawei Yu","Shujun Jia","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14859v2","updated":"2024-10-21T11:25:48Z","published":"2024-03-21T22:08:44Z","title":"Log Probabilities Are a Reliable Estimate of Semantic Plausibility in\n  Base and Instruction-Tuned Language Models","summary":"  Semantic plausibility (e.g. knowing that \"the actor won the award\" is more\nlikely than \"the actor won the battle\") serves as an effective proxy for\ngeneral world knowledge. Language models (LMs) capture vast amounts of world\nknowledge by learning distributional patterns in text, accessible via log\nprobabilities (LogProbs) they assign to plausible vs. implausible outputs. The\nnew generation of instruction-tuned LMs can now also provide explicit estimates\nof plausibility via prompting. Here, we evaluate the effectiveness of LogProbs\nand basic prompting to measure semantic plausibility, both in single-sentence\nminimal pairs (Experiment 1) and short context-dependent scenarios (Experiment\n2). We find that (i) in both base and instruction-tuned LMs, LogProbs offers a\nmore reliable measure of semantic plausibility than direct zero-shot prompting,\nwhich yields inconsistent and often poor results; (ii) instruction-tuning\ngenerally does not alter the sensitivity of LogProbs to semantic plausibility\n(although sometimes decreases it); (iii) across models, context mostly\nmodulates LogProbs in expected ways, as measured by three novel metrics of\ncontext-sensitive plausibility and their match to explicit human plausibility\njudgments. We conclude that, even in the era of prompt-based evaluations,\nLogProbs constitute a useful metric of semantic plausibility, both in base and\ninstruction-tuned LMs.\n","authors":["Carina Kauf","Emmanuele Chersoni","Alessandro Lenci","Evelina Fedorenko","Anna A. Ivanova"],"pdf_url":"https://arxiv.org/pdf/2403.14859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15897v1","updated":"2024-10-21T11:21:21Z","published":"2024-10-21T11:21:21Z","title":"IGMaxHS -- An Incremental MaxSAT Solver with Support for XOR Clauses","summary":"  Recently, a novel, MaxSAT-based method for error correction in quantum\ncomputing has been proposed that requires both incremental MaxSAT solving\ncapabilities and support for XOR constraints, but no dedicated MaxSAT solver\nfulfilling these criteria existed yet. We alleviate that and introduce IGMaxHS,\nwhich is based on the existing solvers iMaxHS and GaussMaxHS, but poses fewer\nrestrictions on the XOR constraints than GaussMaxHS. IGMaxHS is fuzz tested\nwith xwcnfuzz, an extension of wcnfuzz that can directly output XOR\nconstraints. As a result, IGMaxHS is the only solver that reported neither\nincorrect unsatisfiability verdicts nor invalid models nor incoherent cost\nmodel combinations in a final fuzz testing comparison of all three solvers with\n10000 instances. We detail the steps required for implementing Gaussian\nelimination on XOR constraints in CDCL SAT solvers, and extend the recently\nproposed re-entrant incremental MaxSAT solver application program interface to\nallow for incremental addition of XOR constraints. Finally, we show that\nIGMaxHS is capable of decoding quantum color codes through simulation with the\nMunich Quantum Toolkit.\n","authors":["Ole Lübke"],"pdf_url":"https://arxiv.org/pdf/2410.15897v1.pdf","comment":"Presented at the 15th International Workshop on Pragmatics of SAT\n  (PoS 2024, see https://www.pragmaticsofssat.org/2024/ )"},{"id":"http://arxiv.org/abs/2410.15889v1","updated":"2024-10-21T11:06:56Z","published":"2024-10-21T11:06:56Z","title":"Model Mimic Attack: Knowledge Distillation for Provably Transferable\n  Adversarial Examples","summary":"  The vulnerability of artificial neural networks to adversarial perturbations\nin the black-box setting is widely studied in the literature. The majority of\nattack methods to construct these perturbations suffer from an impractically\nlarge number of queries required to find an adversarial example. In this work,\nwe focus on knowledge distillation as an approach to conduct transfer-based\nblack-box adversarial attacks and propose an iterative training of the\nsurrogate model on an expanding dataset. This work is the first, to our\nknowledge, to provide provable guarantees on the success of knowledge\ndistillation-based attack on classification neural networks: we prove that if\nthe student model has enough learning capabilities, the attack on the teacher\nmodel is guaranteed to be found within the finite number of distillation\niterations.\n","authors":["Kirill Lukyanov","Andrew Perminov","Denis Turdakov","Mikhail Pautov"],"pdf_url":"https://arxiv.org/pdf/2410.15889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10621v3","updated":"2024-10-21T11:06:06Z","published":"2024-06-15T12:48:00Z","title":"StrucText-Eval: Evaluating Large Language Model's Reasoning Ability in\n  Structure-Rich Text","summary":"  The effective utilization of structured data, integral to corporate data\nstrategies, has been challenged by the rise of large language models (LLMs)\ncapable of processing unstructured information. This shift prompts the\nquestion: can LLMs interpret structured data directly in its unstructured form?\nWe propose an automatic evaluation data generation method for assessing LLMs'\nreasoning capabilities on structure-rich text to explore this. Our approach\nsupports 8 structured languages and 29 tasks, generating data with adjustable\ncomplexity through controllable nesting and structural width. We introduce\nStrucText-Eval, a benchmark containing 5,800 pre-generated and annotated\nsamples designed to evaluate how well LLMs understand and reason through\nstructured text. StrucText-Eval is divided into two suites: a regular Test\nsuite (3,712 samples) and a Test-Hard suite (2,088 samples), the latter\nemphasizing the gap between human and model performance on more complex tasks.\nExperimental results show that while open-source LLMs achieve a maximum\naccuracy of 74.9\\% on the standard dataset, their performance drops\nsignificantly to 45.8\\% on the harder dataset. In contrast, human participants\nreach an accuracy of 92.6\\% on StrucText-Eval-Hard, highlighting LLMs' current\nlimitations in handling intricate structural information. The benchmark and\ngeneration codes are open sourced in\n\\url{https://github.com/MikeGu721/StrucText-Eval}\n","authors":["Zhouhong Gu","Haoning Ye","Xingzhou Chen","Zeyang Zhou","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.10621v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03615v2","updated":"2024-10-21T11:05:27Z","published":"2024-08-07T08:16:32Z","title":"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in\n  Long-Horizon Tasks","summary":"  Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.\n","authors":["Zaijing Li","Yuquan Xie","Rui Shao","Gongwei Chen","Dongmei Jiang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2408.03615v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15885v1","updated":"2024-10-21T11:02:42Z","published":"2024-10-21T11:02:42Z","title":"How to Build a Pre-trained Multimodal model for Simultaneously Chatting\n  and Decision-making?","summary":"  Existing large pre-trained models typically map text input to text output in\nan end-to-end manner, such as ChatGPT, or map a segment of text input to a\nhierarchy of action decisions, such as OpenVLA. However, humans can\nsimultaneously generate text and actions when receiving specific input signals.\nFor example, a driver can make precise driving decisions while conversing with\na friend in the passenger seat. Motivated by this observation, we consider the\nfollowing question in this work: is it possible to construct a pre-trained\nmodel that can provide both language interaction and precise decision-making\ncapabilities in dynamic open scenarios. We provide a definitive answer to this\nquestion by developing a new model architecture termed Visual Language Action\nmodel for Chatting and Decision Making (VLA4CD), and further demonstrating its\nperformance in challenging autonomous driving tasks. Specifically, we leverage\nLoRA to fine-tune a pre-trained LLM with data of multiple modalities covering\nlanguage, visual, and action. Unlike the existing LoRA operations used for LLM\nfine-tuning, we have designed new computational modules and training cost\nfunctions for VLA4CD. These designs enable VLA4CD to provide continuous-valued\naction decisions while outputting text responses. In contrast, existing LLMs\ncan only output text responses, and current VLA models can only output action\ndecisions. Moreover, these VLA models handle action data by discretizing and\nthen tokenizing the discretized actions, a method unsuitable for complex\ndecision-making tasks involving high-dimensional continuous-valued action\nvectors, such as autonomous driving. The experimental results on CARLA validate\nthat: (1) our proposed model construction method is effective; (2) compared to\nthe SOTA VLA model, VLA4CD can provide more accurate real-time decision-making\nwhile retaining the text interaction capability inherent to LLMs.\n","authors":["Zuojin Tang","Bin Hu","Chenyang Zhao","De Ma","Gang Pan","Bin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15884v1","updated":"2024-10-21T11:02:18Z","published":"2024-10-21T11:02:18Z","title":"Using GPT Models for Qualitative and Quantitative News Analytics in the\n  2024 US Presidental Election Process","summary":"  The paper considers an approach of using Google Search API and GPT-4o model\nfor qualitative and quantitative analyses of news through retrieval-augmented\ngeneration (RAG). This approach was applied to analyze news about the 2024 US\npresidential election process. Different news sources for different time\nperiods have been analyzed. Quantitative scores generated by GPT model have\nbeen analyzed using Bayesian regression to derive trend lines. The\ndistributions found for the regression parameters allow for the analysis of\nuncertainty in the election process. The obtained results demonstrate that\nusing the GPT models for news analysis, one can get informative analytics and\nprovide key insights that can be applied in further analyses of election\nprocesses.\n","authors":["Bohdan M. Pavlyshenko"],"pdf_url":"https://arxiv.org/pdf/2410.15884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15881v1","updated":"2024-10-21T11:01:20Z","published":"2024-10-21T11:01:20Z","title":"MI-VisionShot: Few-shot adaptation of vision-language models for\n  slide-level classification of histopathological images","summary":"  Vision-language supervision has made remarkable strides in learning visual\nrepresentations from textual guidance. In digital pathology, vision-language\nmodels (VLM), pre-trained on curated datasets of histological image-captions,\nhave been adapted to downstream tasks, such as region of interest\nclassification. Zero-shot transfer for slide-level prediction has been\nformulated by MI-Zero, but it exhibits high variability depending on the\ntextual prompts. Inspired by prototypical learning, we propose MI-VisionShot, a\ntraining-free adaptation method on top of VLMs to predict slide-level labels in\nfew-shot learning scenarios. Our framework takes advantage of the excellent\nrepresentation learning of VLM to create prototype-based classifiers under a\nmultiple-instance setting by retrieving the most discriminative patches within\neach slide. Experimentation through different settings shows the ability of\nMI-VisionShot to surpass zero-shot transfer with lower variability, even in\nlow-shot scenarios. Code coming soon at\nthttps://github.com/cvblab/MIVisionShot.\n","authors":["Pablo Meseguer","Rocío del Amor","Valery Naranjo"],"pdf_url":"https://arxiv.org/pdf/2410.15881v1.pdf","comment":"Manuscript accepted for oral presentation at KES-InnovationInMedicine\n  2024 held on Madeira, Portugal"},{"id":"http://arxiv.org/abs/2410.15876v1","updated":"2024-10-21T10:57:45Z","published":"2024-10-21T10:57:45Z","title":"FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL","summary":"  Multi-agent reinforcement learning has demonstrated significant potential in\naddressing complex cooperative tasks across various real-world applications.\nHowever, existing MARL approaches often rely on the restrictive assumption that\nthe number of entities (e.g., agents, obstacles) remains constant between\ntraining and inference. This overlooks scenarios where entities are dynamically\nremoved or added during the inference trajectory -- a common occurrence in\nreal-world environments like search and rescue missions and dynamic combat\nsituations. In this paper, we tackle the challenge of intra-trajectory dynamic\nentity composition under zero-shot out-of-domain (OOD) generalization, where\nsuch dynamic changes cannot be anticipated beforehand. Our empirical studies\nreveal that existing MARL methods suffer significant performance degradation\nand increased uncertainty in these scenarios. In response, we propose\nFlickerFusion, a novel OOD generalization method that acts as a universally\napplicable augmentation technique for MARL backbone methods. Our results show\nthat FlickerFusion not only achieves superior inference rewards but also\nuniquely reduces uncertainty vis-\\`a-vis the backbone, compared to existing\nmethods. For standardized evaluation, we introduce MPEv2, an enhanced version\nof Multi Particle Environments (MPE), consisting of 12 benchmarks. Benchmarks,\nimplementations, and trained models are organized and open-sourced at\nflickerfusion305.github.io, accompanied by ample demo video renderings.\n","authors":["Woosung Koh","Wonbeen Oh","Siyeol Kim","Suhin Shin","Hyeongjin Kim","Jaein Jang","Junghyun Lee","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2410.15876v1.pdf","comment":"NeurIPS '24 Open-World Agents Workshop"},{"id":"http://arxiv.org/abs/2410.04133v2","updated":"2024-10-21T10:56:37Z","published":"2024-10-05T12:12:02Z","title":"An Electrocardiogram Foundation Model Built on over 10 Million\n  Recordings with External Evaluation across Multiple Domains","summary":"  Artificial intelligence (AI) has demonstrated significant potential in ECG\nanalysis and cardiovascular disease assessment. Recently, foundation models\nhave played a remarkable role in advancing medical AI. The development of an\nECG foundation model holds the promise of elevating AI-ECG research to new\nheights. However, building such a model faces several challenges, including\ninsufficient database sample sizes and inadequate generalization across\nmultiple domains. Additionally, there is a notable performance gap between\nsingle-lead and multi-lead ECG analyses. We introduced an ECG Foundation Model\n(ECGFounder), a general-purpose model that leverages real-world ECG annotations\nfrom cardiology experts to broaden the diagnostic capabilities of ECG analysis.\nECGFounder was trained on over 10 million ECGs with 150 label categories from\nthe Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease\ndiagnosis through ECG analysis. The model is designed to be both an effective\nout-of-the-box solution, and a to be fine-tunable for downstream tasks,\nmaximizing usability. Importantly, we extended its application to lower rank\nECGs, and arbitrary single-lead ECGs in particular. ECGFounder is applicable to\nsupporting various downstream tasks in mobile monitoring scenarios.\nExperimental results demonstrate that ECGFounder achieves expert-level\nperformance on internal validation sets, with AUROC exceeding 0.95 for eighty\ndiagnoses. It also shows strong classification performance and generalization\nacross various diagnoses on external validation sets. When fine-tuned,\nECGFounder outperforms baseline models in demographic analysis, clinical event\ndetection, and cross-modality cardiac rhythm diagnosis. The trained model and\ndata will be publicly released upon publication through the bdsp.io. Our code\nis available at https://github.com/bdsp-core/ECGFounder\n","authors":["Jun Li","Aaron Aguirre","Junior Moura","Che Liu","Lanhai Zhong","Chenxi Sun","Gari Clifford","Brandon Westover","Shenda Hong"],"pdf_url":"https://arxiv.org/pdf/2410.04133v2.pdf","comment":"working in progress"},{"id":"http://arxiv.org/abs/2404.07989v3","updated":"2024-10-21T10:54:55Z","published":"2024-04-11T17:59:45Z","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding","summary":"  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n","authors":["Yiwen Tang","Ray Zhang","Jiaming Liu","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Shanghang Zhang","Peng Gao","Hongsheng Li","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07989v3.pdf","comment":"Code and models are released at\n  https://github.com/Ivan-Tang-3D/Any2Point"},{"id":"http://arxiv.org/abs/2310.03059v8","updated":"2024-10-21T10:49:59Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code is released at\nhttps://github.com/Ivan-Tang-3D/Point-PEFT.\n","authors":["Yiwen Tang","Ray Zhang","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2310.03059v8.pdf","comment":"The specialized PEFT framework for 3D pre-trained models, which\n  achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Ivan-Tang-3D/Point-PEFT"},{"id":"http://arxiv.org/abs/2410.15859v1","updated":"2024-10-21T10:39:05Z","published":"2024-10-21T10:39:05Z","title":"Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs","summary":"  Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach.\n","authors":["Xin Ma","Yang Liu","Jingjing Liu","Xiaoxu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.15859v1.pdf","comment":"accepted by NeurIPS 2024. arXiv admin note: text overlap with\n  arXiv:2305.19466 by other authors"},{"id":"http://arxiv.org/abs/2410.15847v1","updated":"2024-10-21T10:19:45Z","published":"2024-10-21T10:19:45Z","title":"Random Token Fusion for Multi-View Medical Diagnosis","summary":"  In multi-view medical diagnosis, deep learning-based models often fuse\ninformation from different imaging perspectives to improve diagnostic\nperformance. However, existing approaches are prone to overfitting and rely\nheavily on view-specific features, which can lead to trivial solutions. In this\nwork, we introduce Random Token Fusion (RTF), a novel technique designed to\nenhance multi-view medical image analysis using vision transformers. By\nintegrating randomness into the feature fusion process during training, RTF\naddresses the issue of overfitting and enhances the robustness and accuracy of\ndiagnostic models without incurring any additional cost at inference. We\nvalidate our approach on standard mammography and chest X-ray benchmark\ndatasets. Through extensive experiments, we demonstrate that RTF consistently\nimproves the performance of existing fusion methods, paving the way for a new\ngeneration of multi-view medical foundation models.\n","authors":["Jingyu Guo","Christos Matsoukas","Fredrik Strand","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2410.15847v1.pdf","comment":"Originally published at the NeurIPS 2024 Workshop on Advancements In\n  Medical Foundation Models: Explainability, Robustness, Security, and Beyond\n  (AIM-FM)"},{"id":"http://arxiv.org/abs/2410.15837v1","updated":"2024-10-21T09:57:42Z","published":"2024-10-21T09:57:42Z","title":"Long-distance Geomagnetic Navigation in GNSS-denied Environments with\n  Deep Reinforcement Learning","summary":"  Geomagnetic navigation has drawn increasing attention with its capacity in\nnavigating through complex environments and its independence from external\nnavigation services like global navigation satellite systems (GNSS). Existing\nstudies on geomagnetic navigation, i.e., matching navigation and bionic\nnavigation, rely on pre-stored map or extensive searches, leading to limited\napplicability or reduced navigation efficiency in unexplored areas. To address\nthe issues with geomagnetic navigation in areas where GNSS is unavailable, this\npaper develops a deep reinforcement learning (DRL)-based mechanism, especially\nfor long-distance geomagnetic navigation. The designed mechanism trains an\nagent to learn and gain the magnetoreception capacity for geomagnetic\nnavigation, rather than using any pre-stored map or extensive and expensive\nsearching approaches. Particularly, we integrate the geomagnetic gradient-based\nparallel approach into geomagnetic navigation. This integration mitigates the\nover-exploration of the learning agent by adjusting the geomagnetic gradient,\nsuch that the obtained gradient is aligned towards the destination. We explore\nthe effectiveness of the proposed approach via detailed numerical simulations,\nwhere we implement twin delayed deep deterministic policy gradient (TD3) in\nrealizing the proposed approach. The results demonstrate that our approach\noutperforms existing metaheuristic and bionic navigation methods in\nlong-distance missions under diverse navigation conditions.\n","authors":["Wenqi Bai","Xiaohui Zhang","Shiliang Zhang","Songnan Yang","Yushuai Li","Tingwen Huang"],"pdf_url":"https://arxiv.org/pdf/2410.15837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15828v1","updated":"2024-10-21T09:46:37Z","published":"2024-10-21T09:46:37Z","title":"LLM4GRN: Discovering Causal Gene Regulatory Networks with LLMs --\n  Evaluation through Synthetic Data Generation","summary":"  Gene regulatory networks (GRNs) represent the causal relationships between\ntranscription factors (TFs) and target genes in single-cell RNA sequencing\n(scRNA-seq) data. Understanding these networks is crucial for uncovering\ndisease mechanisms and identifying therapeutic targets. In this work, we\ninvestigate the potential of large language models (LLMs) for GRN discovery,\nleveraging their learned biological knowledge alone or in combination with\ntraditional statistical methods. We develop a task-based evaluation strategy to\naddress the challenge of unavailable ground truth causal graphs. Specifically,\nwe use the GRNs suggested by LLMs to guide causal synthetic data generation and\ncompare the resulting data against the original dataset. Our statistical and\nbiological assessments show that LLMs can support statistical modeling and data\nsynthesis for biological research.\n","authors":["Tejumade Afonja","Ivaxi Sheth","Ruta Binkyte","Waqar Hanif","Thomas Ulas","Matthias Becker","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2410.15828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15821v1","updated":"2024-10-21T09:39:09Z","published":"2024-10-21T09:39:09Z","title":"The effect of fine-tuning on language model toxicity","summary":"  Fine-tuning language models has become increasingly popular following the\nproliferation of open models and improvements in cost-effective parameter\nefficient fine-tuning. However, fine-tuning can influence model properties such\nas safety. We assess how fine-tuning can impact different open models'\npropensity to output toxic content. We assess the impacts of fine-tuning Gemma,\nLlama, and Phi models on toxicity through three experiments. We compare how\ntoxicity is reduced by model developers during instruction-tuning. We show that\nsmall amounts of parameter-efficient fine-tuning on developer-tuned models via\nlow-rank adaptation on a non-adversarial dataset can significantly alter these\nresults across models. Finally, we highlight the impact of this in the wild,\ndemonstrating how toxicity rates of models fine-tuned by community contributors\ncan deviate in hard-to-predict ways.\n","authors":["Will Hawkins","Brent Mittelstadt","Chris Russell"],"pdf_url":"https://arxiv.org/pdf/2410.15821v1.pdf","comment":"To be presented at NeurIPS 2024 Safe Generative AI Workshop"},{"id":"http://arxiv.org/abs/2402.04494v2","updated":"2024-10-21T09:37:12Z","published":"2024-02-07T00:36:24Z","title":"Amortized Planning with Large-Scale Transformers: A Case Study on Chess","summary":"  This paper uses chess, a landmark planning problem in AI, to assess\ntransformers' performance on a planning task where memorization is futile\n$\\unicode{x2013}$ even at a large scale. To this end, we release ChessBench, a\nlarge-scale benchmark dataset of 10 million chess games with legal move and\nvalue annotations (15 billion data points) provided by Stockfish 16, the\nstate-of-the-art chess engine. We train transformers with up to 270 million\nparameters on ChessBench via supervised learning and perform extensive\nablations to assess the impact of dataset size, model size, architecture type,\nand different prediction targets (state-values, action-values, and behavioral\ncloning). Our largest models learn to predict action-values for novel boards\nquite accurately, implying highly non-trivial generalization. Despite\nperforming no explicit search, our resulting chess policy solves challenging\nchess puzzles and achieves a surprisingly strong Lichess blitz Elo of 2895\nagainst humans (grandmaster level). We also compare to Leela Chess Zero and\nAlphaZero (trained without supervision via self-play) with and without search.\nWe show that, although a remarkably good approximation of Stockfish's\nsearch-based algorithm can be distilled into large-scale transformers via\nsupervised learning, perfect distillation is still beyond reach, thus making\nChessBench well-suited for future research.\n","authors":["Anian Ruoss","Grégoire Delétang","Sourabh Medapati","Jordi Grau-Moya","Li Kevin Wenliang","Elliot Catt","John Reid","Cannada A. Lewis","Joel Veness","Tim Genewein"],"pdf_url":"https://arxiv.org/pdf/2402.04494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15820v1","updated":"2024-10-21T09:36:53Z","published":"2024-10-21T09:36:53Z","title":"MAC Revivo: Artificial Intelligence Paves the Way","summary":"  The vast adoption of Wi-Fi and/or Bluetooth capabilities in Internet of\nThings (IoT) devices, along with the rapid growth of deployed smart devices,\nhas caused significant interference and congestion in the industrial,\nscientific, and medical (ISM) bands. Traditional Wi-Fi Medium Access Control\n(MAC) design faces significant challenges in managing increasingly complex\nwireless environments while ensuring network Quality of Service (QoS)\nperformance. This paper explores the potential integration of advanced\nArtificial Intelligence (AI) methods into the design of Wi-Fi MAC protocols. We\npropose AI-MAC, an innovative approach that employs machine learning algorithms\nto dynamically adapt to changing network conditions, optimize channel access,\nmitigate interference, and ensure deterministic latency. By intelligently\npredicting and managing interference, AI-MAC aims to provide a robust solution\nfor next generation of Wi-Fi networks, enabling seamless connectivity and\nenhanced QoS. Our experimental results demonstrate that AI-MAC significantly\nreduces both interference and latency, paving the way for more reliable and\nefficient wireless communications in the increasingly crowded ISM band.\n","authors":["Jinzhe Pan","Jingqing Wang","Zelin Yun","Zhiyong Xiao","Yuehui Ouyang","Wenchi Cheng","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15819v1","updated":"2024-10-21T09:35:57Z","published":"2024-10-21T09:35:57Z","title":"LiMTR: Time Series Motion Prediction for Diverse Road Users through\n  Multimodal Feature Integration","summary":"  Predicting the behavior of road users accurately is crucial to enable the\nsafe operation of autonomous vehicles in urban or densely populated areas.\nTherefore, there has been a growing interest in time series motion prediction\nresearch, leading to significant advancements in state-of-the-art techniques in\nrecent years. However, the potential of using LiDAR data to capture more\ndetailed local features, such as a person's gaze or posture, remains largely\nunexplored. To address this, we develop a novel multimodal approach for motion\nprediction based on the PointNet foundation model architecture, incorporating\nlocal LiDAR features. Evaluation on the Waymo Open Dataset shows a performance\nimprovement of 6.20% and 1.58% in minADE and mAP respectively, when integrated\nand compared with the previous state-of-the-art MTR. We open-source the code of\nour LiMTR model.\n","authors":["Camiel Oerlemans","Bram Grooten","Michiel Braat","Alaa Alassi","Emilia Silvas","Decebal Constantin Mocanu"],"pdf_url":"https://arxiv.org/pdf/2410.15819v1.pdf","comment":"Accepted at the NeurIPS 2024 workshop Time Series in the Age of Large\n  Models. Code available at https://github.com/Cing2/LiMTR"},{"id":"http://arxiv.org/abs/2410.15814v1","updated":"2024-10-21T09:28:42Z","published":"2024-10-21T09:28:42Z","title":"Kaninfradet3D:A Road-side Camera-LiDAR Fusion 3D Perception Model based\n  on Nonlinear Feature Extraction and Intrinsic Correlation","summary":"  With the development of AI-assisted driving, numerous methods have emerged\nfor ego-vehicle 3D perception tasks, but there has been limited research on\nroadside perception. With its ability to provide a global view and a broader\nsensing range, the roadside perspective is worth developing. LiDAR provides\nprecise three-dimensional spatial information, while cameras offer semantic\ninformation. These two modalities are complementary in 3D detection. However,\nadding camera data does not increase accuracy in some studies since the\ninformation extraction and fusion procedure is not sufficiently reliable.\nRecently, Kolmogorov-Arnold Networks (KANs) have been proposed as replacements\nfor MLPs, which are better suited for high-dimensional, complex data. Both the\ncamera and the LiDAR provide high-dimensional information, and employing KANs\nshould enhance the extraction of valuable features to produce better fusion\noutcomes. This paper proposes Kaninfradet3D, which optimizes the feature\nextraction and fusion modules. To extract features from complex\nhigh-dimensional data, the model's encoder and fuser modules were improved\nusing KAN Layers. Cross-attention was applied to enhance feature fusion, and\nvisual comparisons verified that camera features were more evenly integrated.\nThis addressed the issue of camera features being abnormally concentrated,\nnegatively impacting fusion. Compared to the benchmark, our approach shows\nimprovements of +9.87 mAP and +10.64 mAP in the two viewpoints of the TUMTraf\nIntersection Dataset and an improvement of +1.40 mAP in the roadside end of the\nTUMTraf V2X Cooperative Perception Dataset. The results indicate that\nKaninfradet3D can effectively fuse features, demonstrating the potential of\napplying KANs in roadside perception tasks.\n","authors":["Pei Liu","Nanfang Zheng","Yiqun Li","Junlan Chen","Ziyuan Pu"],"pdf_url":"https://arxiv.org/pdf/2410.15814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15805v1","updated":"2024-10-21T09:22:29Z","published":"2024-10-21T09:22:29Z","title":"RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG Framework for\n  IT Operations and Maintenance","summary":"  With the ever-increasing demands on Question Answering (QA) systems for IT\noperations and maintenance, an efficient and supervised fine-tunable framework\nis necessary to ensure the data security, private deployment and continuous\nupgrading. Although Large Language Models (LLMs) have notably improved the\nopen-domain QA's performance, how to efficiently handle enterprise-exclusive\ncorpora and build domain-specific QA systems are still less-studied for\nindustrial applications. In this paper, we propose a general and comprehensive\nframework based on Retrieval Augmented Generation (RAG) and facilitate the\nwhole business process of establishing QA systems for IT operations and\nmaintenance. In accordance with the prevailing RAG method, our proposed\nframework, named with RAG4ITOps, composes of two major stages: (1) Models\nFine-tuning \\& Data Vectorization, and (2) Online QA System Process. At the\nStage 1, we leverage a contrastive learning method with two negative sampling\nstrategies to fine-tune the embedding model, and design the instruction\ntemplates to fine-tune the LLM with a Retrieval Augmented Fine-Tuning method.\nAt the Stage 2, an efficient process of QA system is built for serving. We\ncollect enterprise-exclusive corpora from the domain of cloud computing, and\nthe extensive experiments show that our method achieves superior results than\ncounterparts on two kinds of QA tasks. Our experiment also provide a case for\napplying the RAG4ITOps to real-world enterprise-level applications.\n","authors":["Tianyang Zhang","Zhuoxuan Jiang","Shengguang Bai","Tianrui Zhang","Lin Lin","Yang Liu","Jiawei Ren"],"pdf_url":"https://arxiv.org/pdf/2410.15805v1.pdf","comment":"Accepted by EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.15804v1","updated":"2024-10-21T09:22:16Z","published":"2024-10-21T09:22:16Z","title":"Deep Learning and Data Augmentation for Detecting Self-Admitted\n  Technical Debt","summary":"  Self-Admitted Technical Debt (SATD) refers to circumstances where developers\nuse textual artifacts to explain why the existing implementation is not\noptimal. Past research in detecting SATD has focused on either identifying SATD\n(classifying SATD items as SATD or not) or categorizing SATD (labeling\ninstances as SATD that pertain to requirement, design, code, test debt, etc.).\nHowever, the performance of these approaches remains suboptimal, particularly\nfor specific types of SATD, such as test and requirement debt, primarily due to\nextremely imbalanced datasets. To address these challenges, we build on earlier\nresearch by utilizing BiLSTM architecture for the binary identification of SATD\nand BERT architecture for categorizing different types of SATD. Despite their\neffectiveness, both architectures struggle with imbalanced data. Therefore, we\nemploy a large language model data augmentation strategy to mitigate this\nissue. Furthermore, we introduce a two-step approach to identify and categorize\nSATD across various datasets derived from different artifacts. Our\ncontributions include providing a balanced dataset for future SATD researchers\nand demonstrating that our approach significantly improves SATD identification\nand categorization performance compared to baseline methods.\n","authors":["Edi Sutoyo","Paris Avgeriou","Andrea Capiluppi"],"pdf_url":"https://arxiv.org/pdf/2410.15804v1.pdf","comment":"Accepted to be published at the 2024 31st Asia-Pacific Software\n  Engineering Conference (APSEC)"},{"id":"http://arxiv.org/abs/2409.07825v3","updated":"2024-10-21T09:14:47Z","published":"2024-09-12T08:15:39Z","title":"Deep Multimodal Learning with Missing Modality: A Survey","summary":"  During multimodal model training and testing, certain data modalities may be\nabsent due to sensor limitations, cost constraints, privacy concerns, or data\nloss, negatively affecting performance. Multimodal learning techniques designed\nto handle missing modalities can mitigate this by ensuring model robustness\neven when some modalities are unavailable. This survey reviews recent progress\nin Multimodal Learning with Missing Modality (MLMM), focusing on deep learning\nmethods. It provides the first comprehensive survey that covers the motivation\nand distinctions between MLMM and standard multimodal learning setups, followed\nby a detailed analysis of current methods, applications, and datasets,\nconcluding with challenges and future directions.\n","authors":["Renjie Wu","Hu Wang","Hsiang-Ting Chen","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2409.07825v3.pdf","comment":"Submitted to ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2410.06062v3","updated":"2024-10-21T09:13:48Z","published":"2024-10-08T14:09:12Z","title":"LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs","summary":"  We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.\n","authors":["Vincent Emonet","Jerven Bolleman","Severine Duvaud","Tarcisio Mendes de Farias","Ana Claudia Sima"],"pdf_url":"https://arxiv.org/pdf/2410.06062v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15794v1","updated":"2024-10-21T09:06:13Z","published":"2024-10-21T09:06:13Z","title":"Habaek: High-performance water segmentation through dataset expansion\n  and inductive bias optimization","summary":"  Water segmentation is critical to disaster response and water resource\nmanagement. Authorities may employ high-resolution photography to monitor\nrivers, lakes, and reservoirs, allowing for more proactive management in\nagriculture, industry, and conservation. Deep learning has improved flood\nmonitoring by allowing models like CNNs, U-Nets, and transformers to handle\nlarge volumes of satellite and aerial data. However, these models usually have\nsignificant processing requirements, limiting their usage in real-time\napplications. This research proposes upgrading the SegFormer model for water\nsegmentation by data augmentation with datasets such as ADE20K and RIWA to\nboost generalization. We examine how inductive bias affects attention-based\nmodels and discover that SegFormer performs better on bigger datasets. To\nfurther demonstrate the function of data augmentation, Low-Rank Adaptation\n(LoRA) is used to lower processing complexity while preserving accuracy. We\nshow that the suggested Habaek model outperforms current models in\nsegmentation, with an Intersection over Union (IoU) ranging from 0.91986 to\n0.94397. In terms of F1-score, recall, accuracy, and precision, Habaek performs\nbetter than rival models, indicating its potential for real-world applications.\nThis study highlights the need to enhance structures and include datasets for\neffective water segmentation.\n","authors":["Hanseon Joo","Eunji Lee","Minjong Cheon"],"pdf_url":"https://arxiv.org/pdf/2410.15794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15792v1","updated":"2024-10-21T09:02:40Z","published":"2024-10-21T09:02:40Z","title":"WildOcc: A Benchmark for Off-Road 3D Semantic Occupancy Prediction","summary":"  3D semantic occupancy prediction is an essential part of autonomous driving,\nfocusing on capturing the geometric details of scenes. Off-road environments\nare rich in geometric information, therefore it is suitable for 3D semantic\noccupancy prediction tasks to reconstruct such scenes. However, most of\nresearches concentrate on on-road environments, and few methods are designed\nfor off-road 3D semantic occupancy prediction due to the lack of relevant\ndatasets and benchmarks. In response to this gap, we introduce WildOcc, to our\nknowledge, the first benchmark to provide dense occupancy annotations for\noff-road 3D semantic occupancy prediction tasks. A ground truth generation\npipeline is proposed in this paper, which employs a coarse-to-fine\nreconstruction to achieve a more realistic result. Moreover, we introduce a\nmulti-modal 3D semantic occupancy prediction framework, which fuses\nspatio-temporal information from multi-frame images and point clouds at voxel\nlevel. In addition, a cross-modality distillation function is introduced, which\ntransfers geometric knowledge from point clouds to image features.\n","authors":["Heng Zhai","Jilin Mei","Chen Min","Liang Chen","Fangzhou Zhao","Yu Hu"],"pdf_url":"https://arxiv.org/pdf/2410.15792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12831v2","updated":"2024-10-21T08:55:49Z","published":"2024-07-03T13:01:54Z","title":"Truth is Universal: Robust Detection of Lies in LLMs","summary":"  Large Language Models (LLMs) have revolutionised natural language processing,\nexhibiting impressive human-like capabilities. In particular, LLMs are capable\nof \"lying\", knowingly outputting false statements. Hence, it is of interest and\nimportance to develop methods to detect when LLMs lie. Indeed, several authors\ntrained classifiers to detect LLM lies based on their internal model\nactivations. However, other researchers showed that these classifiers may fail\nto generalise, for example to negated statements. In this work, we aim to\ndevelop a robust method to detect when an LLM is lying. To this end, we make\nthe following key contributions: (i) We demonstrate the existence of a\ntwo-dimensional subspace, along which the activation vectors of true and false\nstatements can be separated. Notably, this finding is universal and holds for\nvarious LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our\nanalysis explains the generalisation failures observed in previous studies and\nsets the stage for more robust lie detection; (ii) Building upon (i), we\nconstruct an accurate LLM lie detector. Empirically, our proposed classifier\nachieves state-of-the-art performance, attaining 94% accuracy in both\ndistinguishing true from false factual statements and detecting lies generated\nin real-world scenarios.\n","authors":["Lennart Bürger","Fred A. Hamprecht","Boaz Nadler"],"pdf_url":"https://arxiv.org/pdf/2407.12831v2.pdf","comment":"NeurIPS 2024 poster"},{"id":"http://arxiv.org/abs/2410.15787v1","updated":"2024-10-21T08:49:51Z","published":"2024-10-21T08:49:51Z","title":"Arithmetic Transformers Can Length-Generalize in Both Operand Length and\n  Count","summary":"  Transformers often struggle with length generalization, meaning they fail to\ngeneralize to sequences longer than those encountered during training. While\narithmetic tasks are commonly used to study length generalization, certain\ntasks are considered notoriously difficult, e.g., multi-operand addition\n(requiring generalization over both the number of operands and their lengths)\nand multiplication (requiring generalization over both operand lengths). In\nthis work, we achieve approximately 2-3x length generalization on both tasks,\nwhich is the first such achievement in arithmetic Transformers. We design\ntask-specific scratchpads enabling the model to focus on a fixed number of\ntokens per each next-token prediction step, and apply multi-level versions of\nPosition Coupling (Cho et al., 2024; McLeish et al., 2024) to let Transformers\nknow the right position to attend to. On the theory side, we prove that a\n1-layer Transformer using our method can solve multi-operand addition, up to\noperand length and operand count that are exponential in embedding dimension.\n","authors":["Hanseul Cho","Jaeyoung Cha","Srinadh Bhojanapalli","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2410.15787v1.pdf","comment":"38 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.08703v2","updated":"2024-10-21T08:49:18Z","published":"2024-10-11T10:47:02Z","title":"On the token distance modeling ability of higher RoPE attention\n  dimension","summary":"  Length extrapolation algorithms based on Rotary position embedding (RoPE)\nhave shown promising results in extending the context length of language\nmodels. However, understanding how position embedding can capture longer-range\ncontextual information remains elusive. Based on the intuition that different\ndimensions correspond to different frequency of changes in RoPE encoding, we\nconducted a dimension-level analysis to investigate the correlation between a\nhidden dimension of an attention head and its contribution to capturing\nlong-distance dependencies. Using our correlation metric, we identified a\nparticular type of attention heads, which we named Positional Heads, from\nvarious length-extrapolated models. These heads exhibit a strong focus on\nlong-range information interaction and play a pivotal role in long input\nprocessing, as evidence by our ablation. We further demonstrate the correlation\nbetween the efficiency of length extrapolation and the extension of the\nhigh-dimensional attention allocation of these heads. The identification of\nPositional Heads provides insights for future research in long-text\ncomprehension.\n","authors":["Xiangyu Hong","Che Jiang","Biqing Qi","Fandong Meng","Mo Yu","Bowen Zhou","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.08703v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15780v1","updated":"2024-10-21T08:45:26Z","published":"2024-10-21T08:45:26Z","title":"An Efficient System for Automatic Map Storytelling -- A Case Study on\n  Historical Maps","summary":"  Historical maps provide valuable information and knowledge about the past.\nHowever, as they often feature non-standard projections, hand-drawn styles, and\nartistic elements, it is challenging for non-experts to identify and interpret\nthem. While existing image captioning methods have achieved remarkable success\non natural images, their performance on maps is suboptimal as maps are\nunderrepresented in their pre-training process. Despite the recent advance of\nGPT-4 in text recognition and map captioning, it still has a limited\nunderstanding of maps, as its performance wanes when texts (e.g., titles and\nlegends) in maps are missing or inaccurate. Besides, it is inefficient or even\nimpractical to fine-tune the model with users' own datasets. To address these\nproblems, we propose a novel and lightweight map-captioning counterpart.\nSpecifically, we fine-tune the state-of-the-art vision-language model CLIP to\ngenerate captions relevant to historical maps and enrich the captions with\nGPT-3.5 to tell a brief story regarding where, what, when and why of a given\nmap. We propose a novel decision tree architecture to only generate captions\nrelevant to the specified map type. Our system shows invariance to text\nalterations in maps. The system can be easily adapted and extended to other map\ntypes and scaled to a larger map captioning system. The code is open-sourced at\nhttps://github.com/claudaff/automatic-map-storytelling.\n","authors":["Ziyi Liu","Claudio Affolter","Sidi Wu","Yizi Chen","Lorenz Hurni"],"pdf_url":"https://arxiv.org/pdf/2410.15780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15778v1","updated":"2024-10-21T08:42:30Z","published":"2024-10-21T08:42:30Z","title":"Reducing Hallucinations in Vision-Language Models via Latent Space\n  Steering","summary":"  Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.\n","authors":["Sheng Liu","Haotian Ye","James Zou"],"pdf_url":"https://arxiv.org/pdf/2410.15778v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.15770v1","updated":"2024-10-21T08:29:43Z","published":"2024-10-21T08:29:43Z","title":"A roadmap for generative mapping: unlocking the power of generative AI\n  for map-making","summary":"  Maps are broadly relevant across various fields, serving as valuable tools\nfor presenting spatial phenomena and communicating spatial knowledge. However,\nmap-making is still largely confined to those with expertise in GIS and\ncartography due to the specialized software and complex workflow involved, from\ndata processing to visualization. While generative AI has recently demonstrated\nits remarkable capability in creating various types of content and its wide\naccessibility to the general public, its potential in generating maps is yet to\nbe fully realized. This paper highlights the key applications of generative AI\nin map-making, summarizes recent advancements in generative AI, identifies the\nspecific technologies required and the challenges of using current methods, and\nprovides a roadmap for developing a generative mapping system (GMS) to make\nmap-making more accessible.\n","authors":["Sidi Wu","Katharina Henggeler","Yizi Chen","Lorenz Hurni"],"pdf_url":"https://arxiv.org/pdf/2410.15770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15768v1","updated":"2024-10-21T08:28:11Z","published":"2024-10-21T08:28:11Z","title":"Learning to Synthesize Graphics Programs for Geometric Artworks","summary":"  Creating and understanding art has long been a hallmark of human ability.\nWhen presented with finished digital artwork, professional graphic artists can\nintuitively deconstruct and replicate it using various drawing tools, such as\nthe line tool, paint bucket, and layer features, including opacity and blending\nmodes. While most recent research in this field has focused on art generation,\nproposing a range of methods, these often rely on the concept of artwork being\nrepresented as a final image. To bridge the gap between pixel-level results and\nthe actual drawing process, we present an approach that treats a set of drawing\ntools as executable programs. This method predicts a sequence of steps to\nachieve the final image, allowing for understandable and resolution-independent\nreproductions under the usage of a set of drawing commands. Our experiments\ndemonstrate that our program synthesizer, Art2Prog, can comprehensively\nunderstand complex input images and reproduce them using high-quality\nexecutable programs. The experimental results evidence the potential of\nmachines to grasp higher-level information from images and generate compact\nprogram-level descriptions.\n","authors":["Qi Bing","Chaoyi Zhang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2410.15768v1.pdf","comment":"ICPR 2024"},{"id":"http://arxiv.org/abs/2402.16788v4","updated":"2024-10-21T08:27:23Z","published":"2024-02-26T18:01:41Z","title":"Why Transformers Need Adam: A Hessian Perspective","summary":"  SGD performs worse than Adam by a significant margin on Transformers, but the\nreason remains unclear. In this work, we provide an explanation through the\nlens of Hessian: (i) Transformers are \"heterogeneous\": the Hessian spectrum\nacross parameter blocks vary dramatically, a phenomenon we call \"block\nheterogeneity\"; (ii) Heterogeneity hampers SGD: SGD performs worse than Adam on\nproblems with block heterogeneity. To validate (i) and (ii), we check various\nTransformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform\non par with Adam on problems without block heterogeneity, but performs worse\nthan Adam when the heterogeneity exists. Our initial theoretical analysis\nindicates that SGD performs worse because it applies one single learning rate\nto all blocks, which cannot handle the heterogeneity among blocks. This\nlimitation could be ameliorated if we use coordinate-wise learning rates, as\ndesigned in Adam.\n","authors":["Yushun Zhang","Congliang Chen","Tian Ding","Ziniu Li","Ruoyu Sun","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2402.16788v4.pdf","comment":"Advances in Neural Information Processing Systems, 2024"},{"id":"http://arxiv.org/abs/2410.15764v1","updated":"2024-10-21T08:23:31Z","published":"2024-10-21T08:23:31Z","title":"LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec","summary":"  Although discrete speech tokens have exhibited strong potential for language\nmodel-based speech generation, their high bitrates and redundant timbre\ninformation restrict the development of such models. In this work, we propose\nLSCodec, a discrete speech codec that has both low bitrate and speaker\ndecoupling ability. LSCodec adopts a three-stage unsupervised training\nframework with a speaker perturbation technique. A continuous information\nbottleneck is first established, followed by vector quantization that produces\na discrete speaker-decoupled space. A discrete token vocoder finally refines\nacoustic details from LSCodec. By reconstruction experiments, LSCodec\ndemonstrates superior intelligibility and audio quality with only a single\ncodebook and smaller vocabulary size than baselines. The 25Hz version of\nLSCodec also achieves the lowest bitrate (0.25kbps) of codecs so far with\ndecent quality. Voice conversion evaluations prove the satisfactory speaker\ndisentanglement of LSCodec, and ablation study further verifies the\neffectiveness of the proposed training framework.\n","authors":["Yiwei Guo","Zhihan Li","Chenpeng Du","Hankun Wang","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.15764v1.pdf","comment":"5 pages, 2 figures, 4 tables. Submitted to ICASSP 2025. Demo page:\n  https://cantabile-kwok.github.io/LSCodec/"},{"id":"http://arxiv.org/abs/2410.15760v1","updated":"2024-10-21T08:20:19Z","published":"2024-10-21T08:20:19Z","title":"DeepIcon: A Hierarchical Network for Layer-wise Icon Vectorization","summary":"  In contrast to the well-established technique of rasterization, vectorization\nof images poses a significant challenge in the field of computer graphics.\nRecent learning-based methods for converting raster images to vector formats\nfrequently suffer from incomplete shapes, redundant path prediction, and a lack\nof accuracy in preserving the semantics of the original content. These\nshortcomings severely hinder the utility of these methods for further editing\nand manipulation of images. To address these challenges, we present DeepIcon, a\nnovel hierarchical image vectorization network specifically tailored for\ngenerating variable-length icon vector graphics based on the raster image\ninput. Our experimental results indicate that DeepIcon can efficiently produce\nScalable Vector Graphics (SVGs) directly from raster images, bypassing the need\nfor a differentiable rasterizer while also demonstrating a profound\nunderstanding of the image contents.\n","authors":["Qi Bing","Chaoyi Zhang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2410.15760v1.pdf","comment":"Accepted as Oral Presentation at DICTA 2024"},{"id":"http://arxiv.org/abs/2410.15756v1","updated":"2024-10-21T08:15:45Z","published":"2024-10-21T08:15:45Z","title":"Automated Proof Generation for Rust Code via Self-Evolution","summary":"  Ensuring correctness is crucial for code generation. Formal verification\noffers a definitive assurance of correctness, but demands substantial human\neffort in proof construction and hence raises a pressing need for automation.\nThe primary obstacle lies in the severe lack of data - there is much less proof\nthan code for LLMs to train upon. In this paper, we introduce SAFE, a novel\nframework that overcomes the lack of human-written proof to enable automated\nproof generation of Rust code. SAFE establishes a self-evolving cycle where\ndata synthesis and fine-tuning collaborate to enhance the model capability,\nleveraging the definitive power of a symbolic verifier in telling correct proof\nfrom incorrect ones. SAFE also re-purposes the large number of synthesized\nincorrect proofs to train the self-debugging capability of the fine-tuned\nmodels, empowering them to fix incorrect proofs based on the verifier's\nfeedback. SAFE demonstrates superior efficiency and precision compared to\nGPT-4o. Through tens of thousands of synthesized proofs and the self-debugging\nmechanism, we improve the capability of open-source models, initially\nunacquainted with formal verification, to automatically write proof for Rust\ncode. This advancement leads to a significant improvement in performance,\nachieving a 70.50% accuracy rate in a benchmark crafted by human experts, a\nsignificant leap over GPT-4o's performance of 24.46%.\n","authors":["Tianyu Chen","Shuai Lu","Shan Lu","Yeyun Gong","Chenyuan Yang","Xuheng Li","Md Rakib Hossain Misu","Hao Yu","Nan Duan","Peng Cheng","Fan Yang","Shuvendu K Lahiri","Tao Xie","Lidong Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.15756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15747v1","updated":"2024-10-21T08:04:21Z","published":"2024-10-21T08:04:21Z","title":"GIG: Graph Data Imputation With Graph Differential Dependencies","summary":"  Data imputation addresses the challenge of imputing missing values in\ndatabase instances, ensuring consistency with the overall semantics of the\ndataset. Although several heuristics which rely on statistical methods, and\nad-hoc rules have been proposed. These do not generalise well and often lack\ndata context. Consequently, they also lack explainability. The existing\ntechniques also mostly focus on the relational data context making them\nunsuitable for wider application contexts such as in graph data. In this paper,\nwe propose a graph data imputation approach called GIG which relies on graph\ndifferential dependencies (GDDs). GIG, learns the GDDs from a given knowledge\ngraph, and uses these rules to train a transformer model which then predicts\nthe value of missing data within the graph. By leveraging GDDs, GIG incoporates\nsemantic knowledge into the data imputation process making it more reliable and\nexplainable. Experimental results on seven real-world datasets highlight GIG's\neffectiveness compared to existing state-of-the-art approaches.\n","authors":["Jiang Hua","Michael Bewong","Selasi Kwashie","MD Geaur Rahman","Junwei Hu","Xi Guo","Zaiwen Fen"],"pdf_url":"https://arxiv.org/pdf/2410.15747v1.pdf","comment":"12 pages, 4 figures, published to ADC"},{"id":"http://arxiv.org/abs/2410.15748v1","updated":"2024-10-21T08:04:21Z","published":"2024-10-21T08:04:21Z","title":"Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation","summary":"  Formal proofs are challenging to write even for experienced experts. Recent\nprogress in Neural Theorem Proving (NTP) shows promise in expediting this\nprocess. However, the formal corpora available on the Internet are limited\ncompared to the general text, posing a significant data scarcity challenge for\nNTP. To address this issue, this work proposes Alchemy, a general framework for\ndata synthesis that constructs formal theorems through symbolic mutation.\nSpecifically, for each candidate theorem in Mathlib, we identify all invocable\ntheorems that can be used to rewrite or apply to it. Subsequently, we mutate\nthe candidate theorem by replacing the corresponding term in the statement with\nits equivalent form or antecedent. As a result, our method increases the number\nof theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore,\nwe perform continual pretraining and supervised finetuning on this augmented\ncorpus for large language models. Experimental results demonstrate the\neffectiveness of our approach, achieving a 5% absolute performance improvement\non Leandojo benchmark. Additionally, our synthetic data achieve a 2.5% absolute\nperformance gain on the out-of-distribution miniF2F benchmark. To provide\nfurther insights, we conduct a comprehensive analysis of synthetic data\ncomposition and the training paradigm, offering valuable guidance for\ndeveloping a strong theorem prover.\n","authors":["Shaonan Wu","Shuai Lu","Yeyun Gong","Nan Duan","Ping Wei"],"pdf_url":"https://arxiv.org/pdf/2410.15748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15744v1","updated":"2024-10-21T08:01:58Z","published":"2024-10-21T08:01:58Z","title":"Unleashing the Potential of Vision-Language Pre-Training for 3D\n  Zero-Shot Lesion Segmentation via Mask-Attribute Alignment","summary":"  Recent advancements in medical vision-language pre-training models have\ndriven significant progress in zero-shot disease recognition. However,\ntransferring image-level knowledge to pixel-level tasks, such as lesion\nsegmentation in 3D CT scans, remains a critical challenge. Due to the\ncomplexity and variability of pathological visual characteristics, existing\nmethods struggle to align fine-grained lesion features not encountered during\ntraining with disease-related textual representations. In this paper, we\npresent Malenia, a novel multi-scale lesion-level mask-attribute alignment\nframework, specifically designed for 3D zero-shot lesion segmentation. Malenia\nimproves the compatibility between mask representations and their associated\nelemental attributes, explicitly linking the visual features of unseen lesions\nwith the extensible knowledge learned from previously seen ones. Furthermore,\nwe design a Cross-Modal Knowledge Injection module to enhance both visual and\ntextual features with mutually beneficial information, effectively guiding the\ngeneration of segmentation results. Comprehensive experiments across three\ndatasets and 12 lesion categories validate the superior performance of Malenia.\nCodes will be publicly available.\n","authors":["Yankai Jiang","Wenhui Lei","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15737v1","updated":"2024-10-21T07:56:45Z","published":"2024-10-21T07:56:45Z","title":"Who's Who: Large Language Models Meet Knowledge Conflicts in Practice","summary":"  Retrieval-augmented generation (RAG) methods are viable solutions for\naddressing the static memory limits of pre-trained language models.\nNevertheless, encountering conflicting sources of information within the\nretrieval context is an inevitable practical challenge. In such situations, the\nlanguage models are recommended to transparently inform users about the\nconflicts rather than autonomously deciding what to present based on their\ninherent biases. To analyze how current large language models (LLMs) align with\nour recommendation, we introduce WhoQA, a public benchmark dataset to examine\nmodel's behavior in knowledge conflict situations. We induce conflicts by\nasking about a common property among entities having the same name, resulting\nin questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K\nquestions across 13 Wikidata property types and 150K Wikipedia entities. Our\nexperiments show that despite the simplicity of WhoQA questions, knowledge\nconflicts significantly degrades LLMs' performance in RAG settings.\n","authors":["Quang Hieu Pham","Hoang Ngo","Anh Tuan Luu","Dat Quoc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.15737v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15735v1","updated":"2024-10-21T07:53:32Z","published":"2024-10-21T07:53:32Z","title":"AutoTrain: No-code training for state-of-the-art models","summary":"  With the advancements in open-source models, training (or finetuning) models\non custom datasets has become a crucial part of developing solutions which are\ntailored to specific industrial or open-source applications. Yet, there is no\nsingle tool which simplifies the process of training across different types of\nmodalities or tasks. We introduce AutoTrain (aka AutoTrain Advanced) -- an\nopen-source, no code tool/library which can be used to train (or finetune)\nmodels for different kinds of tasks such as: large language model (LLM)\nfinetuning, text classification/regression, token classification,\nsequence-to-sequence task, finetuning of sentence transformers, visual language\nmodel (VLM) finetuning, image classification/regression and even classification\nand regression tasks on tabular data. AutoTrain Advanced is an open-source\nlibrary providing best practices for training models on custom datasets. The\nlibrary is available at https://github.com/huggingface/autotrain-advanced.\nAutoTrain can be used in fully local mode or on cloud machines and works with\ntens of thousands of models shared on Hugging Face Hub and their variations.\n","authors":["Abhishek Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.15735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15726v1","updated":"2024-10-21T07:44:01Z","published":"2024-10-21T07:44:01Z","title":"Reducing annotator bias by belief elicitation","summary":"  Crowdsourced annotations of data play a substantial role in the development\nof Artificial Intelligence (AI). It is broadly recognised that annotations of\ntext data can contain annotator bias, where systematic disagreement in\nannotations can be traced back to differences in the annotators' backgrounds.\nBeing unaware of such annotator bias can lead to representational bias against\nminority group perspectives and therefore several methods have been proposed\nfor recognising bias or preserving perspectives. These methods typically\nrequire either a substantial number of annotators or annotations per data\ninstance. In this study, we propose a simple method for handling bias in\nannotations without requirements on the number of annotators or instances.\nInstead, we ask annotators about their beliefs of other annotators' judgements\nof an instance, under the hypothesis that these beliefs may provide more\nrepresentative and less biased labels than judgements. The method was examined\nin two controlled, survey-based experiments involving Democrats and Republicans\n(n=1,590) asked to judge statements as arguments and then report beliefs about\nothers' judgements. The results indicate that bias, defined as systematic\ndifferences between the two groups of annotators, is consistently reduced when\nasking for beliefs instead of judgements. Our proposed method therefore has the\npotential to reduce the risk of annotator bias, thereby improving the\ngeneralisability of AI systems and preventing harm to unrepresented\nsocio-demographic groups, and we highlight the need for further studies of this\npotential in other tasks and downstream applications.\n","authors":["Terne Sasha Thorn Jakobsen","Andreas Bjerre-Nielsen","Robert Böhm"],"pdf_url":"https://arxiv.org/pdf/2410.15726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15715v1","updated":"2024-10-21T07:34:04Z","published":"2024-10-21T07:34:04Z","title":"Timetable Nodes for Public Transport Network","summary":"  Faster pathfinding in time-dependent transport networks is an important and\nchallenging problem in navigation systems. There are two main types of\ntransport networks: road networks for car driving and public transport route\nnetwork. The solutions that work well in road networks, such as Time-dependent\nContraction Hierarchies and other graph-based approaches, do not usually apply\nin transport networks. In transport networks, non-graph solutions such as CSA\nand RAPTOR show the best results compared to graph-based techniques. In our\nwork, we propose a method that advances graph-based approaches by using\ndifferent optimization techniques from computational geometry to speed up the\nsearch process in transport networks. We apply a new pre-computation step,\nwhich we call timetable nodes (TTN). Our inspiration comes from an iterative\nsearch problem in computational geometry. We implement two versions of the TTN:\none uses a Combined Search Tree (TTN-CST), and the second uses Fractional\nCascading (TTN-FC). Both of these approaches decrease the asymptotic complexity\nof reaching new nodes from $O(k\\times \\log|C|)$ to $O(k + \\log(k) +\n\\log(|C|))$, where $k$ is the number of outgoing edges from a node and $|C|$ is\nthe size of the timetable information (total outgoing edges). Our solution\nsuits any other time-dependent networks and can be integrated into other\npathfinding algorithms. Our experiments indicate that this pre-computation\nsignificantly enhances the performance on high-density graphs. This study\nshowcases how leveraging computational geometry can enhance pathfinding in\ntransport networks, enabling faster pathfinding in scenarios involving large\nnumbers of outgoing edges.\n","authors":["Andrii Rohovyi","Peter J. Stuckey","Toby Walsh"],"pdf_url":"https://arxiv.org/pdf/2410.15715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15714v1","updated":"2024-10-21T07:33:42Z","published":"2024-10-21T07:33:42Z","title":"Offline reinforcement learning for job-shop scheduling problems","summary":"  Recent advances in deep learning have shown significant potential for solving\ncombinatorial optimization problems in real-time. Unlike traditional methods,\ndeep learning can generate high-quality solutions efficiently, which is crucial\nfor applications like routing and scheduling. However, existing approaches like\ndeep reinforcement learning (RL) and behavioral cloning have notable\nlimitations, with deep RL suffering from slow learning and behavioral cloning\nrelying solely on expert actions, which can lead to generalization issues and\nneglect of the optimization objective. This paper introduces a novel offline RL\nmethod designed for combinatorial optimization problems with complex\nconstraints, where the state is represented as a heterogeneous graph and the\naction space is variable. Our approach encodes actions in edge attributes and\nbalances expected rewards with the imitation of expert solutions. We\ndemonstrate the effectiveness of this method on job-shop scheduling and\nflexible job-shop scheduling benchmarks, achieving superior performance\ncompared to state-of-the-art techniques.\n","authors":["Imanol Echeverria","Maialen Murua","Roberto Santana"],"pdf_url":"https://arxiv.org/pdf/2410.15714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15700v1","updated":"2024-10-21T07:18:23Z","published":"2024-10-21T07:18:23Z","title":"InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert\n  Iteration on Large-Scale LEAN Problems","summary":"  Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. The\nmajor learning paradigm is expert iteration, which necessitates a pre-defined\ndataset comprising numerous mathematical problems. In this process, LLMs\nattempt to prove problems within the dataset and iteratively refine their\ncapabilities through self-training on the proofs they discover. We propose to\nuse large scale LEAN problem datasets Lean-workbook for expert iteration with\nmore than 20,000 CPU days. During expert iteration, we found log-linear trends\nbetween solved problem amount with proof length and CPU usage. We train a\ncritic model to select relatively easy problems for policy models to make\ntrials and guide the model to search for deeper proofs. InternLM2.5-StepProver\nachieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet,\nand Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the\nMiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus\nwhich shows a significant improvement compared to only 9.5% of problems proved\nwhen Lean-Workbook-Plus was released. We open-source our models and searched\nproofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.\n","authors":["Zijian Wu","Suozhi Huang","Zhejian Zhou","Huaiyuan Ying","Jiayu Wang","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05392v2","updated":"2024-10-21T07:08:11Z","published":"2024-06-08T07:55:01Z","title":"Deconstructing The Ethics of Large Language Models from Long-standing\n  Issues to New-emerging Dilemmas: A Survey","summary":"  Large Language Models (LLMs) have achieved unparalleled success across\ndiverse language modeling tasks in recent years. However, this progress has\nalso intensified ethical concerns, impacting the deployment of LLMs in everyday\ncontexts. This paper provides a comprehensive survey of ethical challenges\nassociated with LLMs, from longstanding issues such as copyright infringement,\nsystematic bias, and data privacy, to emerging problems like truthfulness and\nsocial norms. We critically analyze existing research aimed at understanding,\nexamining, and mitigating these ethical risks. Our survey underscores\nintegrating ethical standards and societal values into the development of LLMs,\nthereby guiding the development of responsible and ethically aligned language\nmodels.\n","authors":["Chengyuan Deng","Yiqun Duan","Xin Jin","Heng Chang","Yijun Tian","Han Liu","Yichen Wang","Kuofeng Gao","Henry Peng Zou","Yiqiao Jin","Yijia Xiao","Shenghao Wu","Zongxing Xie","Weimin Lyu","Sihong He","Lu Cheng","Haohan Wang","Jun Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.05392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15694v1","updated":"2024-10-21T07:05:07Z","published":"2024-10-21T07:05:07Z","title":"PALMS: Plane-based Accessible Indoor Localization Using Mobile\n  Smartphones","summary":"  In this paper, we present PALMS, an innovative indoor global localization and\nrelocalization system for mobile smartphones that utilizes publicly available\nfloor plans. Unlike most vision-based methods that require constant visual\ninput, our system adopts a dynamic form of localization that considers a single\ninstantaneous observation and odometry data. The core contribution of this work\nis the introduction of a particle filter initialization method that leverages\nthe Certainly Empty Space (CES) constraint along with principal orientation\nmatching. This approach creates a spatial probability distribution of the\ndevice's location, significantly improving localization accuracy and reducing\nparticle filter convergence time. Our experimental evaluations demonstrate that\nPALMS outperforms traditional methods with uniformly initialized particle\nfilters, providing a more efficient and accessible approach to indoor\nwayfinding. By eliminating the need for prior environmental fingerprinting,\nPALMS provides a scalable and practical approach to indoor navigation.\n","authors":["Yunqian Cheng","Roberto Manduchi"],"pdf_url":"https://arxiv.org/pdf/2410.15694v1.pdf","comment":"7 pages, 3 figures, accepted to the 14th International Conference on\n  Indoor Positioning and Indoor Navigation (IPIN) 2024, Best Presentation Award"},{"id":"http://arxiv.org/abs/2410.15693v1","updated":"2024-10-21T07:03:15Z","published":"2024-10-21T07:03:15Z","title":"Geographical Node Clustering and Grouping to Guarantee Data IIDness in\n  Federated Learning","summary":"  Federated learning (FL) is a decentralized AI mechanism suitable for a large\nnumber of devices like in smart IoT. A major challenge of FL is the non-IID\ndataset problem, originating from the heterogeneous data collected by FL\nparticipants, leading to performance deterioration of the trained global model.\nThere have been various attempts to rectify non-IID dataset, mostly focusing on\nmanipulating the collected data. This paper, however, proposes a novel approach\nto ensure data IIDness by properly clustering and grouping mobile IoT nodes\nexploiting their geographical characteristics, so that each FL group can\nachieve IID dataset. We first provide an experimental evidence for the\nindependence and identicalness features of IoT data according to the\ninter-device distance, and then propose Dynamic Clustering and Partial-Steady\nGrouping algorithms that partition FL participants to achieve near-IIDness in\ntheir dataset while considering device mobility. Our mechanism significantly\noutperforms benchmark grouping algorithms at least by 110 times in terms of the\njoint cost between the number of dropout devices and the evenness in per-group\ndevice count, with a mild increase in the number of groups only by up to 0.93\ngroups.\n","authors":["Minkwon Lee","Hyoil Kim","Changhee Joo"],"pdf_url":"https://arxiv.org/pdf/2410.15693v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.06900v4","updated":"2024-10-21T06:56:26Z","published":"2024-02-10T07:55:27Z","title":"Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric","summary":"  In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.\n","authors":["Hyukhun Koh","Dohyung Kim","Minwoo Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2402.06900v4.pdf","comment":"8 page long"},{"id":"http://arxiv.org/abs/2410.15686v1","updated":"2024-10-21T06:54:27Z","published":"2024-10-21T06:54:27Z","title":"NetSafe: Exploring the Topological Safety of Multi-agent Networks","summary":"  Large language models (LLMs) have empowered nodes within multi-agent networks\nwith intelligence, showing growing applications in both academia and industry.\nHowever, how to prevent these networks from generating malicious information\nremains unexplored with previous research on single LLM's safety be challenging\nto transfer. In this paper, we focus on the safety of multi-agent networks from\na topological perspective, investigating which topological properties\ncontribute to safer networks. To this end, we propose a general framework,\nNetSafe along with an iterative RelCom interaction to unify existing diverse\nLLM-based agent frameworks, laying the foundation for generalized topological\nsafety research. We identify several critical phenomena when multi-agent\nnetworks are exposed to attacks involving misinformation, bias, and harmful\ninformation, termed as Agent Hallucination and Aggregation Safety. Furthermore,\nwe find that highly connected networks are more susceptible to the spread of\nadversarial attacks, with task performance in a Star Graph Topology decreasing\nby 29.7%. Besides, our proposed static metrics aligned more closely with\nreal-world dynamic evaluations than traditional graph-theoretic metrics,\nindicating that networks with greater average distances from attackers exhibit\nenhanced safety. In conclusion, our work introduces a new topological\nperspective on the safety of LLM-based multi-agent networks and discovers\nseveral unreported phenomena, paving the way for future research to explore the\nsafety of such networks.\n","authors":["Miao Yu","Shilong Wang","Guibin Zhang","Junyuan Mao","Chenlong Yin","Qijiong Liu","Qingsong Wen","Kun Wang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10532v2","updated":"2024-10-21T06:50:51Z","published":"2024-08-20T04:18:53Z","title":"NutrifyAI: An AI-Powered System for Real-Time Food Detection,\n  Nutritional Analysis, and Personalized Meal Recommendations","summary":"  With diet and nutrition apps reaching 1.4 billion users in 2022 [1], it's not\nsurprise that popular health apps, MyFitnessPal, Noom, and Calorie Counter, are\nsurging in popularity. However, one major setback [2] of nearly all nutrition\napplications is that users must enter food data manually, which is\ntime-consuming and tedious. Thus, there has been an increasing demand for\napplications that can accurately identify food items, analyze their nutritional\ncontent, and offer dietary recommendations in real-time. This paper introduces\na comprehensive system that combines advanced computer vision techniques with\nnutritional analysis, implemented in a versatile mobile and web application.\nThe system is divided into three key concepts: 1) food detection using the\nYOLOv8 model, 2) nutrient analysis via the Edamam Nutrition Analysis API, and\n3) personalized meal recommendations using the Edamam Meal Planning and Recipe\nSearch APIs. Preliminary results showcase the system's effectiveness by\nproviding immediate, accurate dietary insights, with a demonstrated food\nrecognition accuracy of nearly 80%, making it a valuable tool for users to make\ninformed dietary decisions.\n","authors":["Michelle Han","Junyao Chen","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.10532v2.pdf","comment":"4 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.05273v2","updated":"2024-10-21T06:50:05Z","published":"2024-09-12T09:18:09Z","title":"HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers","summary":"  Large Vision-Language-Action (VLA) models, leveraging powerful pre trained\nVision-Language Models (VLMs) backends, have shown promise in robotic control\ndue to their impressive generalization ability. However, the success comes at a\ncost. Their reliance on VLM backends with billions of parameters leads to high\ncomputational costs and inference latency, limiting the testing scenarios to\nmainly quasi-static tasks and hindering performance in dynamic tasks requiring\nrapid interactions. To address these limitations, this paper proposes HiRT, a\nHierarchical Robot Transformer framework that enables flexible frequency and\nperformance trade-off. HiRT keeps VLMs running at low frequencies to capture\ntemporarily invariant features while enabling real-time interaction through a\nhigh-frequency vision-based policy guided by the slowly updated features.\nExperiment results in both simulation and real-world settings demonstrate\nsignificant improvements over baseline methods. Empirically, in static tasks,\nwe double the control frequency and achieve comparable success rates.\nAdditionally, on novel real-world dynamic ma nipulation tasks which are\nchallenging for previous VLA models, HiRT improves the success rate from 48% to\n75%.\n","authors":["Jianke Zhang","Yanjiang Guo","Xiaoyu Chen","Yen-Jen Wang","Yucheng Hu","Chengming Shi","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03320v2","updated":"2024-10-21T06:46:52Z","published":"2022-12-06T20:44:24Z","title":"Reinforcement Learning for Molecular Dynamics Optimization: A Stochastic\n  Pontryagin Maximum Principle Approach","summary":"  In this paper, we present a novel reinforcement learning framework designed\nto optimize molecular dynamics by focusing on the entire trajectory rather than\njust the final molecular configuration. Leveraging a stochastic version of\nPontryagin's Maximum Principle (PMP) and Soft Actor-Critic (SAC) algorithm, our\nframework effectively explores non-convex molecular energy landscapes, escaping\nlocal minima to stabilize in low-energy states. Our approach operates in\ncontinuous state and action spaces without relying on labeled data, making it\napplicable to a wide range of molecular systems. Through extensive\nexperimentation on six distinct molecules, including Bradykinin and Oxytocin,\nwe demonstrate competitive performance against other unsupervised physics-based\nmethods, such as the Greedy and NEMO-based algorithms. Our method's\nadaptability and focus on dynamic trajectory optimization make it suitable for\napplications in areas such as drug discovery and molecular design.\n","authors":["Chandrajit Bajaj","Minh Nguyen","Conrad Li"],"pdf_url":"https://arxiv.org/pdf/2212.03320v2.pdf","comment":"Accepted to the International Conference on Neural Information\n  Processing (ICONIP) 2024. To be published in Springer-Nature Communications\n  in Computer and Information Science (CCIS) Series"},{"id":"http://arxiv.org/abs/2410.15678v1","updated":"2024-10-21T06:42:11Z","published":"2024-10-21T06:42:11Z","title":"Revealing and Mitigating the Local Pattern Shortcuts of Mamba","summary":"  Large language models (LLMs) have advanced significantly due to the attention\nmechanism, but their quadratic complexity and linear memory demands limit their\nperformance on long-context tasks. Recently, researchers introduced Mamba, an\nadvanced model built upon State Space Models(SSMs) that offers linear\ncomplexity and constant memory. Although Mamba is reported to match or surpass\nthe performance of attention-based models, our analysis reveals a performance\ngap: Mamba excels in tasks that involve localized key information but faces\nchallenges with tasks that require handling distributed key information. Our\ncontrolled experiments suggest that this inconsistency arises from Mamba's\nreliance on local pattern shortcuts, which enable the model to remember local\nkey information within its limited memory but hinder its ability to retain more\ndispersed information. Therefore, we introduce a global selection module into\nthe Mamba model to address this issue. Experiments on both existing and\nproposed synthetic tasks, as well as real-world tasks, demonstrate the\neffectiveness of our method. Notably, with the introduction of only 4M extra\nparameters, our approach enables the Mamba model(130M) to achieve a significant\nimprovement on tasks with distributed information, increasing its performance\nfrom 0 to 80.54 points.\n","authors":["Wangjie You","Zecheng Tang","Juntao Li","Lili Yao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12606v2","updated":"2024-10-21T06:27:47Z","published":"2024-10-16T14:24:44Z","title":"Self-Supervised Learning of Disentangled Representations for\n  Multivariate Time-Series","summary":"  Multivariate time-series data in fields like healthcare and industry are\ninformative but challenging due to high dimensionality and lack of labels.\nRecent self-supervised learning methods excel in learning rich representations\nwithout labels but struggle with disentangled embeddings and inductive bias\nissues like transformation-invariance. To address these challenges, we\nintroduce TimeDRL, a framework for multivariate time-series representation\nlearning with dual-level disentangled embeddings. TimeDRL features: (i)\ndisentangled timestamp-level and instance-level embeddings using a [CLS] token\nstrategy; (ii) timestamp-predictive and instance-contrastive tasks for\nrepresentation learning; and (iii) avoidance of augmentation methods to\neliminate inductive biases. Experiments on forecasting and classification\ndatasets show TimeDRL outperforms existing methods, with further validation in\nsemi-supervised settings with limited labeled data.\n","authors":["Ching Chang","Chiao-Tung Chan","Wei-Yao Wang","Wen-Chih Peng","Tien-Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12606v2.pdf","comment":"This submission has been withdrawn to avoid duplication with a full\n  version of the paper that is already available in another arXiv entry\n  (arXiv:2410.12606). The withdrawn version was a short format prepared for a\n  NeurIPS workshop and is no longer necessary as a separate arXiv submission"},{"id":"http://arxiv.org/abs/2406.13165v2","updated":"2024-10-21T06:25:57Z","published":"2024-06-19T02:42:29Z","title":"Cardiac Copilot: Automatic Probe Guidance for Echocardiography with\n  World Model","summary":"  Echocardiography is the only technique capable of real-time imaging of the\nheart and is vital for diagnosing the majority of cardiac diseases. However,\nthere is a severe shortage of experienced cardiac sonographers, due to the\nheart's complex structure and significant operational challenges. To mitigate\nthis situation, we present a Cardiac Copilot system capable of providing\nreal-time probe movement guidance to assist less experienced sonographers in\nconducting freehand echocardiography. This system can enable non-experts,\nespecially in primary departments and medically underserved areas, to perform\ncardiac ultrasound examinations, potentially improving global healthcare\ndelivery. The core innovation lies in proposing a data-driven world model,\nnamed Cardiac Dreamer, for representing cardiac spatial structures. This world\nmodel can provide structure features of any cardiac planes around the current\nprobe position in the latent space, serving as an precise navigation map for\nautonomous plane localization. We train our model with real-world ultrasound\ndata and corresponding probe motion from 110 routine clinical scans with 151K\nsample pairs by three certified sonographers. Evaluations on three standard\nplanes with 37K sample pairs demonstrate that the world model can reduce\nnavigation errors by up to 33\\% and exhibit more stable performance.\n","authors":["Haojun Jiang","Zhenguo Sun","Ning Jia","Meng Li","Yu Sun","Shaqi Luo","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.13165v2.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2410.15669v1","updated":"2024-10-21T06:22:51Z","published":"2024-10-21T06:22:51Z","title":"Learning to Generate and Evaluate Fact-checking Explanations with\n  Transformers","summary":"  In an era increasingly dominated by digital platforms, the spread of\nmisinformation poses a significant challenge, highlighting the need for\nsolutions capable of assessing information veracity. Our research contributes\nto the field of Explainable Artificial Antelligence (XAI) by developing\ntransformer-based fact-checking models that contextualise and justify their\ndecisions by generating human-accessible explanations. Importantly, we also\ndevelop models for automatic evaluation of explanations for fact-checking\nverdicts across different dimensions such as \\texttt{(self)-contradiction},\n\\texttt{hallucination}, \\texttt{convincingness} and \\texttt{overall quality}.\nBy introducing human-centred evaluation methods and developing specialised\ndatasets, we emphasise the need for aligning Artificial Intelligence\n(AI)-generated explanations with human judgements. This approach not only\nadvances theoretical knowledge in XAI but also holds practical implications by\nenhancing the transparency, reliability and users' trust in AI-driven\nfact-checking systems. Furthermore, the development of our metric learning\nmodels is a first step towards potentially increasing efficiency and reducing\nreliance on extensive manual assessment. Based on experimental results, our\nbest performing generative model \\textsc{ROUGE-1} score of 47.77, demonstrating\nsuperior performance in generating fact-checking explanations, particularly\nwhen provided with high-quality evidence. Additionally, the best performing\nmetric learning model showed a moderately strong correlation with human\njudgements on objective dimensions such as \\texttt{(self)-contradiction and\n\\texttt{hallucination}, achieving a Matthews Correlation Coefficient (MCC) of\naround 0.7.}\n","authors":["Darius Feher","Abdullah Khered","Hao Zhang","Riza Batista-Navarro","Viktor Schlegel"],"pdf_url":"https://arxiv.org/pdf/2410.15669v1.pdf","comment":"Forthcoming in Engineering Applications of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2410.15667v1","updated":"2024-10-21T06:11:38Z","published":"2024-10-21T06:11:38Z","title":"RAC: Efficient LLM Factuality Correction with Retrieval Augmentation","summary":"  Large Language Models (LLMs) exhibit impressive results across a wide range\nof natural language processing (NLP) tasks, yet they can often produce\nfactually incorrect outputs. This paper introduces a simple but effective\nlow-latency post-correction method, \\textbf{Retrieval Augmented Correction\n(RAC)}, aimed at enhancing the factual performance of LLMs without requiring\nadditional fine-tuning. Our method is general and can be used with any\ninstruction-tuned LLM, and has greatly reduced latency compared to prior\napproaches. RAC decomposes the LLM's output into atomic facts and applies a\nfine-grained verification and correction process with retrieved content to\nverify and correct the LLM-generated output. Our extensive experiments show\nthat RAC yields up to 30\\% improvements over state-of-the-art baselines across\ntwo popular factuality evaluation datasets, validating its efficacy and\nrobustness in both with and without the integration of Retrieval-Augmented\nGeneration (RAG) across different LLMs.\\footnote{Our code is at\n\\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}\n","authors":["Changmao Li","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2410.15667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15665v1","updated":"2024-10-21T06:09:30Z","published":"2024-10-21T06:09:30Z","title":"Long Term Memory: The Foundation of AI Self-Evolution","summary":"  Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications.\n","authors":["Xun Jiang","Feng Li","Han Zhao","Jiaying Wang","Jun Shao","Shihao Xu","Shu Zhang","Weiling Chen","Xavier Tang","Yize Chen","Mengyue Wu","Weizhi Ma","Mengdi Wang","Tianqiao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15665v1.pdf","comment":"56 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.15656v1","updated":"2024-10-21T05:49:40Z","published":"2024-10-21T05:49:40Z","title":"LightFusionRec: Lightweight Transformers-Based Cross-Domain\n  Recommendation Model","summary":"  This paper presents LightFusionRec, a novel lightweight cross-domain\nrecommendation system that integrates DistilBERT for textual feature extraction\nand FastText for genre embedding. Important issues in recommendation systems,\nsuch as data sparsity, computational efficiency, and cold start issues, are\naddressed in methodology. LightFusionRec uses a small amount of information to\nproduce precise and contextually relevant recommendations for many media\nformats by fusing genre vector embedding with natural language processing\nalgorithms. Tests conducted on extensive movie and book datasets show notable\nenhancements in suggestion quality when compared to conventional methods.\nBecause of its lightweight design, the model can be used for a variety of\npurposes and allows for ondevice inference. LightFusionRec is a noteworthy\ndevelopment in cross-domain recommendation systems, providing accurate and\nscalable recommendations to improve user experience on digital content\nplatforms.\n","authors":["Vansh Kharidia","Dhruvi Paprunia","Prashasti Kanikar"],"pdf_url":"https://arxiv.org/pdf/2410.15656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14023v2","updated":"2024-10-21T05:34:04Z","published":"2024-09-21T05:25:46Z","title":"FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer\n  on UltraScale+ FPGAs","summary":"  Transformer neural networks (TNNs) are being applied across a widening range\nof application domains, including natural language processing (NLP), machine\ntranslation, and computer vision (CV). Their popularity is largely attributed\nto the exceptional performance of their multi-head self-attention blocks when\nanalyzing sequential data and extracting features. To date, there are limited\nhardware accelerators tailored for this mechanism, which is the first step\nbefore designing an accelerator for a complete model. This paper proposes\n\\textit{FAMOUS}, a flexible hardware accelerator for dense multi-head attention\n(MHA) computation of TNNs on field-programmable gate arrays (FPGAs). It is\noptimized for high utilization of processing elements and on-chip memories to\nimprove parallelism and reduce latency. An efficient tiling of large matrices\nhas been employed to distribute memory and computing resources across different\nmodules on various FPGA platforms. The design is evaluated on Xilinx Alveo U55C\nand U200 data center cards containing Ultrascale+ FPGAs. Experimental results\nare presented that show that it can attain a maximum throughput, number of\nparallel attention heads, embedding dimension and tile size of 328 (giga\noperations/second (GOPS)), 8, 768 and 64 respectively on the U55C. Furthermore,\nit is 3.28$\\times$ and 2.6$\\times$ faster than the Intel Xeon Gold 5220R CPU\nand NVIDIA V100 GPU respectively. It is also 1.3$\\times$ faster than the\nfastest state-of-the-art FPGA-based accelerator.\n","authors":["Ehsan Kabir","Md. Arafat Kabir","Austin R. J. Downey","Jason D. Bakos","David Andrews","Miaoqing Huang"],"pdf_url":"https://arxiv.org/pdf/2409.14023v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.13975"},{"id":"http://arxiv.org/abs/2410.15653v1","updated":"2024-10-21T05:30:39Z","published":"2024-10-21T05:30:39Z","title":"Opportunities and Challenges of Generative-AI in Finance","summary":"  Machine Learning and data mining have created widespread impact across\nvarious domains. However, these techniques are limited in their ability to\nreason, understand and generalize w.r.t language specific tasks. The\naforementioned challenges were overcome, with the advancement of LLMs/Gen-AI.\nGen-AI techniques are able to improve understanding of context and nuances in\nlanguage modeling, translation between languages, handle large volumes of data,\nprovide fast, low-latency responses and can be fine-tuned for various tasks and\ndomains.\n  In this manuscript, we present a comprehensive overview of the applications\nof Gen-AI techniques in the finance domain. In particular, we present the\nopportunities and challenges associated with the usage of Gen-AI techniques in\nfinance. We also illustrate the various methodologies which can be used to\ntrain Gen-AI and present the various application areas of Gen-AI techniques in\nthe finance ecosystem.\n  To the best of our knowledge, this work represents the most comprehensive\nsummarization of Gen-AI techniques within the financial domain. The analysis is\ndesigned for a deep overview of areas marked for substantial advancement while\nsimultaneously pin-point those warranting future prioritization. We also hope\nthat this work would serve as a conduit between finance and other domains, thus\nfostering the cross-pollination of innovative concepts and practices.\n","authors":["Akshar Prabhu Desai","Ganesh Satish Mallya","Mohammad Luqman","Tejasvi Ravi","Nithya Kota","Pranjul Yadav"],"pdf_url":"https://arxiv.org/pdf/2410.15653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15650v1","updated":"2024-10-21T05:22:54Z","published":"2024-10-21T05:22:54Z","title":"Voice-Enabled AI Agents can Perform Common Scams","summary":"  Recent advances in multi-modal, highly capable LLMs have enabled\nvoice-enabled AI agents. These agents are enabling new applications, such as\nvoice-enabled autonomous customer service. However, with all AI capabilities,\nthese new capabilities have the potential for dual use.\n  In this work, we show that voice-enabled AI agents can perform the actions\nnecessary to perform common scams. To do so, we select a list of common scams\ncollected by the government and construct voice-enabled agents with directions\nto perform these scams. We conduct experiments on our voice-enabled agents and\nshow that they can indeed perform the actions necessary to autonomously perform\nsuch scams. Our results raise questions around the widespread deployment of\nvoice-enabled AI agents.\n","authors":["Richard Fang","Dylan Bowman","Daniel Kang"],"pdf_url":"https://arxiv.org/pdf/2410.15650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15645v1","updated":"2024-10-21T05:11:19Z","published":"2024-10-21T05:11:19Z","title":"Boosting Jailbreak Transferability for Large Language Models","summary":"  Large language models have drawn significant attention to the challenge of\nsafe alignment, especially regarding jailbreak attacks that circumvent security\nmeasures to produce harmful content. To address the limitations of existing\nmethods like GCG, which perform well in single-model attacks but lack\ntransferability, we propose several enhancements, including a scenario\ninduction template, optimized suffix selection, and the integration of\nre-suffix attack mechanism to reduce inconsistent outputs. Our approach has\nshown superior performance in extensive experiments across various benchmarks,\nachieving nearly 100% success rates in both attack execution and\ntransferability. Notably, our method has won the online first place in the\nAISG-hosted Global Challenge for Safe and Secure LLMs.\n","authors":["Hanqing Liu","Lifeng Zhou","Huanqian Yan"],"pdf_url":"https://arxiv.org/pdf/2410.15645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15644v1","updated":"2024-10-21T05:10:13Z","published":"2024-10-21T05:10:13Z","title":"Procedural Content Generation in Games: A Survey with Insights on\n  Emerging LLM Integration","summary":"  Procedural Content Generation (PCG) is defined as the automatic creation of\ngame content using algorithms. PCG has a long history in both the game industry\nand the academic world. It can increase player engagement and ease the work of\ngame designers. While recent advances in deep learning approaches in PCG have\nenabled researchers and practitioners to create more sophisticated content, it\nis the arrival of Large Language Models (LLMs) that truly disrupted the\ntrajectory of PCG advancement.\n  This survey explores the differences between various algorithms used for PCG,\nincluding search-based methods, machine learning-based methods, other\nfrequently used methods (e.g., noise functions), and the newcomer, LLMs. We\nalso provide a detailed discussion on combined methods. Furthermore, we compare\nthese methods based on the type of content they generate and the publication\ndates of their respective papers. Finally, we identify gaps in the existing\nacademic work and suggest possible directions for future research.\n","authors":["Mahdi Farrokhi Maleki","Richard Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.15644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15642v1","updated":"2024-10-21T05:08:18Z","published":"2024-10-21T05:08:18Z","title":"Resource-Efficient Medical Report Generation using Large Language Models","summary":"  Medical report generation is the task of automatically writing radiology\nreports for chest X-ray images. Manually composing these reports is a\ntime-consuming process that is also prone to human errors. Generating medical\nreports can therefore help reduce the burden on radiologists. In other words,\nwe can promote greater clinical automation in the medical domain. In this work,\nwe propose a new framework leveraging vision-enabled Large Language Models\n(LLM) for the task of medical report generation. We introduce a lightweight\nsolution that achieves better or comparative performance as compared to\nprevious solutions on the task of medical report generation. We conduct\nextensive experiments exploring different model sizes and enhancement\napproaches, such as prefix tuning to improve the text generation abilities of\nthe LLMs. We evaluate our approach on a prominent large-scale radiology report\ndataset - MIMIC-CXR. Our results demonstrate the capability of our\nresource-efficient framework to generate patient-specific reports with strong\nmedical contextual understanding and high precision.\n","authors":[" Abdullah","Ameer Hamza","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.15642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18292v5","updated":"2024-10-21T05:06:15Z","published":"2024-02-28T12:37:30Z","title":"FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time\n  Augmentation","summary":"  Few-shot-learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few labeled\nsamples of the new classes (support set) as reference. So far, plenty of\nalgorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, e.g., obtaining a test accuracy improvement proportion of around\n10% (e.g., from 46.86% to 53.28%) for trained FSL models. Importantly, given\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves.\n","authors":["Yunwei Bai","Ying Kiat Tan","Shiming Chen","Yao Shu","Tsuhan Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18292v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01742v3","updated":"2024-10-21T04:38:08Z","published":"2024-03-04T05:39:23Z","title":"Diffusion-TS: Interpretable Diffusion for General Time Series Generation","summary":"  Denoising diffusion probabilistic models (DDPMs) are becoming the leading\nparadigm for generative models. It has recently shown breakthroughs in audio\nsynthesis, time series imputation and forecasting. In this paper, we propose\nDiffusion-TS, a novel diffusion-based framework that generates multivariate\ntime series samples of high quality by using an encoder-decoder transformer\nwith disentangled temporal representations, in which the decomposition\ntechnique guides Diffusion-TS to capture the semantic meaning of time series\nwhile transformers mine detailed sequential information from the noisy model\ninput. Different from existing diffusion-based approaches, we train the model\nto directly reconstruct the sample instead of the noise in each diffusion step,\ncombining a Fourier-based loss term. Diffusion-TS is expected to generate time\nseries satisfying both interpretablity and realness. In addition, it is shown\nthat the proposed Diffusion-TS can be easily extended to conditional generation\ntasks, such as forecasting and imputation, without any model changes. This also\nmotivates us to further explore the performance of Diffusion-TS under irregular\nsettings. Finally, through qualitative and quantitative experiments, results\nshow that Diffusion-TS achieves the state-of-the-art results on various\nrealistic analyses of time series.\n","authors":["Xinyu Yuan","Yan Qiao"],"pdf_url":"https://arxiv.org/pdf/2403.01742v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02355v2","updated":"2024-10-21T04:32:56Z","published":"2024-10-03T10:06:27Z","title":"AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models","summary":"  Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.\n","authors":["Junfeng Fang","Houcheng Jiang","Kun Wang","Yunshan Ma","Xiang Wang","Xiangnan He","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.02355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15633v1","updated":"2024-10-21T04:30:53Z","published":"2024-10-21T04:30:53Z","title":"Selecting Influential Samples for Long Context Alignment via Homologous\n  Models' Guidance and Contextual Awareness Measurement","summary":"  The expansion of large language models to effectively handle instructions\nwith extremely long contexts has yet to be fully investigated. The primary\nobstacle lies in constructing a high-quality long instruction-following dataset\ndevised for long context alignment. Existing studies have attempted to scale up\nthe available data volume by synthesizing long instruction-following samples.\nHowever, indiscriminately increasing the quantity of data without a\nwell-defined strategy for ensuring data quality may introduce low-quality\nsamples and restrict the final performance. To bridge this gap, we aim to\naddress the unique challenge of long-context alignment, i.e., modeling the\nlong-range dependencies for handling instructions and lengthy input contexts.\nWe propose GATEAU, a novel framework designed to identify the influential and\nhigh-quality samples enriched with long-range dependency relations by utilizing\ncrafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement\n(CAM). Specifically, HMG attempts to measure the difficulty of generating\ncorresponding responses due to the long-range dependencies, using the\nperplexity scores of the response from two homologous models with different\ncontext windows. Also, the role of CAM is to measure the difficulty of\nunderstanding the long input contexts due to long-range dependencies by\nevaluating whether the model's attention is focused on important segments.\nBuilt upon both proposed methods, we select the most challenging samples as the\ninfluential data to effectively frame the long-range dependencies, thereby\nachieving better performance of LLMs. Comprehensive experiments indicate that\nGATEAU effectively identifies samples enriched with long-range dependency\nrelations and the model trained on these selected samples exhibits better\ninstruction-following and long-context understanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15628v1","updated":"2024-10-21T04:24:10Z","published":"2024-10-21T04:24:10Z","title":"Towards Kriging-informed Conditional Diffusion for Regional Sea-Level\n  Data Downscaling","summary":"  Given coarser-resolution projections from global climate models or satellite\ndata, the downscaling problem aims to estimate finer-resolution regional\nclimate data, capturing fine-scale spatial patterns and variability.\nDownscaling is any method to derive high-resolution data from low-resolution\nvariables, often to provide more detailed and local predictions and analyses.\nThis problem is societally crucial for effective adaptation, mitigation, and\nresilience against significant risks from climate change. The challenge arises\nfrom spatial heterogeneity and the need to recover finer-scale features while\nensuring model generalization. Most downscaling methods \\cite{Li2020} fail to\ncapture the spatial dependencies at finer scales and underperform on real-world\nclimate datasets, such as sea-level rise. We propose a novel Kriging-informed\nConditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial\nvariability while preserving fine-scale features. Experimental results on\nclimate data show that our proposed method is more accurate than\nstate-of-the-art downscaling techniques.\n","authors":["Subhankar Ghosh","Arun Sharma","Jayant Gupta","Aneesh Subramanian","Shashi Shekhar"],"pdf_url":"https://arxiv.org/pdf/2410.15628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10786v2","updated":"2024-10-21T04:09:33Z","published":"2024-06-16T02:52:32Z","title":"Exploring the Zero-Shot Capabilities of LLMs Handling Multiple Problems\n  at once","summary":"  Recent studies have proposed placing multiple problems in a single prompt to\nimprove input token utilization for a more efficient LLM inference. We call\nthis MPP, in contrast to conventional SPP that prompts an LLM with a single\nproblem at a time. While MPP has been shown to work comparably well or even\nbetter than SPP under few-shot settings, its zero-shot performance is\nunderexplored, which better reveals the innate multiple problem handling\ncapabilities of LLMs. To address that, we study the zero-shot MPP performance\nof various LLMs on 6 classification and 12 reasoning benchmarks and confirm\nthat LLMs are competent zero-shot multi-problem solvers. We also examine the\nconditions of effectiveness of zero-shot MPP and explore several model-level\nfactors that may enable MPP. We observe that LLMs consistently perform worse\nwith selecting indices of texts of a given class label and with multiple\nmixed-source reasoning problems, indicating a lack of true understanding. We\nalso find that instruction tuning is an important factor than enhances MPP.\n","authors":["Zhengxiang Wang","Jordan Kodner","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2406.10786v2.pdf","comment":"26 pages, 11 figures, 16 tables"},{"id":"http://arxiv.org/abs/2410.15625v1","updated":"2024-10-21T04:08:37Z","published":"2024-10-21T04:08:37Z","title":"Improving Parallel Program Performance Through DSL-Driven Code\n  Generation with LLM Optimizers","summary":"  Mapping computations to processors and assigning data to memory are critical\nfor maximizing performance in parallel programming. These mapping decisions are\nmanaged through the development of specialized low-level system code, called\nmappers, crafted by performance engineers. Each mapper is tailored to a\nspecific application and optimized for the underlying machine architecture, a\nprocess that requires days of refinement and tuning from an expert. Despite\nadvances in system research, automating mapper generation remains a challenge\ndue to the complexity of making millions of decisions to find the optimal\nsolution and generate the solution as code. We introduce an approach that\nleverages recent advances in LLM-based optimizers for mapper design. In under\nten minutes, our method automatically discovers mappers that surpass human\nexpert designs in scientific applications by up to 1.34X speedup. For parallel\nmatrix multiplication algorithms, our mapper achieves up to 1.31X of the\nexpert-designed solution. To achieve this, we simplify the complexity of\nlow-level code generation by introducing a domain-specific language (DSL) that\nabstracts the low-level system programming details and defines a structured\nsearch space for LLMs to explore. To maximize the application performance, we\nuse an LLM optimizer to improve an agentic system that generates the mapper\ncode. As a result, this approach significantly reduces the workload for\nperformance engineers while achieving substantial performance gains across\ndiverse applications. Finally, our results demonstrate the effectiveness of\nLLM-based optimization in system design and suggest its potential for\naddressing other complex system challenges.\n","authors":["Anjiang Wei","Allen Nie","Thiago S. F. X. Teixeira","Rohan Yadav","Wonchan Lee","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2410.15625v1.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2402.18012v3","updated":"2024-10-21T04:06:02Z","published":"2024-02-28T03:09:12Z","title":"Diffusion Models as Constrained Samplers for Optimization with Unknown\n  Constraints","summary":"  Addressing real-world optimization problems becomes particularly challenging\nwhen analytic objective functions or constraints are unavailable. While\nnumerous studies have addressed the issue of unknown objectives, limited\nresearch has focused on scenarios where feasibility constraints are not given\nexplicitly. Overlooking these constraints can lead to spurious solutions that\nare unrealistic in practice. To deal with such unknown constraints, we propose\nto perform optimization within the data manifold using diffusion models. To\nconstrain the optimization process to the data manifold, we reformulate the\noriginal optimization problem as a sampling problem from the product of the\nBoltzmann distribution defined by the objective function and the data\ndistribution learned by the diffusion model. Depending on the differentiability\nof the objective function, we propose two different sampling methods. For\ndifferentiable objectives, we propose a two-stage framework that begins with a\nguided diffusion process for warm-up, followed by a Langevin dynamics stage for\nfurther correction. For non-differentiable objectives, we propose an iterative\nimportance sampling strategy using the diffusion model as the proposal\ndistribution. Comprehensive experiments on a synthetic dataset, six real-world\nblack-box optimization datasets, and a multi-objective molecule optimization\ndataset show that our method achieves better or comparable performance with\nprevious state-of-the-art baselines.\n","authors":["Lingkai Kong","Yuanqi Du","Wenhao Mu","Kirill Neklyudov","Valentin De Bortoli","Dongxia Wu","Haorui Wang","Aaron Ferber","Yi-An Ma","Carla P. Gomes","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.18012v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10038v2","updated":"2024-10-21T04:05:25Z","published":"2023-04-20T01:32:32Z","title":"Open-World Continual Learning: Unifying Novelty Detection and Continual\n  Learning","summary":"  As AI agents are increasingly used in the real open world with unknowns or\nnovelties, they need the ability to (1) recognize objects that (a) they have\nlearned before and (b) detect items that they have never seen or learned, and\n(2) learn the new items incrementally to become more and more knowledgeable and\npowerful. (1) is called novelty detection or out-of-distribution (OOD)\ndetection and (2) is called class incremental learning (CIL), which is a\nsetting of continual learning (CL). In existing research, OOD detection and CIL\nare regarded as two completely different problems. This paper first provides a\ntheoretical proof that good OOD detection for each task within the set of\nlearned tasks (called closed-world OOD detection) is necessary for successful\nCIL. We show this by decomposing CIL into two sub-problems: within-task\nprediction (WP) and task-id prediction (TP), and proving that TP is correlated\nwith closed-world OOD detection. The key theoretical result is that regardless\nof whether WP and OOD detection (or TP) are defined explicitly or implicitly by\na CIL algorithm, good WP and good closed-world OOD detection are necessary and\nsufficient conditions for good CIL, which unifies novelty or OOD detection and\ncontinual learning (CIL, in particular). We call this traditional CIL the\nclosed-world CIL as it does not detect future OOD data in the open world. The\npaper then proves that the theory can be generalized or extended to open-world\nCIL, which is the proposed open-world continual learning, that can perform CIL\nin the open world and detect future or open-world OOD data. Based on the\ntheoretical results, new CIL methods are also designed, which outperform strong\nbaselines in CIL accuracy and in continual OOD detection by a large margin.\n","authors":["Gyuhak Kim","Changnan Xiao","Tatsuya Konishi","Zixuan Ke","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2304.10038v2.pdf","comment":"To appear in Artificial Intelligence Journal. arXiv admin note:\n  substantial text overlap with arXiv:2211.02633"},{"id":"http://arxiv.org/abs/2409.08098v2","updated":"2024-10-21T04:02:23Z","published":"2024-09-12T14:51:43Z","title":"The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal","summary":"  This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.\n","authors":["Huiyuan Xie","Felix Steffek","Joana Ribeiro de Faria","Christine Carter","Jonathan Rutherford"],"pdf_url":"https://arxiv.org/pdf/2409.08098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10160v2","updated":"2024-10-21T03:49:58Z","published":"2024-05-16T14:53:45Z","title":"PIR: Remote Sensing Image-Text Retrieval with Prior Instruction\n  Representation Learning","summary":"  Remote sensing image-text retrieval constitutes a foundational aspect of\nremote sensing interpretation tasks, facilitating the alignment of vision and\nlanguage representations. This paper introduces a prior instruction\nrepresentation (PIR) learning paradigm that draws on prior knowledge to\ninstruct adaptive learning of vision and text representations. Based on PIR, a\ndomain-adapted remote sensing image-text retrieval framework PIR-ITR is\ndesigned to address semantic noise issues in vision-language understanding\ntasks. However, with massive additional data for pre-training the\nvision-language foundation model, remote sensing image-text retrieval is\nfurther developed into an open-domain retrieval task. Continuing with the\nabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote\nsensing image-text retrieval, to address semantic noise in remote sensing\nvision-language representations and further improve open-domain retrieval\nperformance. In vision representation, we utilize the prior-guided knowledge of\nthe remote sensing scene recognition by building a belief matrix to select key\nfeatures for reducing the impact of semantic noise. In text representation, we\nuse the previous time step to cyclically activate the current time step to\nenhance text representation capability. A cluster-wise Affiliation Loss (AL) is\nproposed to constrain the inter-classes and to reduce the semantic confusion\nzones in the common subspace. Comprehensive experiments demonstrate that PIR\ncould enhance vision and text representations and outperform the\nstate-of-the-art methods of closed-domain and open-domain retrieval on two\nbenchmark datasets, RSICD and RSITMD.\n","authors":["Jiancheng Pan","Muyuan Ma","Qing Ma","Cong Bai","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.10160v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.13185v2","updated":"2024-10-21T03:36:05Z","published":"2024-10-17T03:26:37Z","title":"Chain of Ideas: Revolutionizing Research in Novel Idea Development with\n  LLM Agents","summary":"  Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.\n","authors":["Long Li","Weiwen Xu","Jiayan Guo","Ruochen Zhao","Xinxuan Li","Yuqian Yuan","Boqiang Zhang","Yuming Jiang","Yifei Xin","Ronghao Dang","Deli Zhao","Yu Rong","Tian Feng","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.13185v2.pdf","comment":"10 pages,5 figures, conference"},{"id":"http://arxiv.org/abs/2410.15616v1","updated":"2024-10-21T03:35:23Z","published":"2024-10-21T03:35:23Z","title":"Weighted Diversified Sampling for Efficient Data-Driven Single-Cell\n  Gene-Gene Interaction Discovery","summary":"  Gene-gene interactions play a crucial role in the manifestation of complex\nhuman diseases. Uncovering significant gene-gene interactions is a challenging\ntask. Here, we present an innovative approach utilizing data-driven\ncomputational tools, leveraging an advanced Transformer model, to unearth\nnoteworthy gene-gene interactions. Despite the efficacy of Transformer models,\ntheir parameter intensity presents a bottleneck in data ingestion, hindering\ndata efficiency. To mitigate this, we introduce a novel weighted diversified\nsampling algorithm. This algorithm computes the diversity score of each data\nsample in just two passes of the dataset, facilitating efficient subset\ngeneration for interaction discovery. Our extensive experimentation\ndemonstrates that by sampling a mere 1\\% of the single-cell dataset, we achieve\nperformance comparable to that of utilizing the entire dataset.\n","authors":["Yifan Wu","Yuntao Yang","Zirui Liu","Zhao Li","Khushbu Pahwa","Rongbin Li","Wenjin Zheng","Xia Hu","Zhaozhuo Xu"],"pdf_url":"https://arxiv.org/pdf/2410.15616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15607v1","updated":"2024-10-21T03:04:29Z","published":"2024-10-21T03:04:29Z","title":"Reinforced Imitative Trajectory Planning for Urban Automated Driving","summary":"  Reinforcement learning (RL) faces challenges in trajectory planning for urban\nautomated driving due to the poor convergence of RL and the difficulty in\ndesigning reward functions. The convergence problem is alleviated by combining\nRL with supervised learning. However, most existing approaches only reason one\nstep ahead and lack the capability to plan for multiple future steps. Besides,\nalthough inverse reinforcement learning holds promise for solving the reward\nfunction design issue, existing methods for automated driving impose a linear\nstructure assumption on reward functions, making them difficult to apply to\nurban automated driving. In light of these challenges, this paper proposes a\nnovel RL-based trajectory planning method that integrates RL with imitation\nlearning to enable multi-step planning. Furthermore, a transformer-based\nBayesian reward function is developed, providing effective reward signals for\nRL in urban scenarios. Moreover, a hybrid-driven trajectory planning framework\nis proposed to enhance safety and interpretability. The proposed methods were\nvalidated on the large-scale real-world urban automated driving nuPlan dataset.\nThe results demonstrated the significant superiority of the proposed methods\nover the baselines in terms of the closed-loop metrics. The code is available\nat https://github.com/Zigned/nuplan_zigned.\n","authors":["Di Zeng","Ling Zheng","Xiantong Yang","Yinong Li"],"pdf_url":"https://arxiv.org/pdf/2410.15607v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.15605v1","updated":"2024-10-21T03:04:09Z","published":"2024-10-21T03:04:09Z","title":"Deep Active Learning with Manifold-preserving Trajectory Sampling","summary":"  Active learning (AL) is for optimizing the selection of unlabeled data for\nannotation (labeling), aiming to enhance model performance while minimizing\nlabeling effort. The key question in AL is which unlabeled data should be\nselected for annotation. Existing deep AL methods arguably suffer from bias\nincurred by clabeled data, which takes a much lower percentage than unlabeled\ndata in AL context. We observe that such an issue is severe in different types\nof data, such as vision and non-vision data. To address this issue, we propose\na novel method, namely Manifold-Preserving Trajectory Sampling (MPTS), aiming\nto enforce the feature space learned from labeled data to represent a more\naccurate manifold. By doing so, we expect to effectively correct the bias\nincurred by labeled data, which can cause a biased selection of unlabeled data.\nDespite its focus on manifold, the proposed method can be conveniently\nimplemented by performing distribution mapping with MMD (Maximum Mean\nDiscrepancies). Extensive experiments on various vision and non-vision\nbenchmark datasets demonstrate the superiority of our method. Our source code\ncan be found here.\n","authors":["Yingrui Ji","Vijaya Sindhoori Kaza","Nishanth Artham","Tianyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15602v1","updated":"2024-10-21T02:56:44Z","published":"2024-10-21T02:56:44Z","title":"P-YOLOv8: Efficient and Accurate Real-Time Detection of Distracted\n  Driving","summary":"  Distracted driving is a critical safety issue that leads to numerous\nfatalities and injuries worldwide. This study addresses the urgent need for\nefficient and real-time machine learning models to detect distracted driving\nbehaviors. Leveraging the Pretrained YOLOv8 (P-YOLOv8) model, a real-time\nobject detection system is introduced, optimized for both speed and accuracy.\nThis approach addresses the computational constraints and latency limitations\ncommonly associated with conventional detection models. The study demonstrates\nP-YOLOv8 versatility in both object detection and image classification tasks\nusing the Distracted Driver Detection dataset from State Farm, which includes\n22,424 images across ten behavior categories. Our research explores the\napplication of P-YOLOv8 for image classification, evaluating its performance\ncompared to deep learning models such as VGG16, VGG19, and ResNet. Some\ntraditional models often struggle with low accuracy, while others achieve high\naccuracy but come with high computational costs and slow detection speeds,\nmaking them unsuitable for real-time applications. P-YOLOv8 addresses these\nissues by achieving competitive accuracy with significant computational cost\nand efficiency advantages. In particular, P-YOLOv8 generates a lightweight\nmodel with a size of only 2.84 MB and a lower number of parameters, totaling\n1,451,098, due to its innovative architecture. It achieves a high accuracy of\n99.46 percent with this small model size, opening new directions for deployment\non inexpensive and small embedded devices using Tiny Machine Learning (TinyML).\nThe experimental results show robust performance, making P-YOLOv8 a\ncost-effective solution for real-time deployment. This study provides a\ndetailed analysis of P-YOLOv8's architecture, training, and performance\nbenchmarks, highlighting its potential for real-time use in detecting\ndistracted driving.\n","authors":["Mohamed R. Elshamy","Heba M. Emara","Mohamed R. Shoaib","Abdel-Hameed A. Badawy"],"pdf_url":"https://arxiv.org/pdf/2410.15602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15600v1","updated":"2024-10-21T02:53:18Z","published":"2024-10-21T02:53:18Z","title":"Patrol Security Game: Defending Against Adversary with Freedom in Attack\n  Timing, Location, and Duration","summary":"  We explored the Patrol Security Game (PSG), a robotic patrolling problem\nmodeled as an extensive-form Stackelberg game, where the attacker determines\nthe timing, location, and duration of their attack. Our objective is to devise\na patrolling schedule with an infinite time horizon that minimizes the\nattacker's payoff. We demonstrated that PSG can be transformed into a\ncombinatorial minimax problem with a closed-form objective function. By\nconstraining the defender's strategy to a time-homogeneous first-order Markov\nchain (i.e., the patroller's next move depends solely on their current\nlocation), we proved that the optimal solution in cases of zero penalty\ninvolves either minimizing the expected hitting time or return time, depending\non the attacker model, and that these solutions can be computed efficiently.\nAdditionally, we observed that increasing the randomness in the patrol schedule\nreduces the attacker's expected payoff in high-penalty cases. However, the\nminimax problem becomes non-convex in other scenarios. To address this, we\nformulated a bi-criteria optimization problem incorporating two objectives:\nexpected maximum reward and entropy. We proposed three graph-based algorithms\nand one deep reinforcement learning model, designed to efficiently balance the\ntrade-off between these two objectives. Notably, the third algorithm can\nidentify the optimal deterministic patrol schedule, though its runtime grows\nexponentially with the number of patrol spots. Experimental results validate\nthe effectiveness and scalability of our solutions, demonstrating that our\napproaches outperform state-of-the-art baselines on both synthetic and\nreal-world crime datasets.\n","authors":["Hao-Tsung Yang","Ting-Kai Weng","Ting-Yu Chang","Kin Sum Liu","Shan Lin","Jie Gao","Shih-Yu Tsai"],"pdf_url":"https://arxiv.org/pdf/2410.15600v1.pdf","comment":"Under review of TCPS"},{"id":"http://arxiv.org/abs/2410.15597v1","updated":"2024-10-21T02:44:58Z","published":"2024-10-21T02:44:58Z","title":"A Comprehensive Comparative Study of Individual ML Models and Ensemble\n  Strategies for Network Intrusion Detection Systems","summary":"  The escalating frequency of intrusions in networked systems has spurred the\nexploration of new research avenues in devising artificial intelligence (AI)\ntechniques for intrusion detection systems (IDS). Various AI techniques have\nbeen used to automate network intrusion detection tasks, yet each model\npossesses distinct strengths and weaknesses. Selecting the optimal model for a\ngiven dataset can pose a challenge, necessitating the exploration of ensemble\nmethods to enhance generalization and applicability in network intrusion\ndetection. This paper addresses this gap by conducting a comprehensive\nevaluation of diverse individual models and both simple and advanced ensemble\nmethods for network IDS. We introduce an ensemble learning framework tailored\nfor assessing individual models and ensemble methods in network intrusion\ndetection tasks. Our framework encompasses the loading of input datasets,\ntraining of individual models and ensemble methods, and the generation of\nevaluation metrics. Furthermore, we incorporate all features across individual\nmodels and ensemble techniques. The study presents results for our framework,\nencompassing 14 methods, including various bagging, stacking, blending, and\nboosting techniques applied to multiple base learners such as decision trees,\nneural networks, and among others. We evaluate the framework using two distinct\nnetwork intrusion datasets, RoEduNet-SIMARGL2021 and CICIDS-2017, each\npossessing unique characteristics. Additionally, we categorize AI models based\non their performances on our evaluation metrics and via their confusion\nmatrices. Our assessment demonstrates the efficacy of learning across most\nsetups explored in this study. Furthermore, we contribute to the community by\nreleasing our source codes, providing a foundational ensemble learning\nframework for network intrusion detection.\n","authors":["Ismail Bibers","Osvaldo Arreche","Mustafa Abdallah"],"pdf_url":"https://arxiv.org/pdf/2410.15597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03421v2","updated":"2024-10-21T02:43:50Z","published":"2024-10-04T13:31:09Z","title":"One2set + Large Language Model: Best Partners for Keyphrase Generation","summary":"  Keyphrase generation (KPG) aims to automatically generate a collection of\nphrases representing the core concepts of a given document. The dominant\nparadigms in KPG include one2seq and one2set. Recently, there has been\nincreasing interest in applying large language models (LLMs) to KPG. Our\npreliminary experiments reveal that it is challenging for a single model to\nexcel in both recall and precision. Further analysis shows that: 1) the one2set\nparadigm owns the advantage of high recall, but suffers from improper\nassignments of supervision signals during training; 2) LLMs are powerful in\nkeyphrase selection, but existing selection methods often make redundant\nselections. Given these observations, we introduce a generate-then-select\nframework decomposing KPG into two steps, where we adopt a one2set-based model\nas generator to produce candidates and then use an LLM as selector to select\nkeyphrases from these candidates. Particularly, we make two important\nimprovements on our generator and selector: 1) we design an Optimal\nTransport-based assignment strategy to address the above improper assignments;\n2) we model the keyphrase selection as a sequence labeling task to alleviate\nredundant selections. Experimental results on multiple benchmark datasets show\nthat our framework significantly surpasses state-of-the-art models, especially\nin absent keyphrase prediction.\n","authors":["Liangying Shao","Liang Zhang","Minlong Peng","Guoqi Ma","Hao Yue","Mingming Sun","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2410.03421v2.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.19195v2","updated":"2024-10-21T02:30:50Z","published":"2024-02-29T14:26:20Z","title":"Negative Sampling in Knowledge Graph Representation Learning: A Review","summary":"  Knowledge Graph Representation Learning (KGRL), or Knowledge Graph Embedding\n(KGE), is essential for AI applications such as knowledge construction and\ninformation retrieval. These models encode entities and relations into\nlower-dimensional vectors, supporting tasks like link prediction and\nrecommendation systems. Training KGE models relies on both positive and\nnegative samples for effective learning, but generating high-quality negative\nsamples from existing knowledge graphs is challenging. The quality of these\nsamples significantly impacts the model's accuracy. This comprehensive survey\npaper systematically reviews various negative sampling (NS) methods and their\ncontributions to the success of KGRL. Their respective advantages and\ndisadvantages are outlined by categorizing existing NS methods into six\ndistinct categories. Moreover, this survey identifies open research questions\nthat serve as potential directions for future investigations. By offering a\ngeneralization and alignment of fundamental NS concepts, this survey provides\nvaluable insights for designing effective NS methods in the context of KGRL and\nserves as a motivating force for further advancements in the field.\n","authors":["Tiroshan Madushanka","Ryutaro Ichise"],"pdf_url":"https://arxiv.org/pdf/2402.19195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15595v1","updated":"2024-10-21T02:27:24Z","published":"2024-10-21T02:27:24Z","title":"A Comprehensive Survey of Datasets, Theories, Variants, and Applications\n  in Direct Preference Optimization","summary":"  With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community.\n","authors":["Wenyi Xiao","Zechuan Wang","Leilei Gan","Shuai Zhao","Wanggui He","Luu Anh Tuan","Long Chen","Hao Jiang","Zhou Zhao","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15591v1","updated":"2024-10-21T02:19:24Z","published":"2024-10-21T02:19:24Z","title":"AMPLE: Emotion-Aware Multimodal Fusion Prompt Learning for Fake News\n  Detection","summary":"  Detecting fake news in large datasets is challenging due to its diversity and\ncomplexity, with traditional approaches often focusing on textual features\nwhile underutilizing semantic and emotional elements. Current methods also rely\nheavily on large annotated datasets, limiting their effectiveness in more\nnuanced analysis. To address these challenges, this paper introduces\nEmotion-\\textbf{A}ware \\textbf{M}ultimodal Fusion \\textbf{P}rompt\n\\textbf{L}\\textbf{E}arning (\\textbf{AMPLE}) framework to address the above\nissue by combining text sentiment analysis with multimodal data and hybrid\nprompt templates. This framework extracts emotional elements from texts by\nleveraging sentiment analysis tools. It then employs Multi-Head Cross-Attention\n(MCA) mechanisms and similarity-aware fusion methods to integrate multimodal\ndata. The proposed AMPLE framework demonstrates strong performance on two\npublic datasets in both few-shot and data-rich settings, with results\nindicating the potential of emotional aspects in fake news detection.\nFurthermore, the study explores the impact of integrating large language models\nwith this method for text sentiment extraction, revealing substantial room for\nfurther improvement. The code can be found at\n:\\url{https://github.com/xxm1215/MMM2025_few-shot/\n","authors":["Xiaoman Xu","Xiangrun Li","Taihang Wang","Ye Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.15591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11647v3","updated":"2024-10-21T02:11:09Z","published":"2024-01-22T01:57:31Z","title":"LW-FedSSL: Resource-efficient Layer-wise Federated Self-supervised\n  Learning","summary":"  Many studies integrate federated learning (FL) with self-supervised learning\n(SSL) to take advantage of raw data distributed across edge devices. However,\nedge devices often struggle with high computation and communication costs\nimposed by SSL and FL algorithms. To tackle this hindrance, we propose\nLW-FedSSL, a layer-wise federated self-supervised learning approach that allows\nedge devices to incrementally train a single layer of the model at a time. We\nintroduce server-side calibration and representation alignment mechanisms to\nensure LW-FedSSL delivers performance on par with conventional federated\nself-supervised learning (FedSSL) while significantly lowering resource\ndemands. In a pure layer-wise training scheme, training one layer at a time may\nlimit effective interaction between different layers of the model. The\nserver-side calibration mechanism takes advantage of the resource-rich FL\nserver to ensure smooth collaboration between different layers of the global\nmodel. During local training, the representation alignment mechanism encourages\ncloseness between representations of local models and those of the global\nmodel, thereby preserving the layer cohesion established by server-side\ncalibration. With the proposed mechanisms, LW-FedSSL achieves a $3.3 \\times$\nreduction in memory usage, $2.1 \\times$ fewer computational operations (FLOPs),\nand a $3.2 \\times$ lower communication cost while maintaining the same level of\nperformance as its end-to-end training counterpart. Additionally, we explore a\nprogressive training strategy called Prog-FedSSL, which matches end-to-end\ntraining in memory requirements but offers a $1.8 \\times$ reduction in FLOPs\nand communication costs. Although Prog-FedSSL is not as resource-efficient as\nLW-FedSSL, its performance improvements make it a suitable candidate for FL\nenvironments with more lenient resource constraints.\n","authors":["Ye Lin Tun","Chu Myaet Thwal","Le Quang Huy","Minh N. H. Nguyen","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2401.11647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15573v1","updated":"2024-10-21T01:36:42Z","published":"2024-10-21T01:36:42Z","title":"OpenMU: Your Swiss Army Knife for Music Understanding","summary":"  We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.\n","authors":["Mengjie Zhao","Zhi Zhong","Zhuoyuan Mao","Shiqi Yang","Wei-Hsiang Liao","Shusuke Takahashi","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.15573v1.pdf","comment":"Resources: https://github.com/mzhaojp22/openmu"},{"id":"http://arxiv.org/abs/2410.15572v1","updated":"2024-10-21T01:36:08Z","published":"2024-10-21T01:36:08Z","title":"Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka\n  Chatbots: Design Insights and User Perceptions","summary":"  In an era where cultural preservation is increasingly intertwined with\ntechnological innovation, this study introduces a groundbreaking approach to\npromoting and safeguarding the rich heritage of Taiwanese Hakka culture through\nthe development of a Retrieval-Augmented Generation (RAG)-enhanced chatbot.\nTraditional large language models (LLMs), while powerful, often fall short in\ndelivering accurate and contextually rich responses, particularly in culturally\nspecific domains. By integrating external databases with generative AI models,\nRAG technology bridges this gap, empowering chatbots to not only provide\nprecise answers but also resonate deeply with the cultural nuances that are\ncrucial for authentic interactions. This study delves into the intricate\nprocess of augmenting the chatbot's knowledge base with targeted cultural data,\nspecifically curated to reflect the unique aspects of Hakka traditions,\nlanguage, and practices. Through dynamic information retrieval, the\nRAG-enhanced chatbot becomes a versatile tool capable of handling complex\ninquiries that demand an in-depth understanding of Hakka cultural context. This\nis particularly significant in an age where digital platforms often dilute\ncultural identities, making the role of culturally aware AI systems more\ncritical than ever. System usability studies conducted as part of our research\nreveal a marked improvement in both user satisfaction and engagement,\nhighlighting the chatbot's effectiveness in fostering a deeper connection with\nHakka culture. The feedback underscores the potential of RAG technology to not\nonly enhance user experience but also to serve as a vital instrument in the\nbroader mission of ethnic mainstreaming and cultural celebration.\n","authors":["Chen-Chi Chang","Han-Pi Chang","Hung-Shin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.15572v1.pdf","comment":"Accepted to IEEE RASSE 2024"},{"id":"http://arxiv.org/abs/2410.15570v1","updated":"2024-10-21T01:27:29Z","published":"2024-10-21T01:27:29Z","title":"Stacking Small Language Models for Generalizability","summary":"  Recent advances show that large language models (LLMs) generalize strong\nperformance across different natural language benchmarks. However, the large\nsize of LLMs makes training and inference expensive and impractical to run in\nresource-limited settings. This paper introduces a new approach called\nfine-tuning stacks of language models (FSLM), which involves stacking small\nlanguage models (SLM) as an alternative to LLMs. By fine-tuning each SLM to\nperform a specific task, this approach breaks down high level reasoning into\nmultiple lower-level steps that specific SLMs are responsible for. As a result,\nFSLM allows for lower training and inference costs, and also improves model\ninterpretability as each SLM communicates with the subsequent one through\nnatural language. By evaluating FSLM on common natural language benchmarks,\nthis paper highlights promising early results toward generalizable performance\nusing FSLM as a cost-effective alternative to LLMs.\n","authors":["Laurence Liang"],"pdf_url":"https://arxiv.org/pdf/2410.15570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15567v1","updated":"2024-10-21T01:23:34Z","published":"2024-10-21T01:23:34Z","title":"Pruning Foundation Models for High Accuracy without Retraining","summary":"  Despite the superior performance, it is challenging to deploy foundation\nmodels or large language models (LLMs) due to their massive parameters and\ncomputations. While pruning is a promising technique to reduce model size and\naccelerate the inference, the traditional pruning techniques can hardly be\napplied for LLMs as they need to finetune the model on the full dataset with\nmultiple epochs consuming massive data and hardware resources. To deal with\nthis problem, post-training pruning methods are proposed to prune LLMs in\none-shot without retraining. However, their accuracy after pruning may suffer\nfrom certain performance degradation due to the lack of retraining with massive\ndata. To address this issue, in this paper, we first formulate the\npost-training problem for layer-wise LLM compression to simultaneously prune\nmultiple weights in LLMs. Next, we provide an optimal solution for this problem\nand design our post-training pruning algorithm for both unstructured and\nsemi-structured sparsity. Our extensive experiments demonstrate the superior\nperformance of the proposed methods in comparison to SOTA baselines across\nvarious LLM families including transformer-based LLMs and Mamba-based LLMs.\nCode link: https://github.com/piuzha/APT\n","authors":["Pu Zhao","Fei Sun","Xuan Shen","Pinrui Yu","Zhenglun Kong","Yanzhi Wang","Xue Lin"],"pdf_url":"https://arxiv.org/pdf/2410.15567v1.pdf","comment":"Accepted by EMNLP 2024 findings"},{"id":"http://arxiv.org/abs/2410.15555v1","updated":"2024-10-21T01:00:33Z","published":"2024-10-21T01:00:33Z","title":"Bayesian Concept Bottleneck Models with LLM Priors","summary":"  Concept Bottleneck Models (CBMs) have been proposed as a compromise between\nwhite-box and black-box models, aiming to achieve interpretability without\nsacrificing accuracy. The standard training procedure for CBMs is to predefine\na candidate set of human-interpretable concepts, extract their values from the\ntraining data, and identify a sparse subset as inputs to a transparent\nprediction model. However, such approaches are often hampered by the tradeoff\nbetween enumerating a sufficiently large set of concepts to include those that\nare truly relevant versus controlling the cost of obtaining concept\nextractions. This work investigates a novel approach that sidesteps these\nchallenges: BC-LLM iteratively searches over a potentially infinite set of\nconcepts within a Bayesian framework, in which Large Language Models (LLMs)\nserve as both a concept extraction mechanism and prior. BC-LLM is broadly\napplicable and multi-modal. Despite imperfections in LLMs, we prove that BC-LLM\ncan provide rigorous statistical inference and uncertainty quantification. In\nexperiments, it outperforms comparator methods including black-box models,\nconverges more rapidly towards relevant concepts and away from spuriously\ncorrelated ones, and is more robust to out-of-distribution samples.\n","authors":["Jean Feng","Avni Kothari","Luke Zier","Chandan Singh","Yan Shuo Tan"],"pdf_url":"https://arxiv.org/pdf/2410.15555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15554v1","updated":"2024-10-21T00:59:50Z","published":"2024-10-21T00:59:50Z","title":"A Plug-and-Play Fully On-the-Job Real-Time Reinforcement Learning\n  Algorithm for a Direct-Drive Tandem-Wing Experiment Platforms Under Multiple\n  Random Operating Conditions","summary":"  The nonlinear and unstable aerodynamic interference generated by the tandem\nwings of such biomimetic systems poses substantial challenges for motion\ncontrol, especially under multiple random operating conditions. To address\nthese challenges, the Concerto Reinforcement Learning Extension (CRL2E)\nalgorithm has been developed. This plug-and-play, fully on-the-job, real-time\nreinforcement learning algorithm incorporates a novel Physics-Inspired\nRule-Based Policy Composer Strategy with a Perturbation Module alongside a\nlightweight network optimized for real-time control. To validate the\nperformance and the rationality of the module design, experiments were\nconducted under six challenging operating conditions, comparing seven different\nalgorithms. The results demonstrate that the CRL2E algorithm achieves safe and\nstable training within the first 500 steps, improving tracking accuracy by 14\nto 66 times compared to the Soft Actor-Critic, Proximal Policy Optimization,\nand Twin Delayed Deep Deterministic Policy Gradient algorithms. Additionally,\nCRL2E significantly enhances performance under various random operating\nconditions, with improvements in tracking accuracy ranging from 8.3% to 60.4%\ncompared to the Concerto Reinforcement Learning (CRL) algorithm. The\nconvergence speed of CRL2E is 36.11% to 57.64% faster than the CRL algorithm\nwith only the Composer Perturbation and 43.52% to 65.85% faster than the CRL\nalgorithm when both the Composer Perturbation and Time-Interleaved Capability\nPerturbation are introduced, especially in conditions where the standard CRL\nstruggles to converge. Hardware tests indicate that the optimized lightweight\nnetwork structure excels in weight loading and average inference time, meeting\nreal-time control requirements.\n","authors":["Zhang Minghao","Song Bifeng","Yang Xiaojun","Wang Liang"],"pdf_url":"https://arxiv.org/pdf/2410.15554v1.pdf","comment":"63 pages, 32 figures"},{"id":"http://arxiv.org/abs/2404.11049v3","updated":"2024-10-21T00:42:51Z","published":"2024-04-17T03:44:58Z","title":"Stepwise Alignment for Constrained Language Model Policy Optimization","summary":"  Safety and trustworthiness are indispensable requirements for real-world\napplications of AI systems using large language models (LLMs). This paper\nformulates human value alignment as an optimization problem of the language\nmodel policy to maximize reward under a safety constraint, and then proposes an\nalgorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One\nkey idea behind SACPO, supported by theory, is that the optimal policy\nincorporating reward and safety can be directly obtained from a reward-aligned\npolicy. Building on this key idea, SACPO aligns LLMs step-wise with each metric\nwhile leveraging simple yet powerful alignment algorithms such as direct\npreference optimization (DPO). SACPO offers several advantages, including\nsimplicity, stability, computational efficiency, and flexibility of algorithms\nand datasets. Under mild assumptions, our theoretical analysis provides the\nupper bounds on optimality and safety constraint violation. Our experimental\nresults show that SACPO can fine-tune Alpaca-7B better than the\nstate-of-the-art method in terms of both helpfulness and harmlessness.\n","authors":["Akifumi Wachi","Thien Q. Tran","Rei Sato","Takumi Tanabe","Youhei Akimoto"],"pdf_url":"https://arxiv.org/pdf/2404.11049v3.pdf","comment":"Accepted at NeurIPS 2024. Code and models are available at\n  https://github.com/line/sacpo"},{"id":"http://arxiv.org/abs/2410.13147v2","updated":"2024-10-21T00:38:07Z","published":"2024-10-17T02:04:57Z","title":"Utilizing Large Language Models in An Iterative Paradigm with Domain\n  Feedback for Molecule Optimization","summary":"  Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^2$DF. In detail, $\\text{Re}^2$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^2$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^2$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^2$DF enhances Hit\nratio by 6.04% and 5.25%.\n","authors":["Khiem Le","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2410.13147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00352v6","updated":"2024-10-21T17:22:45Z","published":"2023-08-01T07:49:10Z","title":"MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework","summary":"  Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT\n","authors":["Sirui Hong","Mingchen Zhuge","Jonathan Chen","Xiawu Zheng","Yuheng Cheng","Ceyao Zhang","Jinlin Wang","Zili Wang","Steven Ka Shing Yau","Zijuan Lin","Liyang Zhou","Chenyu Ran","Lingfeng Xiao","Chenglin Wu","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2308.00352v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07879v3","updated":"2024-10-21T16:48:18Z","published":"2023-11-14T03:18:28Z","title":"Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting\n  Volunteer Content Moderators","summary":"  Extensive efforts in automated approaches for content moderation have been\nfocused on developing models to identify toxic, offensive, and hateful content\nwith the aim of lightening the load for moderators. Yet, it remains uncertain\nwhether improvements on those tasks have truly addressed moderators' needs in\naccomplishing their work. In this paper, we surface gaps between past research\nefforts that have aimed to provide automation for aspects of content moderation\nand the needs of volunteer content moderators, regarding identifying violations\nof various moderation rules. To do so, we conduct a model review on Hugging\nFace to reveal the availability of models to cover various moderation rules and\nguidelines from three exemplar forums. We further put state-of-the-art LLMs to\nthe test, evaluating how well these models perform in flagging violations of\nplatform rules from one particular forum. Finally, we conduct a user survey\nstudy with volunteer moderators to gain insight into their perspectives on\nuseful moderation models. Overall, we observe a non-trivial gap, as missing\ndeveloped models and LLMs exhibit moderate to low performance on a significant\nportion of the rules. Moderators' reports provide guides for future work on\ndeveloping moderation assistant models.\n","authors":["Yang Trista Cao","Lovely-Frances Domingo","Sarah Ann Gilbert","Michelle Mazurek","Katie Shilton","Hal Daumé III"],"pdf_url":"https://arxiv.org/pdf/2311.07879v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05017v2","updated":"2024-10-21T16:30:03Z","published":"2023-11-08T21:03:43Z","title":"Joint Sensing and Semantic Communications with Multi-Task Deep Learning","summary":"  This paper explores the integration of deep learning techniques for joint\nsensing and communications, with an extension to semantic communications. The\nintegrated system comprises a transmitter and receiver operating over a\nwireless channel, subject to noise and fading. The transmitter employs a deep\nneural network (DNN), namely an encoder, for joint operations of source coding,\nchannel coding, and modulation, while the receiver utilizes another DNN, namely\na decoder, for joint operations of demodulation, channel decoding, and source\ndecoding to reconstruct the data samples. The transmitted signal serves a dual\npurpose, supporting communication with the receiver and enabling sensing. When\na target is present, the reflected signal is received, and another DNN decoder\nis utilized for sensing. This decoder is responsible for detecting the target's\npresence and determining its range. All these DNNs, including one encoder and\ntwo decoders, undergo joint training through multi-task learning, considering\ndata and channel characteristics. This paper extends to incorporate semantic\ncommunications by introducing an additional DNN, another decoder at the\nreceiver, operating as a task classifier. This decoder evaluates the fidelity\nof label classification for received signals, enhancing the integration of\nsemantics within the communication process. The study presents results based on\nusing the CIFAR-10 as the input data and accounting for channel effects like\nAdditive White Gaussian Noise (AWGN) and Rayleigh fading. The results\nunderscore the effectiveness of multi-task deep learning in achieving\nhigh-fidelity joint sensing and semantic communications.\n","authors":["Yalin E. Sagduyu","Tugba Erpek","Aylin Yener","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2311.05017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15859v1","updated":"2024-10-21T10:39:05Z","published":"2024-10-21T10:39:05Z","title":"Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs","summary":"  Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach.\n","authors":["Xin Ma","Yang Liu","Jingjing Liu","Xiaoxu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.15859v1.pdf","comment":"accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.09662v2","updated":"2024-10-21T23:58:45Z","published":"2024-06-14T02:21:53Z","title":"Learning Language Structures through Grounding","summary":"  Language is highly structured, with syntactic and semantic structures, to\nsome extent, agreed upon by speakers of the same language. With implicit or\nexplicit awareness of such structures, humans can learn and use language\nefficiently and generalize to sentences that contain unseen words. Motivated by\nhuman language learning, in this dissertation, we consider a family of machine\nlearning tasks that aim to learn language structures through grounding. We seek\ndistant supervision from other data sources (i.e., grounds), including but not\nlimited to other modalities (e.g., vision), execution results of programs, and\nother languages.\n  We demonstrate the potential of this task formulation and advocate for its\nadoption through three schemes. In Part I, we consider learning syntactic\nparses through visual grounding. We propose the task of visually grounded\ngrammar induction, present the first models to induce syntactic structures from\nvisually grounded text and speech, and find that the visual grounding signals\ncan help improve the parsing quality over language-only models. As a side\ncontribution, we propose a novel evaluation metric that enables the evaluation\nof speech parsing without text or automatic speech recognition systems\ninvolved. In Part II, we propose two execution-aware methods to map sentences\ninto corresponding semantic structures (i.e., programs), significantly\nimproving compositional generalization and few-shot program synthesis. In Part\nIII, we propose methods that learn language structures from annotations in\nother languages. Specifically, we propose a method that sets a new state of the\nart on cross-lingual word alignment. We then leverage the learned word\nalignments to improve the performance of zero-shot cross-lingual dependency\nparsing, by proposing a novel substructure-based projection method that\npreserves structural knowledge learned from the source language.\n","authors":["Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2406.09662v2.pdf","comment":"Ph.D. Thesis"},{"id":"http://arxiv.org/abs/2410.16579v1","updated":"2024-10-21T23:44:03Z","published":"2024-10-21T23:44:03Z","title":"Conflict-Aware Adversarial Training","summary":"  Adversarial training is the most effective method to obtain adversarial\nrobustness for deep neural networks by directly involving adversarial samples\nin the training procedure. To obtain an accurate and robust model, the\nweighted-average method is applied to optimize standard loss and adversarial\nloss simultaneously. In this paper, we argue that the weighted-average method\ndoes not provide the best tradeoff for the standard performance and adversarial\nrobustness. We argue that the failure of the weighted-average method is due to\nthe conflict between the gradients derived from standard and adversarial loss,\nand further demonstrate such a conflict increases with attack budget\ntheoretically and practically. To alleviate this problem, we propose a new\ntrade-off paradigm for adversarial training with a conflict-aware factor for\nthe convex combination of standard and adversarial loss, named\n\\textbf{Conflict-Aware Adversarial Training~(CA-AT)}. Comprehensive\nexperimental results show that CA-AT consistently offers a superior trade-off\nbetween standard performance and adversarial robustness under the settings of\nadversarial training from scratch and parameter-efficient finetuning.\n","authors":["Zhiyu Xue","Haohan Wang","Yao Qin","Ramtin Pedarsani"],"pdf_url":"https://arxiv.org/pdf/2410.16579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16574v1","updated":"2024-10-21T23:14:10Z","published":"2024-10-21T23:14:10Z","title":"How Can We Diagnose and Treat Bias in Large Language Models for Clinical\n  Decision-Making?","summary":"  Recent advancements in Large Language Models (LLMs) have positioned them as\npowerful tools for clinical decision-making, with rapidly expanding\napplications in healthcare. However, concerns about bias remain a significant\nchallenge in the clinical implementation of LLMs, particularly regarding gender\nand ethnicity. This research investigates the evaluation and mitigation of bias\nin LLMs applied to complex clinical cases, focusing on gender and ethnicity\nbiases. We introduce a novel Counterfactual Patient Variations (CPV) dataset\nderived from the JAMA Clinical Challenge. Using this dataset, we built a\nframework for bias evaluation, employing both Multiple Choice Questions (MCQs)\nand corresponding explanations. We explore prompting with eight LLMs and\nfine-tuning as debiasing methods. Our findings reveal that addressing social\nbiases in LLMs requires a multidimensional approach as mitigating gender bias\ncan occur while introducing ethnicity biases, and that gender bias in LLM\nembeddings varies significantly across medical specialities. We demonstrate\nthat evaluating both MCQ response and explanation processes is crucial, as\ncorrect responses can be based on biased \\textit{reasoning}. We provide a\nframework for evaluating LLM bias in real-world clinical cases, offer insights\ninto the complex nature of bias in these models, and present strategies for\nbias mitigation.\n","authors":["Kenza Benkirane","Jackie Kay","Maria Perez-Ortiz"],"pdf_url":"https://arxiv.org/pdf/2410.16574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16571v1","updated":"2024-10-21T23:07:48Z","published":"2024-10-21T23:07:48Z","title":"Implicit Contact Diffuser: Sequential Contact Reasoning with Latent\n  Point Cloud Diffusion","summary":"  Long-horizon contact-rich manipulation has long been a challenging problem,\nas it requires reasoning over both discrete contact modes and continuous object\nmotion. We introduce Implicit Contact Diffuser (ICD), a diffusion-based model\nthat generates a sequence of neural descriptors that specify a series of\ncontact relationships between the object and the environment. This sequence is\nthen used as guidance for an MPC method to accomplish a given task. The key\nadvantage of this approach is that the latent descriptors provide more\ntask-relevant guidance to MPC, helping to avoid local minima for contact-rich\nmanipulation tasks. Our experiments demonstrate that ICD outperforms baselines\non complex, long-horizon, contact-rich manipulation tasks, such as cable\nrouting and notebook folding. Additionally, our experiments also indicate that\n\\methodshort can generalize a target contact relationship to a different\nenvironment. More visualizations can be found on our website\n$\\href{https://implicit-contact-diffuser.github.io/}{https://implicit-contact-diffuser.github.io}$\n","authors":["Zixuan Huang","Yinong He","Yating Lin","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2410.16571v1.pdf","comment":"In submussion"},{"id":"http://arxiv.org/abs/2410.16560v1","updated":"2024-10-21T22:39:52Z","published":"2024-10-21T22:39:52Z","title":"Raising the Stakes: Performance Pressure Improves AI-Assisted Decision\n  Making","summary":"  AI systems are used in many domains to assist with decision making, and\nalthough the potential for AI systems to assist with decision making is much\ndiscussed, human-AI collaboration often underperforms. Investigation into why\nthe performance potential is not realized has revealed many factors, including\n(mis)trust in the AI system and mental models of AI capabilities on subjective\ntasks. Performance pressure is known to influence human decision making\nbehavior, yet how it interacts with human-AI decision making is understudied.\nIn this work, we show the effects of performance pressure on AI advice reliance\nwhen laypeople (Amazon Mechanical Turk crowdworkers) complete a common\nAI-assisted task (fake review detection) and thus have inherently low\nperformance pressure. We manipulate performance pressure by leveraging people's\nloss aversion towards potential monetary gains when completing a task. We find\nthat when the stakes are high, people use AI advice more appropriately than\nwhen stakes are lower, regardless of the presence of an AI explanation.\nFurthermore, when the AI system gives incorrect advice, people correctly\ndiscount the poor advice more often when the stakes are higher than when they\nare lower. We conclude by discussing the implications of how performance\npressure influences AI-assisted decision making and encourage future research\nto incorporate performance pressure analysis.\n","authors":["Nikita Haduong","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2410.16560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16547v1","updated":"2024-10-21T22:18:24Z","published":"2024-10-21T22:18:24Z","title":"PromptHive: Bringing Subject Matter Experts Back to the Forefront with\n  Collaborative Prompt Engineering for Educational Content Creation","summary":"  Involving subject matter experts in prompt engineering can guide LLM outputs\ntoward more helpful, accurate, and tailored content that meets the diverse\nneeds of different domains. However, iterating towards effective prompts can be\nchallenging without adequate interface support for systematic experimentation\nwithin specific task contexts. In this work, we introduce PromptHive, a\ncollaborative interface for prompt authoring, designed to better connect domain\nknowledge with prompt engineering through features that encourage rapid\niteration on prompt variations. We conducted an evaluation study with ten\nsubject matter experts in math and validated our design through two\ncollaborative prompt-writing sessions and a learning gain study with 358\nlearners. Our results elucidate the prompt iteration process and validate the\ntool's usability, enabling non-AI experts to craft prompts that generate\ncontent comparable to human-authored materials while reducing perceived\ncognitive load by half and shortening the authoring process from several months\nto just a few hours.\n","authors":["Mohi Reza","Ioannis Anastasopoulos","Shreya Bhandari","Zachary A. Pardos"],"pdf_url":"https://arxiv.org/pdf/2410.16547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16543v1","updated":"2024-10-21T22:12:00Z","published":"2024-10-21T22:12:00Z","title":"Large language models enabled multiagent ensemble method for efficient\n  EHR data labeling","summary":"  This study introduces a novel multiagent ensemble method powered by LLMs to\naddress a key challenge in ML - data labeling, particularly in large-scale EHR\ndatasets. Manual labeling of such datasets requires domain expertise and is\nlabor-intensive, time-consuming, expensive, and error-prone. To overcome this\nbottleneck, we developed an ensemble LLMs method and demonstrated its\neffectiveness in two real-world tasks: (1) labeling a large-scale unlabeled ECG\ndataset in MIMIC-IV; (2) identifying social determinants of health (SDOH) from\nthe clinical notes of EHR. Trading off benefits and cost, we selected a pool of\ndiverse open source LLMs with satisfactory performance. We treat each LLM's\nprediction as a vote and apply a mechanism of majority voting with minimal\nwinning threshold for ensemble. We implemented an ensemble LLMs application for\nEHR data labeling tasks. By using the ensemble LLMs and natural language\nprocessing, we labeled MIMIC-IV ECG dataset of 623,566 ECG reports with an\nestimated accuracy of 98.2%. We applied the ensemble LLMs method to identify\nSDOH from social history sections of 1,405 EHR clinical notes, also achieving\ncompetitive performance. Our experiments show that the ensemble LLMs can\noutperform individual LLM even the best commercial one, and the method reduces\nhallucination errors. From the research, we found that (1) the ensemble LLMs\nmethod significantly reduces the time and effort required for labeling\nlarge-scale EHR data, automating the process with high accuracy and quality;\n(2) the method generalizes well to other text data labeling tasks, as shown by\nits application to SDOH identification; (3) the ensemble of a group of diverse\nLLMs can outperform or match the performance of the best individual LLM; and\n(4) the ensemble method substantially reduces hallucination errors. This\napproach provides a scalable and efficient solution to data-labeling\nchallenges.\n","authors":["Jingwei Huang","Kuroush Nezafati","Ismael Villanueva-Miranda","Zifan Gu","Ann Marie Navar","Tingyi Wanyan","Qin Zhou","Bo Yao","Ruichen Rong","Xiaowei Zhan","Guanghua Xiao","Eric D. Peterson","Donghan M. Yang","Yang Xie"],"pdf_url":"https://arxiv.org/pdf/2410.16543v1.pdf","comment":"27 pages, 13 figures. Under journal review"},{"id":"http://arxiv.org/abs/2410.16540v1","updated":"2024-10-21T22:07:20Z","published":"2024-10-21T22:07:20Z","title":"A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and\n  Error-Aware Demonstration","summary":"  Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance\nin improving the reasoning capabilities of large language models (LLMs). While\ntheoretical investigations have been conducted to understand CoT, the\nunderlying transformer used in these studies isolates the CoT reasoning process\ninto separated in-context learning steps (Stepwise ICL). In this work, we\ntheoretically show that, compared to Stepwise ICL, the transformer gains better\nerror correction ability and more accurate predictions if the reasoning from\nearlier steps (Coherent CoT) is integrated. Given that this coherent reasoning\nchanges the behavior of the transformer, we further investigate the sensitivity\nof the transformer with Coherent CoT when the demonstration examples are\ncorrupted at the inference stage. Our theoretical results indicate that the\ntransformer is more sensitive to errors in intermediate reasoning steps than\nthe final outcome. Building upon this observation, we propose an improvement on\nCoT by incorporating both correct and incorrect reasoning paths in the\ndemonstration. Our experiments validate the effectiveness of the proposed\napproach.\n","authors":["Yingqian Cui","Pengfei He","Xianfeng Tang","Qi He","Chen Luo","Jiliang Tang","Yue Xing"],"pdf_url":"https://arxiv.org/pdf/2410.16540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16537v1","updated":"2024-10-21T21:55:09Z","published":"2024-10-21T21:55:09Z","title":"QIXAI: A Quantum-Inspired Framework for Enhancing Classical and Quantum\n  Model Transparency and Understanding","summary":"  The impressive performance of deep learning models, particularly\nConvolutional Neural Networks (CNNs), is often hindered by their lack of\ninterpretability, rendering them \"black boxes.\" This opacity raises concerns in\ncritical areas like healthcare, finance, and autonomous systems, where trust\nand accountability are crucial. This paper introduces the QIXAI Framework\n(Quantum-Inspired Explainable AI), a novel approach for enhancing neural\nnetwork interpretability through quantum-inspired techniques. By utilizing\nprinciples from quantum mechanics, such as Hilbert spaces, superposition,\nentanglement, and eigenvalue decomposition, the QIXAI framework reveals how\ndifferent layers of neural networks process and combine features to make\ndecisions.\n  We critically assess model-agnostic methods like SHAP and LIME, as well as\ntechniques like Layer-wise Relevance Propagation (LRP), highlighting their\nlimitations in providing a comprehensive view of neural network operations. The\nQIXAI framework overcomes these limitations by offering deeper insights into\nfeature importance, inter-layer dependencies, and information propagation. A\nCNN for malaria parasite detection is used as a case study to demonstrate how\nquantum-inspired methods like Singular Value Decomposition (SVD), Principal\nComponent Analysis (PCA), and Mutual Information (MI) provide interpretable\nexplanations of model behavior. Additionally, we explore the extension of QIXAI\nto other architectures, including Recurrent Neural Networks (RNNs), Long\nShort-Term Memory (LSTM) networks, Transformers, and Natural Language\nProcessing (NLP) models, and its application to generative models and\ntime-series analysis. The framework applies to both quantum and classical\nsystems, demonstrating its potential to improve interpretability and\ntransparency across a range of models, advancing the broader goal of developing\ntrustworthy AI systems.\n","authors":["John M. Willis"],"pdf_url":"https://arxiv.org/pdf/2410.16537v1.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.17042v3","updated":"2024-10-21T21:54:46Z","published":"2023-10-25T22:45:31Z","title":"StochGradAdam: Accelerating Neural Networks Training with Stochastic\n  Gradient Sampling","summary":"  In this paper, we introduce StochGradAdam, a novel optimizer designed as an\nextension of the Adam algorithm, incorporating stochastic gradient sampling\ntechniques to improve computational efficiency while maintaining robust\nperformance. StochGradAdam optimizes by selectively sampling a subset of\ngradients during training, reducing the computational cost while preserving the\nadvantages of adaptive learning rates and bias corrections found in Adam. Our\nexperimental results, applied to image classification and segmentation tasks,\ndemonstrate that StochGradAdam can achieve comparable or superior performance\nto Adam, even when using fewer gradient updates per iteration. By focusing on\nkey gradient updates, StochGradAdam offers stable convergence and enhanced\nexploration of the loss landscape, while mitigating the impact of noisy\ngradients. The results suggest that this approach is particularly effective for\nlarge-scale models and datasets, providing a promising alternative to\ntraditional optimization techniques for deep learning applications.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2310.17042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16533v1","updated":"2024-10-21T21:48:24Z","published":"2024-10-21T21:48:24Z","title":"Large Body Language Models","summary":"  As virtual agents become increasingly prevalent in human-computer\ninteraction, generating realistic and contextually appropriate gestures in\nreal-time remains a significant challenge. While neural rendering techniques\nhave made substantial progress with static scripts, their applicability to\nhuman-computer interactions remains limited. To address this, we introduce\nLarge Body Language Models (LBLMs) and present LBLM-AVA, a novel LBLM\narchitecture that combines a Transformer-XL large language model with a\nparallelized diffusion model to generate human-like gestures from multimodal\ninputs (text, audio, and video). LBLM-AVA incorporates several key components\nenhancing its gesture generation capabilities, such as multimodal-to-pose\nembeddings, enhanced sequence-to-sequence mapping with redefined attention\nmechanisms, a temporal smoothing module for gesture sequence coherence, and an\nattention-based refinement module for enhanced realism. The model is trained on\nour large-scale proprietary open-source dataset Allo-AVA. LBLM-AVA achieves\nstate-of-the-art performance in generating lifelike and contextually\nappropriate gestures with a 30% reduction in Fr\\'echet Gesture Distance (FGD),\nand a 25% improvement in Fr\\'echet Inception Distance compared to existing\napproaches.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2410.16533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16531v1","updated":"2024-10-21T21:45:22Z","published":"2024-10-21T21:45:22Z","title":"Bayesian scaling laws for in-context learning","summary":"  In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a family of novel Bayesian scaling laws for ICL. In\nexperiments with \\mbox{GPT-2} models of different sizes, our scaling laws\nexceed or match existing scaling laws in accuracy while also offering\ninterpretable terms for task priors, learning efficiency, and per-example\nprobabilities. To illustrate the analytic power that such interpretable scaling\nlaws provide, we report on controlled synthetic dataset experiments designed to\ninform real-world studies of safety alignment. In our experimental protocol, we\nuse SFT to suppress an unwanted existing model capability and then use ICL to\ntry to bring that capability back (many-shot jailbreaking). We then experiment\non real-world instruction-tuned LLMs using capabilities benchmarks as well as a\nnew many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\naccurately predict the conditions under which ICL will cause the suppressed\nbehavior to reemerge, which sheds light on the ineffectiveness of post-training\nat increasing LLM safety.\n","authors":["Aryaman Arora","Dan Jurafsky","Christopher Potts","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2410.16531v1.pdf","comment":"10 pages main text, 26 pages total"},{"id":"http://arxiv.org/abs/2410.16529v1","updated":"2024-10-21T21:37:55Z","published":"2024-10-21T21:37:55Z","title":"Distributed Online Life-Long Learning (DOL3) for Multi-agent Trust and\n  Reputation Assessment in E-commerce","summary":"  Trust and Reputation Assessment of service providers in citizen-focused\nenvironments like e-commerce is vital to maintain the integrity of the\ninteractions among agents. The goals and objectives of both the service\nprovider and service consumer agents are relevant to the goals of the\nrespective citizens (end users). The provider agents often pursue selfish goals\nthat can make the service quality highly volatile, contributing towards the\nnon-stationary nature of the environment. The number of active service\nproviders tends to change over time resulting in an open environment. This\nnecessitates a rapid and continual assessment of the Trust and Reputation. A\nlarge number of service providers in the environment require a distributed\nmulti-agent Trust and Reputation assessment. This paper addresses the problem\nof multi-agent Trust and Reputation Assessment in a non-stationary environment\ninvolving transactions between providers and consumers. In this setting, the\nobserver agents carry out the assessment and communicate their assessed trust\nscores with each other over a network. We propose a novel Distributed Online\nLife-Long Learning (DOL3) algorithm that involves real-time rapid learning of\ntrust and reputation scores of providers. Each observer carries out an adaptive\nlearning and weighted fusion process combining their own assessment along with\nthat of their neighbour in the communication network. Simulation studies reveal\nthat the state-of-the-art methods, which usually involve training a model to\nassess an agent's trust and reputation, do not work well in such an\nenvironment. The simulation results show that the proposed DOL3 algorithm\noutperforms these methods and effectively handles the volatility in such\nenvironments. From the statistical evaluation, it is evident that DOL3 performs\nbetter compared to other models in 90% of the cases.\n","authors":["Hariprasauth Ramamoorthy","Shubhankar Gupta","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2410.16529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08598v2","updated":"2024-10-21T21:32:51Z","published":"2024-06-12T19:05:43Z","title":"Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks","summary":"  As Large Language Models (LLMs) continue to evolve, the search for efficient\nand meaningful evaluation methods is ongoing. Many recent evaluations use LLMs\nas judges to score outputs from other LLMs, often relying on a single large\nmodel like GPT-4o. However, using a single LLM judge is prone to intra-model\nbias, and many tasks - such as those related to emotional intelligence,\ncreative writing, and persuasiveness - may be too subjective for a single model\nto judge fairly. We introduce the Language Model Council (LMC), where a group\nof LLMs collaborate to create tests, respond to them, and evaluate each other's\nresponses to produce a ranking in a democratic fashion. Unlike previous\napproaches that focus on reducing cost or bias by using a panel of smaller\nmodels, our work examines the benefits and nuances of a fully inclusive LLM\nevaluation system. In a detailed case study on emotional intelligence, we\ndeploy a council of 20 recent LLMs to rank each other on open-ended responses\nto interpersonal conflicts. Our results show that the LMC produces rankings\nthat are more separable and more robust, and through a user study, we show that\nthey are more consistent with human evaluations than any individual LLM judge.\nUsing all LLMs for judging can be costly, however, so we use Monte Carlo\nsimulations and hand-curated sub-councils to study hypothetical council\ncompositions and discuss the value of the incremental LLM judge.\n","authors":["Justin Zhao","Flor Miriam Plaza-del-Arco","Benjie Genchel","Amanda Cercas Curry"],"pdf_url":"https://arxiv.org/pdf/2406.08598v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13721v5","updated":"2024-10-21T21:28:13Z","published":"2023-11-22T22:27:54Z","title":"Nova: Generative Language Models for Assembly Code with Hierarchical\n  Attention and Contrastive Learning","summary":"  Binary code analysis is the foundation of crucial tasks in the security\ndomain; thus building effective binary analysis techniques is more important\nthan ever. Large language models (LLMs) although have brought impressive\nimprovement to source code tasks, do not directly generalize to assembly code\ndue to the unique challenges of assembly: (1) the low information density of\nassembly and (2) the diverse optimizations in assembly code. To overcome these\nchallenges, this work proposes a hierarchical attention mechanism that builds\nattention summaries to capture the semantics more effectively and designs\ncontrastive learning objectives to train LLMs to learn assembly optimization.\nEquipped with these techniques, this work develops Nova, a generative LLM for\nassembly code. Nova outperforms existing techniques on binary code\ndecompilation by up to 14.84 -- 21.58% (absolute percentage point improvement)\nhigher Pass@1 and Pass@10, and outperforms the latest binary code similarity\ndetection techniques by up to 6.17% Recall@1, showing promising abilities on\nboth assembly generation and understanding tasks.\n","authors":["Nan Jiang","Chengxiao Wang","Kevin Liu","Xiangzhe Xu","Lin Tan","Xiangyu Zhang","Petr Babkin"],"pdf_url":"https://arxiv.org/pdf/2311.13721v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16520v1","updated":"2024-10-21T21:21:29Z","published":"2024-10-21T21:21:29Z","title":"AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context","summary":"  As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.\n","authors":["Naba Rizvi","Harper Strickland","Daniel Gitelman","Tristan Cooper","Alexis Morales-Flores","Michael Golden","Aekta Kallepalli","Akshat Alurkar","Haaset Owens","Saleha Ahmedi","Isha Khirwadkar","Imani Munyaka","Nedjma Ousidhoum"],"pdf_url":"https://arxiv.org/pdf/2410.16520v1.pdf","comment":"9 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2405.02161v2","updated":"2024-10-21T21:20:29Z","published":"2024-05-03T15:08:25Z","title":"Simulating the Economic Impact of Rationality through Reinforcement\n  Learning and Agent-Based Modelling","summary":"  Agent-based models (ABMs) are simulation models used in economics to overcome\nsome of the limitations of traditional frameworks based on general equilibrium\nassumptions. However, agents within an ABM follow predetermined 'bounded\nrational' behavioural rules which can be cumbersome to design and difficult to\njustify. Here we leverage multi-agent reinforcement learning (RL) to expand the\ncapabilities of ABMs with the introduction of 'fully rational' agents that\nlearn their policy by interacting with the environment and maximising a reward\nfunction. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by\nextending a paradigmatic macro ABM from the economic literature. We show that\ngradually substituting ABM firms in the model with RL agents, trained to\nmaximise profits, allows for studying the impact of rationality on the economy.\nWe find that RL agents spontaneously learn three distinct strategies for\nmaximising profits, with the optimal strategy depending on the level of market\ncompetition and rationality. We also find that RL agents with independent\npolicies, and without the ability to communicate with each other, spontaneously\nlearn to segregate into different strategic groups, thus increasing market\npower and overall profits. Finally, we find that a higher number of rational\n(RL) agents in the economy always improves the macroeconomic environment as\nmeasured by total output. Depending on the specific rational policy, this can\ncome at the cost of higher instability. Our R-MABM framework allows for stable\nmulti-agent learning, is available in open source, and represents a principled\nand robust direction to extend economic simulators.\n","authors":["Simone Brusatin","Tommaso Padoan","Andrea Coletta","Domenico Delli Gatti","Aldo Glielmo"],"pdf_url":"https://arxiv.org/pdf/2405.02161v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.16517v1","updated":"2024-10-21T21:19:49Z","published":"2024-10-21T21:19:49Z","title":"RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean\n  Metric Space","summary":"  Deep Reinforcement Learning (DRL) algorithms have achieved great success in\nsolving many challenging tasks while their black-box nature hinders\ninterpretability and real-world applicability, making it difficult for human\nexperts to interpret and understand DRL policies. Existing works on\ninterpretable reinforcement learning have shown promise in extracting decision\ntree (DT) based policies from DRL policies with most focus on the single-agent\nsettings while prior attempts to introduce DT policies in multi-agent scenarios\nmainly focus on heuristic designs which do not provide any quantitative\nguarantees on the expected return. In this paper, we establish an upper bound\non the return gap between the oracle expert policy and an optimal decision tree\npolicy. This enables us to recast the DT extraction problem into a novel\nnon-euclidean clustering problem over the local observation and action values\nspace of each agent, with action values as cluster labels and the upper bound\non the return gap as clustering loss. Both the algorithm and the upper bound\nare extended to multi-agent decentralized DT extractions by an\niteratively-grow-DT procedure guided by an action-value function conditioned on\nthe current DTs of other agents. Further, we propose the\nReturn-Gap-Minimization Decision Tree (RGMDT) algorithm, which is a\nsurprisingly simple design and is integrated with reinforcement learning\nthrough the utilization of a novel Regularized Information Maximization loss.\nEvaluations on tasks like D4RL show that RGMDT significantly outperforms\nheuristic DT-based baselines and can achieve nearly optimal returns under given\nDT complexity constraints (e.g., maximum number of DT nodes).\n","authors":["Jingdi Chen","Hanhan Zhou","Yongsheng Mei","Carlee Joe-Wong","Gina Adam","Nathaniel D. Bastian","Tian Lan"],"pdf_url":"https://arxiv.org/pdf/2410.16517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05357v2","updated":"2024-10-21T21:17:41Z","published":"2024-08-09T22:08:12Z","title":"SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV\n  Battery Supply Chain Disruptions","summary":"  The electric vehicle (EV) battery supply chain's vulnerability to disruptions\nnecessitates advanced predictive analytics. We present SHIELD (Schema-based\nHierarchical Induction for EV supply chain Disruption), a system integrating\nLarge Language Models (LLMs) with domain expertise for EV battery supply chain\nrisk assessment. SHIELD combines: (1) LLM-driven schema learning to construct a\ncomprehensive knowledge library, (2) a disruption analysis system utilizing\nfine-tuned language models for event extraction, multi-dimensional similarity\nmatching for schema matching, and Graph Convolutional Networks (GCNs) with\nlogical constraints for prediction, and (3) an interactive interface for\nvisualizing results and incorporating expert feedback to enhance\ndecision-making. Evaluated on 12,070 paragraphs from 365 sources (2022-2023),\nSHIELD outperforms baseline GCNs and LLM+prompt methods (e.g., GPT-4o) in\ndisruption prediction. These results demonstrate SHIELD's effectiveness in\ncombining LLM capabilities with domain expertise for enhanced supply chain risk\nassessment.\n","authors":["Zhi-Qi Cheng","Yifei Dong","Aike Shi","Wei Liu","Yuzhi Hu","Jason O'Connor","Alexander G. Hauptmann","Kate S. Whitefoot"],"pdf_url":"https://arxiv.org/pdf/2408.05357v2.pdf","comment":"Oral, EMNLP 2024 Industry Track. 31 pages, 11 figures, Project:\n  https://fly1113.github.io/MFI/"},{"id":"http://arxiv.org/abs/2410.16503v1","updated":"2024-10-21T20:50:51Z","published":"2024-10-21T20:50:51Z","title":"Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for\n  Allocentric Avatar Gesture Animation","summary":"  The scarcity of high-quality, multimodal training data severely hinders the\ncreation of lifelike avatar animations for conversational AI in virtual\nenvironments. Existing datasets often lack the intricate synchronization\nbetween speech, facial expressions, and body movements that characterize\nnatural human communication. To address this critical gap, we introduce\nAllo-AVA, a large-scale dataset specifically designed for text and audio-driven\navatar gesture animation in an allocentric (third person point-of-view)\ncontext. Allo-AVA consists of $\\sim$1,250 hours of diverse video content,\ncomplete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely\nmaps these keypoints to precise timestamps, enabling accurate replication of\nhuman movements (body and facial gestures) in synchronization with speech. This\ncomprehensive resource enables the development and evaluation of more natural,\ncontext-aware avatar animation models, potentially transforming applications\nranging from virtual reality to digital assistants.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2410.16503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02656v2","updated":"2024-10-21T20:50:35Z","published":"2024-10-03T16:43:00Z","title":"Scalable Simulation-free Entropic Unbalanced Optimal Transport","summary":"  The Optimal Transport (OT) problem investigates a transport map that connects\ntwo distributions while minimizing a given cost function. Finding such a\ntransport map has diverse applications in machine learning, such as generative\nmodeling and image-to-image translation. In this paper, we introduce a scalable\nand simulation-free approach for solving the Entropic Unbalanced Optimal\nTransport (EUOT) problem. We derive the dynamical form of this EUOT problem,\nwhich is a generalization of the Schr\\\"odinger bridges (SB) problem. Based on\nthis, we derive dual formulation and optimality conditions of the EUOT problem\nfrom the stochastic optimal control interpretation. By leveraging these\nproperties, we propose a simulation-free algorithm to solve EUOT, called\nSimulation-free EUOT (SF-EUOT). While existing SB models require expensive\nsimulation costs during training and evaluation, our model achieves\nsimulation-free training and one-step generation by utilizing the reciprocal\nproperty. Our model demonstrates significantly improved scalability in\ngenerative modeling and image-to-image translation tasks compared to previous\nSB methods.\n","authors":["Jaemoo Choi","Jaewoong Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02656v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2405.14226v2","updated":"2024-10-21T20:10:37Z","published":"2024-05-23T06:57:04Z","title":"Variational Delayed Policy Optimization","summary":"  In environments with delayed observation, state augmentation by including\nactions within the delay window is adopted to retrieve Markovian property to\nenable reinforcement learning (RL). However, state-of-the-art (SOTA) RL\ntechniques with Temporal-Difference (TD) learning frameworks often suffer from\nlearning inefficiency, due to the significant expansion of the augmented state\nspace with the delay. To improve learning efficiency without sacrificing\nperformance, this work introduces a novel framework called Variational Delayed\nPolicy Optimization (VDPO), which reformulates delayed RL as a variational\ninference problem. This problem is further modelled as a two-step iterative\noptimization problem, where the first step is TD learning in the delay-free\nenvironment with a small state space, and the second step is behaviour cloning\nwhich can be addressed much more efficiently than TD learning. We not only\nprovide a theoretical analysis of VDPO in terms of sample complexity and\nperformance, but also empirically demonstrate that VDPO can achieve consistent\nperformance with SOTA methods, with a significant enhancement of sample\nefficiency (approximately 50\\% less amount of samples) in the MuJoCo benchmark.\n","authors":["Qingyuan Wu","Simon Sinong Zhan","Yixuan Wang","Yuhui Wang","Chung-Wei Lin","Chen Lv","Qi Zhu","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2405.14226v2.pdf","comment":"NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2409.10695v2","updated":"2024-10-21T20:01:13Z","published":"2024-09-16T19:52:24Z","title":"Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large\n  Language Models","summary":"  We introduce Playground v3 (PGv3), our latest text-to-image model that\nachieves state-of-the-art (SoTA) performance across multiple testing\nbenchmarks, excels in graphic design abilities and introduces new capabilities.\nUnlike traditional text-to-image generative models that rely on pre-trained\nlanguage models like T5 or CLIP text encoders, our approach fully integrates\nLarge Language Models (LLMs) with a novel structure that leverages text\nconditions exclusively from a decoder-only LLM. Additionally, to enhance image\ncaptioning quality-we developed an in-house captioner, capable of generating\ncaptions with varying levels of detail, enriching the diversity of text\nstructures. We also introduce a new benchmark CapsBench to evaluate detailed\nimage captioning performance. Experimental results demonstrate that PGv3 excels\nin text prompt adherence, complex reasoning, and accurate text rendering. User\npreference studies indicate the super-human graphic design ability of our model\nfor common design applications, such as stickers, posters, and logo designs.\nFurthermore, PGv3 introduces new capabilities, including precise RGB color\ncontrol and robust multilingual understanding.\n","authors":["Bingchen Liu","Ehsan Akhgari","Alexander Visheratin","Aleks Kamko","Linmiao Xu","Shivam Shrirao","Chase Lambert","Joao Souza","Suhail Doshi","Daiqing Li"],"pdf_url":"https://arxiv.org/pdf/2409.10695v2.pdf","comment":"Project page: https://playground.com/pg-v3"},{"id":"http://arxiv.org/abs/2409.01344v2","updated":"2024-10-21T19:49:41Z","published":"2024-09-02T15:58:24Z","title":"Pairing Analogy-Augmented Generation with Procedural Memory for\n  Procedural Q&A","summary":"  Large language models struggle to synthesize disparate pieces of information\ninto a coherent plan when approaching a complex procedural task. In this work,\nwe introduce a novel formalism and structure for such procedural knowledge.\nBased on this formalism, we present a novel procedural knowledge dataset called\nLCStep, which we created from LangChain tutorials. To leverage this procedural\nknowledge to solve new tasks, we propose analogy-augmented generation (AAG),\nwhich draws inspiration from the human ability to assimilate past experiences\nto solve unfamiliar problems. AAG uses a custom procedure memory store to\nretrieve and adapt specialized domain knowledge to answer new procedural tasks.\nWe demonstrate that AAG outperforms few-shot and RAG baselines on LCStep,\nRecipeNLG, and CHAMP datasets under a pairwise LLM-based evaluation,\ncorroborated by human evaluation in the case of RecipeNLG.\n","authors":["K Roth","Rushil Gupta","Simon Halle","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16427v3","updated":"2024-10-21T19:47:54Z","published":"2024-09-24T19:47:21Z","title":"HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI\n  Interactions","summary":"  AI agents are increasingly autonomous in their interactions with human users\nand tools, leading to increased interactional safety risks. We present\nHAICOSYSTEM, a framework examining AI agent safety within diverse and complex\nsocial interactions. HAICOSYSTEM features a modular sandbox environment that\nsimulates multi-turn interactions between human users and AI agents, where the\nAI agents are equipped with a variety of tools (e.g., patient management\nplatforms) to navigate diverse scenarios (e.g., a user attempting to access\nother patients' profiles). To examine the safety of AI agents in these\ninteractions, we develop a comprehensive multi-dimensional evaluation framework\nthat uses metrics covering operational, content-related, societal, and legal\nrisks. Through running 1840 simulations based on 92 scenarios across seven\ndomains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEM\ncan emulate realistic user-AI interactions and complex tool use by AI agents.\nOur experiments show that state-of-the-art LLMs, both proprietary and\nopen-sourced, exhibit safety risks in over 50\\% cases, with models generally\nshowing higher risks when interacting with simulated malicious users. Our\nfindings highlight the ongoing challenge of building agents that can safely\nnavigate complex interactions, particularly when faced with malicious users. To\nfoster the AI agent safety ecosystem, we release a code platform that allows\npractitioners to create custom scenarios, simulate interactions, and evaluate\nthe safety and performance of their agents.\n","authors":["Xuhui Zhou","Hyunwoo Kim","Faeze Brahman","Liwei Jiang","Hao Zhu","Ximing Lu","Frank Xu","Bill Yuchen Lin","Yejin Choi","Niloofar Mireshghallah","Ronan Le Bras","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2409.16427v3.pdf","comment":"Both the second and third authors contributed equally"},{"id":"http://arxiv.org/abs/2410.16458v1","updated":"2024-10-21T19:34:40Z","published":"2024-10-21T19:34:40Z","title":"STAR: A Simple Training-free Approach for Recommendations using Large\n  Language Models","summary":"  Recent progress in large language models (LLMs) offers promising new\napproaches for recommendation system (RecSys) tasks. While the current\nstate-of-the-art methods rely on fine-tuning LLMs to achieve optimal results,\nthis process is costly and introduces significant engineering complexities.\nConversely, methods that bypass fine-tuning and use LLMs directly are less\nresource-intensive but often fail to fully capture both semantic and\ncollaborative information, resulting in sub-optimal performance compared to\ntheir fine-tuned counterparts. In this paper, we propose a Simple Training-free\nApproach for Recommendation (STAR), a framework that utilizes LLMs and can be\napplied to various recommendation tasks without the need for fine-tuning. Our\napproach involves a retrieval stage that uses semantic embeddings from LLMs\ncombined with collaborative user information to retrieve candidate items. We\nthen apply an LLM for pairwise ranking to enhance next-item prediction.\nExperimental results on the Amazon Review dataset show competitive performance\nfor next item prediction, even with our retrieval stage alone. Our full method\nachieves Hits@10 performance of +23.8% on Beauty, +37.5% on Toys and Games, and\n-1.8% on Sports and Outdoors relative to the best supervised models. This\nframework offers an effective alternative to traditional supervised models,\nhighlighting the potential of LLMs in recommendation systems without extensive\ntraining or custom architectures.\n","authors":["Dong-Ho Lee","Adam Kraft","Long Jin","Nikhil Mehta","Taibai Xu","Lichan Hong","Ed H. Chi","Xinyang Yi"],"pdf_url":"https://arxiv.org/pdf/2410.16458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16454v1","updated":"2024-10-21T19:28:37Z","published":"2024-10-21T19:28:37Z","title":"Does your LLM truly unlearn? An embarrassingly simple approach to\n  recover unlearned knowledge","summary":"  Large language models (LLMs) have shown remarkable proficiency in generating\ntext, benefiting from extensive training on vast textual corpora. However, LLMs\nmay also acquire unwanted behaviors from the diverse and sensitive nature of\ntheir training data, which can include copyrighted and private content. Machine\nunlearning has been introduced as a viable solution to remove the influence of\nsuch problematic content without the need for costly and time-consuming\nretraining. This process aims to erase specific knowledge from LLMs while\npreserving as much model utility as possible. Despite the effectiveness of\ncurrent unlearning methods, little attention has been given to whether existing\nunlearning methods for LLMs truly achieve forgetting or merely hide the\nknowledge, which current unlearning benchmarks fail to detect. This paper\nreveals that applying quantization to models that have undergone unlearning can\nrestore the \"forgotten\" information. To thoroughly evaluate this phenomenon, we\nconduct comprehensive experiments using various quantization techniques across\nmultiple precision levels. We find that for unlearning methods with utility\nconstraints, the unlearned model retains an average of 21\\% of the intended\nforgotten knowledge in full precision, which significantly increases to 83\\%\nafter 4-bit quantization. Based on our empirical findings, we provide a\ntheoretical explanation for the observed phenomenon and propose a\nquantization-robust unlearning strategy to mitigate this intricate issue...\n","authors":["Zhiwei Zhang","Fali Wang","Xiaomin Li","Zongyu Wu","Xianfeng Tang","Hui Liu","Qi He","Wenpeng Yin","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16454v1.pdf","comment":"21 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.15512v2","updated":"2024-10-21T18:57:08Z","published":"2024-09-03T06:02:02Z","title":"PixelBytes: Catching Unified Embedding for Multimodal Generation","summary":"  This report introduces PixelBytes Embedding, a novel approach for unified\nmultimodal representation learning. Our method captures diverse inputs in a\nsingle, cohesive representation, enabling emergent properties for multimodal\nsequence generation, particularly for text and pixelated images. Inspired by\nstate-of-the-art sequence models such as Image Transformers, PixelCNN, and\nMamba-Bytes, PixelBytes aims to address the challenges of integrating different\ndata types. We explore various model architectures, including Recurrent Neural\nNetworks (RNNs), State Space Models (SSMs), and Attention-based models,\nfocusing on bidirectional processing and our innovative PxBy embedding\ntechnique. Our experiments, conducted on a specialized PixelBytes Pok{\\'e}mon\ndataset, demonstrate that bidirectional sequence models with PxBy embedding and\nconvolutional layers can generate coherent multimodal sequences. This work\ncontributes to the advancement of integrated AI models capable of understanding\nand generating multimodal data in a unified manner.\n","authors":["Fabien Furfaro"],"pdf_url":"https://arxiv.org/pdf/2409.15512v2.pdf","comment":"This article is an earlier version of my work arXiv:2410.01820\n  \"PixelBytes: Catching Unified Representation for Multimodal Generation.\""},{"id":"http://arxiv.org/abs/2410.16431v1","updated":"2024-10-21T18:51:34Z","published":"2024-10-21T18:51:34Z","title":"Conjuring Semantic Similarity","summary":"  The semantic similarity between sample expressions measures the distance\nbetween their latent 'meaning'. Such meanings are themselves typically\nrepresented by textual expressions, often insufficient to differentiate\nconcepts at fine granularity. We propose a novel approach whereby the semantic\nsimilarity among textual expressions is based not on other expressions they can\nbe rephrased as, but rather based on the imagery they evoke. While this is not\npossible with humans, generative models allow us to easily visualize and\ncompare generated images, or their distribution, evoked by a textual prompt.\nTherefore, we characterize the semantic similarity between two textual\nexpressions simply as the distance between image distributions they induce, or\n'conjure.' We show that by choosing the Jensen-Shannon divergence between the\nreverse-time diffusion stochastic differential equations (SDEs) induced by each\ntextual expression, this can be directly computed via Monte-Carlo sampling. Our\nmethod contributes a novel perspective on semantic similarity that not only\naligns with human-annotated scores, but also opens up new avenues for the\nevaluation of text-conditioned generative models while offering better\ninterpretability of their learnt representations.\n","authors":["Tian Yu Liu","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2410.16431v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.14535v2","updated":"2024-10-21T07:28:25Z","published":"2024-10-18T15:23:29Z","title":"Comparing Differentiable and Dynamic Ray Tracing: Introducing the\n  Multipath Lifetime Map","summary":"  With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle\ncommunications, radio propagation modeling tools must adapt to the rapidly\nchanging nature of the radio channel. Recently, both Differentiable and Dynamic\nRay Tracing frameworks have emerged to address these challenges. However, there\nis often confusion about how these approaches differ and which one should be\nused in specific contexts. In this paper, we provide an overview of these two\ntechniques and a comparative analysis against two state-of-the-art tools:\n3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise\ncharacterization of the scope of these methods, we introduce a novel\nsimulation-based metric, the Multipath Lifetime Map, which enables the\nevaluation of spatial and temporal coherence in radio channels only based on\nthe geometrical description of the environment. Finally, our metrics are\nevaluated on a classic urban street canyon scenario, yielding similar results\nto those obtained from measurement campaigns.\n","authors":["Jérome Eertmans","Enrico Maria Vittuci","Vittorio Degli Esposti","Laurent Jacques","Claude Oestges"],"pdf_url":"https://arxiv.org/pdf/2410.14535v2.pdf","comment":"5 pages, 5 figures, 1 table, submitted to EuCAP 2025"},{"id":"http://arxiv.org/abs/2410.14485v2","updated":"2024-10-21T08:26:40Z","published":"2024-10-18T14:10:16Z","title":"CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and\n  Fully-Connected Neural Networks for Causally Constrained Predictions","summary":"  Artificial Neural Networks (ANNs), including fully-connected networks and\ntransformers, are highly flexible and powerful function approximators, widely\napplied in fields like computer vision and natural language processing.\nHowever, their inability to inherently respect causal structures can limit\ntheir robustness, making them vulnerable to covariate shift and difficult to\ninterpret/explain. This poses significant challenges for their reliability in\nreal-world applications. In this paper, we introduce Causal Fully-Connected\nNeural Networks (CFCNs) and Causal Transformers (CaTs), two general model\nfamilies designed to operate under predefined causal constraints, as specified\nby a Directed Acyclic Graph (DAG). These models retain the powerful function\napproximation abilities of traditional neural networks while adhering to the\nunderlying structural constraints, improving robustness, reliability, and\ninterpretability at inference time. This approach opens new avenues for\ndeploying neural networks in more demanding, real-world scenarios where\nrobustness and explainability is critical.\n","authors":["Matthew J. Vowels","Mathieu Rochat","Sina Akbari"],"pdf_url":"https://arxiv.org/pdf/2410.14485v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14380v2","updated":"2024-10-21T01:28:32Z","published":"2024-10-18T11:07:26Z","title":"Dual-Label Learning With Irregularly Present Labels","summary":"  In multi-task learning, we often encounter the case when the presence of\nlabels across samples exhibits irregular patterns: samples can be fully\nlabeled, partially labeled or unlabeled. Taking drug analysis as an example,\nmultiple toxicity properties of a drug molecule may not be concurrently\navailable due to experimental limitations. It triggers a demand for a new\ntraining and inference mechanism that could accommodate irregularly present\nlabels and maximize the utility of any available label information. In this\nwork, we focus on the two-label learning task, and propose a novel training and\ninference framework, Dual-Label Learning (DLL). The DLL framework formulates\nthe problem into a dual-function system, in which the two functions should\nsimultaneously satisfy standard supervision, structural duality and\nprobabilistic duality. DLL features a dual-tower model architecture that\nexplicitly captures the information exchange between labels, aimed at\nmaximizing the utility of partially available labels in understanding label\ncorrelation. During training, label imputation for missing labels is conducted\nas part of the forward propagation process, while during inference, labels are\nregarded as unknowns of a bivariate system of equations and are solved jointly.\nTheoretical analysis guarantees the feasibility of DLL, and extensive\nexperiments are conducted to verify that by explicitly modeling label\ncorrelation and maximizing the utility of available labels, our method makes\nconsistently better predictions than baseline approaches by up to a 10% gain in\nF1-score or MAPE. Remarkably, our method provided with data at a label missing\nrate as high as 60% can achieve similar or even better results than baseline\napproaches at a label missing rate of only 10%.\n","authors":["Mingqian Li","Qiao Han","Yiteng Zhai","Ruifeng Li","Yao Yang","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10270v3","updated":"2024-10-21T08:13:45Z","published":"2024-10-14T08:21:25Z","title":"QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis","summary":"  Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.\n","authors":["Abhijit Manatkar","Ashlesha Akella","Parthivi Gupta","Krishnasuri Narayanam"],"pdf_url":"https://arxiv.org/pdf/2410.10270v3.pdf","comment":"Accepted for EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.14153v2","updated":"2024-10-21T06:56:03Z","published":"2024-10-18T03:44:10Z","title":"Wireless Human-Machine Collaboration in Industry 5.0","summary":"  Wireless Human-Machine Collaboration (WHMC) represents a critical advancement\nfor Industry 5.0, enabling seamless interaction between humans and machines\nacross geographically distributed systems. As the WHMC systems become\nincreasingly important for achieving complex collaborative control tasks,\nensuring their stability is essential for practical deployment and long-term\noperation. Stability analysis certifies how the closed-loop system will behave\nunder model randomness, which is essential for systems operating with wireless\ncommunications. However, the fundamental stability analysis of the WHMC systems\nremains an unexplored challenge due to the intricate interplay between the\nstochastic nature of wireless communications, dynamic human operations, and the\ninherent complexities of control system dynamics. This paper establishes a\nfundamental WHMC model incorporating dual wireless loops for machine and human\ncontrol. Our framework accounts for practical factors such as short-packet\ntransmissions, fading channels, and advanced HARQ schemes. We model human\ncontrol lag as a Markov process, which is crucial for capturing the stochastic\nnature of human interactions. Building on this model, we propose a stochastic\ncycle-cost-based approach to derive a stability condition for the WHMC system,\nexpressed in terms of wireless channel statistics, human dynamics, and control\nparameters. Our findings are validated through extensive numerical simulations\nand a proof-of-concept experiment, where we developed and tested a novel\nwireless collaborative cart-pole control system. The results confirm the\neffectiveness of our approach and provide a robust framework for future\nresearch on WHMC systems in more complex environments.\n","authors":["Gaoyang Pang","Wanchun Liu","Dusit Niyato","Daniel Quevedo","Branka Vucetic","Yonghui Li"],"pdf_url":"https://arxiv.org/pdf/2410.14153v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2406.08472v2","updated":"2024-10-21T17:59:13Z","published":"2024-06-12T17:56:31Z","title":"RILe: Reinforced Imitation Learning","summary":"  Reinforcement Learning has achieved significant success in generating complex\nbehavior but often requires extensive reward function engineering. Adversarial\nvariants of Imitation Learning and Inverse Reinforcement Learning offer an\nalternative by learning policies from expert demonstrations via a\ndiscriminator. However, these methods struggle in complex tasks where randomly\nsampling expert-like behaviors is challenging. This limitation stems from their\nreliance on policy-agnostic discriminators, which provide insufficient guidance\nfor agent improvement, especially as task complexity increases and expert\nbehavior becomes more distinct. We introduce RILe (Reinforced Imitation\nLearning environment), a novel trainer-student system that learns a dynamic\nreward function based on the student's performance and alignment with expert\ndemonstrations. In RILe, the student learns an action policy while the trainer,\nusing reinforcement learning, continuously updates itself via the\ndiscriminator's feedback to optimize the alignment between the student and the\nexpert. The trainer optimizes for long-term cumulative rewards from the\ndiscriminator, enabling it to provide nuanced feedback that accounts for the\ncomplexity of the task and the student's current capabilities. This approach\nallows for greater exploration of agent actions by providing graduated feedback\nrather than binary expert/non-expert classifications. By reducing dependence on\npolicy-agnostic discriminators, RILe enables better performance in complex\nsettings where traditional methods falter, outperforming existing methods by 2x\nin complex simulated robot-locomotion tasks.\n","authors":["Mert Albaba","Sammy Christen","Thomas Langarek","Christoph Gebhardt","Otmar Hilliges","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2406.08472v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16267v1","updated":"2024-10-21T17:59:11Z","published":"2024-10-21T17:59:11Z","title":"xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video\n  Even in VLMs","summary":"  We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html\n","authors":["Michael S. Ryoo","Honglu Zhou","Shrikant Kendre","Can Qin","Le Xue","Manli Shu","Silvio Savarese","Ran Xu","Caiming Xiong","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2410.16267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16255v1","updated":"2024-10-21T17:56:47Z","published":"2024-10-21T17:56:47Z","title":"Revisiting Deep Feature Reconstruction for Logical and Structural\n  Industrial Anomaly Detection","summary":"  Industrial anomaly detection is crucial for quality control and predictive\nmaintenance, but it presents challenges due to limited training data, diverse\nanomaly types, and external factors that alter object appearances. Existing\nmethods commonly detect structural anomalies, such as dents and scratches, by\nleveraging multi-scale features from image patches extracted through deep\npre-trained networks. However, significant memory and computational demands\noften limit their practical application. Additionally, detecting logical\nanomalies-such as images with missing or excess elements-requires an\nunderstanding of spatial relationships that traditional patch-based methods\nfail to capture. In this work, we address these limitations by focusing on Deep\nFeature Reconstruction (DFR), a memory- and compute-efficient approach for\ndetecting structural anomalies. We further enhance DFR into a unified\nframework, called ULSAD, which is capable of detecting both structural and\nlogical anomalies. Specifically, we refine the DFR training objective to\nimprove performance in structural anomaly detection, while introducing an\nattention-based loss mechanism using a global autoencoder-like network to\nhandle logical anomaly detection. Our empirical evaluation across five\nbenchmark datasets demonstrates the performance of ULSAD in detecting and\nlocalizing both structural and logical anomalies, outperforming eight\nstate-of-the-art methods. An extensive ablation study further highlights the\ncontribution of each component to the overall performance improvement. Our code\nis available at https://github.com/sukanyapatra1997/ULSAD-2024.git\n","authors":["Sukanya Patra","Souhaib Ben Taieb"],"pdf_url":"https://arxiv.org/pdf/2410.16255v1.pdf","comment":"Accepted in Transactions on Machine Learning Research (TMLR). Link to\n  OpenReview: https://openreview.net/forum?id=kdTC4ktHPD"},{"id":"http://arxiv.org/abs/2410.16253v1","updated":"2024-10-21T17:56:09Z","published":"2024-10-21T17:56:09Z","title":"Distribution Learning with Valid Outputs Beyond the Worst-Case","summary":"  Generative models at times produce \"invalid\" outputs, such as images with\ngeneration artifacts and unnatural sounds. Validity-constrained distribution\nlearning attempts to address this problem by requiring that the learned\ndistribution have a provably small fraction of its mass in invalid parts of\nspace -- something which standard loss minimization does not always ensure. To\nthis end, a learner in this model can guide the learning via \"validity\nqueries\", which allow it to ascertain the validity of individual examples.\nPrior work on this problem takes a worst-case stance, showing that proper\nlearning requires an exponential number of validity queries, and demonstrating\nan improper algorithm which -- while generating guarantees in a wide-range of\nsettings -- makes an atypical polynomial number of validity queries. In this\nwork, we take a first step towards characterizing regimes where guaranteeing\nvalidity is easier than in the worst-case. We show that when the data\ndistribution lies in the model class and the log-loss is minimized, the number\nof samples required to ensure validity has a weak dependence on the validity\nrequirement. Additionally, we show that when the validity region belongs to a\nVC-class, a limited number of validity queries are often sufficient.\n","authors":["Nick Rittler","Kamalika Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2410.16253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16247v1","updated":"2024-10-21T17:52:01Z","published":"2024-10-21T17:52:01Z","title":"Implicit Regularization for Tubal Tensor Factorizations via Gradient\n  Descent","summary":"  We provide a rigorous analysis of implicit regularization in an\noverparametrized tensor factorization problem beyond the lazy training regime.\nFor matrix factorization problems, this phenomenon has been studied in a number\nof works. A particular challenge has been to design universal initialization\nstrategies which provably lead to implicit regularization in gradient-descent\nmethods. At the same time, it has been argued by Cohen et. al. 2016 that more\ngeneral classes of neural networks can be captured by considering tensor\nfactorizations. However, in the tensor case, implicit regularization has only\nbeen rigorously established for gradient flow or in the lazy training regime.\nIn this paper, we prove the first tensor result of its kind for gradient\ndescent rather than gradient flow. We focus on the tubal tensor product and the\nassociated notion of low tubal rank, encouraged by the relevance of this model\nfor image data. We establish that gradient descent in an overparametrized\ntensor factorization model with a small random initialization exhibits an\nimplicit bias towards solutions of low tubal rank. Our theoretical findings are\nillustrated in an extensive set of numerical simulations show-casing the\ndynamics predicted by our theory as well as the crucial role of using a small\nrandom initialization.\n","authors":["Santhosh Karnik","Anna Veselovska","Mark Iwen","Felix Krahmer"],"pdf_url":"https://arxiv.org/pdf/2410.16247v1.pdf","comment":"58 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.21042v2","updated":"2024-10-21T17:50:10Z","published":"2024-05-31T17:33:07Z","title":"Comparing the information content of probabilistic representation spaces","summary":"  Probabilistic representation spaces convey information about a dataset, and\nto understand the effects of factors such as training loss and network\narchitecture, we seek to compare the information content of such spaces.\nHowever, most existing methods to compare representation spaces assume\nrepresentations are points, and neglect the distributional nature of\nprobabilistic representations. Here, instead of building upon point-based\nmeasures of comparison, we build upon classic methods from literature on hard\nclustering. We generalize two information-theoretic methods of comparing hard\nclustering assignments to be applicable to general probabilistic representation\nspaces. We then propose a practical method of estimation that is based on\nfingerprinting a representation space with a sample of the dataset and is\napplicable when the communicated information is only a handful of bits. With\nunsupervised disentanglement as a motivating problem, we find information\nfragments that are repeatedly contained in individual latent dimensions in VAE\nand InfoGAN ensembles. Then, by comparing the full latent spaces of models, we\nfind highly consistent information content across datasets, methods, and\nhyperparameters, even though there is often a point during training with\nsubstantial variety across repeat runs. Finally, we leverage the\ndifferentiability of the proposed method and perform model fusion by\nsynthesizing the information content of multiple weak learners, each incapable\nof representing the global structure of a dataset. Across the case studies, the\ndirect comparison of information content provides a natural basis for\nunderstanding the processing of information.\n","authors":["Kieran A. Murphy","Sam Dillavou","Dani S. Bassett"],"pdf_url":"https://arxiv.org/pdf/2405.21042v2.pdf","comment":"Code:\n  https://github.com/murphyka/representation-space-info-comparison"},{"id":"http://arxiv.org/abs/2410.16239v1","updated":"2024-10-21T17:42:41Z","published":"2024-10-21T17:42:41Z","title":"MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays,\n  ECGs, and Diagnostic Report","summary":"  In this paper, we introduce a novel Multi-Modal Contrastive Pre-training\nFramework that synergistically combines X-rays, electrocardiograms (ECGs), and\nradiology/cardiology reports. Our approach leverages transformers to encode\nthese diverse modalities into a unified representation space, aiming to enhance\ndiagnostic accuracy and facilitate comprehensive patient assessments. We\nutilize LoRA-Peft to significantly reduce trainable parameters in the LLM and\nincorporate recent linear attention dropping strategy in the Vision\nTransformer(ViT) for smoother attention. Furthermore, we provide novel\nmultimodal attention explanations and retrieval for our model. To the best of\nour knowledge, we are the first to propose an integrated model that combines\nX-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing\ncontrastive loss, MoRE effectively aligns modality-specific features into a\ncoherent embedding, which supports various downstream tasks such as zero-shot\nclassification and multimodal retrieval. Employing our proposed methodology, we\nachieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and\nPtbXl downstream datasets, surpassing existing multimodal approaches. Our\nproposed framework shows significant improvements in capturing intricate\ninter-modal relationships and its robustness in medical diagnosis that\nestablishes a framework for future research in multimodal learning in the\nhealthcare sector.\n","authors":["Samrajya Thapa","Koushik Howlader","Subhankar Bhattacharjee","Wei le"],"pdf_url":"https://arxiv.org/pdf/2410.16239v1.pdf","comment":"10 pages, 5 figures, 9 tables. Supplementary detail in Appendix. Code\n  made available in Github for reproducibility"},{"id":"http://arxiv.org/abs/2405.12235v6","updated":"2024-10-21T17:34:08Z","published":"2024-05-14T23:50:01Z","title":"Hypergraph: A Unified and Uniform Definition with Application to\n  Chemical Hypergraph and More","summary":"  The conventional definition of hypergraph has two major issues: (1) there is\nnot a standard definition of directed hypergraph and (2) there is not a formal\ndefinition of nested hypergraph. To resolve these issues, we propose a new\ndefinition of hypergraph that unifies the concepts of undirected, directed and\nnested hypergraphs, and that is uniform in using hyperedge as a single\nconstruct for representing high-order correlations among things, i.e., nodes\nand hyperedges. Specifically, we define a hyperedge to be a simple hyperedge, a\nnesting hyperedge, or a directed hyperedge. With this new definition, a\nhypergraph is nested if it has nesting hyperedge(s), and is directed if it has\ndirected hyperedge(s). Otherwise, a hypergraph is a simple hypergraph. The\nuniformity and power of this new definition, with visualization, should\nfacilitate the use of hypergraph for representing (hierarchical) high-order\ncorrelations in general and chemical systems in particular. Graph has been\nwidely used as a mathematical structure for machine learning on molecular\nstructures and 3D molecular geometries. However, graph has a major limitation:\nit can represent only pairwise correlations between nodes. Hypergraph extends\ngraph with high-order correlations among nodes. This extension is significant\nor essential for machine learning on chemical systems. For molecules, this is\nsignificant as it allows the direct, explicit representation of multicenter\nbonds and molecular substructures. For chemical reactions, this is essential\nsince most chemical reactions involve multiple participants. We propose the use\nof chemical hypergraph, a multilevel hypergraph with simple, nesting and\ndirected hyperedges, as a single mathematical structure for representing\nchemical systems. We apply the new definition of hypergraph to chemical\nhypergraph and, as simplified versions, molecular hypergraph and chemical\nreaction hypergraph.\n","authors":["Daniel T. Chang"],"pdf_url":"https://arxiv.org/pdf/2405.12235v6.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.03623 by other authors"},{"id":"http://arxiv.org/abs/2410.16222v1","updated":"2024-10-21T17:27:01Z","published":"2024-10-21T17:27:01Z","title":"A Realistic Threat Model for Large Language Model Jailbreaks","summary":"  A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. In their original settings, these methods all\nlargely succeed in coercing the target output, but their attacks vary\nsubstantially in fluency and computational effort. In this work, we propose a\nunified threat model for the principled comparison of these methods. Our threat\nmodel combines constraints in perplexity, measuring how far a jailbreak\ndeviates from natural text, and computational budget, in total FLOPs. For the\nformer, we build an N-gram model on 1T tokens, which, in contrast to\nmodel-based perplexity, allows for an LLM-agnostic and inherently interpretable\nevaluation. We adapt popular attacks to this new, realistic threat model, with\nwhich we, for the first time, benchmark these attacks on equal footing. After a\nrigorous comparison, we not only find attack success rates against safety-tuned\nmodern models to be lower than previously presented but also find that attacks\nbased on discrete optimization significantly outperform recent LLM-based\nattacks. Being inherently interpretable, our threat model allows for a\ncomprehensive analysis and comparison of jailbreak attacks. We find that\neffective attacks exploit and abuse infrequent N-grams, either selecting\nN-grams absent from real-world text or rare ones, e.g. specific to code\ndatasets.\n","authors":["Valentyn Boreiko","Alexander Panfilov","Vaclav Voracek","Matthias Hein","Jonas Geiping"],"pdf_url":"https://arxiv.org/pdf/2410.16222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17355v2","updated":"2024-10-21T17:27:00Z","published":"2024-08-30T15:39:34Z","title":"Bidirectional Decoding: Improving Action Chunking via Closed-Loop\n  Resampling","summary":"  Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. Yet, its reported effects on the learned policy are\ninconsistent: some studies find it crucial for achieving strong results, while\nothers observe decreased performance. In this paper, we first dissect how\naction chunking impacts the divergence between a learner and a demonstrator. We\nfind that action chunking allows the learner to better capture the temporal\ndependencies in demonstrations but at the cost of reduced reactivity in\nstochastic environments. To address this tradeoff, we propose Bidirectional\nDecoding (BID), a test-time inference algorithm that bridges action chunking\nwith closed-loop operations. BID samples multiple predictions at each time step\nand searches for the optimal one based on two criteria: (i) backward coherence,\nwhich favors samples that align with previous decisions; (ii) forward contrast,\nwhich seeks samples of high likelihood for future plans. By coupling decisions\nwithin and across action chunks, BID promotes consistency over time while\nmaintaining reactivity to unexpected changes. Experimental results show that\nBID boosts the performance of two state-of-the-art generative policies across\nseven simulation benchmarks and two real-world tasks. Code and videos are\navailable at https://bid-robot.github.io.\n","authors":["Yuejiang Liu","Jubayer Ibn Hamid","Annie Xie","Yoonho Lee","Maximilian Du","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2408.17355v2.pdf","comment":"Project website: https://bid-robot.github.io/"},{"id":"http://arxiv.org/abs/2406.01583v2","updated":"2024-10-21T17:25:44Z","published":"2024-06-03T17:58:43Z","title":"Decomposing and Interpreting Image Representations via Text in ViTs\n  Beyond CLIP","summary":"  Recent work has explored how individual components of the CLIP-ViT model\ncontribute to the final representation by leveraging the shared image-text\nrepresentation space of CLIP. These components, such as attention heads and\nMLPs, have been shown to capture distinct image features like shape, color or\ntexture. However, understanding the role of these components in arbitrary\nvision transformers (ViTs) is challenging. To this end, we introduce a general\nframework which can identify the roles of various components in ViTs beyond\nCLIP. Specifically, we (a) automate the decomposition of the final\nrepresentation into contributions from different model components, and (b)\nlinearly map these contributions to CLIP space to interpret them via text.\nAdditionally, we introduce a novel scoring function to rank components by their\nimportance with respect to specific features. Applying our framework to various\nViT variants (e.g. DeiT, DINO, DINOv2, Swin, MaxViT), we gain insights into the\nroles of different components concerning particular image features. These\ninsights facilitate applications such as image retrieval using text\ndescriptions or reference images, visualizing token importance heatmaps, and\nmitigating spurious correlations. We release our code to reproduce the\nexperiments at https://github.com/SriramB-98/vit-decompose\n","authors":["Sriram Balasubramanian","Samyadeep Basu","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2406.01583v2.pdf","comment":"NeurIPS 2024, 31 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.13714v2","updated":"2024-10-21T17:21:16Z","published":"2024-10-17T16:14:49Z","title":"Generation through the lens of learning theory","summary":"  We study generation through the lens of statistical learning theory. First,\nwe abstract and formalize the results of Gold [1967], Angluin [1979, 1980], and\nKleinberg and Mullainathan [2024] for language identification/generation in the\nlimit in terms of a binary hypothesis class defined over an abstract instance\nspace. Then, we formalize a different paradigm of generation studied by\nKleinberg and Mullainathan [2024], which we call ``uniform generation,\" and\nprovide a characterization of which hypothesis classes are uniformly\ngeneratable. As is standard in statistical learning theory, our\ncharacterization is in terms of the finiteness of a new combinatorial dimension\nwe call the Closure dimension. By doing so, we are able to compare\ngeneratability with predictability (captured via PAC and online learnability)\nand show that these two properties of hypothesis classes are\n\\emph{incompatible} - there are classes that are generatable but not\npredictable and vice versa.\n","authors":["Vinod Raman","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2410.13714v2.pdf","comment":"Minor edits"},{"id":"http://arxiv.org/abs/2410.16212v1","updated":"2024-10-21T17:12:06Z","published":"2024-10-21T17:12:06Z","title":"Comprehensive benchmarking of large language models for RNA secondary\n  structure prediction","summary":"  Inspired by the success of large language models (LLM) for DNA and proteins,\nseveral LLM for RNA have been developed recently. RNA-LLM uses large datasets\nof RNA sequences to learn, in a self-supervised way, how to represent each RNA\nbase with a semantically rich numerical vector. This is done under the\nhypothesis that obtaining high-quality RNA representations can enhance\ndata-costly downstream tasks. Among them, predicting the secondary structure is\na fundamental task for uncovering RNA functional mechanisms. In this work we\npresent a comprehensive experimental analysis of several pre-trained RNA-LLM,\ncomparing them for the RNA secondary structure prediction task in an unified\ndeep learning framework. The RNA-LLM were assessed with increasing\ngeneralization difficulty on benchmark datasets. Results showed that two LLM\nclearly outperform the other models, and revealed significant challenges for\ngeneralization in low-homology scenarios.\n","authors":["L. I. Zablocki","L. A. Bugnon","M. Gerard","L. Di Persia","G. Stegmayer","D. H. Milone"],"pdf_url":"https://arxiv.org/pdf/2410.16212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16208v1","updated":"2024-10-21T17:11:21Z","published":"2024-10-21T17:11:21Z","title":"Compute-Constrained Data Selection","summary":"  Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. These experiments show the\nvalidity of this model in real-world experiments. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective.\n","authors":["Junjie Oscar Yin","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.16208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16207v1","updated":"2024-10-21T17:10:43Z","published":"2024-10-21T17:10:43Z","title":"CoT-TL: Low-Resource Temporal Knowledge Representation of Planning\n  Instructions Using Chain-of-Thought Reasoning","summary":"  Autonomous agents often face the challenge of interpreting uncertain natural\nlanguage instructions for planning tasks. Representing these instructions as\nLinear Temporal Logic (LTL) enables planners to synthesize actionable plans. We\nintroduce CoT-TL, a data-efficient in-context learning framework for\ntranslating natural language specifications into LTL representations. CoT-TL\naddresses the limitations of large language models, which typically rely on\nextensive fine-tuning data, by extending chain-of-thought reasoning and\nsemantic roles to align with the requirements of formal logic creation. This\napproach enhances the transparency and rationale behind LTL generation,\nfostering user trust. CoT-TL achieves state-of-the-art accuracy across three\ndiverse datasets in low-data scenarios, outperforming existing methods without\nfine-tuning or intermediate translations. To improve reliability and minimize\nhallucinations, we incorporate model checking to validate the syntax of the\ngenerated LTL output. We further demonstrate CoT-TL's effectiveness through\nablation studies and evaluations on unseen LTL structures and formulas in a new\ndataset. Finally, we validate CoT-TL's practicality by integrating it into a\nQuadCopter for multi-step drone planning based on natural language\ninstructions.\n","authors":["Kumar Manas","Stefan Zwicklbauer","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2410.16207v1.pdf","comment":"Accepted for publication in Proceedings of the 2024 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2024), Abu\n  Dhabi 14-18 October 2024"},{"id":"http://arxiv.org/abs/2410.16204v1","updated":"2024-10-21T17:05:50Z","published":"2024-10-21T17:05:50Z","title":"Systematic Review: Text Processing Algorithms in Machine Learning and\n  Deep Learning for Mental Health Detection on Social Media","summary":"  The global rise in depression necessitates innovative detection methods for\nearly intervention. Social media provides a unique opportunity to identify\ndepression through user-generated posts. This systematic review evaluates\nmachine learning (ML) models for depression detection on social media, focusing\non biases and methodological challenges throughout the ML lifecycle. A search\nof PubMed, IEEE Xplore, and Google Scholar identified 47 relevant studies\npublished after 2010. The Prediction model Risk Of Bias ASsessment Tool\n(PROBAST) was utilized to assess methodological quality and risk of bias.\nSignificant biases impacting model reliability and generalizability were found.\nThere is a predominant reliance on Twitter (63.8%) and English-language content\n(over 90%), with most studies focusing on users from the United States and\nEurope. Non-probability sampling methods (approximately 80%) limit\nrepresentativeness. Only 23% of studies explicitly addressed linguistic nuances\nlike negations, crucial for accurate sentiment analysis. Inconsistent\nhyperparameter tuning was observed, with only 27.7% properly tuning models.\nAbout 17% did not adequately partition data into training, validation, and test\nsets, risking overfitting. While 74.5% used appropriate evaluation metrics for\nimbalanced data, others relied on accuracy without addressing class imbalance,\npotentially skewing results. Reporting transparency varied, often lacking\ncritical methodological details. These findings highlight the need to diversify\ndata sources, standardize preprocessing protocols, ensure consistent model\ndevelopment practices, address class imbalance, and enhance reporting\ntransparency. By overcoming these challenges, future research can develop more\nrobust and generalizable ML models for depression detection on social media,\ncontributing to improved mental health outcomes globally.\n","authors":["Yuchen Cao","Jianglai Dai","Zhongyan Wang","Yeyubei Zhang","Xiaorui Shen","Yunchong Liu","Yexin Tian"],"pdf_url":"https://arxiv.org/pdf/2410.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19323v2","updated":"2024-10-21T17:05:15Z","published":"2024-05-29T17:54:22Z","title":"Are Large Language Models Chameleons? An Attempt to Simulate Social\n  Surveys","summary":"  Can large language models (LLMs) simulate social surveys? To answer this\nquestion, we conducted millions of simulations in which LLMs were asked to\nanswer subjective questions. A comparison of different LLM responses with the\nEuropean Social Survey (ESS) data suggests that the effect of prompts on bias\nand variability is fundamental, highlighting major cultural, age, and gender\nbiases. We further discussed statistical methods for measuring the difference\nbetween LLM answers and survey data and proposed a novel measure inspired by\nJaccard similarity, as LLM-generated responses are likely to have a smaller\nvariance. Our experiments also reveal that it is important to analyze the\nrobustness and variability of prompts before using LLMs to simulate social\nsurveys, as their imitation abilities are approximate at best.\n","authors":["Mingmeng Geng","Sihong He","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2405.19323v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.16201v1","updated":"2024-10-21T17:03:20Z","published":"2024-10-21T17:03:20Z","title":"Theoretical Limitations of Ensembles in the Age of Overparameterization","summary":"  Classic tree-based ensembles generalize better than any single decision tree.\nIn contrast, recent empirical studies find that modern ensembles of\n(overparameterized) neural networks may not provide any inherent generalization\nadvantage over single but larger neural networks. This paper clarifies how\nmodern overparameterized ensembles differ from their classic underparameterized\ncounterparts, using ensembles of random feature (RF) regressors as a basis for\ndeveloping theory. In contrast to the underparameterized regime, where\nensembling typically induces regularization and increases generalization, we\nprove that infinite ensembles of overparameterized RF regressors become\npointwise equivalent to (single) infinite-width RF regressors. This\nequivalence, which is exact for ridgeless models and approximate for small\nridge penalties, implies that overparameterized ensembles and single large\nmodels exhibit nearly identical generalization. As a consequence, we can\ncharacterize the predictive variance amongst ensemble members, and demonstrate\nthat it quantifies the expected effects of increasing capacity rather than\ncapturing any conventional notion of uncertainty. Our results challenge common\nassumptions about the advantages of ensembles in overparameterized settings,\nprompting a reconsideration of how well intuitions from underparameterized\nensembles transfer to deep ensembles and the overparameterized regime.\n","authors":["Niclas Dern","John P. Cunningham","Geoff Pleiss"],"pdf_url":"https://arxiv.org/pdf/2410.16201v1.pdf","comment":"26 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.16195v1","updated":"2024-10-21T16:59:01Z","published":"2024-10-21T16:59:01Z","title":"A Trust-Region Method for Graphical Stein Variational Inference","summary":"  Stein variational inference (SVI) is a sample-based approximate Bayesian\ninference technique that generates a sample set by jointly optimizing the\nsamples' locations to minimize an information-theoretic measure of discrepancy\nwith the target probability distribution. SVI thus provides a fast and\nsignificantly more sample-efficient approach to Bayesian inference than\ntraditional (random-sampling-based) alternatives. However, the optimization\ntechniques employed in existing SVI methods struggle to address problems in\nwhich the target distribution is high-dimensional, poorly-conditioned, or\nnon-convex, which severely limits the range of their practical applicability.\nIn this paper, we propose a novel trust-region optimization approach for SVI\nthat successfully addresses each of these challenges. Our method builds upon\nprior work in SVI by leveraging conditional independences in the target\ndistribution (to achieve high-dimensional scaling) and second-order information\n(to address poor conditioning), while additionally providing an effective\nadaptive step control procedure, which is essential for ensuring convergence on\nchallenging non-convex optimization problems. Experimental results show our\nmethod achieves superior numerical performance, both in convergence rate and\nsample accuracy, and scales better in high-dimensional distributions, than\nprevious SVI techniques.\n","authors":["Liam Pavlovic","David M. Rosen"],"pdf_url":"https://arxiv.org/pdf/2410.16195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13791v3","updated":"2024-10-21T16:55:31Z","published":"2024-06-19T19:35:14Z","title":"IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards\n  for Better Well-Being","summary":"  Sustainable Development Goals (SDGs) give the UN a road map for development\nwith Agenda 2030 as a target. SDG3 \"Good Health and Well-Being\" ensures healthy\nlives and promotes well-being for all ages. Digital technologies can support\nSDG3. Burnout and even depression could be reduced by encouraging better\npreventive health. Due to the lack of patient knowledge and focus to take care\nof their health, it is necessary to help patients before it is too late. New\ntrends such as positive psychology and mindfulness are highly encouraged in the\nUSA. Digital Twins (DTs) can help with the continuous monitoring of emotion\nusing physiological signals (e.g., collected via wearables). DTs facilitate\nmonitoring and provide constant health insight to improve quality of life and\nwell-being with better personalization. Healthcare DTs challenges are\nstandardizing data formats, communication protocols, and data exchange\nmechanisms. As an example, ISO has the ISO/IEC JTC 1/SC 41 Internet of Things\n(IoT) and DTs Working Group, with standards such as \"ISO/IEC 21823-3:2021 IoT -\nInteroperability for IoT Systems - Part 3 Semantic interoperability\", \"ISO/IEC\nCD 30178 - IoT - Data format, value and coding\". To achieve those data\nintegration and knowledge challenges, we designed the Mental Health Knowledge\nGraph (ontology and dataset) to boost mental health. As an example, explicit\nknowledge is described such as chocolate contains magnesium which is\nrecommended for depression. The Knowledge Graph (KG) acquires knowledge from\nontology-based mental health projects classified within the LOV4IoT ontology\ncatalog (Emotion, Depression, and Mental Health). Furthermore, the KG is mapped\nto standards when possible. Standards from ETSI SmartM2M can be used such as\nSAREF4EHAW to represent medical devices and sensors, but also ITU/WHO, ISO,\nW3C, NIST, and IEEE standards relevant to mental health can be considered.\n","authors":["Amelie Gyrard","Seyedali Mohammadi","Manas Gaur","Antonio Kung"],"pdf_url":"https://arxiv.org/pdf/2406.13791v3.pdf","comment":"20 pages, Book chapter, Smart Technologies for Achieving Good Health\n  and Well-Being: Towards Sustainable Development Goal, Taylor & Francis"},{"id":"http://arxiv.org/abs/2409.18169v3","updated":"2024-10-21T16:51:22Z","published":"2024-09-26T17:55:22Z","title":"Harmful Fine-tuning Attacks and Defenses for Large Language Models: A\n  Survey","summary":"  Recent research demonstrates that the nascent fine-tuning-as-a-service\nbusiness model exposes serious safety concerns -- fine-tuning over a few\nharmful data uploaded by the users can compromise the safety alignment of the\nmodel. The attack, known as harmful fine-tuning, has raised a broad research\ninterest among the community. However, as the attack is still new, \\textbf{we\nobserve from our miserable submission experience that there are general\nmisunderstandings within the research community.} We in this paper aim to clear\nsome common concerns for the attack setting, and formally establish the\nresearch problem. Specifically, we first present the threat model of the\nproblem, and introduce the harmful fine-tuning attack and its variants. Then we\nsystematically survey the existing literature on attacks/defenses/mechanical\nanalysis of the problem. Finally, we outline future research directions that\nmight contribute to the development of the field. Additionally, we present a\nlist of questions of interest, which might be useful to refer to when reviewers\nin the peer review process question the realism of the\nexperiment/attack/defense setting. A curated list of relevant papers is\nmaintained and made accessible at:\n\\url{https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers}.\n","authors":["Tiansheng Huang","Sihao Hu","Fatih Ilhan","Selim Furkan Tekin","Ling Liu"],"pdf_url":"https://arxiv.org/pdf/2409.18169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20539v2","updated":"2024-10-21T16:44:58Z","published":"2024-05-30T23:31:25Z","title":"SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement\n  Learning Agents","summary":"  Reinforcement learning (RL) is an actively growing field that is seeing\nincreased usage in real-world, safety-critical applications -- making it\nparamount to ensure the robustness of RL algorithms against adversarial\nattacks. In this work we explore a particularly stealthy form of training-time\nattacks against RL -- backdoor poisoning. Here the adversary intercepts the\ntraining of an RL agent with the goal of reliably inducing a particular action\nwhen the agent observes a pre-determined trigger at inference time. We uncover\ntheoretical limitations of prior work by proving their inability to generalize\nacross domains and MDPs. Motivated by this, we formulate a novel poisoning\nattack framework which interlinks the adversary's objectives with those of\nfinding an optimal policy -- guaranteeing attack success in the limit. Using\ninsights from our theoretical analysis we develop ``SleeperNets'' as a\nuniversal backdoor attack which exploits a newly proposed threat model and\nleverages dynamic reward poisoning techniques. We evaluate our attack in 6\nenvironments spanning multiple domains and demonstrate significant improvements\nin attack success over existing methods, while preserving benign episodic\nreturn.\n","authors":["Ethan Rathbun","Christopher Amato","Alina Oprea"],"pdf_url":"https://arxiv.org/pdf/2405.20539v2.pdf","comment":"23 pages, 14 figures, NeurIPS"},{"id":"http://arxiv.org/abs/2410.16179v1","updated":"2024-10-21T16:44:51Z","published":"2024-10-21T16:44:51Z","title":"MagicPIG: LSH Sampling for Efficient LLM Generation","summary":"  Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.\n","authors":["Zhuoming Chen","Ranajoy Sadhukhan","Zihao Ye","Yang Zhou","Jianyu Zhang","Niklas Nolte","Yuandong Tian","Matthijs Douze","Leon Bottou","Zhihao Jia","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14540v2","updated":"2024-10-21T16:40:09Z","published":"2024-05-23T13:22:59Z","title":"This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian\n  Optimization","summary":"  Bayesian Optimization (BO) has proven to be very successful at optimizing a\nstatic, noisy, costly-to-evaluate black-box function $f : \\mathcal{S} \\to\n\\mathbb{R}$. However, optimizing a black-box which is also a function of time\n(i.e., a dynamic function) $f : \\mathcal{S} \\times \\mathcal{T} \\to \\mathbb{R}$\nremains a challenge, since a dynamic Bayesian Optimization (DBO) algorithm has\nto keep track of the optimum over time. This changes the nature of the\noptimization problem in at least three aspects: (i) querying an arbitrary point\nin $\\mathcal{S} \\times \\mathcal{T}$ is impossible, (ii) past observations\nbecome less and less relevant for keeping track of the optimum as time goes by\nand (iii) the DBO algorithm must have a high sampling frequency so it can\ncollect enough relevant observations to keep track of the optimum through time.\nIn this paper, we design a Wasserstein distance-based criterion able to\nquantify the relevancy of an observation with respect to future predictions.\nThen, we leverage this criterion to build W-DBO, a DBO algorithm able to remove\nirrelevant observations from its dataset on the fly, thus maintaining\nsimultaneously a good predictive performance and a high sampling frequency,\neven in continuous-time optimization tasks with unknown horizon. Numerical\nexperiments establish the superiority of W-DBO, which outperforms\nstate-of-the-art methods by a comfortable margin.\n","authors":["Anthony Bardou","Patrick Thiran","Giovanni Ranieri"],"pdf_url":"https://arxiv.org/pdf/2405.14540v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07059v2","updated":"2024-10-21T16:34:48Z","published":"2024-07-09T17:31:47Z","title":"Differentiable Optimization of Similarity Scores Between Models and\n  Brains","summary":"  How do we know if two systems - biological or artificial - process\ninformation in a similar way? Similarity measures such as linear regression,\nCentered Kernel Alignment (CKA), Normalized Bures Similarity (NBS), and angular\nProcrustes distance, are often used to quantify this similarity. However, it is\ncurrently unclear what drives high similarity scores and even what constitutes\na \"good\" score. Here, we introduce a novel tool to investigate these questions\nby differentiating through similarity measures to directly maximize the score.\nSurprisingly, we find that high similarity scores do not guarantee encoding\ntask-relevant information in a manner consistent with neural data; and this is\nparticularly acute for CKA and even some variations of cross-validated and\nregularized linear regression. We find no consistent threshold for a good\nsimilarity score - it depends on both the measure and the dataset. In addition,\nsynthetic datasets optimized to maximize similarity scores initially learn the\nhighest variance principal component of the target dataset, but some methods\nlike angular Procrustes capture lower variance dimensions much earlier than\nmethods like CKA. To shed light on this, we mathematically derive the\nsensitivity of CKA, angular Procrustes, and NBS to the variance of principal\ncomponent dimensions, and explain the emphasis CKA places on high variance\ncomponents. Finally, by jointly optimizing multiple similarity measures, we\ncharacterize their allowable ranges and reveal that some similarity measures\nare more constraining than others. While current measures offer a seemingly\nstraightforward way to quantify the similarity between neural systems, our work\nunderscores the need for careful interpretation. We hope the tools we developed\nwill be used by practitioners to better understand current and future\nsimilarity measures.\n","authors":["Nathan Cloos","Moufan Li","Markus Siegel","Scott L. Brincat","Earl K. Miller","Guangyu Robert Yang","Christopher J. Cueva"],"pdf_url":"https://arxiv.org/pdf/2407.07059v2.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.16195v2","updated":"2024-10-21T16:32:24Z","published":"2024-05-25T11:57:43Z","title":"Adaptive $Q$-Network: On-the-fly Target Selection for Deep Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) is well known for being highly sensitive to\nhyperparameters, requiring practitioners substantial efforts to optimize them\nfor the problem at hand. This also limits the applicability of RL in real-world\nscenarios. In recent years, the field of automated Reinforcement Learning\n(AutoRL) has grown in popularity by trying to address this issue. However,\nthese approaches typically hinge on additional samples to select\nwell-performing hyperparameters, hindering sample-efficiency and practicality.\nFurthermore, most AutoRL methods are heavily based on already existing AutoML\nmethods, which were originally developed neglecting the additional challenges\ninherent to RL due to its non-stationarities. In this work, we propose a new\napproach for AutoRL, called Adaptive $Q$-Network (AdaQN), that is tailored to\nRL to take into account the non-stationarity of the optimization procedure\nwithout requiring additional samples. AdaQN learns several $Q$-functions, each\none trained with different hyperparameters, which are updated online using the\n$Q$-function with the smallest approximation error as a shared target. Our\nselection scheme simultaneously handles different hyperparameters while coping\nwith the non-stationarity induced by the RL optimization procedure and being\northogonal to any critic-based RL algorithm. We demonstrate that AdaQN is\ntheoretically sound and empirically validate it in MuJoCo control problems and\nAtari $2600$ games, showing benefits in sample-efficiency, overall performance,\nrobustness to stochasticity and training stability.\n","authors":["Théo Vincent","Fabian Wahren","Jan Peters","Boris Belousov","Carlo D'Eramo"],"pdf_url":"https://arxiv.org/pdf/2405.16195v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.13995v2","updated":"2024-10-21T16:27:48Z","published":"2024-10-17T19:50:28Z","title":"Adversarial Inception for Bounded Backdoor Poisoning in Deep\n  Reinforcement Learning","summary":"  Recent works have demonstrated the vulnerability of Deep Reinforcement\nLearning (DRL) algorithms against training-time, backdoor poisoning attacks.\nThese attacks induce pre-determined, adversarial behavior in the agent upon\nobserving a fixed trigger during deployment while allowing the agent to solve\nits intended task during training. Prior attacks rely on arbitrarily large\nperturbations to the agent's rewards to achieve both of these objectives -\nleaving them open to detection. Thus, in this work, we propose a new class of\nbackdoor attacks against DRL which achieve state of the art performance while\nminimally altering the agent's rewards. These \"inception\" attacks train the\nagent to associate the targeted adversarial behavior with high returns by\ninducing a disjunction between the agent's chosen action and the true action\nexecuted in the environment during training. We formally define these attacks\nand prove they can achieve both adversarial objectives. We then devise an\nonline inception attack which significantly out-performs prior attacks under\nbounded reward constraints.\n","authors":["Ethan Rathbun","Christopher Amato","Alina Oprea"],"pdf_url":"https://arxiv.org/pdf/2410.13995v2.pdf","comment":"10 pages, 5 figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2410.16161v1","updated":"2024-10-21T16:25:14Z","published":"2024-10-21T16:25:14Z","title":"DMM: Distributed Matrix Mechanism for Differentially-Private Federated\n  Learning using Packed Secret Sharing","summary":"  Federated Learning (FL) has gained lots of traction recently, both in\nindustry and academia. In FL, a machine learning model is trained using data\nfrom various end-users arranged in committees across several rounds. Since such\ndata can often be sensitive, a primary challenge in FL is providing privacy\nwhile still retaining utility of the model. Differential Privacy (DP) has\nbecome the main measure of privacy in the FL setting. DP comes in two flavors:\ncentral and local. In the former, a centralized server is trusted to receive\nthe users' raw gradients from a training step, and then perturb their\naggregation with some noise before releasing the next version of the model. In\nthe latter (more private) setting, noise is applied on users' local devices,\nand only the aggregation of users' noisy gradients is revealed even to the\nserver. Great strides have been made in increasing the privacy-utility\ntrade-off in the central DP setting, by utilizing the so-called matrix\nmechanism. However, progress has been mostly stalled in the local DP setting.\nIn this work, we introduce the distributed matrix mechanism to achieve the\nbest-of-both-worlds; local DP and also better privacy-utility trade-off from\nthe matrix mechanism. We accomplish this by proposing a cryptographic protocol\nthat securely transfers sensitive values across rounds, which makes use of\npacked secret sharing. This protocol accommodates the dynamic participation of\nusers per training round required by FL, including those that may drop out from\nthe computation. We provide experiments which show that our mechanism indeed\nsignificantly improves the privacy-utility trade-off of FL models compared to\nprevious local DP mechanisms, with little added overhead.\n","authors":["Alexander Bienstock","Ujjwal Kumar","Antigoni Polychroniadou"],"pdf_url":"https://arxiv.org/pdf/2410.16161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16159v1","updated":"2024-10-21T16:22:19Z","published":"2024-10-21T16:22:19Z","title":"Metric as Transform: Exploring beyond Affine Transform for Interpretable\n  Neural Network","summary":"  Artificial Neural Networks of varying architectures are generally paired with\naffine transformation at the core. However, we find dot product neurons with\nglobal influence less interpretable as compared to local influence of euclidean\ndistance (as used in Radial Basis Function Network). In this work, we explore\nthe generalization of dot product neurons to $l^p$-norm, metrics, and beyond.\nWe find that metrics as transform performs similarly to affine transform when\nused in MultiLayer Perceptron or Convolutional Neural Network. Moreover, we\nexplore various properties of Metrics, compare it with Affine, and present\nmultiple cases where metrics seem to provide better interpretability. We\ndevelop an interpretable local dictionary based Neural Networks and use it to\nunderstand and reject adversarial examples.\n","authors":["Suman Sapkota"],"pdf_url":"https://arxiv.org/pdf/2410.16159v1.pdf","comment":"22 pages, 20 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.16154v1","updated":"2024-10-21T16:21:09Z","published":"2024-10-21T16:21:09Z","title":"Unsupervised Replay Strategies for Continual Learning with Limited Data","summary":"  Artificial neural networks (ANNs) show limited performance with scarce or\nimbalanced training data and face challenges with continuous learning, such as\nforgetting previously learned data after new tasks training. In contrast, the\nhuman brain can learn continuously and from just a few examples. This research\nexplores the impact of 'sleep', an unsupervised phase incorporating stochastic\nactivation with local Hebbian learning rules, on ANNs trained incrementally\nwith limited and imbalanced datasets, specifically MNIST and Fashion MNIST. We\ndiscovered that introducing a sleep phase significantly enhanced accuracy in\nmodels trained with limited data. When a few tasks were trained sequentially,\nsleep replay not only rescued previously learned information that had been\ncatastrophically forgetting following new task training but often enhanced\nperformance in prior tasks, especially those trained with limited data. This\nstudy highlights the multifaceted role of sleep replay in augmenting learning\nefficiency and facilitating continual learning in ANNs.\n","authors":["Anthony Bazhenov","Pahan Dewasurendra","Giri P. Krishnan","Jean Erik Delanois"],"pdf_url":"https://arxiv.org/pdf/2410.16154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16152v1","updated":"2024-10-21T16:19:34Z","published":"2024-10-21T16:19:34Z","title":"Warped Diffusion: Solving Video Inverse Problems with Image Diffusion\n  Models","summary":"  Using image models naively for solving inverse video problems often suffers\nfrom flickering, texture-sticking, and temporal inconsistency in generated\nvideos. To tackle these problems, in this paper, we view frames as continuous\nfunctions in the 2D space, and videos as a sequence of continuous warping\ntransformations between different frames. This perspective allows us to train\nfunction space diffusion models only on images and utilize them to solve\ntemporally correlated inverse problems. The function space diffusion models\nneed to be equivariant with respect to the underlying spatial transformations.\nTo ensure temporal consistency, we introduce a simple post-hoc test-time\nguidance towards (self)-equivariant solutions. Our method allows us to deploy\nstate-of-the-art latent diffusion models such as Stable Diffusion XL to solve\nvideo inverse problems. We demonstrate the effectiveness of our method for\nvideo inpainting and $8\\times$ video super-resolution, outperforming existing\ntechniques based on noise transformations. We provide generated video results:\nhttps://giannisdaras.github.io/warped\\_diffusion.github.io/.\n","authors":["Giannis Daras","Weili Nie","Karsten Kreis","Alex Dimakis","Morteza Mardani","Nikola Borislavov Kovachki","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2410.16152v1.pdf","comment":"Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16151v1","updated":"2024-10-21T16:18:31Z","published":"2024-10-21T16:18:31Z","title":"Small Contributions, Small Networks: Efficient Neural Network Pruning\n  Based on Relative Importance","summary":"  Recent advancements have scaled neural networks to unprecedented sizes,\nachieving remarkable performance across a wide range of tasks. However,\ndeploying these large-scale models on resource-constrained devices poses\nsignificant challenges due to substantial storage and computational\nrequirements. Neural network pruning has emerged as an effective technique to\nmitigate these limitations by reducing model size and complexity. In this\npaper, we introduce an intuitive and interpretable pruning method based on\nactivation statistics, rooted in information theory and statistical analysis.\nOur approach leverages the statistical properties of neuron activations to\nidentify and remove weights with minimal contributions to neuron outputs.\nSpecifically, we build a distribution of weight contributions across the\ndataset and utilize its parameters to guide the pruning process. Furthermore,\nwe propose a Pruning-aware Training strategy that incorporates an additional\nregularization term to enhance the effectiveness of our pruning method.\nExtensive experiments on multiple datasets and network architectures\ndemonstrate that our method consistently outperforms several baseline and\nstate-of-the-art pruning techniques.\n","authors":["Mostafa Hussien","Mahmoud Afifi","Kim Khoa Nguyen","Mohamed Cheriet"],"pdf_url":"https://arxiv.org/pdf/2410.16151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16150v1","updated":"2024-10-21T16:18:19Z","published":"2024-10-21T16:18:19Z","title":"Modelling Structured Data Learning with Restricted Boltzmann Machines in\n  the Teacher-Student Setting","summary":"  Restricted Boltzmann machines (RBM) are generative models capable to learn\ndata with a rich underlying structure. We study the teacher-student setting\nwhere a student RBM learns structured data generated by a teacher RBM. The\namount of structure in the data is controlled by adjusting the number of hidden\nunits of the teacher and the correlations in the rows of the weights, a.k.a.\npatterns. In the absence of correlations, we validate the conjecture that the\nperformance is independent of the number of teacher patters and hidden units of\nthe student RBMs, and we argue that the teacher-student setting can be used as\na toy model for studying the lottery ticket hypothesis. Beyond this regime, we\nfind that the critical amount of data required to learn the teacher patterns\ndecreases with both their number and correlations. In both regimes, we find\nthat, even with an relatively large dataset, it becomes impossible to learn the\nteacher patterns if the inference temperature used for regularization is kept\ntoo low. In our framework, the student can learn teacher patterns one-to-one or\nmany-to-one, generalizing previous findings about the teacher-student setting\nwith two hidden units to any arbitrary finite number of hidden units.\n","authors":["Robin Thériault","Francesco Tosello","Daniele Tantari"],"pdf_url":"https://arxiv.org/pdf/2410.16150v1.pdf","comment":"51 pages, 21 figures"},{"id":"http://arxiv.org/abs/2410.16146v1","updated":"2024-10-21T16:17:01Z","published":"2024-10-21T16:17:01Z","title":"Towards Combating Frequency Simplicity-biased Learning for Domain\n  Generalization","summary":"  Domain generalization methods aim to learn transferable knowledge from source\ndomains that can generalize well to unseen target domains. Recent studies show\nthat neural networks frequently suffer from a simplicity-biased learning\nbehavior which leads to over-reliance on specific frequency sets, namely as\nfrequency shortcuts, instead of semantic information, resulting in poor\ngeneralization performance. Despite previous data augmentation techniques\nsuccessfully enhancing generalization performances, they intend to apply more\nfrequency shortcuts, thereby causing hallucinations of generalization\nimprovement. In this paper, we aim to prevent such learning behavior of\napplying frequency shortcuts from a data-driven perspective. Given the\ntheoretical justification of models' biased learning behavior on different\nspatial frequency components, which is based on the dataset frequency\nproperties, we argue that the learning behavior on various frequency components\ncould be manipulated by changing the dataset statistical structure in the\nFourier domain. Intuitively, as frequency shortcuts are hidden in the dominant\nand highly dependent frequencies of dataset structure, dynamically perturbating\nthe over-reliance frequency components could prevent the application of\nfrequency shortcuts. To this end, we propose two effective data augmentation\nmodules designed to collaboratively and adaptively adjust the frequency\ncharacteristic of the dataset, aiming to dynamically influence the learning\nbehavior of the model and ultimately serving as a strategy to mitigate shortcut\nlearning. Code is available at AdvFrequency\n(https://github.com/C0notSilly/AdvFrequency).\n","authors":["Xilin He","Jingyu Hu","Qinliang Lin","Cheng Luo","Weicheng Xie","Siyang Song","Muhammad Haris Khan","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2410.16146v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16138v1","updated":"2024-10-21T16:04:50Z","published":"2024-10-21T16:04:50Z","title":"Theoretical Insights into Line Graph Transformation on Graph Learning","summary":"  Line graph transformation has been widely studied in graph theory, where each\nnode in a line graph corresponds to an edge in the original graph. This has\ninspired a series of graph neural networks (GNNs) applied to transformed line\ngraphs, which have proven effective in various graph representation learning\ntasks. However, there is limited theoretical study on how line graph\ntransformation affects the expressivity of GNN models. In this study, we focus\non two types of graphs known to be challenging to the Weisfeiler-Leman (WL)\ntests: Cai-F\\\"urer-Immerman (CFI) graphs and strongly regular graphs, and show\nthat applying line graph transformation helps exclude these challenging graph\nproperties, thus potentially assist WL tests in distinguishing these graphs. We\nempirically validate our findings by conducting a series of experiments that\ncompare the accuracy and efficiency of graph isomorphism tests and GNNs on both\nline-transformed and original graphs across these graph structure types.\n","authors":["Fan Yang","Xingyue Huang"],"pdf_url":"https://arxiv.org/pdf/2410.16138v1.pdf","comment":"21 pages, code available at\n  https://github.com/lukeyf/graphs-and-lines"},{"id":"http://arxiv.org/abs/2410.16135v1","updated":"2024-10-21T16:00:04Z","published":"2024-10-21T16:00:04Z","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference\n  on GPUs","summary":"  To date, 2:4 sparsity has stood as the only sparse pattern that can be\naccelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often\npossesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios,\nmeaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity,\ndo not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity\nis promising in addressing these limitations of 2:4 sparsity. However,\nregarding accuracy, the effects of V:N:M sparsity on broader Transformer\nmodels, such as vision Transformers and large language models (LLMs), are\nlargely unexamined. Moreover, Some specific issues related to V:N:M sparsity,\nsuch as how to select appropriate V and M values, remain unresolved. In this\nstudy, we thoroughly investigate the application of V:N:M sparsity in vision\nmodels and LLMs across multiple tasks, from pertaining to downstream tasks. We\npropose three key approaches to enhance the applicability and accuracy of\nV:N:M-sparse Transformers, including heuristic V and M selection,\nV:N:M-specific channel permutation, and three-staged LoRA training techniques.\nExperimental results show that, with our methods, the DeiT-small achieves\nlossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy\neven at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5\nsparsity performs comparably or better than training-free 2:4 sparse\nalternatives on downstream tasks. More importantly, V:N:M-sparse Transformers\noffer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity.\nOverall, our exploration largely facilitates the V:N:M sparsity to act as a\ntruly effective acceleration solution for Transformers in cost-sensitive\ninference scenarios.\n","authors":["Kang Zhao","Tao Yuan","Han Bao","Zhenfeng Su","Chang Gao","Zhaofeng Sun","Zichen Liang","Liping Jing","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14134v2","updated":"2024-10-21T15:59:18Z","published":"2024-08-26T09:29:56Z","title":"Exploring the Potential of Large Language Models for Heterophilic Graphs","summary":"  Large language models (LLMs) have presented significant opportunities to\nenhance various machine learning applications, including graph neural networks\n(GNNs). By leveraging the vast open-world knowledge within LLMs, we can more\neffectively interpret and utilize textual data to better characterize\nheterophilic graphs, where neighboring nodes often have different labels.\nHowever, existing approaches for heterophilic graphs overlook the rich textual\ndata associated with nodes, which could unlock deeper insights into their\nheterophilic contexts. In this work, we explore the potential of LLMs for\nmodeling heterophilic graphs and propose a novel two-stage framework:\nLLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first\nstage, we fine-tune the LLM to better identify homophilic and heterophilic\nedges based on the textual content of their nodes. In the second stage, we\nadaptively manage message propagation in GNNs for different edge types based on\nnode features, structures, and heterophilic or homophilic characteristics. To\ncope with the computational demands when deploying LLMs in practical scenarios,\nwe further explore model distillation techniques to fine-tune smaller, more\nefficient models that maintain competitive performance. Extensive experiments\nvalidate the effectiveness of our framework, demonstrating the feasibility of\nusing LLMs to enhance node classification on heterophilic graphs.\n","authors":["Yuxia Wu","Shujie Li","Yuan Fang","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.14134v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.13502v2","updated":"2024-10-21T15:58:30Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to problems that\nare more complex than the ones on which they have been trained. Empirical\ninvestigations of such questions are impeded by two major flaws of current\nevaluations: (i) much of the evaluation data is contaminated, in the sense that\nit has already been seen during training, and (ii) benchmark datasets do not\ncapture how problem proofs may be arbitrarily complex in various ways. As a\nstep towards addressing these issues, we present a framework for evaluating\nLLMs on problems with arbitrarily complex arithmetic proofs, called MathGAP.\nMathGAP generates problems that follow fixed proof specifications -- along with\nchain-of-thought reasoning annotations -- enabling systematic studies on\ngeneralization with respect to arithmetic proof complexity. We apply MathGAP to\nanalyze how in-context learning interacts with generalization to problems that\nhave more complex proofs. We find that among the models tested, most show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for GPT-4o. Surprisingly, providing in-context examples from\nthe same distribution as the test set is not always beneficial for performance.\nIn particular, zero-shot prompting as well as demonstrating a diverse range of\nexamples that are less complex than the test data sometimes yield similar or\nhigher accuracies.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schölkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.00299v4","updated":"2024-10-21T15:56:23Z","published":"2024-06-29T03:37:29Z","title":"Human-Agent Joint Learning for Efficient Robot Manipulation Skill\n  Acquisition","summary":"  Employing a teleoperation system for gathering demonstrations offers the\npotential for more efficient learning of robot manipulation. However,\nteleoperating a robot arm equipped with a dexterous hand or gripper, via a\nteleoperation system presents inherent challenges due to the task's high\ndimensionality, complexity of motion, and differences between physiological\nstructures. In this study, we introduce a novel system for joint learning\nbetween human operators and robots, that enables human operators to share\ncontrol of a robot end-effector with a learned assistive agent, simplifies the\ndata collection process, and facilitates simultaneous human demonstration\ncollection and robot manipulation training. As data accumulates, the assistive\nagent gradually learns. Consequently, less human effort and attention are\nrequired, enhancing the efficiency of the data collection process. It also\nallows the human operator to adjust the control ratio to achieve a trade-off\nbetween manual and automated control. We conducted experiments in both\nsimulated environments and physical real-world settings. Through user studies\nand quantitative evaluations, it is evident that the proposed system could\nenhance data collection efficiency and reduce the need for human adaptation\nwhile ensuring the collected data is of sufficient quality for downstream\ntasks. \\textit{For more details, please refer to our webpage\nhttps://norweig1an.github.io/HAJL.github.io/.\n","authors":["Shengcheng Luo","Quanquan Peng","Jun Lv","Kaiwen Hong","Katherine Rose Driggs-Campbell","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2407.00299v4.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16128v1","updated":"2024-10-21T15:55:04Z","published":"2024-10-21T15:55:04Z","title":"SMART: Self-learning Meta-strategy Agent for Reasoning Tasks","summary":"  Tasks requiring deductive reasoning, especially those involving multiple\nsteps, often demand adaptive strategies such as intermediate generation of\nrationales or programs, as no single approach is universally optimal. While\nLanguage Models (LMs) can enhance their outputs through iterative\nself-refinement and strategy adjustments, they frequently fail to apply the\nmost effective strategy in their first attempt. This inefficiency raises the\nquestion: Can LMs learn to select the optimal strategy in the first attempt,\nwithout a need for refinement? To address this challenge, we introduce SMART\n(Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that\nenables LMs to autonomously learn and select the most effective strategies for\nvarious reasoning tasks. We model the strategy selection process as a Markov\nDecision Process and leverage reinforcement learning-driven continuous\nself-improvement to allow the model to find the suitable strategy to solve a\ngiven task. Unlike traditional self-refinement methods that rely on multiple\ninference passes or external feedback, SMART allows an LM to internalize the\noutcomes of its own reasoning processes and adjust its strategy accordingly,\naiming for correct solutions on the first attempt. Our experiments across\nvarious reasoning datasets and with different model architectures demonstrate\nthat SMART significantly enhances the ability of models to choose optimal\nstrategies without external guidance (+15 points on the GSM8K dataset). By\nachieving higher accuracy with a single inference pass, SMART not only improves\nperformance but also reduces computational costs for refinement-based\nstrategies, paving the way for more efficient and intelligent reasoning in LMs.\n","authors":["Rongxing Liu","Kumar Shridhar","Manish Prajapat","Patrick Xia","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.16128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16124v1","updated":"2024-10-21T15:51:30Z","published":"2024-10-21T15:51:30Z","title":"MNIST-Nd: a set of naturalistic datasets to benchmark clustering across\n  dimensions","summary":"  Driven by advances in recording technology, large-scale high-dimensional\ndatasets have emerged across many scientific disciplines. Especially in\nbiology, clustering is often used to gain insights into the structure of such\ndatasets, for instance to understand the organization of different cell types.\nHowever, clustering is known to scale poorly to high dimensions, even though\nthe exact impact of dimensionality is unclear as current benchmark datasets are\nmostly two-dimensional. Here we propose MNIST-Nd, a set of synthetic datasets\nthat share a key property of real-world datasets, namely that individual\nsamples are noisy and clusters do not perfectly separate. MNIST-Nd is obtained\nby training mixture variational autoencoders with 2 to 64 latent dimensions on\nMNIST, resulting in six datasets with comparable structure but varying\ndimensionality. It thus offers the chance to disentangle the impact of\ndimensionality on clustering. Preliminary common clustering algorithm\nbenchmarks on MNIST-Nd suggest that Leiden is the most robust for growing\ndimensions.\n","authors":["Polina Turishcheva","Laura Hansel","Martin Ritzert","Marissa A. Weis","Alexander S. Ecker"],"pdf_url":"https://arxiv.org/pdf/2410.16124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16122v1","updated":"2024-10-21T15:50:04Z","published":"2024-10-21T15:50:04Z","title":"Integer linear programming for unsupervised training set selection in\n  molecular machine learning","summary":"  Integer linear programming (ILP) is an elegant approach to solve linear\noptimization problems, naturally described using integer decision variables.\nWithin the context of physics-inspired machine learning applied to chemistry,\nwe demonstrate the relevance of an ILP formulation to select molecular training\nsets for predictions of size-extensive properties. We show that our algorithm\noutperforms existing unsupervised training set selection approaches, especially\nwhen predicting properties of molecules larger than those present in the\ntraining set. We argue that the reason for the improved performance is due to\nthe selection that is based on the notion of local similarity (i.e., per-atom)\nand a unique ILP approach that finds optimal solutions efficiently. Altogether,\nthis work provides a practical algorithm to improve the performance of\nphysics-inspired machine learning models and offers insights into the\nconceptual differences with existing training set selection approaches.\n","authors":["Matthieu Haeberle","Puck van Gerwen","Ruben Laplaza","Ksenia R. Briling","Jan Weinreich","Friedrich Eisenbrand","Clemence Corminboeuf"],"pdf_url":"https://arxiv.org/pdf/2410.16122v1.pdf","comment":"31 pages + SI (15 pages)"},{"id":"http://arxiv.org/abs/2410.16121v1","updated":"2024-10-21T15:48:34Z","published":"2024-10-21T15:48:34Z","title":"Extracting Spatiotemporal Data from Gradients with Large Language Models","summary":"  Recent works show that sensitive user data can be reconstructed from gradient\nupdates, breaking the key privacy promise of federated learning. While success\nwas demonstrated primarily on image data, these methods do not directly\ntransfer to other domains, such as spatiotemporal data. To understand privacy\nrisks in spatiotemporal federated learning, we first propose Spatiotemporal\nGradient Inversion Attack (ST-GIA), a gradient attack algorithm tailored to\nspatiotemporal data that successfully reconstructs the original location from\ngradients. Furthermore, the absence of priors in attacks on spatiotemporal data\nhas hindered the accurate reconstruction of real client data. To address this\nlimitation, we propose ST-GIA+, which utilizes an auxiliary language model to\nguide the search for potential locations, thereby successfully reconstructing\nthe original data from gradients. In addition, we design an adaptive defense\nstrategy to mitigate gradient inversion attacks in spatiotemporal federated\nlearning. By dynamically adjusting the perturbation levels, we can offer\ntailored protection for varying rounds of training data, thereby achieving a\nbetter trade-off between privacy and utility than current state-of-the-art\nmethods. Through intensive experimental analysis on three real-world datasets,\nwe reveal that the proposed defense strategy can well preserve the utility of\nspatiotemporal federated learning with effective security protection.\n","authors":["Lele Zheng","Yang Cao","Renhe Jiang","Kenjiro Taura","Yulong Shen","Sheng Li","Masatoshi Yoshikawa"],"pdf_url":"https://arxiv.org/pdf/2410.16121v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.08529"},{"id":"http://arxiv.org/abs/2410.16119v1","updated":"2024-10-21T15:47:03Z","published":"2024-10-21T15:47:03Z","title":"SeaDAG: Semi-autoregressive Diffusion for Conditional Directed Acyclic\n  Graph Generation","summary":"  We introduce SeaDAG, a semi-autoregressive diffusion model for conditional\ngeneration of Directed Acyclic Graphs (DAGs). Considering their inherent\nlayer-wise structure, we simulate layer-wise autoregressive generation by\ndesigning different denoising speed for different layers. Unlike conventional\nautoregressive generation that lacks a global graph structure view, our method\nmaintains a complete graph structure at each diffusion step, enabling\noperations such as property control that require the full graph structure.\nLeveraging this capability, we evaluate the DAG properties during training by\nemploying a graph property decoder. We explicitly train the model to learn\ngraph conditioning with a condition loss, which enhances the diffusion model's\ncapacity to generate graphs that are both realistic and aligned with specified\nproperties. We evaluate our method on two representative conditional DAG\ngeneration tasks: (1) circuit generation from truth tables, where precise DAG\nstructures are crucial for realizing circuit functionality, and (2) molecule\ngeneration based on quantum properties. Our approach demonstrates promising\nresults, generating high-quality and realistic DAGs that closely align with\ngiven conditions.\n","authors":["Xinyi Zhou","Xing Li","Yingzhao Lian","Yiwen Wang","Lei Chen","Mingxuan Yuan","Jianye Hao","Guangyong Chen","Pheng Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2410.16119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02176v3","updated":"2024-10-21T15:37:16Z","published":"2024-06-04T10:12:09Z","title":"AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local\n  Neural Fields","summary":"  We present AROMA (Attentive Reduced Order Model with Attention), a framework\ndesigned to enhance the modeling of partial differential equations (PDEs) using\nlocal neural fields. Our flexible encoder-decoder architecture can obtain\nsmooth latent representations of spatial physical fields from a variety of data\ntypes, including irregular-grid inputs and point clouds. This versatility\neliminates the need for patching and allows efficient processing of diverse\ngeometries. The sequential nature of our latent representation can be\ninterpreted spatially and permits the use of a conditional transformer for\nmodeling the temporal dynamics of PDEs. By employing a diffusion-based\nformulation, we achieve greater stability and enable longer rollouts compared\nto conventional MSE training. AROMA's superior performance in simulating 1D and\n2D equations underscores the efficacy of our approach in capturing complex\ndynamical behaviors.\n","authors":["Louis Serrano","Thomas X Wang","Etienne Le Naour","Jean-Noël Vittaut","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2406.02176v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16106v1","updated":"2024-10-21T15:34:44Z","published":"2024-10-21T15:34:44Z","title":"Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation","summary":"  Statistical inference with finite-sample validity for the value function of a\ngiven policy in Markov decision processes (MDPs) is crucial for ensuring the\nreliability of reinforcement learning. Temporal Difference (TD) learning,\narguably the most widely used algorithm for policy evaluation, serves as a\nnatural framework for this purpose.In this paper, we study the consistency\nproperties of TD learning with Polyak-Ruppert averaging and linear function\napproximation, and obtain three significant improvements over existing results.\nFirst, we derive a novel sharp high-dimensional probability convergence\nguarantee that depends explicitly on the asymptotic variance and holds under\nweak conditions. We further establish refined high-dimensional Berry-Esseen\nbounds over the class of convex sets that guarantee faster rates than those in\nthe literature. Finally, we propose a plug-in estimator for the asymptotic\ncovariance matrix, designed for efficient online computation. These results\nenable the construction of confidence regions and simultaneous confidence\nintervals for the linear parameters of the value function, with guaranteed\nfinite-sample coverage. We demonstrate the applicability of our theoretical\nfindings through numerical experiments.\n","authors":["Weichen Wu","Gen Li","Yuting Wei","Alessandro Rinaldo"],"pdf_url":"https://arxiv.org/pdf/2410.16106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16105v1","updated":"2024-10-21T15:34:33Z","published":"2024-10-21T15:34:33Z","title":"Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep\n  Learning","summary":"  Deep neural networks (DNNs) suffer from the spectral bias, wherein DNNs\ntypically exhibit a tendency to prioritize the learning of lower-frequency\ncomponents of a function, struggling to capture its high-frequency features.\nThis paper is to address this issue. Notice that a function having only low\nfrequency components may be well-represented by a shallow neural network (SNN),\na network having only a few layers. By observing that composition of low\nfrequency functions can effectively approximate a high-frequency function, we\npropose to learn a function containing high-frequency components by composing\nseveral SNNs, each of which learns certain low-frequency information from the\ngiven data. We implement the proposed idea by exploiting the multi-grade deep\nlearning (MGDL) model, a recently introduced model that trains a DNN\nincrementally, grade by grade, a current grade learning from the residue of the\nprevious grade only an SNN composed with the SNNs trained in the preceding\ngrades as features. We apply MGDL to synthetic, manifold, colored images, and\nMNIST datasets, all characterized by presence of high-frequency features. Our\nstudy reveals that MGDL excels at representing functions containing\nhigh-frequency information. Specifically, the neural networks learned in each\ngrade adeptly capture some low-frequency information, allowing their\ncompositions with SNNs learned in the previous grades effectively representing\nthe high-frequency features. Our experimental results underscore the efficacy\nof MGDL in addressing the spectral bias inherent in DNNs. By leveraging MGDL,\nwe offer insights into overcoming spectral bias limitation of DNNs, thereby\nenhancing the performance and applicability of deep learning models in tasks\nrequiring the representation of high-frequency information. This study confirms\nthat the proposed method offers a promising solution to address the spectral\nbias of DNNs.\n","authors":["Ronglong Fang","Yuesheng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.16105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16103v1","updated":"2024-10-21T15:31:06Z","published":"2024-10-21T15:31:06Z","title":"LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics","summary":"  We introduce LDAdam, a memory-efficient optimizer for training large models,\nthat performs adaptive optimization steps within lower dimensional subspaces,\nwhile consistently exploring the full parameter space during training. This\nstrategy keeps the optimizer's memory footprint to a fraction of the model\nsize. LDAdam relies on a new projection-aware update rule for the optimizer\nstates that allows for transitioning between subspaces, i.e., estimation of the\nstatistics of the projected gradients. To mitigate the errors due to low-rank\nprojection, LDAdam integrates a new generalized error feedback mechanism, which\nexplicitly accounts for both gradient and optimizer state compression. We prove\nthe convergence of LDAdam under standard assumptions, and show that LDAdam\nallows for accurate and efficient fine-tuning and pre-training of language\nmodels.\n","authors":["Thomas Robert","Mher Safaryan","Ionut-Vlad Modoranu","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2410.16103v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2410.16100v1","updated":"2024-10-21T15:27:18Z","published":"2024-10-21T15:27:18Z","title":"ExDBN: Exact learning of Dynamic Bayesian Networks","summary":"  Causal learning from data has received much attention in recent years. One\nway of capturing causal relationships is by utilizing Bayesian networks. There,\none recovers a weighted directed acyclic graph, in which random variables are\nrepresented by vertices, and the weights associated with each edge represent\nthe strengths of the causal relationships between them. This concept is\nextended to capture dynamic effects by introducing a dependency on past data,\nwhich may be captured by the structural equation model, which is utilized in\nthe present contribution to formulate a score-based learning approach. A\nmixed-integer quadratic program is formulated and an algorithmic solution\nproposed, in which the pre-generation of exponentially many acyclicity\nconstraints is avoided by utilizing the so-called branch-and-cut (\"lazy\nconstraint\") method. Comparing the novel approach to the state of the art, we\nshow that the proposed approach turns out to produce excellent results when\napplied to small and medium-sized synthetic instances of up to 25 time-series.\nLastly, two interesting applications in bio-science and finance, to which the\nmethod is directly applied, further stress the opportunities in developing\nhighly accurate, globally convergent solvers that can handle modest instances.\n","authors":["Pavel Rytíř","Aleš Wodecki","Georgios Korpas","Jakub Mareček"],"pdf_url":"https://arxiv.org/pdf/2410.16100v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2402.06955v3","updated":"2024-10-21T15:26:26Z","published":"2024-02-10T13:51:09Z","title":"Feature Mapping in Physics-Informed Neural Networks (PINNs)","summary":"  In this paper, the training dynamics of PINNs with a feature mapping layer\nvia the limiting Conjugate Kernel and Neural Tangent Kernel is investigated,\nshedding light on the convergence of PINNs; Although the commonly used\nFourier-based feature mapping has achieved great success, we show its\ninadequacy in some physics scenarios. Via these two scopes, we propose\nconditionally positive definite Radial Basis Function as a better alternative.\nLastly, we explore the feature mapping numerically in wide neural networks. Our\nempirical results reveal the efficacy of our method in diverse forward and\ninverse problem sets. Composing feature functions is found to be a practical\nway to address the expressivity and generalisability trade-off, viz., tuning\nthe bandwidth of the kernels and the surjectivity of the feature mapping\nfunction. This simple technique can be implemented for coordinate inputs and\nbenefits the broader PINNs research.\n","authors":["Chengxi Zeng","Tilo Burghardt","Alberto M Gambaruto"],"pdf_url":"https://arxiv.org/pdf/2402.06955v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08160v3","updated":"2024-10-21T15:22:58Z","published":"2024-09-12T15:52:22Z","title":"On the Role of Context in Reading Time Prediction","summary":"  We present a new perspective on how readers integrate context during\nreal-time language comprehension. Our proposals build on surprisal theory,\nwhich posits that the processing effort of a linguistic unit (e.g., a word) is\nan affine function of its in-context information content. We first observe that\nsurprisal is only one out of many potential ways that a contextual predictor\ncan be derived from a language model. Another one is the pointwise mutual\ninformation (PMI) between a unit and its context, which turns out to yield the\nsame predictive power as surprisal when controlling for unigram frequency.\nMoreover, both PMI and surprisal are correlated with frequency. This means that\nneither PMI nor surprisal contains information about context alone. In response\nto this, we propose a technique where we project surprisal onto the orthogonal\ncomplement of frequency, yielding a new contextual predictor that is\nuncorrelated with frequency. Our experiments show that the proportion of\nvariance in reading times explained by context is a lot smaller when context is\nrepresented by the orthogonalized predictor. From an interpretability\nstandpoint, this indicates that previous studies may have overstated the role\nthat context has in predicting reading times.\n","authors":["Andreas Opedal","Eleanor Chodroff","Ryan Cotterell","Ethan Gotlieb Wilcox"],"pdf_url":"https://arxiv.org/pdf/2409.08160v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.13203v2","updated":"2024-10-21T15:21:56Z","published":"2024-10-17T04:10:36Z","title":"TabSeq: A Framework for Deep Learning on Tabular Data via Sequential\n  Ordering","summary":"  Effective analysis of tabular data still poses a significant problem in deep\nlearning, mainly because features in tabular datasets are often heterogeneous\nand have different levels of relevance. This work introduces TabSeq, a novel\nframework for the sequential ordering of features, addressing the vital\nnecessity to optimize the learning process. Features are not always equally\ninformative, and for certain deep learning models, their random arrangement can\nhinder the model's learning capacity. Finding the optimum sequence order for\nsuch features could improve the deep learning models' learning process. The\nnovel feature ordering technique we provide in this work is based on clustering\nand incorporates both local ordering and global ordering. It is designed to be\nused with a multi-head attention mechanism in a denoising autoencoder network.\nOur framework uses clustering to align comparable features and improve data\norganization. Multi-head attention focuses on essential characteristics,\nwhereas the denoising autoencoder highlights important aspects by rebuilding\nfrom distorted inputs. This method improves the capability to learn from\ntabular data while lowering redundancy. Our research, demonstrating improved\nperformance through appropriate feature sequence rearrangement using raw\nantibody microarray and two other real-world biomedical datasets, validates the\nimpact of feature ordering. These results demonstrate that feature ordering can\nbe a viable approach to improved deep learning of tabular data.\n","authors":["Al Zadid Sultan Bin Habib","Kesheng Wang","Mary-Anne Hartley","Gianfranco Doretto","Donald A. Adjeroh"],"pdf_url":"https://arxiv.org/pdf/2410.13203v2.pdf","comment":"This paper has been accepted for presentation at the 27th\n  International Conference on Pattern Recognition (ICPR 2024) in Kolkata, India"},{"id":"http://arxiv.org/abs/2408.08381v4","updated":"2024-10-21T15:18:39Z","published":"2024-08-15T18:54:31Z","title":"Pre-processing and Compression: Understanding Hidden Representation\n  Refinement Across Imaging Domains via Intrinsic Dimension","summary":"  In recent years, there has been interest in how geometric properties such as\nintrinsic dimension (ID) of a neural network's hidden representations change\nthrough its layers, and how such properties are predictive of important model\nbehavior such as generalization ability. However, evidence has begun to emerge\nthat such behavior can change significantly depending on the domain of the\nnetwork's training data, such as natural versus medical images. Here, we\nfurther this inquiry by exploring how the ID of a network's learned\nrepresentations changes through its layers, in essence, characterizing how the\nnetwork successively refines the information content of input data to be used\nfor predictions. Analyzing eleven natural and medical image datasets across six\nnetwork architectures, we find that how ID changes through the network differs\nnoticeably between natural and medical image models. Specifically, medical\nimage models peak in representation ID earlier in the network, implying a\ndifference in the image features and their abstractness that are typically used\nfor downstream tasks in these domains. Additionally, we discover a strong\ncorrelation of this peak representation ID with the ID of the data in its input\nspace, implying that the intrinsic information content of a model's learned\nrepresentations is guided by that of the data it was trained on. Overall, our\nfindings emphasize notable discrepancies in network behavior between natural\nand non-natural imaging domains regarding hidden representation information\ncontent, and provide further insights into how a network's learned features are\nshaped by its training data.\n","authors":["Nicholas Konz","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.08381v4.pdf","comment":"Published in NeurIPS 2024 Workshop on Scientific Methods for\n  Understanding Deep Learning (SciForDL)"},{"id":"http://arxiv.org/abs/2406.15819v2","updated":"2024-10-21T15:07:13Z","published":"2024-06-22T11:17:50Z","title":"Automatic AI Model Selection for Wireless Systems: Online Learning via\n  Digital Twinning","summary":"  In modern wireless network architectures, such as O-RAN, artificial\nintelligence (AI)-based applications are deployed at intelligent controllers to\ncarry out functionalities like scheduling or power control. The AI \"apps\" are\nselected on the basis of contextual information such as network conditions,\ntopology, traffic statistics, and design goals. The mapping between context and\nAI model parameters is ideally done in a zero-shot fashion via an automatic\nmodel selection (AMS) mapping that leverages only contextual information\nwithout requiring any current data. This paper introduces a general methodology\nfor the online optimization of AMS mappings. Optimizing an AMS mapping is\nchallenging, as it requires exposure to data collected from many different\ncontexts. Therefore, if carried out online, this initial optimization phase\nwould be extremely time consuming. A possible solution is to leverage a digital\ntwin of the physical system to generate synthetic data from multiple simulated\ncontexts. However, given that the simulator at the digital twin is imperfect, a\ndirect use of simulated data for the optimization of the AMS mapping would\nyield poor performance when tested in the real system. This paper proposes a\nnovel method for the online optimization of AMS mapping that corrects for the\nbias of the simulator by means of limited real data collected from the physical\nsystem. Experimental results for a graph neural network-based power control app\ndemonstrate the significant advantages of the proposed approach.\n","authors":["Qiushuo Hou","Matteo Zecchin","Sangwoo Park","Yunlong Cai","Guanding Yu","Kaushik Chowdhury","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2406.15819v2.pdf","comment":"submitted for a journal publication"},{"id":"http://arxiv.org/abs/2410.16077v1","updated":"2024-10-21T14:55:59Z","published":"2024-10-21T14:55:59Z","title":"CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian\n  Product Routing in Mixture-of-Experts","summary":"  Large language models (LLM) have been attracting much attention from the\ncommunity recently, due to their remarkable performance in all kinds of\ndownstream tasks. According to the well-known scaling law, scaling up a dense\nLLM enhances its capabilities, but also significantly increases the\ncomputational complexity. Mixture-of-Experts (MoE) models address that by\nallowing the model size to grow without substantially raising training or\ninference costs. Yet MoE models face challenges regarding knowledge sharing\namong experts, making their performance somehow sensitive to routing accuracy.\nTo tackle that, previous works introduced shared experts and combined their\noutputs with those of the top $K$ routed experts in an ``addition'' manner. In\nthis paper, inspired by collective matrix factorization to learn shared\nknowledge among data, we propose CartesianMoE, which implements more effective\nknowledge sharing among experts in more like a ``multiplication'' manner.\nExtensive experimental results indicate that CartesianMoE outperforms previous\nMoE models for building LLMs, in terms of both perplexity and downstream task\nperformance. And we also find that CartesianMoE achieves better expert routing\nrobustness.\n","authors":["Zhenpeng Su","Xing Wu","Zijia Lin","Yizhe Xiong","Minxuan Lv","Guangyuan Ma","Hui Chen","Songlin Hu","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2410.16077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16073v1","updated":"2024-10-21T14:53:12Z","published":"2024-10-21T14:53:12Z","title":"On the Geometry of Regularization in Adversarial Training:\n  High-Dimensional Asymptotics and Generalization Bounds","summary":"  Regularization, whether explicit in terms of a penalty in the loss or\nimplicit in the choice of algorithm, is a cornerstone of modern machine\nlearning. Indeed, controlling the complexity of the model class is particularly\nimportant when data is scarce, noisy or contaminated, as it translates a\nstatistical belief on the underlying structure of the data. This work\ninvestigates the question of how to choose the regularization norm $\\lVert\n\\cdot \\rVert$ in the context of high-dimensional adversarial training for\nbinary classification. To this end, we first derive an exact asymptotic\ndescription of the robust, regularized empirical risk minimizer for various\ntypes of adversarial attacks and regularization norms (including non-$\\ell_p$\nnorms). We complement this analysis with a uniform convergence analysis,\nderiving bounds on the Rademacher Complexity for this class of problems.\nLeveraging our theoretical results, we quantitatively characterize the\nrelationship between perturbation size and the optimal choice of $\\lVert \\cdot\n\\rVert$, confirming the intuition that, in the data scarce regime, the type of\nregularization becomes increasingly important for adversarial training as\nperturbations grow in size.\n","authors":["Matteo Vilucchio","Nikolaos Tsilivis","Bruno Loureiro","Julia Kempe"],"pdf_url":"https://arxiv.org/pdf/2410.16073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07012v2","updated":"2024-10-21T14:53:11Z","published":"2024-03-09T10:01:49Z","title":"A PID-Controlled Non-Negative Tensor Factorization Model for Analyzing\n  Missing Data in NILM","summary":"  With the growing demand for energy and increased environmental awareness,\nNon-Intrusive Load Monitoring (NILM) has become an essential tool in smart grid\nand energy management. By analyzing total power load data, NILM infers the\nenergy usage of individual appliances without the need for separate sensors,\nenabling real-time monitoring from a few locations. This approach helps users\nunderstand consumption patterns, enhance energy efficiency, and detect\nanomalies for effective energy management. However, NILM datasets often suffer\nfrom issues such as sensor failures and data loss, compromising data integrity,\nthereby impacting subsequent analysis and applications. Traditional imputation\nmethods, such as linear interpolation and matrix factorization, struggle with\nnonlinear relationships and are sensitive to sparse data, resulting in\ninformation loss. To address these challenges, this paper proposes a\nProportional-Integral-Derivative (PID) Controlled Non-Negative Latent\nFactorization of Tensor (PNLF) model, which dynamically adjusts parameter\ngradients to improve convergence, stability, and accuracy. Experimental results\nshow that the PNLF model significantly outperforms state-of-the-art tensor\ncompletion models in both accuracy and efficiency. By addressing data loss\nissues, this study enhances load disaggregation precision and optimizes energy\nmanagement, providing reliable data support for smart grid applications and\npolicy formulation.\n","authors":["DengYu Shi"],"pdf_url":"https://arxiv.org/pdf/2403.07012v2.pdf","comment":"13papegs 8figures"},{"id":"http://arxiv.org/abs/2410.09940v2","updated":"2024-10-21T14:36:35Z","published":"2024-10-13T17:51:21Z","title":"Generalized Group Data Attribution","summary":"  Data Attribution (DA) methods quantify the influence of individual training\ndata points on model outputs and have broad applications such as\nexplainability, data selection, and noisy label identification. However,\nexisting DA methods are often computationally intensive, limiting their\napplicability to large-scale machine learning models. To address this\nchallenge, we introduce the Generalized Group Data Attribution (GGDA)\nframework, which computationally simplifies DA by attributing to groups of\ntraining points instead of individual ones. GGDA is a general framework that\nsubsumes existing attribution methods and can be applied to new DA techniques\nas they emerge. It allows users to optimize the trade-off between efficiency\nand fidelity based on their needs. Our empirical results demonstrate that GGDA\napplied to popular DA methods such as Influence Functions, TracIn, and TRAK\nresults in upto 10x-50x speedups over standard DA methods while gracefully\ntrading off attribution fidelity. For downstream applications such as dataset\npruning and noisy label identification, we demonstrate that GGDA significantly\nimproves computational efficiency and maintains effectiveness, enabling\npractical applications in large-scale machine learning scenarios that were\npreviously infeasible.\n","authors":["Dan Ley","Suraj Srinivas","Shichang Zhang","Gili Rusak","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2410.09940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16705v4","updated":"2024-10-21T14:33:15Z","published":"2023-10-25T15:20:53Z","title":"Wasserstein Gradient Flow over Variational Parameter Space for\n  Variational Inference","summary":"  Variational inference (VI) can be cast as an optimization problem in which\nthe variational parameters are tuned to closely align a variational\ndistribution with the true posterior. The optimization task can be approached\nthrough vanilla gradient descent in black-box VI or natural-gradient descent in\nnatural-gradient VI. In this work, we reframe VI as the optimization of an\nobjective that concerns probability distributions defined over a\n\\textit{variational parameter space}. Subsequently, we propose Wasserstein\ngradient descent for tackling this optimization problem. Notably, the\noptimization techniques, namely black-box VI and natural-gradient VI, can be\nreinterpreted as specific instances of the proposed Wasserstein gradient\ndescent. To enhance the efficiency of optimization, we develop practical\nmethods for numerically solving the discrete gradient flows. We validate the\neffectiveness of the proposed methods through empirical experiments on a\nsynthetic dataset, supplemented by theoretical analyses.\n","authors":["Dai Hai Nguyen","Tetsuya Sakurai","Hiroshi Mamitsuka"],"pdf_url":"https://arxiv.org/pdf/2310.16705v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16052v1","updated":"2024-10-21T14:28:26Z","published":"2024-10-21T14:28:26Z","title":"Near-Optimal Algorithm for Non-Stationary Kernelized Bandits","summary":"  This paper studies a non-stationary kernelized bandit (KB) problem, also\ncalled time-varying Bayesian optimization, where one seeks to minimize the\nregret under an unknown reward function that varies over time. In particular,\nwe focus on a near-optimal algorithm whose regret upper bound matches the\nregret lower bound. For this goal, we show the first algorithm-independent\nregret lower bound for non-stationary KB with squared exponential and Mat\\'ern\nkernels, which reveals that an existing optimization-based KB algorithm with\nslight modification is near-optimal. However, this existing algorithm suffers\nfrom feasibility issues due to its huge computational cost. Therefore, we\npropose a novel near-optimal algorithm called restarting phased elimination\nwith random permutation (R-PERP), which bypasses the huge computational cost. A\ntechnical key point is the simple permutation procedures of query candidates,\nwhich enable us to derive a novel tighter confidence bound tailored to the\nnon-stationary problems.\n","authors":["Shogo Iwazaki","Shion Takeno"],"pdf_url":"https://arxiv.org/pdf/2410.16052v1.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.06446v2","updated":"2024-10-21T14:28:18Z","published":"2024-10-09T01:12:07Z","title":"Machine Unlearning in Forgettability Sequence","summary":"  Machine unlearning (MU) is becoming a promising paradigm to achieve the\n\"right to be forgotten\", where the training trace of any chosen data points\ncould be eliminated, while maintaining the model utility on general testing\nsamples after unlearning. With the advancement of forgetting research, many\nfundamental open questions remain unanswered: do different samples exhibit\nvarying levels of difficulty in being forgotten? Further, does the sequence in\nwhich samples are forgotten, determined by their respective difficulty levels,\ninfluence the performance of forgetting algorithms? In this paper, we identify\nkey factor affecting unlearning difficulty and the performance of unlearning\nalgorithms. We find that samples with higher privacy risks are more likely to\nbe unlearning, indicating that the unlearning difficulty varies among different\nsamples which motives a more precise unlearning mode. Built upon this insight,\nwe propose a general unlearning framework, dubbed RSU, which consists of\nRanking module and SeqUnlearn module.\n","authors":["Junjie Chen","Qian Chen","Jian Lou","Xiaoyu Zhang","Kai Wu","Zilong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06446v2.pdf","comment":"The senior authors of the draft are not fully convinced that the\n  novelty is significant enough for this submission compared to the latest\n  research progress in this area. Additionally, the senior authors have\n  identified writing issues. Based on these two reasons, we have decided to\n  withdraw the draft from arXiv"},{"id":"http://arxiv.org/abs/2410.16041v1","updated":"2024-10-21T14:14:29Z","published":"2024-10-21T14:14:29Z","title":"GFlowNets for Hamiltonian decomposition in groups of compatible\n  operators","summary":"  Quantum computing presents a promising alternative for the direct simulation\nof quantum systems with the potential to explore chemical problems beyond the\ncapabilities of classical methods. However, current quantum algorithms are\nconstrained by hardware limitations and the increased number of measurements\nrequired to achieve chemical accuracy. To address the measurement challenge,\ntechniques for grouping commuting and anti-commuting terms, driven by\nheuristics, have been developed to reduce the number of measurements needed in\nquantum algorithms on near-term quantum devices. In this work, we propose a\nprobabilistic framework using GFlowNets to group fully (FC) or qubit-wise\ncommuting (QWC) terms within a given Hamiltonian. The significance of this\napproach is demonstrated by the reduced number of measurements for the found\ngroupings; 51% and 67% reduction factors respectively for FC and QWC\npartitionings with respect to greedy coloring algorithms, highlighting the\npotential of GFlowNets for future applications in the measurement problem.\nFurthermore, the flexibility of our algorithm extends its applicability to\nother resource optimization problems in Hamiltonian simulation, such as circuit\ndesign.\n","authors":["Isaac L. Huidobro-Meezs","Jun Dai","Guillaume Rabusseau","Rodrigo A. Vargas-Hernández"],"pdf_url":"https://arxiv.org/pdf/2410.16041v1.pdf","comment":"8 pages, 2 figures. Accepted for Machine Learning and the Physical\n  Sciences Workshop, NeurIPS 2024. Submission Number: 167"},{"id":"http://arxiv.org/abs/2410.16032v1","updated":"2024-10-21T14:06:53Z","published":"2024-10-21T14:06:53Z","title":"TimeMixer++: A General Time Series Pattern Machine for Universal\n  Predictive Analysis","summary":"  Time series analysis plays a critical role in numerous applications,\nsupporting tasks such as forecasting, classification, anomaly detection, and\nimputation. In this work, we present the time series pattern machine (TSPM), a\nmodel designed to excel in a broad range of time series tasks through powerful\nrepresentation and pattern extraction capabilities. Traditional time series\nmodels often struggle to capture universal patterns, limiting their\neffectiveness across diverse tasks. To address this, we define multiple scales\nin the time domain and various resolutions in the frequency domain, employing\nvarious mixing strategies to extract intricate, task-adaptive time series\npatterns. Specifically, we introduce a general-purpose TSPM that processes\nmulti-scale time series using (1) multi-resolution time imaging (MRTI), (2)\ntime image decomposition (TID), (3) multi-scale mixing (MCM), and (4)\nmulti-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI\ntransforms multi-scale time series into multi-resolution time images, capturing\npatterns across both temporal and frequency domains. TID leverages dual-axis\nattention to extract seasonal and trend patterns, while MCM hierarchically\naggregates these patterns across scales. MRM adaptively integrates all\nrepresentations across resolutions. This method achieves state-of-the-art\nperformance across 8 time series analytical tasks, consistently surpassing both\ngeneral-purpose and task-specific models. Our work marks a promising step\ntoward the next generation of TSPMs, paving the way for further advancements in\ntime series analysis.\n","authors":["Shiyu Wang","Jiawei Li","Xiaoming Shi","Zhou Ye","Baichuan Mo","Wenze Lin","Shengtong Ju","Zhixuan Chu","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2410.16032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16029v1","updated":"2024-10-21T14:05:06Z","published":"2024-10-21T14:05:06Z","title":"Natural GaLore: Accelerating GaLore for memory-efficient LLM Training\n  and Fine-tuning","summary":"  Training LLMs presents significant memory challenges due to growing size of\ndata, weights, and optimizer states. Techniques such as data and model\nparallelism, gradient checkpointing, and offloading strategies address this\nissue but are often infeasible due to hardware constraints. To mitigate memory\nusage, alternative methods like Parameter-Efficient-Fine-Tuning (PEFT) and\nGaLore approximate weights or optimizer states. PEFT methods, such as LoRA,\nhave gained popularity for fine-tuning LLMs, though they require a full-rank\nwarm start. In contrast, GaLore allows full-parameter learning while being more\nmemory-efficient. This work introduces Natural GaLore, a simple drop in\nreplacement for AdamW, which efficiently applies the inverse Empirical Fisher\nInformation Matrix to low-rank gradients using Woodbury's Identity. We\ndemonstrate that incorporating second-order information speeds up optimization\nsignificantly, especially when the iteration budget is limited. Empirical\npretraining on 60M, 130M, 350M, and 1.1B parameter Llama models on C4 data\ndemonstrate significantly lower perplexity over GaLore without additional\nmemory overhead. By fine-tuning RoBERTa on the GLUE benchmark using Natural\nGaLore, we demonstrate significant reduction in gap 86.05% vs 86.28% for\nfull-finetuning. Furthermore, fine-tuning the TinyLlama 1.1B model for function\ncalling using the TinyAgent framework shows that Natural GaLore achieving\n83.09% accuracy on the TinyAgent dataset, significantly outperforms 16-bit LoRA\nat 80.06% and even surpasses GPT4-Turbo by 4%, all while using 30% less memory.\n  All code to reproduce the results are available at:\nhttps://github.com/selfsupervised-ai/Natural-GaLore.git\n","authors":["Arijit Das"],"pdf_url":"https://arxiv.org/pdf/2410.16029v1.pdf","comment":"10 pages, 3 tables, 3 figures"},{"id":"http://arxiv.org/abs/2403.04202v6","updated":"2024-10-21T13:47:44Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v6.pdf","comment":"Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA)\n  https://ojs.aaai.org/index.php/AIES/article/view/31736"},{"id":"http://arxiv.org/abs/2410.16013v1","updated":"2024-10-21T13:45:02Z","published":"2024-10-21T13:45:02Z","title":"Information-Theoretic Minimax Regret Bounds for Reinforcement Learning\n  based on Duality","summary":"  We study agents acting in an unknown environment where the agent's goal is to\nfind a robust policy. We consider robust policies as policies that achieve high\ncumulative rewards for all possible environments. To this end, we consider\nagents minimizing the maximum regret over different environment parameters,\nleading to the study of minimax regret. This research focuses on deriving\ninformation-theoretic bounds for minimax regret in Markov Decision Processes\n(MDPs) with a finite time horizon. Building on concepts from supervised\nlearning, such as minimum excess risk (MER) and minimax excess risk, we use\nrecent bounds on the Bayesian regret to derive minimax regret bounds.\nSpecifically, we establish minimax theorems and use bounds on the Bayesian\nregret to perform minimax regret analysis using these minimax theorems. Our\ncontributions include defining a suitable minimax regret in the context of\nMDPs, finding information-theoretic bounds for it, and applying these bounds in\nvarious scenarios.\n","authors":["Raghav Bongole","Amaury Gouverneur","Borja Rodríguez-Gálvez","Tobias J. Oechtering","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2410.16013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16012v1","updated":"2024-10-21T13:43:02Z","published":"2024-10-21T13:43:02Z","title":"Massimo: Public Queue Monitoring and Management using Mass-Spring Model","summary":"  An efficient system of a queue control and regulation in public spaces is\nvery important in order to avoid the traffic jams and to improve the customer\nsatisfaction. This article offers a detailed road map based on a merger of\nintelligent systems and creating an efficient systems of queues in public\nplaces. Through the utilization of different technologies i.e. computer vision,\nmachine learning algorithms, deep learning our system provide accurate\ninformation about the place is crowded or not and the necessary efforts to be\ntaken.\n","authors":["Abhijeet Kumar","Unnati Singh","Rajdeep Chatterjee","Tathagata Bandyopadhyay"],"pdf_url":"https://arxiv.org/pdf/2410.16012v1.pdf","comment":"8 pages, 6 figures, 3 algorithms, 3 tables"},{"id":"http://arxiv.org/abs/2410.16008v1","updated":"2024-10-21T13:41:27Z","published":"2024-10-21T13:41:27Z","title":"Resilient Temporal GCN for Smart Grid State Estimation Under Topology\n  Inaccuracies","summary":"  State Estimation is a crucial task in power systems. Graph Neural Networks\nhave demonstrated significant potential in state estimation for power systems\nby effectively analyzing measurement data and capturing the complex\ninteractions and interrelations among the measurements through the system's\ngraph structure. However, the information about the system's graph structure\nmay be inaccurate due to noise, attack or lack of accurate information about\nthe topology of the system. This paper studies these scenarios under topology\nuncertainties and evaluates the impact of the topology uncertainties on the\nperformance of a Temporal Graph Convolutional Network (TGCN) for state\nestimation in power systems. In order to make the model resilient to topology\nuncertainties, modifications in the TGCN model are proposed to incorporate a\nknowledge graph, generated based on the measurement data. This knowledge graph\nsupports the assumed uncertain system graph. Two variations of the TGCN\narchitecture are introduced to integrate the knowledge graph, and their\nperformances are evaluated and compared to demonstrate improved resilience\nagainst topology uncertainties. The evaluation results indicate that while the\ntwo proposed architecture show different performance, they both improve the\nperformance of the TGCN state estimation under topology uncertainties.\n","authors":["Seyed Hamed Haghshenas","Mia Naeini"],"pdf_url":"https://arxiv.org/pdf/2410.16008v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.10576v2","updated":"2024-10-21T13:39:32Z","published":"2024-06-15T09:31:03Z","title":"Bypass Back-propagation: Optimization-based Structural Pruning for Large\n  Language Models via Policy Gradient","summary":"  In contrast to moderate-size neural network pruning, structural weight\npruning on the Large-Language Models (LLMs) imposes a novel challenge on the\nefficiency of the pruning algorithms, due to the heavy computation/memory\ndemands of the LLMs. Recent efficient LLM pruning methods typically operate at\nthe post-training phase without the expensive weight finetuning, however, their\npruning criteria often rely on heuristically hand-crafted metrics, potentially\nleading to suboptimal performance. We instead propose a novel\noptimization-based structural pruning that learns the pruning masks in a\nprobabilistic space directly by optimizing the loss of the pruned model. To\npreserve the efficiency, our method eliminates the back-propagation through the\nLLM per se during the optimization, requiring only the forward pass of the LLM.\nWe achieve this by learning an underlying Bernoulli distribution to sample\nbinary pruning masks, where we decouple the Bernoulli parameters from the LLM\nloss, thus facilitating an efficient optimization via a policy gradient\nestimator without back-propagation. As a result, our method is able to 1)\noperate at structural granularities of channels, heads, and layers, 2) support\nglobal and heterogeneous pruning (i.e., our method automatically determines\ndifferent redundancy for different layers), and 3) optionally initialize with a\nmetric-based method (for our Bernoulli distributions). Extensive experiments on\nLLaMA, LLaMA-2, LLaMA-3, Vicuna, and Mistral using the C4 and WikiText2\ndatasets demonstrate that our method operates for 2.7 hours with around 35GB\nmemory for the 13B models on a single A100 GPU, and our pruned models\noutperform the state-of-the-arts w.r.t. both perplexity and the majority of\nvarious zero-shot tasks. Codes will be released.\n","authors":["Yuan Gao","Zujing Liu","Weizhong Zhang","Bo Du","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2406.10576v2.pdf","comment":"Initially submitted on June 15, 2024, this version mainly changed the\n  title, and added several experiments: such as 1) experiments on LLaMA-3,\n  Mistral, 2) additional baseline methods (i.e., Bosai -- Everybody Prune Now),\n  and 3) post-pruning finetuned performance (i.e., first prune then finetune)"},{"id":"http://arxiv.org/abs/2404.14212v4","updated":"2024-10-21T13:34:22Z","published":"2024-04-22T14:21:37Z","title":"Toward Routing River Water in Land Surface Models with Recurrent Neural\n  Networks","summary":"  Machine learning is playing an increasing role in hydrology, supplementing or\nreplacing physics-based models. One notable example is the use of recurrent\nneural networks (RNNs) for forecasting streamflow given observed precipitation\nand geographic characteristics. Training of such a model over the continental\nUnited States (CONUS) demonstrated that a single set of model parameters can be\nused across independent catchments, and that RNNs can outperform physics-based\nmodels. In this work, we take a next step and study the performance of RNNs for\nriver routing in land surface models (LSMs). Instead of observed precipitation,\nthe LSM-RNN uses instantaneous runoff calculated from physics-based models as\nan input. We train the model with data from river basins spanning the globe and\ntest using historical streamflow measurements. The model demonstrates skill at\ngeneralization across basins (predicting streamflow in catchments not used in\ntraining) and across time (predicting streamflow during years not used in\ntraining). We compare the predictions from the LSM-RNN to an existing\nphysics-based model calibrated with a similar dataset and find that the LSM-RNN\noutperforms the physics-based model. Our results show that RNNs are effective\nfor global streamflow prediction from runoff inputs and motivate the\ndevelopment of complete routing models that can capture nested sub-basis\nconnections.\n","authors":["Mauricio Lima","Katherine Deck","Oliver R. A. Dunbar","Tapio Schneider"],"pdf_url":"https://arxiv.org/pdf/2404.14212v4.pdf","comment":"32 pages, 11 figures; submitted in HESS (EGU) with CCBY license"},{"id":"http://arxiv.org/abs/2410.15998v1","updated":"2024-10-21T13:29:08Z","published":"2024-10-21T13:29:08Z","title":"1024m at SMM4H 2024: Tasks 3, 5 & 6 -- Ensembles of Transformers and\n  Large Language Models for Medical Text Classification","summary":"  Social media is a great source of data for users reporting information and\nregarding their health and how various things have had an effect on them. This\npaper presents various approaches using Transformers and Large Language Models\nand their ensembles, their performance along with advantages and drawbacks for\nvarious tasks of SMM4H'24 - Classifying texts on impact of nature and outdoor\nspaces on the author's mental health (Task 3), Binary classification of tweets\nreporting their children's health disorders like Asthma, Autism, ADHD and\nSpeech disorder (task 5), Binary classification of users self-reporting their\nage (task 6).\n","authors":["Ram Mohan Rao Kadiyala","M. V. P. Chandra Sekhara Rao"],"pdf_url":"https://arxiv.org/pdf/2410.15998v1.pdf","comment":"short paper , acl 2024"},{"id":"http://arxiv.org/abs/2410.15997v1","updated":"2024-10-21T13:28:28Z","published":"2024-10-21T13:28:28Z","title":"MultiRC: Joint Learning for Time Series Anomaly Prediction and Detection\n  with Multi-scale Reconstructive Contrast","summary":"  Many methods have been proposed for unsupervised time series anomaly\ndetection. Despite some progress, research on predicting future anomalies is\nstill relatively scarce. Predicting anomalies is particularly challenging due\nto the diverse reaction time and the lack of labeled data. To address these\nchallenges, we propose MultiRC to integrate reconstructive and contrastive\nlearning for joint learning of anomaly prediction and detection, with\nmulti-scale structure and adaptive dominant period mask to deal with the\ndiverse reaction time. MultiRC also generates negative samples to provide\nessential training momentum for the anomaly prediction tasks and prevent model\ndegradation. We evaluate seven benchmark datasets from different fields. For\nboth anomaly prediction and detection tasks, MultiRC outperforms existing\nstate-of-the-art methods.\n","authors":["Shiyan Hu","Kai Zhao","Xiangfei Qiu","Yang Shu","Jilin Hu","Bin Yang","Chenjuan Guo"],"pdf_url":"https://arxiv.org/pdf/2410.15997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15990v1","updated":"2024-10-21T13:20:15Z","published":"2024-10-21T13:20:15Z","title":"Augmenting Legal Decision Support Systems with LLM-based NLI for\n  Analyzing Social Media Evidence","summary":"  This paper presents our system description and error analysis of our entry\nfor NLLP 2024 shared task on Legal Natural Language Inference (L-NLI)\n\\citep{hagag2024legallenssharedtask2024}. The task required classifying these\nrelationships as entailed, contradicted, or neutral, indicating any association\nbetween the review and the complaint. Our system emerged as the winning\nsubmission, significantly outperforming other entries with a substantial margin\nand demonstrating the effectiveness of our approach in legal text analysis. We\nprovide a detailed analysis of the strengths and limitations of each model and\napproach tested, along with a thorough error analysis and suggestions for\nfuture improvements. This paper aims to contribute to the growing field of\nlegal NLP by offering insights into advanced techniques for natural language\ninference in legal contexts, making it accessible to both experts and newcomers\nin the field.\n","authors":["Ram Mohan Rao Kadiyala","Siddartha Pullakhandam","Kanwal Mehreen","Subhasya Tippareddy","Ashay Srivastava"],"pdf_url":"https://arxiv.org/pdf/2410.15990v1.pdf","comment":"8 pages , accepted to emnlp 2024"},{"id":"http://arxiv.org/abs/2410.15987v1","updated":"2024-10-21T13:16:58Z","published":"2024-10-21T13:16:58Z","title":"Analyzing Closed-loop Training Techniques for Realistic Traffic Agent\n  Models in Autonomous Highway Driving Simulations","summary":"  Simulation plays a crucial role in the rapid development and safe deployment\nof autonomous vehicles. Realistic traffic agent models are indispensable for\nbridging the gap between simulation and the real world. Many existing\napproaches for imitating human behavior are based on learning from\ndemonstration. However, these approaches are often constrained by focusing on\nindividual training strategies. Therefore, to foster a broader understanding of\nrealistic traffic agent modeling, in this paper, we provide an extensive\ncomparative analysis of different training principles, with a focus on\nclosed-loop methods for highway driving simulation. We experimentally compare\n(i) open-loop vs. closed-loop multi-agent training, (ii) adversarial vs.\ndeterministic supervised training, (iii) the impact of reinforcement losses,\nand (iv) the impact of training alongside log-replayed agents to identify\nsuitable training techniques for realistic agent modeling. Furthermore, we\nidentify promising combinations of different closed-loop training methods.\n","authors":["Matthias Bitzer","Reinis Cimurs","Benjamin Coors","Johannes Goth","Sebastian Ziesche","Philipp Geiger","Maximilian Naumann"],"pdf_url":"https://arxiv.org/pdf/2410.15987v1.pdf","comment":"15 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.15986v1","updated":"2024-10-21T13:16:29Z","published":"2024-10-21T13:16:29Z","title":"A quantitative Robbins-Siegmund theorem","summary":"  The Robbins-Siegmund theorem is one of the most important results in\nstochastic optimization, where it is widely used to prove the convergence of\nstochastic algorithms. We provide a quantitative version of the theorem,\nestablishing a bound on how far one needs to look in order to locate a region\nof metastability in the sense of Tao. Our proof involves a metastable analogue\nof Doob's theorem for $L_1$-supermartingales along with a series of technical\nlemmas that make precise how quantitative information propagates through sums\nand products of stochastic processes. In this way, our paper establishes a\ngeneral methodology for finding metastable bounds for stochastic processes that\ncan be reduced to supermartingales, and therefore for obtaining quantitative\nconvergence information across a broad class of stochastic algorithms whose\nconvergence proof relies on some variation of the Robbins-Siegmund theorem. We\nconclude by discussing how our general quantitative result might be used in\npractice.\n","authors":["Morenikeji Neri","Thomas Powell"],"pdf_url":"https://arxiv.org/pdf/2410.15986v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2410.06003v3","updated":"2024-10-21T13:12:35Z","published":"2024-10-08T13:04:02Z","title":"Is the MMI Criterion Necessary for Interpretability? Degenerating\n  Non-causal Features to Plain Noise for Self-Rationalization","summary":"  An important line of research in the field of explainability is to extract a\nsmall subset of crucial rationales from the full input. The most widely used\ncriterion for rationale extraction is the maximum mutual information (MMI)\ncriterion. However, in certain datasets, there are spurious features\nnon-causally correlated with the label and also get high mutual information,\ncomplicating the loss landscape of MMI. Although some penalty-based methods\nhave been developed to penalize the spurious features (e.g., invariance\npenalty, intervention penalty, etc) to help MMI work better, these are merely\nremedial measures. In the optimization objectives of these methods, spurious\nfeatures are still distinguished from plain noise, which hinders the discovery\nof causal rationales. This paper aims to develop a new criterion that treats\nspurious features as plain noise, allowing the model to work on datasets rich\nin spurious features as if it were working on clean datasets, thereby making\nrationale extraction easier. We theoretically observe that removing either\nplain noise or spurious features from the input does not alter the conditional\ndistribution of the remaining components relative to the task label. However,\nsignificant changes in the conditional distribution occur only when causal\nfeatures are eliminated. Based on this discovery, the paper proposes a\ncriterion for \\textbf{M}aximizing the \\textbf{R}emaining \\textbf{D}iscrepancy\n(MRD). Experiments on six widely used datasets show that our MRD criterion\nimproves rationale quality (measured by the overlap with human-annotated\nrationales) by up to $10.4\\%$ as compared to several recent competitive MMI\nvariants. Code: \\url{https://github.com/jugechengzi/Rationalization-MRD}.\n","authors":["Wei Liu","Zhiying Deng","Zhongyu Niu","Jun Wang","Haozhao Wang","YuanKai Zhang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.06003v3.pdf","comment":"Accepted at NeurIPS 2024. arXiv admin note: text overlap with\n  arXiv:2309.13391"},{"id":"http://arxiv.org/abs/2410.15982v1","updated":"2024-10-21T13:12:22Z","published":"2024-10-21T13:12:22Z","title":"State Estimation Using Sparse DEIM and Recurrent Neural Networks","summary":"  Discrete Empirical Interpolation Method (DEIM) estimates a function from its\npointwise incomplete observations. In particular, this method can be used to\nestimate the state of a dynamical system from observational data gathered by\nsensors. However, when the number of observations are limited, DEIM returns\nlarge estimation errors. Sparse DEIM (S-DEIM) was recently developed to address\nthis problem by introducing a kernel vector which previous DEIM-based methods\nhad ignored. Unfortunately, estimating the optimal kernel vector in S-DEIM is a\ndifficult task. Here, we introduce a data-driven method to estimate this kernel\nvector from sparse observational time series using recurrent neural networks.\nUsing numerical examples, we demonstrate that this machine learning approach\ntogether with S-DEIM leads to nearly optimal state estimations.\n","authors":["Mohammad Farazmand"],"pdf_url":"https://arxiv.org/pdf/2410.15982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11786v2","updated":"2024-10-21T13:11:44Z","published":"2024-10-15T17:05:25Z","title":"Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.\n","authors":["Tsz Ting Chung","Leyang Cui","Lemao Liu","Xinting Huang","Shuming Shi","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.11786v2.pdf","comment":"14 pages, 5 figures, 10 tables, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15981v1","updated":"2024-10-21T13:06:38Z","published":"2024-10-21T13:06:38Z","title":"Visual Representation Learning Guided By Multi-modal Prior Knowledge","summary":"  Despite the remarkable success of deep neural networks (DNNs) in computer\nvision, they fail to remain high-performing when facing distribution shifts\nbetween training and testing data. In this paper, we propose Knowledge-Guided\nVisual representation learning (KGV), a distribution-based learning approach\nleveraging multi-modal prior knowledge, to improve generalization under\ndistribution shift. We use prior knowledge from two distinct modalities: 1) a\nknowledge graph (KG) with hierarchical and association relationships; and 2)\ngenerated synthetic images of visual elements semantically represented in the\nKG. The respective embeddings are generated from the given modalities in a\ncommon latent space, i.e., visual embeddings from original and synthetic images\nas well as knowledge graph embeddings (KGEs). These embeddings are aligned via\na novel variant of translation-based KGE methods, where the node and relation\nembeddings of the KG are modeled as Gaussian distributions and translations\nrespectively. We claim that incorporating multi-model prior knowledge enables\nmore regularized learning of image representations. Thus, the models are able\nto better generalize across different data distributions. We evaluate KGV on\ndifferent image classification tasks with major or minor distribution shifts,\nnamely road sign classification across datasets from Germany, China, and\nRussia, image classification with the mini-ImageNet dataset and its variants,\nas well as the DVM-CAR dataset. The results demonstrate that KGV consistently\nexhibits higher accuracy and data efficiency than the baselines across all\nexperiments.\n","authors":["Hongkuan Zhou","Lavdim Halilaj","Sebastian Monka","Stefan Schmid","Yuqicheng Zhu","Bo Xiong","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2410.15981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15974v1","updated":"2024-10-21T13:00:09Z","published":"2024-10-21T13:00:09Z","title":"Large Language Models for Cross-lingual Emotion Detection","summary":"  This paper presents a detailed system description of our entry for the WASSA\n2024 Task 2, focused on cross-lingual emotion detection. We utilized a\ncombination of large language models (LLMs) and their ensembles to effectively\nunderstand and categorize emotions across different languages. Our approach not\nonly outperformed other submissions with a large margin, but also demonstrated\nthe strength of integrating multiple models to enhance performance.\nAdditionally, We conducted a thorough comparison of the benefits and\nlimitations of each model used. An error analysis is included along with\nsuggested areas for future improvement. This paper aims to offer a clear and\ncomprehensive understanding of advanced techniques in emotion detection, making\nit accessible even to those new to the field.\n","authors":["Ram Mohan Rao Kadiyala"],"pdf_url":"https://arxiv.org/pdf/2410.15974v1.pdf","comment":"6 pages , accepted to acl 2024"},{"id":"http://arxiv.org/abs/2410.15973v1","updated":"2024-10-21T12:59:58Z","published":"2024-10-21T12:59:58Z","title":"Karush-Kuhn-Tucker Condition-Trained Neural Networks (KKT Nets)","summary":"  This paper presents a novel approach to solving convex optimization problems\nby leveraging the fact that, under certain regularity conditions, any set of\nprimal or dual variables satisfying the Karush-Kuhn-Tucker (KKT) conditions is\nnecessary and sufficient for optimality. Similar to Theory-Trained Neural\nNetworks (TTNNs), the parameters of the convex optimization problem are input\nto the neural network, and the expected outputs are the optimal primal and dual\nvariables. A choice for the loss function in this case is a loss, which we\nrefer to as the KKT Loss, that measures how well the network's outputs satisfy\nthe KKT conditions. We demonstrate the effectiveness of this approach using a\nlinear program as an example. For this problem, we observe that minimizing the\nKKT Loss alone outperforms training the network with a weighted sum of the KKT\nLoss and a Data Loss (the mean-squared error between the ground truth optimal\nsolutions and the network's output). Moreover, minimizing only the Data Loss\nyields inferior results compared to those obtained by minimizing the KKT Loss.\nWhile the approach is promising, the obtained primal and dual solutions are not\nsufficiently close to the ground truth optimal solutions. In the future, we aim\nto develop improved models to obtain solutions closer to the ground truth and\nextend the approach to other problem classes.\n","authors":["Shreya Arvind","Rishabh Pomaje","Rajshekhar V Bhat"],"pdf_url":"https://arxiv.org/pdf/2410.15973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15954v1","updated":"2024-10-21T12:34:02Z","published":"2024-10-21T12:34:02Z","title":"TS-ACL: A Time Series Analytic Continual Learning Framework for\n  Privacy-Preserving and Class-Incremental Pattern Recognition","summary":"  Class-incremental Learning (CIL) in Time Series Classification (TSC) aims to\nincrementally train models using the streaming time series data that arrives\ncontinuously. The main problem in this scenario is catastrophic forgetting,\ni.e., training models with new samples inevitably leads to the forgetting of\npreviously learned knowledge. Among existing methods, the replay-based methods\nachieve satisfactory performance but compromise privacy, while exemplar-free\nmethods protect privacy but suffer from low accuracy. However, more critically,\nowing to their reliance on gradient-based update techniques, these existing\nmethods fundamentally cannot solve the catastrophic forgetting problem. In TSC\nscenarios with continuously arriving data and temporally shifting\ndistributions, these methods become even less practical. In this paper, we\npropose a Time Series Analytic Continual Learning framework, called TS-ACL.\nInspired by analytical learning, TS-ACL transforms neural network updates into\ngradient-free linear regression problems, thereby fundamentally mitigating\ncatastrophic forgetting. Specifically, employing a pre-trained and frozen\nfeature extraction encoder, TS-ACL only needs to update its analytic classifier\nrecursively in a lightweight manner that is highly suitable for real-time\napplications and large-scale data processing. Additionally, we theoretically\ndemonstrate that the model obtained recursively through the TS-ACL is exactly\nequivalent to a model trained on the complete dataset in a centralized manner,\nthereby establishing the property of absolute knowledge memory. Extensive\nexperiments validate the superior performance of our TS-ACL.\n","authors":["Kejia Fan","Jiaxu Li","Songning Lai","Linpu Lv","Anfeng Liu","Jianheng Tang","Houbing Herbert Song","Huiping Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.15954v1.pdf","comment":"11 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.15952v1","updated":"2024-10-21T12:32:39Z","published":"2024-10-21T12:32:39Z","title":"User-centric evaluation of explainability of AI with and for humans: a\n  comprehensive empirical study","summary":"  This study is located in the Human-Centered Artificial Intelligence (HCAI)\nand focuses on the results of a user-centered assessment of commonly used\neXplainable Artificial Intelligence (XAI) algorithms, specifically\ninvestigating how humans understand and interact with the explanations provided\nby these algorithms. To achieve this, we employed a multi-disciplinary approach\nthat included state-of-the-art research methods from social sciences to measure\nthe comprehensibility of explanations generated by a state-of-the-art lachine\nlearning model, specifically the Gradient Boosting Classifier (XGBClassifier).\nWe conducted an extensive empirical user study involving interviews with 39\nparticipants from three different groups, each with varying expertise in data\nscience, data visualization, and domain-specific knowledge related to the\ndataset used for training the machine learning model. Participants were asked a\nseries of questions to assess their understanding of the model's explanations.\nTo ensure replicability, we built the model using a publicly available dataset\nfrom the UC Irvine Machine Learning Repository, focusing on edible and\nnon-edible mushrooms. Our findings reveal limitations in existing XAI methods\nand confirm the need for new design principles and evaluation techniques that\naddress the specific information needs and user perspectives of different\nclasses of AI stakeholders. We believe that the results of our research and the\ncross-disciplinary methodology we developed can be successfully adapted to\nvarious data types and user profiles, thus promoting dialogue and address\nopportunities in HCAI research. To support this, we are making the data\nresulting from our study publicly available.\n","authors":["Szymon Bobek","Paloma Korycińska","Monika Krakowska","Maciej Mozolewski","Dorota Rak","Magdalena Zych","Magdalena Wójcik","Grzegorz J. Nalepa"],"pdf_url":"https://arxiv.org/pdf/2410.15952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15927v1","updated":"2024-10-21T11:55:06Z","published":"2024-10-21T11:55:06Z","title":"GReFEL: Geometry-Aware Reliable Facial Expression Learning under Bias\n  and Imbalanced Data Distribution","summary":"  Reliable facial expression learning (FEL) involves the effective learning of\ndistinctive facial expression characteristics for more reliable, unbiased and\naccurate predictions in real-life settings. However, current systems struggle\nwith FEL tasks because of the variance in people's facial expressions due to\ntheir unique facial structures, movements, tones, and demographics. Biased and\nimbalanced datasets compound this challenge, leading to wrong and biased\nprediction labels. To tackle these, we introduce GReFEL, leveraging Vision\nTransformers and a facial geometry-aware anchor-based reliability balancing\nmodule to combat imbalanced data distributions, bias, and uncertainty in facial\nexpression learning. Integrating local and global data with anchors that learn\ndifferent facial data points and structural features, our approach adjusts\nbiased and mislabeled emotions caused by intra-class disparity, inter-class\nsimilarity, and scale sensitivity, resulting in comprehensive, accurate, and\nreliable facial expression predictions. Our model outperforms current\nstate-of-the-art methodologies, as demonstrated by extensive experiments on\nvarious datasets.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Karlo Serbetar","Dong Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2410.15927v1.pdf","comment":"ACCV 2024. Extended version of ARBEx (arXiv:2305.01486)"},{"id":"http://arxiv.org/abs/2405.14468v2","updated":"2024-10-21T11:54:16Z","published":"2024-05-23T11:55:49Z","title":"Neural Collapse versus Low-rank Bias: Is Deep Neural Collapse Really\n  Optimal?","summary":"  Deep neural networks (DNNs) exhibit a surprising structure in their final\nlayer known as neural collapse (NC), and a growing body of works has currently\ninvestigated the propagation of neural collapse to earlier layers of DNNs -- a\nphenomenon called deep neural collapse (DNC). However, existing theoretical\nresults are restricted to special cases: linear models, only two layers or\nbinary classification. In contrast, we focus on non-linear models of arbitrary\ndepth in multi-class classification and reveal a surprising qualitative shift.\nAs soon as we go beyond two layers or two classes, DNC stops being optimal for\nthe deep unconstrained features model (DUFM) -- the standard theoretical\nframework for the analysis of collapse. The main culprit is a low-rank bias of\nmulti-layer regularization schemes: this bias leads to optimal solutions of\neven lower rank than the neural collapse. We support our theoretical findings\nwith experiments on both DUFM and real data, which show the emergence of the\nlow-rank structure in the solution found by gradient descent.\n","authors":["Peter Súkeník","Marco Mondelli","Christoph Lampert"],"pdf_url":"https://arxiv.org/pdf/2405.14468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15923v1","updated":"2024-10-21T11:53:49Z","published":"2024-10-21T11:53:49Z","title":"Automatic Differentiation of Optimization Algorithms with Time-Varying\n  Updates","summary":"  Numerous Optimization Algorithms have a time-varying update rule thanks to,\nfor instance, a changing step size, momentum parameter or, Hessian\napproximation. In this paper, we apply unrolled or automatic differentiation to\na time-varying iterative process and provide convergence (rate) guarantees for\nthe resulting derivative iterates. We adapt these convergence results and apply\nthem to proximal gradient descent with variable step size and FISTA when\nsolving partly smooth problems. We confirm our findings numerically by solving\n$\\ell_1$ and $\\ell_2$-regularized linear and logisitc regression respectively.\nOur theoretical and numerical results show that the convergence rate of the\nalgorithm is reflected in its derivative iterates.\n","authors":["Sheheryar Mehmood","Peter Ochs"],"pdf_url":"https://arxiv.org/pdf/2410.15923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08958v2","updated":"2024-10-21T11:49:53Z","published":"2024-02-14T05:58:43Z","title":"Towards Next-Level Post-Training Quantization of Hyper-Scale\n  Transformers","summary":"  With the increasing complexity of generative AI models, post-training\nquantization (PTQ) has emerged as a promising solution for deploying\nhyper-scale models on edge devices such as mobile and TVs. Existing PTQ\nschemes, however, consume considerable time and resources, which could be a\nbottleneck in real situations where frequent model updates and multiple\nhyperparameter tunings are required. As a cost-effective alternative,\nlearning-free PTQ schemes have been proposed. However, the performance is\nsomewhat limited because they cannot consider the inter-layer dependency within\nthe attention module, which is a significant feature of Transformers. In this\npaper, we thus propose a novel PTQ algorithm that balances accuracy and\nefficiency. The key idea of the proposed algorithm called aespa is to perform\nquantization layer-wise for efficiency while targeting attention-wise\nreconstruction to consider the cross-layer dependency. Through extensive\nexperiments on various language models and complexity analysis, we demonstrate\nthat aespa is accurate and efficient in quantizing Transformer models.\n","authors":["Junhan Kim","Chungman Lee","Eulrang Cho","Kyungphil Park","Ho-young Kim","Joonyoung Kim","Yongkweon Jeon"],"pdf_url":"https://arxiv.org/pdf/2402.08958v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2310.07446v5","updated":"2024-10-21T11:45:04Z","published":"2023-10-11T12:48:45Z","title":"ProbTS: Benchmarking Point and Distributional Forecasting across Diverse\n  Prediction Horizons","summary":"  Delivering precise point and distributional forecasts across a spectrum of\nprediction horizons represents a significant and enduring challenge in the\napplication of time-series forecasting within various industries. Prior\nresearch on developing deep learning models for time-series forecasting has\noften concentrated on isolated aspects, such as long-term point forecasting or\nshort-term probabilistic estimations. This narrow focus may result in skewed\nmethodological choices and hinder the adaptability of these models to uncharted\nscenarios. While there is a rising trend in developing universal forecasting\nmodels, a thorough understanding of their advantages and drawbacks, especially\nregarding essential forecasting needs like point and distributional forecasts\nacross short and long horizons, is still lacking. In this paper, we present\nProbTS, a benchmark tool designed as a unified platform to evaluate these\nfundamental forecasting needs and to conduct a rigorous comparative analysis of\nnumerous cutting-edge studies from recent years. We dissect the distinctive\ndata characteristics arising from disparate forecasting requirements and\nelucidate how these characteristics can skew methodological preferences in\ntypical research trajectories, which often fail to fully accommodate essential\nforecasting needs. Building on this, we examine the latest models for universal\ntime-series forecasting and discover that our analyses of methodological\nstrengths and weaknesses are also applicable to these universal models.\nFinally, we outline the limitations inherent in current research and underscore\nseveral avenues for future exploration.\n","authors":["Jiawen Zhang","Xumeng Wen","Zhenwei Zhang","Shun Zheng","Jia Li","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.07446v5.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.15910v1","updated":"2024-10-21T11:33:14Z","published":"2024-10-21T11:33:14Z","title":"Diverse Policies Recovering via Pointwise Mutual Information Weighted\n  Imitation Learning","summary":"  Recovering a spectrum of diverse policies from a set of expert trajectories\nis an important research topic in imitation learning. After determining a\nlatent style for a trajectory, previous diverse policies recovering methods\nusually employ a vanilla behavioral cloning learning objective conditioned on\nthe latent style, treating each state-action pair in the trajectory with equal\nimportance. Based on an observation that in many scenarios, behavioral styles\nare often highly relevant with only a subset of state-action pairs, this paper\npresents a new principled method in diverse polices recovery. In particular,\nafter inferring or assigning a latent style for a trajectory, we enhance the\nvanilla behavioral cloning by incorporating a weighting mechanism based on\npointwise mutual information. This additional weighting reflects the\nsignificance of each state-action pair's contribution to learning the style,\nthus allowing our method to focus on state-action pairs most representative of\nthat style. We provide theoretical justifications for our new objective, and\nextensive empirical evaluations confirm the effectiveness of our method in\nrecovering diverse policies from expert data.\n","authors":["Hanlin Yang","Jian Yao","Weiming Liu","Qing Wang","Hanmin Qin","Hansheng Kong","Kirk Tang","Jiechao Xiong","Chao Yu","Kai Li","Junliang Xing","Hongwu Chen","Juchao Zhuo","Qiang Fu","Yang Wei","Haobo Fu"],"pdf_url":"https://arxiv.org/pdf/2410.15910v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.02063v5","updated":"2024-10-21T11:26:32Z","published":"2024-05-03T12:48:21Z","title":"Few-sample Variational Inference of Bayesian Neural Networks with\n  Arbitrary Nonlinearities","summary":"  Bayesian Neural Networks (BNNs) extend traditional neural networks to provide\nuncertainties associated with their outputs. On the forward pass through a BNN,\npredictions (and their uncertainties) are made either by Monte Carlo sampling\nnetwork weights from the learned posterior or by analytically propagating\nstatistical moments through the network. Though flexible, Monte Carlo sampling\nis computationally expensive and can be infeasible or impractical under\nresource constraints or for large networks. While moment propagation can\nameliorate the computational costs of BNN inference, it can be difficult or\nimpossible for networks with arbitrary nonlinearities, thereby restricting the\npossible set of network layers permitted with such a scheme. In this work, we\ndemonstrate a simple yet effective approach for propagating statistical moments\nthrough arbitrary nonlinearities with only 3 deterministic samples, enabling\nfew-sample variational inference of BNNs without restricting the set of network\nlayers used. Furthermore, we leverage this approach to demonstrate a novel\nnonlinear activation function that we use to inject physics-informed prior\ninformation into output nodes of a BNN.\n","authors":["David J. Schodt"],"pdf_url":"https://arxiv.org/pdf/2405.02063v5.pdf","comment":"Comment 1: Fixed plot markers in figure 6 to match legend and to\n  improve grayscale appearance Comment 2: Fixed mistyped value for optimizer\n  learning rate"},{"id":"http://arxiv.org/abs/2410.15899v1","updated":"2024-10-21T11:23:23Z","published":"2024-10-21T11:23:23Z","title":"On the Design and Performance of Machine Learning Based Error Correcting\n  Decoders","summary":"  This paper analyzes the design and competitiveness of four neural network\n(NN) architectures recently proposed as decoders for forward error correction\n(FEC) codes. We first consider the so-called single-label neural network (SLNN)\nand the multi-label neural network (MLNN) decoders which have been reported to\nachieve near maximum likelihood (ML) performance. Here, we show analytically\nthat SLNN and MLNN decoders can always achieve ML performance, regardless of\nthe code dimensions -- although at the cost of computational complexity -- and\nno training is in fact required. We then turn our attention to two\ntransformer-based decoders: the error correction code transformer (ECCT) and\nthe cross-attention message passing transformer (CrossMPT). We compare their\nperformance against traditional decoders, and show that ordered statistics\ndecoding outperforms these transformer-based decoders. The results in this\npaper cast serious doubts on the application of NN-based FEC decoders in the\nshort and medium block length regime.\n","authors":["Yuncheng Yuan","Péter Scheepers","Lydia Tasiou","Yunus Can Gültekin","Federico Corradi","Alex Alvarado"],"pdf_url":"https://arxiv.org/pdf/2410.15899v1.pdf","comment":"6 pages, 4 figures, submitted for possible presentation in a\n  conference"},{"id":"http://arxiv.org/abs/2310.04539v3","updated":"2024-10-21T11:12:02Z","published":"2023-10-06T19:06:13Z","title":"Generating Less Certain Adversarial Examples Improves Robust\n  Generalization","summary":"  This paper revisits the robust overfitting phenomenon of adversarial\ntraining. Observing that models with better robust generalization performance\nare less certain in predicting adversarially generated training inputs, we\nargue that overconfidence in predicting adversarial examples is a potential\ncause. Therefore, we hypothesize that generating less certain adversarial\nexamples improves robust generalization, and propose a formal definition of\nadversarial certainty that captures the variance of the model's predicted\nlogits on adversarial examples. Our theoretical analysis of synthetic\ndistributions characterizes the connection between adversarial certainty and\nrobust generalization. Accordingly, built upon the notion of adversarial\ncertainty, we develop a general method to search for models that can generate\ntraining-time adversarial inputs with reduced certainty, while maintaining the\nmodel's capability in distinguishing adversarial examples. Extensive\nexperiments on image benchmarks demonstrate that our method effectively learns\nmodels with consistently improved robustness and mitigates robust overfitting,\nconfirming the importance of generating less certain adversarial examples for\nrobust generalization.\n","authors":["Minxing Zhang","Michael Backes","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04539v3.pdf","comment":"Published in Transactions of Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2408.13296v2","updated":"2024-10-21T11:10:00Z","published":"2024-08-23T14:48:02Z","title":"The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An\n  Exhaustive Review of Technologies, Research, Best Practices, Applied Research\n  Challenges and Opportunities","summary":"  This report examines the fine-tuning of Large Language Models (LLMs),\nintegrating theoretical insights with practical applications. It outlines the\nhistorical evolution of LLMs from traditional Natural Language Processing (NLP)\nmodels to their pivotal role in AI. A comparison of fine-tuning methodologies,\nincluding supervised, unsupervised, and instruction-based approaches,\nhighlights their applicability to different tasks. The report introduces a\nstructured seven-stage pipeline for fine-tuning LLMs, spanning data\npreparation, model initialization, hyperparameter tuning, and model deployment.\nEmphasis is placed on managing imbalanced datasets and optimization techniques.\nParameter-efficient methods like Low-Rank Adaptation (LoRA) and Half\nFine-Tuning are explored for balancing computational efficiency with\nperformance. Advanced techniques such as memory fine-tuning, Mixture of Experts\n(MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized\nnetworks and multi-agent collaboration. The report also examines novel\napproaches like Proximal Policy Optimization (PPO) and Direct Preference\nOptimization (DPO), which align LLMs with human preferences, alongside pruning\nand routing optimizations to improve efficiency. Further sections cover\nvalidation frameworks, post-deployment monitoring, and inference optimization,\nwith attention to deploying LLMs on distributed and cloud-based platforms.\nEmerging areas such as multimodal LLMs, fine-tuning for audio and speech, and\nchallenges related to scalability, privacy, and accountability are also\naddressed. This report offers actionable insights for researchers and\npractitioners navigating LLM fine-tuning in an evolving landscape.\n","authors":["Venkatesh Balavadhani Parthasarathy","Ahtsham Zafar","Aafaq Khan","Arsalan Shahid"],"pdf_url":"https://arxiv.org/pdf/2408.13296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15889v1","updated":"2024-10-21T11:06:56Z","published":"2024-10-21T11:06:56Z","title":"Model Mimic Attack: Knowledge Distillation for Provably Transferable\n  Adversarial Examples","summary":"  The vulnerability of artificial neural networks to adversarial perturbations\nin the black-box setting is widely studied in the literature. The majority of\nattack methods to construct these perturbations suffer from an impractically\nlarge number of queries required to find an adversarial example. In this work,\nwe focus on knowledge distillation as an approach to conduct transfer-based\nblack-box adversarial attacks and propose an iterative training of the\nsurrogate model on an expanding dataset. This work is the first, to our\nknowledge, to provide provable guarantees on the success of knowledge\ndistillation-based attack on classification neural networks: we prove that if\nthe student model has enough learning capabilities, the attack on the teacher\nmodel is guaranteed to be found within the finite number of distillation\niterations.\n","authors":["Kirill Lukyanov","Andrew Perminov","Denis Turdakov","Mikhail Pautov"],"pdf_url":"https://arxiv.org/pdf/2410.15889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15884v1","updated":"2024-10-21T11:02:18Z","published":"2024-10-21T11:02:18Z","title":"Using GPT Models for Qualitative and Quantitative News Analytics in the\n  2024 US Presidental Election Process","summary":"  The paper considers an approach of using Google Search API and GPT-4o model\nfor qualitative and quantitative analyses of news through retrieval-augmented\ngeneration (RAG). This approach was applied to analyze news about the 2024 US\npresidential election process. Different news sources for different time\nperiods have been analyzed. Quantitative scores generated by GPT model have\nbeen analyzed using Bayesian regression to derive trend lines. The\ndistributions found for the regression parameters allow for the analysis of\nuncertainty in the election process. The obtained results demonstrate that\nusing the GPT models for news analysis, one can get informative analytics and\nprovide key insights that can be applied in further analyses of election\nprocesses.\n","authors":["Bohdan M. Pavlyshenko"],"pdf_url":"https://arxiv.org/pdf/2410.15884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15882v1","updated":"2024-10-21T11:01:44Z","published":"2024-10-21T11:01:44Z","title":"Distributed Learning for UAV Swarms","summary":"  Unmanned Aerial Vehicle (UAV) swarms are increasingly deployed in dynamic,\ndata-rich environments for applications such as environmental monitoring and\nsurveillance. These scenarios demand efficient data processing while\nmaintaining privacy and security, making Federated Learning (FL) a promising\nsolution. FL allows UAVs to collaboratively train global models without sharing\nraw data, but challenges arise due to the non-Independent and Identically\nDistributed (non-IID) nature of the data collected by UAVs. In this study, we\nshow an integration of the state-of-the-art FL methods to UAV Swarm application\nand invetigate the performance of multiple aggregation methods (namely FedAvg,\nFedProx, FedOpt, and MOON) with a particular focus on tackling non-IID on a\nvariety of datasets, specifically MNIST for baseline performance, CIFAR10 for\nnatural object classification, EuroSAT for environment monitoring, and CelebA\nfor surveillance. These algorithms were selected to cover improved techniques\non both client-side updates and global aggregation. Results show that while all\nalgorithms perform comparably on IID data, their performance deteriorates\nsignificantly under non-IID conditions. FedProx demonstrated the most stable\noverall performance, emphasising the importance of regularising local updates\nin non-IID environments to mitigate drastic deviations in local models.\n","authors":["Chen Hu","Hanchi Ren","Jingjing Deng","Xianghua Xie"],"pdf_url":"https://arxiv.org/pdf/2410.15882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15876v1","updated":"2024-10-21T10:57:45Z","published":"2024-10-21T10:57:45Z","title":"FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL","summary":"  Multi-agent reinforcement learning has demonstrated significant potential in\naddressing complex cooperative tasks across various real-world applications.\nHowever, existing MARL approaches often rely on the restrictive assumption that\nthe number of entities (e.g., agents, obstacles) remains constant between\ntraining and inference. This overlooks scenarios where entities are dynamically\nremoved or added during the inference trajectory -- a common occurrence in\nreal-world environments like search and rescue missions and dynamic combat\nsituations. In this paper, we tackle the challenge of intra-trajectory dynamic\nentity composition under zero-shot out-of-domain (OOD) generalization, where\nsuch dynamic changes cannot be anticipated beforehand. Our empirical studies\nreveal that existing MARL methods suffer significant performance degradation\nand increased uncertainty in these scenarios. In response, we propose\nFlickerFusion, a novel OOD generalization method that acts as a universally\napplicable augmentation technique for MARL backbone methods. Our results show\nthat FlickerFusion not only achieves superior inference rewards but also\nuniquely reduces uncertainty vis-\\`a-vis the backbone, compared to existing\nmethods. For standardized evaluation, we introduce MPEv2, an enhanced version\nof Multi Particle Environments (MPE), consisting of 12 benchmarks. Benchmarks,\nimplementations, and trained models are organized and open-sourced at\nflickerfusion305.github.io, accompanied by ample demo video renderings.\n","authors":["Woosung Koh","Wonbeen Oh","Siyeol Kim","Suhin Shin","Hyeongjin Kim","Jaein Jang","Junghyun Lee","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2410.15876v1.pdf","comment":"NeurIPS '24 Open-World Agents Workshop"},{"id":"http://arxiv.org/abs/2410.15875v1","updated":"2024-10-21T10:57:25Z","published":"2024-10-21T10:57:25Z","title":"Enabling Asymmetric Knowledge Transfer in Multi-Task Learning with\n  Self-Auxiliaries","summary":"  Knowledge transfer in multi-task learning is typically viewed as a dichotomy;\npositive transfer, which improves the performance of all tasks, or negative\ntransfer, which hinders the performance of all tasks. In this paper, we\ninvestigate the understudied problem of asymmetric task relationships, where\nknowledge transfer aids the learning of certain tasks while hindering the\nlearning of others. We propose an optimisation strategy that includes\nadditional cloned tasks named self-auxiliaries into the learning process to\nflexibly transfer knowledge between tasks asymmetrically. Our method can\nexploit asymmetric task relationships, benefiting from the positive transfer\ncomponent while avoiding the negative transfer component. We demonstrate that\nasymmetric knowledge transfer provides substantial improvements in performance\ncompared to existing multi-task optimisation strategies on benchmark computer\nvision problems.\n","authors":["Olivier Graffeuille","Yun Sing Koh","Joerg Wicker","Moritz Lehmann"],"pdf_url":"https://arxiv.org/pdf/2410.15875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04133v2","updated":"2024-10-21T10:56:37Z","published":"2024-10-05T12:12:02Z","title":"An Electrocardiogram Foundation Model Built on over 10 Million\n  Recordings with External Evaluation across Multiple Domains","summary":"  Artificial intelligence (AI) has demonstrated significant potential in ECG\nanalysis and cardiovascular disease assessment. Recently, foundation models\nhave played a remarkable role in advancing medical AI. The development of an\nECG foundation model holds the promise of elevating AI-ECG research to new\nheights. However, building such a model faces several challenges, including\ninsufficient database sample sizes and inadequate generalization across\nmultiple domains. Additionally, there is a notable performance gap between\nsingle-lead and multi-lead ECG analyses. We introduced an ECG Foundation Model\n(ECGFounder), a general-purpose model that leverages real-world ECG annotations\nfrom cardiology experts to broaden the diagnostic capabilities of ECG analysis.\nECGFounder was trained on over 10 million ECGs with 150 label categories from\nthe Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease\ndiagnosis through ECG analysis. The model is designed to be both an effective\nout-of-the-box solution, and a to be fine-tunable for downstream tasks,\nmaximizing usability. Importantly, we extended its application to lower rank\nECGs, and arbitrary single-lead ECGs in particular. ECGFounder is applicable to\nsupporting various downstream tasks in mobile monitoring scenarios.\nExperimental results demonstrate that ECGFounder achieves expert-level\nperformance on internal validation sets, with AUROC exceeding 0.95 for eighty\ndiagnoses. It also shows strong classification performance and generalization\nacross various diagnoses on external validation sets. When fine-tuned,\nECGFounder outperforms baseline models in demographic analysis, clinical event\ndetection, and cross-modality cardiac rhythm diagnosis. The trained model and\ndata will be publicly released upon publication through the bdsp.io. Our code\nis available at https://github.com/bdsp-core/ECGFounder\n","authors":["Jun Li","Aaron Aguirre","Junior Moura","Che Liu","Lanhai Zhong","Chenxi Sun","Gari Clifford","Brandon Westover","Shenda Hong"],"pdf_url":"https://arxiv.org/pdf/2410.04133v2.pdf","comment":"working in progress"},{"id":"http://arxiv.org/abs/2404.07989v3","updated":"2024-10-21T10:54:55Z","published":"2024-04-11T17:59:45Z","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding","summary":"  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n","authors":["Yiwen Tang","Ray Zhang","Jiaming Liu","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Shanghang Zhang","Peng Gao","Hongsheng Li","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07989v3.pdf","comment":"Code and models are released at\n  https://github.com/Ivan-Tang-3D/Any2Point"},{"id":"http://arxiv.org/abs/2310.03059v8","updated":"2024-10-21T10:49:59Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code is released at\nhttps://github.com/Ivan-Tang-3D/Point-PEFT.\n","authors":["Yiwen Tang","Ray Zhang","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2310.03059v8.pdf","comment":"The specialized PEFT framework for 3D pre-trained models, which\n  achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Ivan-Tang-3D/Point-PEFT"},{"id":"http://arxiv.org/abs/2410.15859v1","updated":"2024-10-21T10:39:05Z","published":"2024-10-21T10:39:05Z","title":"Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs","summary":"  Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach.\n","authors":["Xin Ma","Yang Liu","Jingjing Liu","Xiaoxu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.15859v1.pdf","comment":"accepted by NeurIPS 2024. arXiv admin note: text overlap with\n  arXiv:2305.19466 by other authors"},{"id":"http://arxiv.org/abs/2410.15858v1","updated":"2024-10-21T10:37:17Z","published":"2024-10-21T10:37:17Z","title":"Towards Optimal Adapter Placement for Efficient Transfer Learning","summary":"  Parameter-efficient transfer learning (PETL) aims to adapt pre-trained models\nto new downstream tasks while minimizing the number of fine-tuned parameters.\nAdapters, a popular approach in PETL, inject additional capacity into existing\nnetworks by incorporating low-rank projections, achieving performance\ncomparable to full fine-tuning with significantly fewer parameters. This paper\ninvestigates the relationship between the placement of an adapter and its\nperformance. We observe that adapter location within a network significantly\nimpacts its effectiveness, and that the optimal placement is task-dependent. To\nexploit this observation, we introduce an extended search space of adapter\nconnections, including long-range and recurrent adapters. We demonstrate that\neven randomly selected adapter placements from this expanded space yield\nimproved results, and that high-performing placements often correlate with high\ngradient rank. Our findings reveal that a small number of strategically placed\nadapters can match or exceed the performance of the common baseline of adding\nadapters in every block, opening a new avenue for research into optimal adapter\nplacement strategies.\n","authors":["Aleksandra I. Nowak","Otniel-Bogdan Mercea","Anurag Arnab","Jonas Pfeiffer","Yann Dauphin","Utku Evci"],"pdf_url":"https://arxiv.org/pdf/2410.15858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15854v1","updated":"2024-10-21T10:30:24Z","published":"2024-10-21T10:30:24Z","title":"TEXEL: A neuromorphic processor with on-chip learning for beyond-CMOS\n  device integration","summary":"  Recent advances in memory technologies, devices and materials have shown\ngreat potential for integration into neuromorphic electronic systems. However,\na significant gap remains between the development of these materials and the\nrealization of large-scale, fully functional systems. One key challenge is\ndetermining which devices and materials are best suited for specific functions\nand how they can be paired with CMOS circuitry. To address this, we introduce\nTEXEL, a mixed-signal neuromorphic architecture designed to explore the\nintegration of on-chip learning circuits and novel two- and three-terminal\ndevices. TEXEL serves as an accessible platform to bridge the gap between\nCMOS-based neuromorphic computation and the latest advancements in emerging\ndevices. In this paper, we demonstrate the readiness of TEXEL for device\nintegration through comprehensive chip measurements and simulations. TEXEL\nprovides a practical system for testing bio-inspired learning algorithms\nalongside emerging devices, establishing a tangible link between brain-inspired\ncomputation and cutting-edge device research.\n","authors":["Hugh Greatorex","Ole Richter","Michele Mastella","Madison Cotteret","Philipp Klein","Maxime Fabre","Arianna Rubino","Willian Soares Girão","Junren Chen","Martin Ziegler","Laura Bégon-Lours","Giacomo Indiveri","Elisabetta Chicca"],"pdf_url":"https://arxiv.org/pdf/2410.15854v1.pdf","comment":"17 pages, 7 figures. Supplementary material: 8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2304.03552v2","updated":"2024-10-21T10:29:32Z","published":"2023-04-07T09:22:28Z","title":"A physics-informed neural network framework for modeling\n  obstacle-related equations","summary":"  Deep learning has been highly successful in some applications. Nevertheless,\nits use for solving partial differential equations (PDEs) has only been of\nrecent interest with current state-of-the-art machine learning libraries, e.g.,\nTensorFlow or PyTorch. Physics-informed neural networks (PINNs) are an\nattractive tool for solving partial differential equations based on sparse and\nnoisy data. Here extend PINNs to solve obstacle-related PDEs which present a\ngreat computational challenge because they necessitate numerical methods that\ncan yield an accurate approximation of the solution that lies above a given\nobstacle. The performance of the proposed PINNs is demonstrated in multiple\nscenarios for linear and nonlinear PDEs subject to regular and irregular\nobstacles.\n","authors":["Hamid El Bahja","Jan Christian Hauffen","Peter Jung","Bubacarr Bah","Issa Karambal"],"pdf_url":"https://arxiv.org/pdf/2304.03552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15851v1","updated":"2024-10-21T10:27:57Z","published":"2024-10-21T10:27:57Z","title":"R2I-rPPG: A Robust Region of Interest Selection Method for Remote\n  Photoplethysmography to Extract Heart Rate","summary":"  The COVID-19 pandemic has underscored the need for low-cost, scalable\napproaches to measuring contactless vital signs, either during initial triage\nat a healthcare facility or virtual telemedicine visits. Remote\nphotoplethysmography (rPPG) can accurately estimate heart rate (HR) when\napplied to close-up videos of healthy volunteers in well-lit laboratory\nsettings. However, results from such highly optimized laboratory studies may\nnot be readily translated to healthcare settings. One significant barrier to\nthe practical application of rPPG in health care is the accurate localization\nof the region of interest (ROI). Clinical or telemedicine visits may involve\nsub-optimal lighting, movement artifacts, variable camera angle, and subject\ndistance. This paper presents an rPPG ROI selection method based on 3D facial\nlandmarks and patient head yaw angle. We then demonstrate the robustness of\nthis ROI selection method when coupled to the Plane-Orthogonal-to-Skin (POS)\nrPPG method when applied to videos of patients presenting to an Emergency\nDepartment for respiratory complaints. Our results demonstrate the\neffectiveness of our proposed approach in improving the accuracy and robustness\nof rPPG in a challenging clinical environment.\n","authors":["Sandeep Nagar","Mark Hasegawa-Johnson","David G. Beiser","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2410.15851v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.15849v1","updated":"2024-10-21T10:25:52Z","published":"2024-10-21T10:25:52Z","title":"Focus Where It Matters: Graph Selective State Focused Attention Networks","summary":"  Traditional graph neural networks (GNNs) lack scalability and lose individual\nnode characteristics due to over-smoothing, especially in the case of deeper\nnetworks. This results in sub-optimal feature representation, affecting the\nmodel's performance on tasks involving dynamically changing graphs. To address\nthis issue, we present Graph Selective States Focused Attention Networks\n(GSANs) based neural network architecture for graph-structured data. The GSAN\nis enabled by multi-head masked self-attention (MHMSA) and selective state\nspace modeling (S3M) layers to overcome the limitations of GNNs. In GSAN, the\nMHMSA allows GSAN to dynamically emphasize crucial node connections,\nparticularly in evolving graph environments. The S3M layer enables the network\nto adjust dynamically in changing node states and improving predictions of node\nbehavior in varying contexts without needing primary knowledge of the graph\nstructure. Furthermore, the S3M layer enhances the generalization of unseen\nstructures and interprets how node states influence link importance. With this,\nGSAN effectively outperforms inductive and transductive tasks and overcomes the\nissues that traditional GNNs experience. To analyze the performance behavior of\nGSAN, a set of state-of-the-art comparative experiments are conducted on graphs\nbenchmark datasets, including $Cora$, $Citeseer$, $Pubmed$ network citation,\nand $protein-protein-interaction$ datasets, as an outcome, GSAN improved the\nclassification accuracy by $1.56\\%$, $8.94\\%$, $0.37\\%$, and $1.54\\%$ on\n$F1-score$ respectively.\n","authors":["Shikhar Vashistha","Neetesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.15849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15664v3","updated":"2024-10-21T10:22:17Z","published":"2024-06-21T21:44:27Z","title":"Flat Posterior Does Matter For Bayesian Model Averaging","summary":"  Bayesian neural network (BNN) approximates the posterior distribution of\nmodel parameters and utilizes the posterior for prediction via Bayesian Model\nAveraging (BMA). The quality of the posterior approximation is critical for\nachieving accurate and robust predictions. It is known that flatness in the\nloss landscape is strongly associated with generalization performance, and it\nnecessitates consideration to improve the quality of the posterior\napproximation. In this work, we empirically demonstrate that BNNs often\nstruggle to capture the flatness. Moreover, we provide both experimental and\ntheoretical evidence showing that BMA can be ineffective without ensuring\nflatness. To address this, we propose Sharpness-Aware Bayesian Model Averaging\n(SA-BMA), a novel optimizer that seeks flat posteriors by calculating\ndivergence in the parameter space. SA-BMA aligns with the intrinsic nature of\nBNN and the generalized version of existing sharpness-aware optimizers for DNN.\nIn addition, we suggest a Bayesian Transfer Learning scheme to efficiently\nleverage pre-trained DNN. We validate the efficacy of SA-BMA in enhancing\ngeneralization performance in few-shot classification and distribution shift by\nensuring flat posterior.\n","authors":["Sungjun Lim","Jeyoon Yeom","Sooyon Kim","Hoyoon Byun","Jinho Kang","Yohan Jung","Jiyoung Jung","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2406.15664v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15847v1","updated":"2024-10-21T10:19:45Z","published":"2024-10-21T10:19:45Z","title":"Random Token Fusion for Multi-View Medical Diagnosis","summary":"  In multi-view medical diagnosis, deep learning-based models often fuse\ninformation from different imaging perspectives to improve diagnostic\nperformance. However, existing approaches are prone to overfitting and rely\nheavily on view-specific features, which can lead to trivial solutions. In this\nwork, we introduce Random Token Fusion (RTF), a novel technique designed to\nenhance multi-view medical image analysis using vision transformers. By\nintegrating randomness into the feature fusion process during training, RTF\naddresses the issue of overfitting and enhances the robustness and accuracy of\ndiagnostic models without incurring any additional cost at inference. We\nvalidate our approach on standard mammography and chest X-ray benchmark\ndatasets. Through extensive experiments, we demonstrate that RTF consistently\nimproves the performance of existing fusion methods, paving the way for a new\ngeneration of multi-view medical foundation models.\n","authors":["Jingyu Guo","Christos Matsoukas","Fredrik Strand","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2410.15847v1.pdf","comment":"Originally published at the NeurIPS 2024 Workshop on Advancements In\n  Medical Foundation Models: Explainability, Robustness, Security, and Beyond\n  (AIM-FM)"},{"id":"http://arxiv.org/abs/2410.15846v1","updated":"2024-10-21T10:16:56Z","published":"2024-10-21T10:16:56Z","title":"Modelling Concurrent RTP Flows for End-to-end Predictions of QoS in Real\n  Time Communications","summary":"  The Real-time Transport Protocol (RTP)-based real-time communications (RTC)\napplications, exemplified by video conferencing, have experienced an\nunparalleled surge in popularity and development in recent years. In pursuit of\noptimizing their performance, the prediction of Quality of Service (QoS)\nmetrics emerges as a pivotal endeavor, bolstering network monitoring and\nproactive solutions. However, contemporary approaches are confined to\nindividual RTP flows and metrics, falling short in relationship capture and\ncomputational efficiency. To this end, we propose Packet-to-Prediction (P2P), a\nnovel deep learning (DL) framework that hinges on raw packets to simultaneously\nprocess concurrent RTP flows and perform end-to-end prediction of multiple QoS\nmetrics. Specifically, we implement a streamlined architecture, namely\nlength-free Transformer with cross and neighbourhood attention, capable of\nhandling an unlimited number of RTP flows, and employ a multi-task learning\nparadigm to forecast four key metrics in a single shot. Our work is based on\nextensive traffic collected during real video calls, and conclusively, P2P\nexcels comparative models in both prediction performance and temporal\nefficiency.\n","authors":["Tailai Song","Paolo Garza","Michela Meo","Maurizio Matteo Munafò"],"pdf_url":"https://arxiv.org/pdf/2410.15846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15840v1","updated":"2024-10-21T10:03:03Z","published":"2024-10-21T10:03:03Z","title":"Private, Efficient and Scalable Kernel Learning for Medical Image\n  Analysis","summary":"  Medical imaging is key in modern medicine. From magnetic resonance imaging\n(MRI) to microscopic imaging for blood cell detection, diagnostic medical\nimaging reveals vital insights into patient health. To predict diseases or\nprovide individualized therapies, machine learning techniques like kernel\nmethods have been widely used. Nevertheless, there are multiple challenges for\nimplementing kernel methods. Medical image data often originates from various\nhospitals and cannot be combined due to privacy concerns, and the high\ndimensionality of image data presents another significant obstacle. While\nrandomised encoding offers a promising direction, existing methods often\nstruggle with a trade-off between accuracy and efficiency. Addressing the need\nfor efficient privacy-preserving methods on distributed image data, we\nintroduce OKRA (Orthonormal K-fRAmes), a novel randomized encoding-based\napproach for kernel-based machine learning. This technique, tailored for widely\nused kernel functions, significantly enhances scalability and speed compared to\ncurrent state-of-the-art solutions. Through experiments conducted on various\nclinical image datasets, we evaluated model quality, computational performance,\nand resource overhead. Additionally, our method outperforms comparable\napproaches\n","authors":["Anika Hannemann","Arjhun Swaminathan","Ali Burak Ünal","Mete Akgün"],"pdf_url":"https://arxiv.org/pdf/2410.15840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15827v1","updated":"2024-10-21T09:44:37Z","published":"2024-10-21T09:44:37Z","title":"Explainability of Highly Associated Fuzzy Churn Patterns in Binary\n  Classification","summary":"  Customer churn, particularly in the telecommunications sector, influences\nboth costs and profits. As the explainability of models becomes increasingly\nimportant, this study emphasizes not only the explainability of customer churn\nthrough machine learning models, but also the importance of identifying\nmultivariate patterns and setting soft bounds for intuitive interpretation. The\nmain objective is to use a machine learning model and fuzzy-set theory with\ntop-\\textit{k} HUIM to identify highly associated patterns of customer churn\nwith intuitive identification, referred to as Highly Associated Fuzzy Churn\nPatterns (HAFCP). Moreover, this method aids in uncovering association rules\namong multiple features across low, medium, and high distributions. Such\ndiscoveries are instrumental in enhancing the explainability of findings.\nExperiments show that when the top-5 HAFCPs are included in five datasets, a\nmixture of performance results is observed, with some showing notable\nimprovements. It becomes clear that high importance features enhance\nexplanatory power through their distribution and patterns associated with other\nfeatures. As a result, the study introduces an innovative approach that\nimproves the explainability and effectiveness of customer churn prediction\nmodels.\n","authors":["D. Y. C. Wang","Lars Arne Jordanger","Jerry Chun-Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2410.15827v1.pdf","comment":"18 pages single columns, 4 figures, This paper is an extended version\n  of a work originally presented at the 6th International Workshop on\n  Utility-Driven Mining and Learning (held in conjunction with the 28th\n  Pacific-Asia Conference on Knowledge Discovery and Data Mining - PAKDD 2024)\n  on May 7, 2024"},{"id":"http://arxiv.org/abs/2402.04494v2","updated":"2024-10-21T09:37:12Z","published":"2024-02-07T00:36:24Z","title":"Amortized Planning with Large-Scale Transformers: A Case Study on Chess","summary":"  This paper uses chess, a landmark planning problem in AI, to assess\ntransformers' performance on a planning task where memorization is futile\n$\\unicode{x2013}$ even at a large scale. To this end, we release ChessBench, a\nlarge-scale benchmark dataset of 10 million chess games with legal move and\nvalue annotations (15 billion data points) provided by Stockfish 16, the\nstate-of-the-art chess engine. We train transformers with up to 270 million\nparameters on ChessBench via supervised learning and perform extensive\nablations to assess the impact of dataset size, model size, architecture type,\nand different prediction targets (state-values, action-values, and behavioral\ncloning). Our largest models learn to predict action-values for novel boards\nquite accurately, implying highly non-trivial generalization. Despite\nperforming no explicit search, our resulting chess policy solves challenging\nchess puzzles and achieves a surprisingly strong Lichess blitz Elo of 2895\nagainst humans (grandmaster level). We also compare to Leela Chess Zero and\nAlphaZero (trained without supervision via self-play) with and without search.\nWe show that, although a remarkably good approximation of Stockfish's\nsearch-based algorithm can be distilled into large-scale transformers via\nsupervised learning, perfect distillation is still beyond reach, thus making\nChessBench well-suited for future research.\n","authors":["Anian Ruoss","Grégoire Delétang","Sourabh Medapati","Jordi Grau-Moya","Li Kevin Wenliang","Elliot Catt","John Reid","Cannada A. Lewis","Joel Veness","Tim Genewein"],"pdf_url":"https://arxiv.org/pdf/2402.04494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15819v1","updated":"2024-10-21T09:35:57Z","published":"2024-10-21T09:35:57Z","title":"LiMTR: Time Series Motion Prediction for Diverse Road Users through\n  Multimodal Feature Integration","summary":"  Predicting the behavior of road users accurately is crucial to enable the\nsafe operation of autonomous vehicles in urban or densely populated areas.\nTherefore, there has been a growing interest in time series motion prediction\nresearch, leading to significant advancements in state-of-the-art techniques in\nrecent years. However, the potential of using LiDAR data to capture more\ndetailed local features, such as a person's gaze or posture, remains largely\nunexplored. To address this, we develop a novel multimodal approach for motion\nprediction based on the PointNet foundation model architecture, incorporating\nlocal LiDAR features. Evaluation on the Waymo Open Dataset shows a performance\nimprovement of 6.20% and 1.58% in minADE and mAP respectively, when integrated\nand compared with the previous state-of-the-art MTR. We open-source the code of\nour LiMTR model.\n","authors":["Camiel Oerlemans","Bram Grooten","Michiel Braat","Alaa Alassi","Emilia Silvas","Decebal Constantin Mocanu"],"pdf_url":"https://arxiv.org/pdf/2410.15819v1.pdf","comment":"Accepted at the NeurIPS 2024 workshop Time Series in the Age of Large\n  Models. Code available at https://github.com/Cing2/LiMTR"},{"id":"http://arxiv.org/abs/2410.15815v1","updated":"2024-10-21T09:28:46Z","published":"2024-10-21T09:28:46Z","title":"Solvation Free Energies from Neural Thermodynamic Integration","summary":"  We propose to compute solvation free energies via thermodynamic integration\nalong a neural-network potential interpolating between two target Hamiltonians.\nWe use a stochastic interpolant to define an interpolation between the\ndistributions at the level of samples and optimize a neural network potential\nto match the corresponding equilibrium potential at every intermediate\ntime-step. Once the alignment between the interpolating samples and the\ninterpolating potentials is sufficiently accurate, the free-energy difference\nbetween the two Hamiltonians can be estimated using (neural) thermodynamic\nintegration. We validate our method to compute solvation free energies on\nseveral benchmark systems: a Lennard-Jones particle in a Lennard-Jones fluid,\nas well as the insertion of both water and methane solutes in a water solvent\nat atomistic resolution.\n","authors":["Bálint Máté","François Fleuret","Tristan Bereau"],"pdf_url":"https://arxiv.org/pdf/2410.15815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15808v1","updated":"2024-10-21T09:23:50Z","published":"2024-10-21T09:23:50Z","title":"Mean-Field Simulation-Based Inference for Cosmological Initial\n  Conditions","summary":"  Reconstructing cosmological initial conditions (ICs) from late-time\nobservations is a difficult task, which relies on the use of computationally\nexpensive simulators alongside sophisticated statistical methods to navigate\nmulti-million dimensional parameter spaces. We present a simple method for\nBayesian field reconstruction based on modeling the posterior distribution of\nthe initial matter density field to be diagonal Gaussian in Fourier space, with\nits covariance and the mean estimator being the trainable parts of the\nalgorithm. Training and sampling are extremely fast (training: $\\sim 1 \\,\n\\mathrm{h}$ on a GPU, sampling: $\\lesssim 3 \\, \\mathrm{s}$ for 1000 samples at\nresolution $128^3$), and our method supports industry-standard\n(non-differentiable) $N$-body simulators. We verify the fidelity of the\nobtained IC samples in terms of summary statistics.\n","authors":["Oleg Savchenko","Florian List","Guillermo Franco Abellán","Noemi Anau Montel","Christoph Weniger"],"pdf_url":"https://arxiv.org/pdf/2410.15808v1.pdf","comment":"Accepted for the NeurIPS 2024 workshop Machine Learning and the\n  Physical Sciences; 5 + 4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.15804v1","updated":"2024-10-21T09:22:16Z","published":"2024-10-21T09:22:16Z","title":"Deep Learning and Data Augmentation for Detecting Self-Admitted\n  Technical Debt","summary":"  Self-Admitted Technical Debt (SATD) refers to circumstances where developers\nuse textual artifacts to explain why the existing implementation is not\noptimal. Past research in detecting SATD has focused on either identifying SATD\n(classifying SATD items as SATD or not) or categorizing SATD (labeling\ninstances as SATD that pertain to requirement, design, code, test debt, etc.).\nHowever, the performance of these approaches remains suboptimal, particularly\nfor specific types of SATD, such as test and requirement debt, primarily due to\nextremely imbalanced datasets. To address these challenges, we build on earlier\nresearch by utilizing BiLSTM architecture for the binary identification of SATD\nand BERT architecture for categorizing different types of SATD. Despite their\neffectiveness, both architectures struggle with imbalanced data. Therefore, we\nemploy a large language model data augmentation strategy to mitigate this\nissue. Furthermore, we introduce a two-step approach to identify and categorize\nSATD across various datasets derived from different artifacts. Our\ncontributions include providing a balanced dataset for future SATD researchers\nand demonstrating that our approach significantly improves SATD identification\nand categorization performance compared to baseline methods.\n","authors":["Edi Sutoyo","Paris Avgeriou","Andrea Capiluppi"],"pdf_url":"https://arxiv.org/pdf/2410.15804v1.pdf","comment":"Accepted to be published at the 2024 31st Asia-Pacific Software\n  Engineering Conference (APSEC)"},{"id":"http://arxiv.org/abs/2410.02711v2","updated":"2024-10-21T09:22:05Z","published":"2024-10-03T17:35:38Z","title":"NETS: A Non-Equilibrium Transport Sampler","summary":"  We propose an algorithm, termed the Non-Equilibrium Transport Sampler (NETS),\nto sample from unnormalized probability distributions. NETS can be viewed as a\nvariant of annealed importance sampling (AIS) based on Jarzynski's equality, in\nwhich the stochastic differential equation used to perform the non-equilibrium\nsampling is augmented with an additional learned drift term that lowers the\nimpact of the unbiasing weights used in AIS. We show that this drift is the\nminimizer of a variety of objective functions, which can all be estimated in an\nunbiased fashion without backpropagating through solutions of the stochastic\ndifferential equations governing the sampling. We also prove that some these\nobjectives control the Kullback-Leibler divergence of the estimated\ndistribution from its target. NETS is shown to be unbiased and, in addition,\nhas a tunable diffusion coefficient which can be adjusted post-training to\nmaximize the effective sample size. We demonstrate the efficacy of the method\non standard benchmarks, high-dimensional Gaussian mixture distributions, and a\nmodel from statistical lattice field theory, for which it surpasses the\nperformances of related work and existing baselines.\n","authors":["Michael S. Albergo","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2410.02711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15800v1","updated":"2024-10-21T09:16:06Z","published":"2024-10-21T09:16:06Z","title":"On the VC dimension of deep group convolutional neural networks","summary":"  We study the generalization capabilities of Group Convolutional Neural\nNetworks (GCNNs) with ReLU activation function by deriving upper and lower\nbounds for their Vapnik-Chervonenkis (VC) dimension. Specifically, we analyze\nhow factors such as the number of layers, weights, and input dimension affect\nthe VC dimension. We further compare the derived bounds to those known for\nother types of neural networks. Our findings extend previous results on the VC\ndimension of continuous GCNNs with two layers, thereby providing new insights\ninto the generalization properties of GCNNs, particularly regarding the\ndependence on the input resolution of the data.\n","authors":["Anna Sepliarskaia","Sophie Langer","Johannes Schmidt-Hieber"],"pdf_url":"https://arxiv.org/pdf/2410.15800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07825v3","updated":"2024-10-21T09:14:47Z","published":"2024-09-12T08:15:39Z","title":"Deep Multimodal Learning with Missing Modality: A Survey","summary":"  During multimodal model training and testing, certain data modalities may be\nabsent due to sensor limitations, cost constraints, privacy concerns, or data\nloss, negatively affecting performance. Multimodal learning techniques designed\nto handle missing modalities can mitigate this by ensuring model robustness\neven when some modalities are unavailable. This survey reviews recent progress\nin Multimodal Learning with Missing Modality (MLMM), focusing on deep learning\nmethods. It provides the first comprehensive survey that covers the motivation\nand distinctions between MLMM and standard multimodal learning setups, followed\nby a detailed analysis of current methods, applications, and datasets,\nconcluding with challenges and future directions.\n","authors":["Renjie Wu","Hu Wang","Hsiang-Ting Chen","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2409.07825v3.pdf","comment":"Submitted to ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2407.13432v2","updated":"2024-10-21T09:12:21Z","published":"2024-07-18T12:01:09Z","title":"The Art of Imitation: Learning Long-Horizon Manipulation Tasks from Few\n  Demonstrations","summary":"  Task Parametrized Gaussian Mixture Models (TP-GMM) are a sample-efficient\nmethod for learning object-centric robot manipulation tasks. However, there are\nseveral open challenges to applying TP-GMMs in the wild. In this work, we\ntackle three crucial challenges synergistically. First, end-effector velocities\nare non-Euclidean and thus hard to model using standard GMMs. We thus propose\nto factorize the robot's end-effector velocity into its direction and\nmagnitude, and model them using Riemannian GMMs. Second, we leverage the\nfactorized velocities to segment and sequence skills from complex demonstration\ntrajectories. Through the segmentation, we further align skill trajectories and\nhence leverage time as a powerful inductive bias. Third, we present a method to\nautomatically detect relevant task parameters per skill from visual\nobservations. Our approach enables learning complex manipulation tasks from\njust five demonstrations while using only RGB-D observations. Extensive\nexperimental evaluations on RLBench demonstrate that our approach achieves\nstate-of-the-art performance with 20-fold improved sample efficiency. Our\npolicies generalize across different environments, object instances, and object\npositions, while the learned skills are reusable.\n","authors":["Jan Ole von Hartz","Tim Welschehold","Abhinav Valada","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2407.13432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20648v2","updated":"2024-10-21T08:52:10Z","published":"2024-05-31T07:30:24Z","title":"Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision\n  Models For Video Captioning and Summarization","summary":"  Video is an increasingly prominent and information-dense medium, yet it poses\nsubstantial challenges for language models. A typical video consists of a\nsequence of shorter segments, or shots, that collectively form a coherent\nnarrative. Each shot is analogous to a word in a sentence where multiple data\nstreams of information (such as visual and auditory data) must be processed\nsimultaneously. Comprehension of the entire video requires not only\nunderstanding the visual-audio information of each shot but also requires that\nthe model links the ideas between each shot to generate a larger,\nall-encompassing story. Despite significant progress in the field, current\nworks often overlook videos' more granular shot-by-shot semantic information.\nIn this project, we propose a family of efficient large language vision models\n(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By\nleveraging better pretraining and data collection strategies, we extend the\nabilities of existing small LLVMs from being able to understand a picture to\nbeing able to understand a sequence of frames. Specifically, we show that\nShotluck Holmes achieves better performance than state-of-the-art results on\nthe Shot2Story video captioning and summary task with significantly smaller and\nmore computationally efficient models.\n","authors":["Richard Luo","Austin Peng","Adithya Vasudev","Rishabh Jain"],"pdf_url":"https://arxiv.org/pdf/2405.20648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15787v1","updated":"2024-10-21T08:49:51Z","published":"2024-10-21T08:49:51Z","title":"Arithmetic Transformers Can Length-Generalize in Both Operand Length and\n  Count","summary":"  Transformers often struggle with length generalization, meaning they fail to\ngeneralize to sequences longer than those encountered during training. While\narithmetic tasks are commonly used to study length generalization, certain\ntasks are considered notoriously difficult, e.g., multi-operand addition\n(requiring generalization over both the number of operands and their lengths)\nand multiplication (requiring generalization over both operand lengths). In\nthis work, we achieve approximately 2-3x length generalization on both tasks,\nwhich is the first such achievement in arithmetic Transformers. We design\ntask-specific scratchpads enabling the model to focus on a fixed number of\ntokens per each next-token prediction step, and apply multi-level versions of\nPosition Coupling (Cho et al., 2024; McLeish et al., 2024) to let Transformers\nknow the right position to attend to. On the theory side, we prove that a\n1-layer Transformer using our method can solve multi-operand addition, up to\noperand length and operand count that are exponential in embedding dimension.\n","authors":["Hanseul Cho","Jaeyoung Cha","Srinadh Bhojanapalli","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2410.15787v1.pdf","comment":"38 pages, 16 figures"},{"id":"http://arxiv.org/abs/2408.16457v2","updated":"2024-10-21T08:47:29Z","published":"2024-08-29T11:45:01Z","title":"HYGENE: A Diffusion-based Hypergraph Generation Method","summary":"  Hypergraphs are powerful mathematical structures that can model complex,\nhigh-order relationships in various domains, including social networks,\nbioinformatics, and recommender systems. However, generating realistic and\ndiverse hypergraphs remains challenging due to their inherent complexity and\nlack of effective generative models. In this paper, we introduce a\ndiffusion-based Hypergraph Generation (HYGENE) method that addresses these\nchallenges through a progressive local expansion approach. HYGENE works on the\nbipartite representation of hypergraphs, starting with a single pair of\nconnected nodes and iteratively expanding it to form the target hypergraph. At\neach step, nodes and hyperedges are added in a localized manner using a\ndenoising diffusion process, which allows for the construction of the global\nstructure before refining local details. Our experiments demonstrated the\neffectiveness of HYGENE, proving its ability to closely mimic a variety of\nproperties in hypergraphs. To the best of our knowledge, this is the first\nattempt to employ deep learning models for hypergraph generation, and our work\naims to lay the groundwork for future research in this area.\n","authors":["Dorian Gailhard","Enzo Tartaglione","Lirida Naviner","Jhony H. Giraldo"],"pdf_url":"https://arxiv.org/pdf/2408.16457v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.11529 by other authors"},{"id":"http://arxiv.org/abs/2410.06717v2","updated":"2024-10-21T08:45:30Z","published":"2024-10-09T09:41:28Z","title":"Exact full-RSB SAT/UNSAT transition in infinitely wide two-layer neural\n  networks","summary":"  We analyze the problem of storing random pattern-label associations using two\nclasses of continuous non-convex weights models, namely the perceptron with\nnegative margin and an infinite-width two-layer neural network with\nnon-overlapping receptive fields and generic activation function. Using a\nfull-RSB ansatz we compute the exact value of the SAT/UNSAT transition.\nFurthermore, in the case of the negative perceptron we show that the overlap\ndistribution of typical states displays an overlap gap (a disconnected support)\nin certain regions of the phase diagram defined by the value of the margin and\nthe density of patterns to be stored. This implies that some recent theorems\nthat ensure convergence of Approximate Message Passing (AMP) based algorithms\nto capacity are not applicable. Finally, we show that Gradient Descent is not\nable to reach the maximal capacity, irrespectively of the presence of an\noverlap gap for typical states. This finding, similarly to what occurs in\nbinary weight models, suggests that gradient-based algorithms are biased\ntowards highly atypical states, whose inaccessibility determines the\nalgorithmic threshold.\n","authors":["Brandon L. Annesi","Enrico M. Malatesta","Francesco Zamponi"],"pdf_url":"https://arxiv.org/pdf/2410.06717v2.pdf","comment":"38 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.18698v2","updated":"2024-10-21T08:43:41Z","published":"2024-07-26T12:23:54Z","title":"Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended\n  Text Generation","summary":"  Decoding from the output distributions of large language models to produce\nhigh-quality text is a complex challenge in language modeling. Various\napproaches, such as beam search, sampling with temperature, $k-$sampling,\nnucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive\nsearch, have been proposed to address this problem, aiming to improve\ncoherence, diversity, as well as resemblance to human-generated text. In this\nstudy, we introduce adaptive contrastive search, a novel decoding strategy\nextending contrastive search by incorporating an adaptive degeneration penalty,\nguided by the estimated uncertainty of the model at each generation step. This\nstrategy is designed to enhance both the creativity and diversity of the\nlanguage modeling process while at the same time producing coherent and\nhigh-quality generated text output. Our findings indicate performance\nenhancement in both aspects, across different model architectures and datasets,\nunderscoring the effectiveness of our method in text generation tasks. Our code\nbase, datasets, and models are publicly available.\n","authors":["Esteban Garces Arias","Julian Rodemann","Meimingwei Li","Christian Heumann","Matthias Aßenmacher"],"pdf_url":"https://arxiv.org/pdf/2407.18698v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15778v1","updated":"2024-10-21T08:42:30Z","published":"2024-10-21T08:42:30Z","title":"Reducing Hallucinations in Vision-Language Models via Latent Space\n  Steering","summary":"  Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.\n","authors":["Sheng Liu","Haotian Ye","James Zou"],"pdf_url":"https://arxiv.org/pdf/2410.15778v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.15777v1","updated":"2024-10-21T08:42:10Z","published":"2024-10-21T08:42:10Z","title":"High-Fidelity Transfer of Functional Priors for Wide Bayesian Neural\n  Networks by Learning Activations","summary":"  Function-space priors in Bayesian Neural Networks provide a more intuitive\napproach to embedding beliefs directly into the model's output, thereby\nenhancing regularization, uncertainty quantification, and risk-aware\ndecision-making. However, imposing function-space priors on BNNs is\nchallenging. We address this task through optimization techniques that explore\nhow trainable activations can accommodate complex priors and match intricate\ntarget function distributions. We discuss critical learning challenges,\nincluding identifiability, loss construction, and symmetries that arise in this\ncontext. Furthermore, we enable evidence maximization to facilitate model\nselection by conditioning the functional priors on additional hyperparameters.\nOur empirical findings demonstrate that even BNNs with a single wide hidden\nlayer, when equipped with these adaptive trainable activations and conditioning\nstrategies, can effectively achieve high-fidelity function-space priors,\nproviding a robust and flexible framework for enhancing Bayesian neural network\nperformance.\n","authors":["Marcin Sendera","Amin Sorkhei","Tomasz Kuśmierczyk"],"pdf_url":"https://arxiv.org/pdf/2410.15777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09640v2","updated":"2024-10-21T08:33:44Z","published":"2024-10-12T20:33:37Z","title":"Provable Acceleration of Nesterov's Accelerated Gradient for Rectangular\n  Matrix Factorization and Linear Neural Networks","summary":"  We study the convergence rate of first-order methods for rectangular matrix\nfactorization, which is a canonical nonconvex optimization problem.\nSpecifically, given a rank-$r$ matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, we\nprove that gradient descent (GD) can find a pair of $\\epsilon$-optimal\nsolutions $\\mathbf{X}_T\\in\\mathbb{R}^{m\\times d}$ and\n$\\mathbf{Y}_T\\in\\mathbb{R}^{n\\times d}$, where $d\\geq r$, satisfying\n$\\lVert\\mathbf{X}_T\\mathbf{Y}_T^\\top-\\mathbf{A}\\rVert_\\mathrm{F}\\leq\\epsilon\\lVert\\mathbf{A}\\rVert_\\mathrm{F}$\nin $T=O(\\kappa^2\\log\\frac{1}{\\epsilon})$ iterations with high probability,\nwhere $\\kappa$ denotes the condition number of $\\mathbf{A}$. Furthermore, we\nprove that Nesterov's accelerated gradient (NAG) attains an iteration\ncomplexity of $O(\\kappa\\log\\frac{1}{\\epsilon})$, which is the best-known bound\nof first-order methods for rectangular matrix factorization. Different from\nsmall balanced random initialization in the existing literature, we adopt an\nunbalanced initialization, where $\\mathbf{X}_0$ is large and $\\mathbf{Y}_0$ is\n$0$. Moreover, our initialization and analysis can be further extended to\nlinear neural networks, where we prove that NAG can also attain an accelerated\nlinear convergence rate. In particular, we only require the width of the\nnetwork to be greater than or equal to the rank of the output label matrix. In\ncontrast, previous results achieving the same rate require excessive widths\nthat additionally depend on the condition number and the rank of the input data\nmatrix.\n","authors":["Zhenghao Xu","Yuqing Wang","Tuo Zhao","Rachel Ward","Molei Tao"],"pdf_url":"https://arxiv.org/pdf/2410.09640v2.pdf","comment":"30 pages (checklist included), fix typos"},{"id":"http://arxiv.org/abs/2410.10504v2","updated":"2024-10-21T08:33:05Z","published":"2024-10-14T13:46:58Z","title":"A Kernelizable Primal-Dual Formulation of the Multilinear Singular Value\n  Decomposition","summary":"  The ability to express a learning task in terms of a primal and a dual\noptimization problem lies at the core of a plethora of machine learning\nmethods. For example, Support Vector Machine (SVM), Least-Squares Support\nVector Machine (LS-SVM), Ridge Regression (RR), Lasso Regression (LR),\nPrincipal Component Analysis (PCA), and more recently Singular Value\nDecomposition (SVD) have all been defined either in terms of primal weights or\nin terms of dual Lagrange multipliers. The primal formulation is\ncomputationally advantageous in the case of large sample size while the dual is\npreferred for high-dimensional data. Crucially, said learning problems can be\nmade nonlinear through the introduction of a feature map in the primal problem,\nwhich corresponds to applying the kernel trick in the dual. In this paper we\nderive a primal-dual formulation of the Multilinear Singular Value\nDecomposition (MLSVD), which recovers as special cases both PCA and SVD.\nBesides enabling computational gains through the derived primal formulation, we\npropose a nonlinear extension of the MLSVD using feature maps, which results in\na dual problem where a kernel tensor arises. We discuss potential applications\nin the context of signal analysis and deep learning.\n","authors":["Frederiek Wesel","Kim Batselier"],"pdf_url":"https://arxiv.org/pdf/2410.10504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15772v1","updated":"2024-10-21T08:32:02Z","published":"2024-10-21T08:32:02Z","title":"Mislabeled examples detection viewed as probing machine learning models:\n  concepts, survey and extensive benchmark","summary":"  Mislabeled examples are ubiquitous in real-world machine learning datasets,\nadvocating the development of techniques for automatic detection. We show that\nmost mislabeled detection methods can be viewed as probing trained machine\nlearning models using a few core principles. We formalize a modular framework\nthat encompasses these methods, parameterized by only 4 building blocks, as\nwell as a Python library that demonstrates that these principles can actually\nbe implemented. The focus is on classifier-agnostic concepts, with an emphasis\non adapting methods developed for deep learning models to non-deep classifiers\nfor tabular data. We benchmark existing methods on (artificial) Completely At\nRandom (NCAR) as well as (realistic) Not At Random (NNAR) labeling noise from a\nvariety of tasks with imperfect labeling rules. This benchmark provides new\ninsights as well as limitations of existing methods in this setup.\n","authors":["Thomas George","Pierre Nodet","Alexis Bondu","Vincent Lemaire"],"pdf_url":"https://arxiv.org/pdf/2410.15772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16788v4","updated":"2024-10-21T08:27:23Z","published":"2024-02-26T18:01:41Z","title":"Why Transformers Need Adam: A Hessian Perspective","summary":"  SGD performs worse than Adam by a significant margin on Transformers, but the\nreason remains unclear. In this work, we provide an explanation through the\nlens of Hessian: (i) Transformers are \"heterogeneous\": the Hessian spectrum\nacross parameter blocks vary dramatically, a phenomenon we call \"block\nheterogeneity\"; (ii) Heterogeneity hampers SGD: SGD performs worse than Adam on\nproblems with block heterogeneity. To validate (i) and (ii), we check various\nTransformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform\non par with Adam on problems without block heterogeneity, but performs worse\nthan Adam when the heterogeneity exists. Our initial theoretical analysis\nindicates that SGD performs worse because it applies one single learning rate\nto all blocks, which cannot handle the heterogeneity among blocks. This\nlimitation could be ameliorated if we use coordinate-wise learning rates, as\ndesigned in Adam.\n","authors":["Yushun Zhang","Congliang Chen","Tian Ding","Ziniu Li","Ruoyu Sun","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2402.16788v4.pdf","comment":"Advances in Neural Information Processing Systems, 2024"},{"id":"http://arxiv.org/abs/2403.19381v2","updated":"2024-10-21T08:26:52Z","published":"2024-03-28T12:42:25Z","title":"On Uncertainty Quantification for Near-Bayes Optimal Algorithms","summary":"  Bayesian modelling allows for the quantification of predictive uncertainty\nwhich is crucial in safety-critical applications. Yet for many machine learning\n(ML) algorithms, it is difficult to construct or implement their Bayesian\ncounterpart. In this work we present a promising approach to address this\nchallenge, based on the hypothesis that commonly used ML algorithms are\nefficient across a wide variety of tasks and may thus be near Bayes-optimal\nw.r.t. an unknown task distribution. We prove that it is possible to recover\nthe Bayesian posterior defined by the task distribution, which is unknown but\noptimal in this setting, by building a martingale posterior using the\nalgorithm. We further propose a practical uncertainty quantification method\nthat apply to general ML algorithms. Experiments based on a variety of non-NN\nand NN algorithms demonstrate the efficacy of our method.\n","authors":["Ziyu Wang","Chris Holmes"],"pdf_url":"https://arxiv.org/pdf/2403.19381v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15765v1","updated":"2024-10-21T08:24:44Z","published":"2024-10-21T08:24:44Z","title":"SeisLM: a Foundation Model for Seismic Waveforms","summary":"  We introduce the Seismic Language Model (SeisLM), a foundational model\ndesigned to analyze seismic waveforms -- signals generated by Earth's\nvibrations such as the ones originating from earthquakes. SeisLM is pretrained\non a large collection of open-source seismic datasets using a self-supervised\ncontrastive loss, akin to BERT in language modeling. This approach allows the\nmodel to learn general seismic waveform patterns from unlabeled data without\nbeing tied to specific downstream tasks. When fine-tuned, SeisLM excels in\nseismological tasks like event detection, phase-picking, onset time regression,\nand foreshock-aftershock classification. The code has been made publicly\navailable on https://github.com/liutianlin0121/seisLM.\n","authors":["Tianlin Liu","Jannes Münchmeyer","Laura Laurenti","Chris Marone","Maarten V. de Hoop","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2410.15765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15762v1","updated":"2024-10-21T08:21:25Z","published":"2024-10-21T08:21:25Z","title":"Solving Sparse \\& High-Dimensional-Output Regression via Compression","summary":"  Multi-Output Regression (MOR) has been widely used in scientific data\nanalysis for decision-making. Unlike traditional regression models, MOR aims to\nsimultaneously predict multiple real-valued outputs given an input. However,\nthe increasing dimensionality of the outputs poses significant challenges\nregarding interpretability and computational scalability for modern MOR\napplications. As a first step to address these challenges, this paper proposes\na Sparse \\& High-dimensional-Output REgression (SHORE) model by incorporating\nadditional sparsity requirements to resolve the output interpretability, and\nthen designs a computationally efficient two-stage optimization framework\ncapable of solving SHORE with provable accuracy via compression on outputs.\nTheoretically, we show that the proposed framework is computationally scalable\nwhile maintaining the same order of training loss and prediction loss\nbefore-and-after compression under arbitrary or relatively weak sample set\nconditions. Empirically, numerical results further validate the theoretical\nfindings, showcasing the efficiency and accuracy of the proposed framework.\n","authors":["Renyuan Li","Zhehui Chen","Guanyi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15762v1.pdf","comment":"Admitted in Neurips 2024"},{"id":"http://arxiv.org/abs/2410.15761v1","updated":"2024-10-21T08:21:00Z","published":"2024-10-21T08:21:00Z","title":"Learning-to-Defer for Extractive Question Answering","summary":"  Pre-trained language models have profoundly impacted the field of extractive\nquestion-answering, leveraging large-scale textual corpora to enhance\ncontextual language understanding. Despite their success, these models struggle\nin complex scenarios that demand nuanced interpretation or inferential\nreasoning beyond immediate textual cues. Furthermore, their size poses\ndeployment challenges on resource-constrained devices. Addressing these\nlimitations, we introduce an adapted two-stage Learning-to-Defer mechanism that\nenhances decision-making by enabling selective deference to human experts or\nlarger models without retraining language models in the context of\nquestion-answering. This approach not only maintains computational efficiency\nbut also significantly improves model reliability and accuracy in ambiguous\ncontexts. We establish the theoretical soundness of our methodology by proving\nBayes and $(\\mathcal{H}, \\mathcal{R})$--consistency of our surrogate loss\nfunction, guaranteeing the optimality of the final solution. Empirical\nevaluations on the SQuADv2 dataset illustrate performance gains from\nintegrating human expertise and leveraging larger models. Our results further\ndemonstrate that deferring a minimal number of queries allows the smaller model\nto achieve performance comparable to their larger counterparts while preserving\ncomputing efficiency, thus broadening the applicability of pre-trained language\nmodels in diverse operational environments.\n","authors":["Montreuil Yannis","Carlier Axel","Ng Lai Xing","Ooi Wei Tsang"],"pdf_url":"https://arxiv.org/pdf/2410.15761v1.pdf","comment":"25 pages, 17 main paper"},{"id":"http://arxiv.org/abs/2410.15742v1","updated":"2024-10-21T08:01:08Z","published":"2024-10-21T08:01:08Z","title":"DeepVigor+: Scalable and Accurate Semi-Analytical Fault Resilience\n  Analysis for Deep Neural Network","summary":"  Growing exploitation of Machine Learning (ML) in safety-critical applications\nnecessitates rigorous safety analysis. Hardware reliability assessment is a\nmajor concern with respect to measuring the level of safety. Quantifying the\nreliability of emerging ML models, including Deep Neural Networks (DNNs), is\nhighly complex due to their enormous size in terms of the number of parameters\nand computations. Conventionally, Fault Injection (FI) is applied to perform a\nreliability measurement. However, performing FI on modern-day DNNs is\nprohibitively time-consuming if an acceptable confidence level is to be\nachieved. In order to speed up FI for large DNNs, statistical FI has been\nproposed. However, the run-time for the large DNN models is still considerably\nlong.\n  In this work, we introduce DeepVigor+, a scalable, fast and accurate\nsemi-analytical method as an efficient alternative for reliability measurement\nin DNNs. DeepVigor+ implements a fault propagation analysis model and attempts\nto acquire Vulnerability Factors (VFs) as reliability metrics in an optimal\nway. The results indicate that DeepVigor+ obtains VFs for DNN models with an\nerror less than 1\\% and 14.9 up to 26.9 times fewer simulations than the\nbest-known state-of-the-art statistical FI enabling an accurate reliability\nanalysis for emerging DNNs within a few minutes.\n","authors":["Mohammad Hasan Ahmadilivani","Jaan Raik","Masoud Daneshtalab","Maksim Jenihhin"],"pdf_url":"https://arxiv.org/pdf/2410.15742v1.pdf","comment":"14 pages, 9 figures, 8 tables, 16 equations. The source code is\n  accessible via: https://github.com/mhahmadilivany/DeepVigor"},{"id":"http://arxiv.org/abs/2410.13286v2","updated":"2024-10-21T08:00:06Z","published":"2024-10-17T07:32:24Z","title":"A Human-in-the-Loop Fairness-Aware Model Selection Framework for Complex\n  Fairness Objective Landscapes","summary":"  Fairness-aware Machine Learning (FairML) applications are often characterized\nby complex social objectives and legal requirements, frequently involving\nmultiple, potentially conflicting notions of fairness. Despite the well-known\nImpossibility Theorem of Fairness and extensive theoretical research on the\nstatistical and socio-technical trade-offs between fairness metrics, many\nFairML tools still optimize or constrain for a single fairness objective.\nHowever, this one-sided optimization can inadvertently lead to violations of\nother relevant notions of fairness. In this socio-technical and empirical\nstudy, we frame fairness as a many-objective (MaO) problem by treating fairness\nmetrics as conflicting objectives. We introduce ManyFairHPO, a\nhuman-in-the-loop, fairness-aware model selection framework that enables\npractitioners to effectively navigate complex and nuanced fairness objective\nlandscapes. ManyFairHPO aids in the identification, evaluation, and balancing\nof fairness metric conflicts and their related social consequences, leading to\nmore informed and socially responsible model-selection decisions. Through a\ncomprehensive empirical evaluation and a case study on the Law School\nAdmissions problem, we demonstrate the effectiveness of ManyFairHPO in\nbalancing multiple fairness objectives, mitigating risks such as\nself-fulfilling prophecies, and providing interpretable insights to guide\nstakeholders in making fairness-aware modeling decisions.\n","authors":["Jake Robertson","Thorsten Schmidt","Frank Hutter","Noor Awad"],"pdf_url":"https://arxiv.org/pdf/2410.13286v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14066v2","updated":"2024-10-21T07:50:28Z","published":"2024-10-17T22:28:07Z","title":"Lightweight Correlation-Aware Table Compression","summary":"  The growing adoption of data lakes for managing relational data necessitates\nefficient, open storage formats that provide high scan performance and\ncompetitive compression ratios. While existing formats achieve fast scans\nthrough lightweight encoding techniques, they have reached a plateau in terms\nof minimizing storage footprint. Recently, correlation-aware compression\nschemes have been shown to reduce file sizes further. Yet, current approaches\neither incur significant scan overheads or require manual specification of\ncorrelations, limiting their practicability. We present $\\texttt{Virtual}$, a\nframework that integrates seamlessly with existing open formats to\nautomatically leverage data correlations, achieving substantial compression\ngains while having minimal scan performance overhead. Experiments on data-gov\ndatasets show that $\\texttt{Virtual}$ reduces file sizes by up to 40% compared\nto Apache Parquet.\n","authors":["Mihail Stoian","Alexander van Renen","Jan Kobiolka","Ping-Lin Kuo","Josif Grabocka","Andreas Kipf"],"pdf_url":"https://arxiv.org/pdf/2410.14066v2.pdf","comment":"Third Table Representation Learning Workshop (TRL @ NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.15729v1","updated":"2024-10-21T07:44:57Z","published":"2024-10-21T07:44:57Z","title":"Two-stage Learning-to-Defer for Multi-Task Learning","summary":"  The Learning-to-Defer approach has been explored for classification and, more\nrecently, regression tasks separately. Many contemporary learning tasks,\nhowever, involves both classification and regression components. In this paper,\nwe introduce a Learning-to-Defer approach for multi-task learning that\nencompasses both classification and regression tasks. Our two-stage approach\nutilizes a rejector that defers decisions to the most accurate agent among a\npre-trained joint classifier-regressor models and one or more external experts.\nWe show that our surrogate loss is $(\\mathcal{H}, \\mathcal{F}, \\mathcal{R})$\nand Bayes--consistent, ensuring an effective approximation of the optimal\nsolution. Additionally, we derive learning bounds that demonstrate the benefits\nof employing multiple confident experts along a rich model in a two-stage\nlearning framework. Empirical experiments conducted on electronic health record\nanalysis tasks underscore the performance enhancements achieved through our\nmethod.\n","authors":["Montreuil Yannis","Yeo Shu Heng","Carlier Axel","Ng Lai Xing","Ooi Wei Tsang"],"pdf_url":"https://arxiv.org/pdf/2410.15729v1.pdf","comment":"32 pages, 17 main paper"},{"id":"http://arxiv.org/abs/2410.15728v1","updated":"2024-10-21T07:44:44Z","published":"2024-10-21T07:44:44Z","title":"Object-Centric Temporal Consistency via Conditional Autoregressive\n  Inductive Biases","summary":"  Unsupervised object-centric learning from videos is a promising approach\ntowards learning compositional representations that can be applied to various\ndownstream tasks, such as prediction and reasoning. Recently, it was shown that\npretrained Vision Transformers (ViTs) can be useful to learn object-centric\nrepresentations on real-world video datasets. However, while these approaches\nsucceed at extracting objects from the scenes, the slot-based representations\nfail to maintain temporal consistency across consecutive frames in a video,\ni.e. the mapping of objects to slots changes across the video. To address this,\nwe introduce Conditional Autoregressive Slot Attention (CA-SA), a framework\nthat enhances the temporal consistency of extracted object-centric\nrepresentations in video-centric vision tasks. Leveraging an autoregressive\nprior network to condition representations on previous timesteps and a novel\nconsistency loss function, CA-SA predicts future slot representations and\nimposes consistency across frames. We present qualitative and quantitative\nresults showing that our proposed method outperforms the considered baselines\non downstream tasks, such as video prediction and visual question-answering\ntasks.\n","authors":["Cristian Meo","Akihiro Nakano","Mircea Lică","Aniket Didolkar","Masahiro Suzuki","Anirudh Goyal","Mengmi Zhang","Justin Dauwels","Yutaka Matsuo","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2410.15728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00843v2","updated":"2024-10-21T07:43:39Z","published":"2024-06-30T22:33:47Z","title":"A Unified Approach to Extract Interpretable Rules from Tree Ensembles\n  via Integer Programming","summary":"  Tree ensemble methods represent a popular machine learning model, known for\ntheir effectiveness in supervised classification and regression tasks. Their\nperformance derives from aggregating predictions of multiple decision trees,\nwhich are renowned for their interpretability properties. However, tree\nensemble methods do not reliably exhibit interpretable output. Our work aims to\nextract an optimized list of rules from a trained tree ensemble, providing the\nuser with a condensed, interpretable model that retains most of the predictive\npower of the full model. Our approach consists of solving a clean and neat set\npartitioning problem formulated through Integer Programming. The proposed\nmethod works with either tabular or time series data, for both classification\nand regression tasks, and does not require parameter tuning under the most\ncommon setting. Through rigorous computational experiments, we offer\nstatistically significant evidence that our method is competitive with other\nrule extraction methods and effectively handles time series.\n","authors":["Lorenzo Bonasera","Emilio Carrizosa"],"pdf_url":"https://arxiv.org/pdf/2407.00843v2.pdf","comment":"- Fixed several typos - Related work have been expanded - Discussion\n  of computational results has been improved for clearness"},{"id":"http://arxiv.org/abs/2410.15723v1","updated":"2024-10-21T07:42:43Z","published":"2024-10-21T07:42:43Z","title":"S-CFE: Simple Counterfactual Explanations","summary":"  We study the problem of finding optimal sparse, manifold-aligned\ncounterfactual explanations for classifiers. Canonically, this can be\nformulated as an optimization problem with multiple non-convex components,\nincluding classifier loss functions and manifold alignment (or\n\\emph{plausibility}) metrics. The added complexity of enforcing\n\\emph{sparsity}, or shorter explanations, complicates the problem further.\nExisting methods often focus on specific models and plausibility measures,\nrelying on convex $\\ell_1$ regularizers to enforce sparsity. In this paper, we\ntackle the canonical formulation using the accelerated proximal gradient (APG)\nmethod, a simple yet efficient first-order procedure capable of handling smooth\nnon-convex objectives and non-smooth $\\ell_p$ (where $0 \\leq p < 1$)\nregularizers. This enables our approach to seamlessly incorporate various\nclassifiers and plausibility measures while producing sparser solutions. Our\nalgorithm only requires differentiable data-manifold regularizers and supports\nbox constraints for bounded feature ranges, ensuring the generated\ncounterfactuals remain \\emph{actionable}. Finally, experiments on real-world\ndatasets demonstrate that our approach effectively produces sparse,\nmanifold-aligned counterfactual explanations while maintaining proximity to the\nfactual data and computational efficiency.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sai Ganesh Nagarajan","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2410.15723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15721v1","updated":"2024-10-21T07:39:44Z","published":"2024-10-21T07:39:44Z","title":"Learning signals defined on graphs with optimal transport and Gaussian\n  process regression","summary":"  In computational physics, machine learning has now emerged as a powerful\ncomplementary tool to explore efficiently candidate designs in engineering\nstudies. Outputs in such supervised problems are signals defined on meshes, and\na natural question is the extension of general scalar output regression models\nto such complex outputs. Changes between input geometries in terms of both size\nand adjacency structure in particular make this transition non-trivial. In this\nwork, we propose an innovative strategy for Gaussian process regression where\ninputs are large and sparse graphs with continuous node attributes and outputs\nare signals defined on the nodes of the associated inputs. The methodology\nrelies on the combination of regularized optimal transport, dimension reduction\ntechniques, and the use of Gaussian processes indexed by graphs. In addition to\nenabling signal prediction, the main point of our proposal is to come with\nconfidence intervals on node values, which is crucial for uncertainty\nquantification and active learning. Numerical experiments highlight the\nefficiency of the method to solve real problems in fluid dynamics and solid\nmechanics.\n","authors":["Raphaël Carpintero Perez","Sébastien da Veiga","Josselin Garnier","Brian Staber"],"pdf_url":"https://arxiv.org/pdf/2410.15721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15716v1","updated":"2024-10-21T07:34:17Z","published":"2024-10-21T07:34:17Z","title":"Traffic Matrix Estimation based on Denoising Diffusion Probabilistic\n  Model","summary":"  The traffic matrix estimation (TME) problem has been widely researched for\ndecades of years. Recent progresses in deep generative models offer new\nopportunities to tackle TME problems in a more advanced way. In this paper, we\nleverage the powerful ability of denoising diffusion probabilistic models\n(DDPMs) on distribution learning, and for the first time adopt DDPM to address\nthe TME problem. To ensure a good performance of DDPM on learning the\ndistributions of TMs, we design a preprocessing module to reduce the dimensions\nof TMs while keeping the data variety of each OD flow. To improve the\nestimation accuracy, we parameterize the noise factors in DDPM and transform\nthe TME problem into a gradient-descent optimization problem. Finally, we\ncompared our method with the state-of-the-art TME methods using two real-world\nTM datasets, the experimental results strongly demonstrate the superiority of\nour method on both TM synthesis and TM estimation.\n","authors":["Xinyu Yuan","Yan Qiao","Pei Zhao","Rongyao Hu","Benchu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15714v1","updated":"2024-10-21T07:33:42Z","published":"2024-10-21T07:33:42Z","title":"Offline reinforcement learning for job-shop scheduling problems","summary":"  Recent advances in deep learning have shown significant potential for solving\ncombinatorial optimization problems in real-time. Unlike traditional methods,\ndeep learning can generate high-quality solutions efficiently, which is crucial\nfor applications like routing and scheduling. However, existing approaches like\ndeep reinforcement learning (RL) and behavioral cloning have notable\nlimitations, with deep RL suffering from slow learning and behavioral cloning\nrelying solely on expert actions, which can lead to generalization issues and\nneglect of the optimization objective. This paper introduces a novel offline RL\nmethod designed for combinatorial optimization problems with complex\nconstraints, where the state is represented as a heterogeneous graph and the\naction space is variable. Our approach encodes actions in edge attributes and\nbalances expected rewards with the imitation of expert solutions. We\ndemonstrate the effectiveness of this method on job-shop scheduling and\nflexible job-shop scheduling benchmarks, achieving superior performance\ncompared to state-of-the-art techniques.\n","authors":["Imanol Echeverria","Maialen Murua","Roberto Santana"],"pdf_url":"https://arxiv.org/pdf/2410.15714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10185v2","updated":"2024-10-21T07:30:29Z","published":"2022-06-21T08:39:12Z","title":"Federated Stochastic Approximation under Markov Noise and Heterogeneity:\n  Applications in Reinforcement Learning","summary":"  Since reinforcement learning algorithms are notoriously data-intensive, the\ntask of sampling observations from the environment is usually split across\nmultiple agents. However, transferring these observations from the agents to a\ncentral location can be prohibitively expensive in terms of communication cost,\nand it can also compromise the privacy of each agent's local behavior policy.\nFederated reinforcement learning is a framework in which $N$ agents\ncollaboratively learn a global model, without sharing their individual data and\npolicies. This global model is the unique fixed point of the average of $N$\nlocal operators, corresponding to the $N$ agents. Each agent maintains a local\ncopy of the global model and updates it using locally sampled data. In this\npaper, we show that by careful collaboration of the agents in solving this\njoint fixed point problem, we can find the global model $N$ times faster, also\nknown as linear speedup. We first propose a general framework for federated\nstochastic approximation with Markovian noise and heterogeneity, showing linear\nspeedup in convergence. We then apply this framework to federated reinforcement\nlearning algorithms, examining the convergence of federated on-policy TD,\noff-policy TD, and $Q$-learning.\n","authors":["Sajad Khodadadian","Pranay Sharma","Gauri Joshi","Siva Theja Maguluri"],"pdf_url":"https://arxiv.org/pdf/2206.10185v2.pdf","comment":"80 pages, 0 figure, accepted to ICML 2022 for long presentation"},{"id":"http://arxiv.org/abs/2410.15706v1","updated":"2024-10-21T07:24:26Z","published":"2024-10-21T07:24:26Z","title":"Estimating Individual Dose-Response Curves under Unobserved Confounders\n  from Observational Data","summary":"  Estimating an individual's potential response to continuously varied\ntreatments is crucial for addressing causal questions across diverse domains,\nfrom healthcare to social sciences. However, existing methods are limited\neither to estimating causal effects of binary treatments, or scenarios where\nall confounding variables are measurable. In this work, we present ContiVAE, a\nnovel framework for estimating causal effects of continuous treatments,\nmeasured by individual dose-response curves, considering the presence of\nunobserved confounders using observational data. Leveraging a variational\nauto-encoder with a Tilted Gaussian prior distribution, ContiVAE models the\nhidden confounders as latent variables, and is able to predict the potential\noutcome of any treatment level for each individual while effectively capture\nthe heterogeneity among individuals. Experiments on semi-synthetic datasets\nshow that ContiVAE outperforms existing methods by up to 62%, demonstrating its\nrobustness and flexibility. Application on a real-world dataset illustrates its\npractical utility.\n","authors":["Shutong Chen","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2410.15706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15704v1","updated":"2024-10-21T07:20:41Z","published":"2024-10-21T07:20:41Z","title":"Residual vector quantization for KV cache compression in large language\n  model","summary":"  KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.\n","authors":["Ankur Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.15704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15698v1","updated":"2024-10-21T07:13:45Z","published":"2024-10-21T07:13:45Z","title":"Solving Continual Offline RL through Selective Weights Activation on\n  Aligned Spaces","summary":"  Continual offline reinforcement learning (CORL) has shown impressive ability\nin diffusion-based lifelong learning systems by modeling the joint\ndistributions of trajectories. However, most research only focuses on limited\ncontinual task settings where the tasks have the same observation and action\nspace, which deviates from the realistic demands of training agents in various\nenvironments. In view of this, we propose Vector-Quantized Continual Diffuser,\nnamed VQ-CD, to break the barrier of different spaces between various tasks.\nSpecifically, our method contains two complementary sections, where the\nquantization spaces alignment provides a unified basis for the selective\nweights activation. In the quantized spaces alignment, we leverage vector\nquantization to align the different state and action spaces of various tasks,\nfacilitating continual training in the same space. Then, we propose to leverage\na unified diffusion model attached by the inverse dynamic model to master all\ntasks by selectively activating different weights according to the task-related\nsparse masks. Finally, we conduct extensive experiments on 15 continual\nlearning (CL) tasks, including conventional CL task settings (identical state\nand action spaces) and general CL task settings (various state and action\nspaces). Compared with 16 baselines, our method reaches the SOTA performance.\n","authors":["Jifeng Hu","Sili Huang","Li Shen","Zhejian Yang","Shengchao Hu","Shisong Tang","Hechang Chen","Yi Chang","Dacheng Tao","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11802v2","updated":"2024-10-21T07:11:07Z","published":"2024-10-15T17:23:49Z","title":"FoundTS: Comprehensive and Unified Benchmarking of Foundation Models for\n  Time Series Forecasting","summary":"  Time Series Forecasting (TSF) is key functionality in numerous fields,\nincluding in finance, weather services, and energy management. While TSF\nmethods are emerging these days, many of them require domain-specific data\ncollection and model training and struggle with poor generalization performance\non new domains. Foundation models aim to overcome this limitation. Pre-trained\non large-scale language or time series data, they exhibit promising inferencing\ncapabilities in new or unseen data. This has spurred a surge in new TSF\nfoundation models. We propose a new benchmark, FoundTS, to enable thorough and\nfair evaluation and comparison of such models. FoundTS covers a variety of TSF\nfoundation models, including those based on large language models and those\npretrained on time series. Next, FoundTS supports different forecasting\nstrategies, including zero-shot, few-shot, and full-shot, thereby facilitating\nmore thorough evaluations. Finally, FoundTS offers a pipeline that standardizes\nevaluation processes such as dataset splitting, loading, normalization, and\nfew-shot sampling, thereby facilitating fair evaluations. Building on this, we\nreport on an extensive evaluation of TSF foundation models on a broad range of\ndatasets from diverse domains and with different statistical characteristics.\nSpecifically, we identify pros and cons and inherent limitations of existing\nfoundation models, and we identify directions for future model design. We make\nour code and datasets available at\nhttps://anonymous.4open.science/r/FoundTS-C2B0.\n","authors":["Zhe Li","Xiangfei Qiu","Peng Chen","Yihang Wang","Hanyin Cheng","Yang Shu","Jilin Hu","Chenjuan Guo","Aoying Zhou","Qingsong Wen","Christian S. Jensen","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.11802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05392v2","updated":"2024-10-21T07:08:11Z","published":"2024-06-08T07:55:01Z","title":"Deconstructing The Ethics of Large Language Models from Long-standing\n  Issues to New-emerging Dilemmas: A Survey","summary":"  Large Language Models (LLMs) have achieved unparalleled success across\ndiverse language modeling tasks in recent years. However, this progress has\nalso intensified ethical concerns, impacting the deployment of LLMs in everyday\ncontexts. This paper provides a comprehensive survey of ethical challenges\nassociated with LLMs, from longstanding issues such as copyright infringement,\nsystematic bias, and data privacy, to emerging problems like truthfulness and\nsocial norms. We critically analyze existing research aimed at understanding,\nexamining, and mitigating these ethical risks. Our survey underscores\nintegrating ethical standards and societal values into the development of LLMs,\nthereby guiding the development of responsible and ethically aligned language\nmodels.\n","authors":["Chengyuan Deng","Yiqun Duan","Xin Jin","Heng Chang","Yijun Tian","Han Liu","Yichen Wang","Kuofeng Gao","Henry Peng Zou","Yiqiao Jin","Yijia Xiao","Shenghao Wu","Zongxing Xie","Weimin Lyu","Sihong He","Lu Cheng","Haohan Wang","Jun Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.05392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15689v1","updated":"2024-10-21T06:59:04Z","published":"2024-10-21T06:59:04Z","title":"Enhancing SNN-based Spatio-Temporal Learning: A Benchmark Dataset and\n  Cross-Modality Attention Model","summary":"  Spiking Neural Networks (SNNs), renowned for their low power consumption,\nbrain-inspired architecture, and spatio-temporal representation capabilities,\nhave garnered considerable attention in recent years. Similar to Artificial\nNeural Networks (ANNs), high-quality benchmark datasets are of great importance\nto the advances of SNNs. However, our analysis indicates that many prevalent\nneuromorphic datasets lack strong temporal correlation, preventing SNNs from\nfully exploiting their spatio-temporal representation capabilities. Meanwhile,\nthe integration of event and frame modalities offers more comprehensive visual\nspatio-temporal information. Yet, the SNN-based cross-modality fusion remains\nunderexplored.\n  In this work, we present a neuromorphic dataset called DVS-SLR that can\nbetter exploit the inherent spatio-temporal properties of SNNs. Compared to\nexisting datasets, it offers advantages in terms of higher temporal\ncorrelation, larger scale, and more varied scenarios. In addition, our\nneuromorphic dataset contains corresponding frame data, which can be used for\ndeveloping SNN-based fusion methods. By virtue of the dual-modal feature of the\ndataset, we propose a Cross-Modality Attention (CMA) based fusion method. The\nCMA model efficiently utilizes the unique advantages of each modality, allowing\nfor SNNs to learn both temporal and spatial attention scores from the\nspatio-temporal features of event and frame modalities, subsequently allocating\nthese scores across modalities to enhance their synergy. Experimental results\ndemonstrate that our method not only improves recognition accuracy but also\nensures robustness across diverse scenarios.\n","authors":["Shibo Zhou","Bo Yang","Mengwen Yuan","Runhao Jiang","Rui Yan","Gang Pan","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.15689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15688v1","updated":"2024-10-21T06:57:09Z","published":"2024-10-21T06:57:09Z","title":"MIK: Modified Isolation Kernel for Biological Sequence Visualization,\n  Classification, and Clustering","summary":"  The t-Distributed Stochastic Neighbor Embedding (t-SNE) has emerged as a\npopular dimensionality reduction technique for visualizing high-dimensional\ndata. It computes pairwise similarities between data points by default using an\nRBF kernel and random initialization (in low-dimensional space), which\nsuccessfully captures the overall structure but may struggle to preserve the\nlocal structure efficiently. This research proposes a novel approach called the\nModified Isolation Kernel (MIK) as an alternative to the Gaussian kernel, which\nis built upon the concept of the Isolation Kernel. MIK uses adaptive density\nestimation to capture local structures more accurately and integrates\nrobustness measures. It also assigns higher similarity values to nearby points\nand lower values to distant points. Comparative research using the normal\nGaussian kernel, the isolation kernel, and several initialization techniques,\nincluding random, PCA, and random walk initializations, are used to assess the\nproposed approach (MIK). Additionally, we compare the computational efficiency\nof all $3$ kernels with $3$ different initialization methods. Our experimental\nresults demonstrate several advantages of the proposed kernel (MIK) and\ninitialization method selection. It exhibits improved preservation of the local\nand global structure and enables better visualization of clusters and\nsubclusters in the embedded space. These findings contribute to advancing\ndimensionality reduction techniques and provide researchers and practitioners\nwith an effective tool for data exploration, visualization, and analysis in\nvarious domains.\n","authors":["Sarwan Ali","Prakash Chourasia","Haris Mansoor","Bipin koirala","Murray Patterson"],"pdf_url":"https://arxiv.org/pdf/2410.15688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01967v2","updated":"2024-10-21T06:51:57Z","published":"2024-08-04T09:08:55Z","title":"A multi-task deep learning approach for lane-level pavement performance\n  prediction with segment-level data","summary":"  The elaborate pavement performance prediction is an important premise of\nimplementing preventive maintenance. Our survey reveals that in practice, the\npavement performance is usually measured at segment-level, where an unique\nperformance value is obtained for all lanes within one segment of 1km length.\nIt still lacks more elaborate performance analysis at lane-level due to costly\ndata collection and difficulty in prediction modeling. Therefore, this study\ndeveloped a multi-task deep learning approach to predict the lane-level\npavement performance with a large amount of historical segment-level\nperformance measurement data. The unified prediction framework can effectively\naddress inherent correlation and differences across lanes. In specific, the\nprediction framework firstly employed an Long Short-Term Memory (LSTM) layer to\ncapture the segment-level pavement deterioration pattern. Then multiple\ntask-specific LSTM layers were designed based on number of lanes to capture\nlane-level differences in pavement performance. Finally, we concatenated\nmultiple task-specific LSTM outputs with auxiliary features for further\ntraining and obtained the lane-level predictions after fully connected layer.\nThe aforementioned prediction framework was validated with a real case in\nChina. It revealed a better model performance regardless of one-way 2-lane,\n3-lane, and 4-lane scenarios, all lower than 10% in terms of mean absolute\npercentage error. The proposed prediction framework also outperforms other\nensemble learning and shallow machine learning methods in almost every lane.\n","authors":["Bo Wang","Wenbo Zhang","Yunpeng LI"],"pdf_url":"https://arxiv.org/pdf/2408.01967v2.pdf","comment":"24 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2212.03320v2","updated":"2024-10-21T06:46:52Z","published":"2022-12-06T20:44:24Z","title":"Reinforcement Learning for Molecular Dynamics Optimization: A Stochastic\n  Pontryagin Maximum Principle Approach","summary":"  In this paper, we present a novel reinforcement learning framework designed\nto optimize molecular dynamics by focusing on the entire trajectory rather than\njust the final molecular configuration. Leveraging a stochastic version of\nPontryagin's Maximum Principle (PMP) and Soft Actor-Critic (SAC) algorithm, our\nframework effectively explores non-convex molecular energy landscapes, escaping\nlocal minima to stabilize in low-energy states. Our approach operates in\ncontinuous state and action spaces without relying on labeled data, making it\napplicable to a wide range of molecular systems. Through extensive\nexperimentation on six distinct molecules, including Bradykinin and Oxytocin,\nwe demonstrate competitive performance against other unsupervised physics-based\nmethods, such as the Greedy and NEMO-based algorithms. Our method's\nadaptability and focus on dynamic trajectory optimization make it suitable for\napplications in areas such as drug discovery and molecular design.\n","authors":["Chandrajit Bajaj","Minh Nguyen","Conrad Li"],"pdf_url":"https://arxiv.org/pdf/2212.03320v2.pdf","comment":"Accepted to the International Conference on Neural Information\n  Processing (ICONIP) 2024. To be published in Springer-Nature Communications\n  in Computer and Information Science (CCIS) Series"},{"id":"http://arxiv.org/abs/2410.15681v1","updated":"2024-10-21T06:43:04Z","published":"2024-10-21T06:43:04Z","title":"Federated Learning with MMD-based Early Stopping for Adaptive GNSS\n  Interference Classification","summary":"  Federated learning (FL) enables multiple devices to collaboratively train a\nglobal model while maintaining data on local servers. Each device trains the\nmodel on its local server and shares only the model updates (i.e., gradient\nweights) during the aggregation step. A significant challenge in FL is managing\nthe feature distribution of novel, unbalanced data across devices. In this\npaper, we propose an FL approach using few-shot learning and aggregation of the\nmodel weights on a global server. We introduce a dynamic early stopping method\nto balance out-of-distribution classes based on representation learning,\nspecifically utilizing the maximum mean discrepancy of feature embeddings\nbetween local and global models. An exemplary application of FL is\norchestrating machine learning models along highways for interference\nclassification based on snapshots from global navigation satellite system\n(GNSS) receivers. Extensive experiments on four GNSS datasets from two\nreal-world highways and controlled environments demonstrate that our FL method\nsurpasses state-of-the-art techniques in adapting to both novel interference\nclasses and multipath scenarios.\n","authors":["Nishant S. Gaikwad","Lucas Heublein","Nisha L. Raichur","Tobias Feigl","Christopher Mutschler","Felix Ott"],"pdf_url":"https://arxiv.org/pdf/2410.15681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12606v2","updated":"2024-10-21T06:27:47Z","published":"2024-10-16T14:24:44Z","title":"Self-Supervised Learning of Disentangled Representations for\n  Multivariate Time-Series","summary":"  Multivariate time-series data in fields like healthcare and industry are\ninformative but challenging due to high dimensionality and lack of labels.\nRecent self-supervised learning methods excel in learning rich representations\nwithout labels but struggle with disentangled embeddings and inductive bias\nissues like transformation-invariance. To address these challenges, we\nintroduce TimeDRL, a framework for multivariate time-series representation\nlearning with dual-level disentangled embeddings. TimeDRL features: (i)\ndisentangled timestamp-level and instance-level embeddings using a [CLS] token\nstrategy; (ii) timestamp-predictive and instance-contrastive tasks for\nrepresentation learning; and (iii) avoidance of augmentation methods to\neliminate inductive biases. Experiments on forecasting and classification\ndatasets show TimeDRL outperforms existing methods, with further validation in\nsemi-supervised settings with limited labeled data.\n","authors":["Ching Chang","Chiao-Tung Chan","Wei-Yao Wang","Wen-Chih Peng","Tien-Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12606v2.pdf","comment":"This submission has been withdrawn to avoid duplication with a full\n  version of the paper that is already available in another arXiv entry\n  (arXiv:2410.12606). The withdrawn version was a short format prepared for a\n  NeurIPS workshop and is no longer necessary as a separate arXiv submission"},{"id":"http://arxiv.org/abs/2409.00608v2","updated":"2024-10-21T06:26:03Z","published":"2024-09-01T04:23:48Z","title":"TinyAgent: Function Calling at the Edge","summary":"  Recent large language models (LLMs) have enabled the development of advanced\nagentic systems that can integrate various tools and APIs to fulfill user\nqueries through function calling. However, the deployment of these LLMs on the\nedge has not been explored since they typically require cloud-based\ninfrastructure due to their substantial model size and computational demands.\nTo this end, we present TinyAgent, an end-to-end framework for training and\ndeploying task-specific small language model agents capable of function calling\nfor driving agentic systems at the edge. We first show how to enable accurate\nfunction calling for open-source models via the LLMCompiler framework. We then\nsystematically curate a high-quality dataset for function calling, which we use\nto fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient\ninference, we introduce a novel tool retrieval method to reduce the input\nprompt length and utilize quantization to further accelerate the inference\nspeed. As a driving application, we demonstrate a local Siri-like system for\nApple's MacBook that can execute user commands through text or voice input. Our\nresults show that our models can achieve, and even surpass, the\nfunction-calling capabilities of larger models like GPT-4-Turbo, while being\nfully deployed at the edge. We open-source our dataset, models, and installable\npackage and provide a demo video for our MacBook assistant agent.\n","authors":["Lutfi Eren Erdogan","Nicholas Lee","Siddharth Jha","Sehoon Kim","Ryan Tabrizi","Suhong Moon","Coleman Hooper","Gopala Anumanchipalli","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2409.00608v2.pdf","comment":"EMNLP 2024 Demo"},{"id":"http://arxiv.org/abs/2410.15667v1","updated":"2024-10-21T06:11:38Z","published":"2024-10-21T06:11:38Z","title":"RAC: Efficient LLM Factuality Correction with Retrieval Augmentation","summary":"  Large Language Models (LLMs) exhibit impressive results across a wide range\nof natural language processing (NLP) tasks, yet they can often produce\nfactually incorrect outputs. This paper introduces a simple but effective\nlow-latency post-correction method, \\textbf{Retrieval Augmented Correction\n(RAC)}, aimed at enhancing the factual performance of LLMs without requiring\nadditional fine-tuning. Our method is general and can be used with any\ninstruction-tuned LLM, and has greatly reduced latency compared to prior\napproaches. RAC decomposes the LLM's output into atomic facts and applies a\nfine-grained verification and correction process with retrieved content to\nverify and correct the LLM-generated output. Our extensive experiments show\nthat RAC yields up to 30\\% improvements over state-of-the-art baselines across\ntwo popular factuality evaluation datasets, validating its efficacy and\nrobustness in both with and without the integration of Retrieval-Augmented\nGeneration (RAG) across different LLMs.\\footnote{Our code is at\n\\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}\n","authors":["Changmao Li","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2410.15667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15665v1","updated":"2024-10-21T06:09:30Z","published":"2024-10-21T06:09:30Z","title":"Long Term Memory: The Foundation of AI Self-Evolution","summary":"  Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications.\n","authors":["Xun Jiang","Feng Li","Han Zhao","Jiaying Wang","Jun Shao","Shihao Xu","Shu Zhang","Weiling Chen","Xavier Tang","Yize Chen","Mengyue Wu","Weizhi Ma","Mengdi Wang","Tianqiao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15665v1.pdf","comment":"56 pages, 13 figures"},{"id":"http://arxiv.org/abs/2301.00201v2","updated":"2024-10-21T06:06:49Z","published":"2022-12-31T13:48:42Z","title":"Exploring Singularities in point clouds with the graph Laplacian: An\n  explicit approach","summary":"  We develop theory and methods that use the graph Laplacian to analyze the\ngeometry of the underlying manifold of datasets. Our theory provides\ntheoretical guarantees and explicit bounds on the functional forms of the graph\nLaplacian when it acts on functions defined close to singularities of the\nunderlying manifold. We use these explicit bounds to develop tests for\nsingularities and propose methods that can be used to estimate geometric\nproperties of singularities in the datasets.\n","authors":["Martin Andersson","Benny Avelin"],"pdf_url":"https://arxiv.org/pdf/2301.00201v2.pdf","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.15661v1","updated":"2024-10-21T06:03:49Z","published":"2024-10-21T06:03:49Z","title":"Scalable Data Ablation Approximations for Language Models through\n  Modular Training and Merging","summary":"  Training data compositions for Large Language Models (LLMs) can significantly\naffect their downstream performance. However, a thorough data ablation study\nexploring large sets of candidate data mixtures is typically prohibitively\nexpensive since the full effect is seen only after training the models; this\ncan lead practitioners to settle for sub-optimal data mixtures. We propose an\nefficient method for approximating data ablations which trains individual\nmodels on subsets of a training corpus and reuses them across evaluations of\ncombinations of subsets. In continued pre-training experiments, we find that,\ngiven an arbitrary evaluation set, the perplexity score of a single model\ntrained on a candidate set of data is strongly correlated with perplexity\nscores of parameter averages of models trained on distinct partitions of that\ndata. From this finding, we posit that researchers and practitioners can\nconduct inexpensive simulations of data ablations by maintaining a pool of\nmodels that were each trained on partitions of a large training corpus, and\nassessing candidate data mixtures by evaluating parameter averages of\ncombinations of these models. This approach allows for substantial improvements\nin amortized training efficiency -- scaling only linearly with respect to new\ndata -- by enabling reuse of previous training computation, opening new avenues\nfor improving model performance through rigorous, incremental data assessment\nand mixing.\n","authors":["Clara Na","Ian Magnusson","Ananya Harsh Jha","Tom Sherborne","Emma Strubell","Jesse Dodge","Pradeep Dasigi"],"pdf_url":"https://arxiv.org/pdf/2410.15661v1.pdf","comment":"EMNLP 2024. 17 pages"},{"id":"http://arxiv.org/abs/2305.18512v2","updated":"2024-10-21T05:59:50Z","published":"2023-05-29T17:09:26Z","title":"A Rainbow in Deep Network Black Boxes","summary":"  A central question in deep learning is to understand the functions learned by\ndeep networks. What is their approximation class? Do the learned weights and\nrepresentations depend on initialization? Previous empirical work has evidenced\nthat kernels defined by network activations are similar across initializations.\nFor shallow networks, this has been theoretically studied with random feature\nmodels, but an extension to deep networks has remained elusive. Here, we\nprovide a deep extension of such random feature models, which we call the\nrainbow model. We prove that rainbow networks define deterministic\n(hierarchical) kernels in the infinite-width limit. The resulting functions\nthus belong to a data-dependent RKHS which does not depend on the weight\nrandomness. We also verify numerically our modeling assumptions on deep CNNs\ntrained on image classification tasks, and show that the trained networks\napproximately satisfy the rainbow hypothesis. In particular, rainbow networks\nsampled from the corresponding random feature model achieve similar performance\nas the trained networks. Our results highlight the central role played by the\ncovariances of network weights at each layer, which are observed to be low-rank\nas a result of feature learning.\n","authors":["Florentin Guth","Brice Ménard","Gaspar Rochette","Stéphane Mallat"],"pdf_url":"https://arxiv.org/pdf/2305.18512v2.pdf","comment":"59 pages, 10 figures. To appear at JMLR"},{"id":"http://arxiv.org/abs/2410.15658v1","updated":"2024-10-21T05:56:31Z","published":"2024-10-21T05:56:31Z","title":"Calibration of ordinal regression networks","summary":"  Recent studies have shown that deep neural networks are not well-calibrated\nand produce over-confident predictions. The miscalibration issue primarily\nstems from the minimization of cross-entropy, which aims to align predicted\nsoftmax probabilities with one-hot labels. In ordinal regression tasks, this\nproblem is compounded by an additional challenge: the expectation that softmax\nprobabilities should exhibit unimodal distribution is not met with\ncross-entropy. Rather, the ordinal regression literature has focused on\nunimodality and overlooked calibration. To address these issues, we propose a\nnovel loss function that introduces order-aware calibration, ensuring that\nprediction confidence adheres to ordinal relationships between classes. It\nincorporates soft ordinal encoding and label-smoothing-based regularization to\nenforce both calibration and unimodality. Extensive experiments across three\npopular ordinal regression benchmarks demonstrate that our approach achieves\nstate-of-the-art calibration without compromising accuracy.\n","authors":["Daehwan Kim","Haejun Chung","Ikbeom Jang"],"pdf_url":"https://arxiv.org/pdf/2410.15658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15655v1","updated":"2024-10-21T05:47:07Z","published":"2024-10-21T05:47:07Z","title":"Accounting for Missing Covariates in Heterogeneous Treatment Estimation","summary":"  Many applications of causal inference require using treatment effects\nestimated on a study population to make decisions in a separate target\npopulation. We consider the challenging setting where there are covariates that\nare observed in the target population that were not seen in the original study.\nOur goal is to estimate the tightest possible bounds on heterogeneous treatment\neffects conditioned on such newly observed covariates. We introduce a novel\npartial identification strategy based on ideas from ecological inference; the\nmain idea is that estimates of conditional treatment effects for the full\ncovariate set must marginalize correctly when restricted to only the covariates\nobserved in both populations. Furthermore, we introduce a bias-corrected\nestimator for these bounds and prove that it enjoys fast convergence rates and\nstatistical guarantees (e.g., asymptotic normality). Experimental results on\nboth real and synthetic data demonstrate that our framework can produce bounds\nthat are much tighter than would otherwise be possible.\n","authors":["Khurram Yamin","Vibhhu Sharma","Ed Kennedy","Bryan Wilder"],"pdf_url":"https://arxiv.org/pdf/2410.15655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12629v2","updated":"2024-10-21T05:38:10Z","published":"2022-12-24T01:39:02Z","title":"Concentration of the Langevin Algorithm's Stationary Distribution","summary":"  A canonical algorithm for log-concave sampling is the Langevin Algorithm, aka\nthe Langevin Diffusion run with some discretization stepsize $\\eta > 0$. This\ndiscretization leads the Langevin Algorithm to have a stationary distribution\n$\\pi_{\\eta}$ which differs from the stationary distribution $\\pi$ of the\nLangevin Diffusion, and it is an important challenge to understand whether the\nwell-known properties of $\\pi$ extend to $\\pi_{\\eta}$. In particular, while\nconcentration properties such as isoperimetry and rapidly decaying tails are\nclassically known for $\\pi$, the analogous properties for $\\pi_{\\eta}$ are open\nquestions with algorithmic implications. This note provides a first step in\nthis direction by establishing concentration results for $\\pi_{\\eta}$ that\nmirror classical results for $\\pi$. Specifically, we show that for any\nnontrivial stepsize $\\eta > 0$, $\\pi_{\\eta}$ is sub-exponential (respectively,\nsub-Gaussian) when the potential is convex (respectively, strongly convex).\nMoreover, the concentration bounds we show are essentially tight. We also show\nthat these concentration bounds extend to all iterates along the trajectory of\nthe Langevin Algorithm, and to inexact implementations which use sub-Gaussian\nestimates of the gradient.\n  Key to our analysis is the use of a rotation-invariant moment generating\nfunction (aka Bessel function) to study the stationary dynamics of the Langevin\nAlgorithm. This technique may be of independent interest because it enables\ndirectly analyzing the discrete-time stationary distribution $\\pi_{\\eta}$\nwithout going through the continuous-time stationary distribution $\\pi$ as an\nintermediary.\n","authors":["Jason M. Altschuler","Kunal Talwar"],"pdf_url":"https://arxiv.org/pdf/2212.12629v2.pdf","comment":"Added Section 6 (extensions to concentration of the trajectory and\n  inexact gradients)"},{"id":"http://arxiv.org/abs/2409.14023v2","updated":"2024-10-21T05:34:04Z","published":"2024-09-21T05:25:46Z","title":"FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer\n  on UltraScale+ FPGAs","summary":"  Transformer neural networks (TNNs) are being applied across a widening range\nof application domains, including natural language processing (NLP), machine\ntranslation, and computer vision (CV). Their popularity is largely attributed\nto the exceptional performance of their multi-head self-attention blocks when\nanalyzing sequential data and extracting features. To date, there are limited\nhardware accelerators tailored for this mechanism, which is the first step\nbefore designing an accelerator for a complete model. This paper proposes\n\\textit{FAMOUS}, a flexible hardware accelerator for dense multi-head attention\n(MHA) computation of TNNs on field-programmable gate arrays (FPGAs). It is\noptimized for high utilization of processing elements and on-chip memories to\nimprove parallelism and reduce latency. An efficient tiling of large matrices\nhas been employed to distribute memory and computing resources across different\nmodules on various FPGA platforms. The design is evaluated on Xilinx Alveo U55C\nand U200 data center cards containing Ultrascale+ FPGAs. Experimental results\nare presented that show that it can attain a maximum throughput, number of\nparallel attention heads, embedding dimension and tile size of 328 (giga\noperations/second (GOPS)), 8, 768 and 64 respectively on the U55C. Furthermore,\nit is 3.28$\\times$ and 2.6$\\times$ faster than the Intel Xeon Gold 5220R CPU\nand NVIDIA V100 GPU respectively. It is also 1.3$\\times$ faster than the\nfastest state-of-the-art FPGA-based accelerator.\n","authors":["Ehsan Kabir","Md. Arafat Kabir","Austin R. J. Downey","Jason D. Bakos","David Andrews","Miaoqing Huang"],"pdf_url":"https://arxiv.org/pdf/2409.14023v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.13975"},{"id":"http://arxiv.org/abs/2410.15651v1","updated":"2024-10-21T05:23:42Z","published":"2024-10-21T05:23:42Z","title":"Understanding and Alleviating Memory Consumption in RLHF for LLMs","summary":"  Fine-tuning with Reinforcement Learning with Human Feedback (RLHF) is\nessential for aligning large language models (LLMs). However, RLHF often\nencounters significant memory challenges. This study is the first to examine\nmemory usage in the RLHF context, exploring various memory management\nstrategies and unveiling the reasons behind excessive memory consumption.\nAdditionally, we introduce a simple yet effective approach that substantially\nreduces the memory required for RLHF fine-tuning.\n","authors":["Jin Zhou","Hanmei Yang"," Steven"," Tang","Mingcan Xiang","Hui Guan","Tongping Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15648v1","updated":"2024-10-21T05:16:59Z","published":"2024-10-21T05:16:59Z","title":"Linking Model Intervention to Causal Interpretation in Model Explanation","summary":"  Intervention intuition is often used in model explanation where the\nintervention effect of a feature on the outcome is quantified by the difference\nof a model prediction when the feature value is changed from the current value\nto the baseline value. Such a model intervention effect of a feature is\ninherently association. In this paper, we will study the conditions when an\nintuitive model intervention effect has a causal interpretation, i.e., when it\nindicates whether a feature is a direct cause of the outcome. This work links\nthe model intervention effect to the causal interpretation of a model. Such an\ninterpretation capability is important since it indicates whether a machine\nlearning model is trustworthy to domain experts. The conditions also reveal the\nlimitations of using a model intervention effect for causal interpretation in\nan environment with unobserved features. Experiments on semi-synthetic datasets\nhave been conducted to validate theorems and show the potential for using the\nmodel intervention effect for model interpretation.\n","authors":["Debo Cheng","Ziqi Xu","Jiuyong Li","Lin Liu","Kui Yu","Thuc Duy Le","Jixue Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18292v5","updated":"2024-10-21T05:06:15Z","published":"2024-02-28T12:37:30Z","title":"FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time\n  Augmentation","summary":"  Few-shot-learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few labeled\nsamples of the new classes (support set) as reference. So far, plenty of\nalgorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, e.g., obtaining a test accuracy improvement proportion of around\n10% (e.g., from 46.86% to 53.28%) for trained FSL models. Importantly, given\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves.\n","authors":["Yunwei Bai","Ying Kiat Tan","Shiming Chen","Yao Shu","Tsuhan Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18292v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16546v2","updated":"2024-10-21T05:06:01Z","published":"2024-09-25T01:39:02Z","title":"AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization","summary":"  Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.\n","authors":["Yifan Tan","Haoze Wang","Chao Yan","Yangdong Deng"],"pdf_url":"https://arxiv.org/pdf/2409.16546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02033v4","updated":"2024-10-21T05:02:20Z","published":"2023-02-03T23:41:53Z","title":"An Asymptotically Optimal Algorithm for the Convex Hull Membership\n  Problem","summary":"  We study the convex hull membership (CHM) problem in the pure exploration\nsetting where one aims to efficiently and accurately determine if a given point\nlies in the convex hull of means of a finite set of distributions. We give a\ncomplete characterization of the sample complexity of the CHM problem in the\none-dimensional case. We present the first asymptotically optimal algorithm\ncalled Thompson-CHM, whose modular design consists of a stopping rule and a\nsampling rule. In addition, we extend the algorithm to settings that generalize\nseveral important problems in the multi-armed bandit literature. Furthermore,\nwe discuss the extension of Thompson-CHM to higher dimensions. Finally, we\nprovide numerical experiments to demonstrate the empirical behavior of the\nalgorithm matches our theoretical results for realistic time horizons.\n","authors":["Gang Qiao","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2302.02033v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15640v1","updated":"2024-10-21T04:58:33Z","published":"2024-10-21T04:58:33Z","title":"Deep Graph Attention Networks","summary":"  Graphs are useful for representing various realworld objects. However, graph\nneural networks (GNNs) tend to suffer from over-smoothing, where the\nrepresentations of nodes of different classes become similar as the number of\nlayers increases, leading to performance degradation. A method that does not\nrequire protracted tuning of the number of layers is needed to effectively\nconstruct a graph attention network (GAT), a type of GNN. Therefore, we\nintroduce a method called \"DeepGAT\" for predicting the class to which nodes\nbelong in a deep GAT. It avoids over-smoothing in a GAT by ensuring that nodes\nin different classes are not similar at each layer. Using DeepGAT to predict\nclass labels, a 15-layer network is constructed without the need to tune the\nnumber of layers. DeepGAT prevented over-smoothing and achieved a 15-layer GAT\nwith similar performance to a 2-layer GAT, as indicated by the similar\nattention coefficients. DeepGAT enables the training of a large network to\nacquire similar attention coefficients to a network with few layers. It avoids\nthe over-smoothing problem and obviates the need to tune the number of layers,\nthus saving time and enhancing GNN performance.\n","authors":["Jun Kato","Airi Mita","Keita Gobara","Akihiro Inokuchi"],"pdf_url":"https://arxiv.org/pdf/2410.15640v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.15637v1","updated":"2024-10-21T04:50:57Z","published":"2024-10-21T04:50:57Z","title":"Large Deviations and Improved Mean-squared Error Rates of Nonlinear SGD:\n  Heavy-tailed Noise and Power of Symmetry","summary":"  We study large deviations and mean-squared error (MSE) guarantees of a\ngeneral framework of nonlinear stochastic gradient methods in the online\nsetting, in the presence of heavy-tailed noise. Unlike existing works that rely\non the closed form of a nonlinearity (typically clipping), our framework treats\nthe nonlinearity in a black-box manner, allowing us to provide unified\nguarantees for a broad class of bounded nonlinearities, including many popular\nones, like sign, quantization, normalization, as well as component-wise and\njoint clipping. We provide several strong results for a broad range of\nstep-sizes in the presence of heavy-tailed noise with symmetric probability\ndensity function, positive in a neighbourhood of zero and potentially unbounded\nmoments. In particular, for non-convex costs we provide a large deviation upper\nbound for the minimum norm-squared of gradients, showing an asymptotic tail\ndecay on an exponential scale, at a rate $\\sqrt{t} / \\log(t)$. We establish the\naccompanying rate function, showing an explicit dependence on the choice of\nstep-size, nonlinearity, noise and problem parameters. Next, for non-convex\ncosts and the minimum norm-squared of gradients, we derive the optimal MSE rate\n$\\widetilde{\\mathcal{O}}(t^{-1/2})$. Moreover, for strongly convex costs and\nthe last iterate, we provide an MSE rate that can be made arbitrarily close to\nthe optimal rate $\\mathcal{O}(t^{-1})$, improving on the state-of-the-art\nresults in the presence of heavy-tailed noise. Finally, we establish almost\nsure convergence of the minimum norm-squared of gradients, providing an\nexplicit rate, which can be made arbitrarily close to $o(t^{-1/4})$.\n","authors":["Aleksandar Armacki","Shuhua Yu","Dragana Bajovic","Dusan Jakovetic","Soummya Kar"],"pdf_url":"https://arxiv.org/pdf/2410.15637v1.pdf","comment":"30 pages. arXiv admin note: text overlap with arXiv:2410.13954"},{"id":"http://arxiv.org/abs/2403.01742v3","updated":"2024-10-21T04:38:08Z","published":"2024-03-04T05:39:23Z","title":"Diffusion-TS: Interpretable Diffusion for General Time Series Generation","summary":"  Denoising diffusion probabilistic models (DDPMs) are becoming the leading\nparadigm for generative models. It has recently shown breakthroughs in audio\nsynthesis, time series imputation and forecasting. In this paper, we propose\nDiffusion-TS, a novel diffusion-based framework that generates multivariate\ntime series samples of high quality by using an encoder-decoder transformer\nwith disentangled temporal representations, in which the decomposition\ntechnique guides Diffusion-TS to capture the semantic meaning of time series\nwhile transformers mine detailed sequential information from the noisy model\ninput. Different from existing diffusion-based approaches, we train the model\nto directly reconstruct the sample instead of the noise in each diffusion step,\ncombining a Fourier-based loss term. Diffusion-TS is expected to generate time\nseries satisfying both interpretablity and realness. In addition, it is shown\nthat the proposed Diffusion-TS can be easily extended to conditional generation\ntasks, such as forecasting and imputation, without any model changes. This also\nmotivates us to further explore the performance of Diffusion-TS under irregular\nsettings. Finally, through qualitative and quantitative experiments, results\nshow that Diffusion-TS achieves the state-of-the-art results on various\nrealistic analyses of time series.\n","authors":["Xinyu Yuan","Yan Qiao"],"pdf_url":"https://arxiv.org/pdf/2403.01742v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15628v1","updated":"2024-10-21T04:24:10Z","published":"2024-10-21T04:24:10Z","title":"Towards Kriging-informed Conditional Diffusion for Regional Sea-Level\n  Data Downscaling","summary":"  Given coarser-resolution projections from global climate models or satellite\ndata, the downscaling problem aims to estimate finer-resolution regional\nclimate data, capturing fine-scale spatial patterns and variability.\nDownscaling is any method to derive high-resolution data from low-resolution\nvariables, often to provide more detailed and local predictions and analyses.\nThis problem is societally crucial for effective adaptation, mitigation, and\nresilience against significant risks from climate change. The challenge arises\nfrom spatial heterogeneity and the need to recover finer-scale features while\nensuring model generalization. Most downscaling methods \\cite{Li2020} fail to\ncapture the spatial dependencies at finer scales and underperform on real-world\nclimate datasets, such as sea-level rise. We propose a novel Kriging-informed\nConditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial\nvariability while preserving fine-scale features. Experimental results on\nclimate data show that our proposed method is more accurate than\nstate-of-the-art downscaling techniques.\n","authors":["Subhankar Ghosh","Arun Sharma","Jayant Gupta","Aneesh Subramanian","Shashi Shekhar"],"pdf_url":"https://arxiv.org/pdf/2410.15628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15709v2","updated":"2024-10-21T04:21:36Z","published":"2023-06-27T08:36:13Z","title":"Privacy-Preserving Community Detection for Locally Distributed Multiple\n  Networks","summary":"  Modern multi-layer networks are commonly stored and analyzed in a local and\ndistributed fashion because of the privacy, ownership, and communication costs.\nThe literature on the model-based statistical methods for community detection\nbased on these data is still limited. This paper proposes a new method for\nconsensus community detection and estimation in a multi-layer stochastic block\nmodel using locally stored and computed network data with privacy protection. A\nnovel algorithm named privacy-preserving Distributed Spectral Clustering\n(ppDSC) is developed. To preserve the edges' privacy, we adopt the randomized\nresponse (RR) mechanism to perturb the network edges, which satisfies the\nstrong notion of differential privacy. The ppDSC algorithm is performed on the\nsquared RR-perturbed adjacency matrices to prevent possible cancellation of\ncommunities among different layers. To remove the bias incurred by RR and the\nsquared network matrices, we develop a two-step bias-adjustment procedure. Then\nwe perform eigen-decomposition on the debiased matrices, aggregation of the\nlocal eigenvectors using an orthogonal Procrustes transformation, and k-means\nclustering. We provide theoretical analysis on the statistical errors of ppDSC\nin terms of eigen-vector estimation. In addition, the blessings and curses of\nnetwork heterogeneity are well-explained by our bounds.\n","authors":["Xiao Guo","Xiang Li","Xiangyu Chang","Shujie Ma"],"pdf_url":"https://arxiv.org/pdf/2306.15709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15625v1","updated":"2024-10-21T04:08:37Z","published":"2024-10-21T04:08:37Z","title":"Improving Parallel Program Performance Through DSL-Driven Code\n  Generation with LLM Optimizers","summary":"  Mapping computations to processors and assigning data to memory are critical\nfor maximizing performance in parallel programming. These mapping decisions are\nmanaged through the development of specialized low-level system code, called\nmappers, crafted by performance engineers. Each mapper is tailored to a\nspecific application and optimized for the underlying machine architecture, a\nprocess that requires days of refinement and tuning from an expert. Despite\nadvances in system research, automating mapper generation remains a challenge\ndue to the complexity of making millions of decisions to find the optimal\nsolution and generate the solution as code. We introduce an approach that\nleverages recent advances in LLM-based optimizers for mapper design. In under\nten minutes, our method automatically discovers mappers that surpass human\nexpert designs in scientific applications by up to 1.34X speedup. For parallel\nmatrix multiplication algorithms, our mapper achieves up to 1.31X of the\nexpert-designed solution. To achieve this, we simplify the complexity of\nlow-level code generation by introducing a domain-specific language (DSL) that\nabstracts the low-level system programming details and defines a structured\nsearch space for LLMs to explore. To maximize the application performance, we\nuse an LLM optimizer to improve an agentic system that generates the mapper\ncode. As a result, this approach significantly reduces the workload for\nperformance engineers while achieving substantial performance gains across\ndiverse applications. Finally, our results demonstrate the effectiveness of\nLLM-based optimization in system design and suggest its potential for\naddressing other complex system challenges.\n","authors":["Anjiang Wei","Allen Nie","Thiago S. F. X. Teixeira","Rohan Yadav","Wonchan Lee","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2410.15625v1.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.15624v1","updated":"2024-10-21T04:08:19Z","published":"2024-10-21T04:08:19Z","title":"Test-time Adaptation for Cross-modal Retrieval with Query Shift","summary":"  The success of most existing cross-modal retrieval methods heavily relies on\nthe assumption that the given queries follow the same distribution of the\nsource domain. However, such an assumption is easily violated in real-world\nscenarios due to the complexity and diversity of queries, thus leading to the\nquery shift problem. Specifically, query shift refers to the online query\nstream originating from the domain that follows a different distribution with\nthe source one. In this paper, we observe that query shift would not only\ndiminish the uniformity (namely, within-modality scatter) of the query modality\nbut also amplify the gap between query and gallery modalities. Based on the\nobservations, we propose a novel method dubbed Test-time adaptation for\nCross-modal Retrieval (TCR). In brief, TCR employs a novel module to refine the\nquery predictions (namely, retrieval results of the query) and a joint\nobjective to prevent query shift from disturbing the common space, thus\nachieving online adaptation for the cross-modal retrieval models with query\nshift. Expensive experiments demonstrate the effectiveness of the proposed TCR\nagainst query shift. The code will be released upon acceptance.\n","authors":["Haobin Li","Peng Hu","Qianjun Zhang","Xi Peng","Xiting Liu","Mouxing Yang"],"pdf_url":"https://arxiv.org/pdf/2410.15624v1.pdf","comment":"22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2402.18012v3","updated":"2024-10-21T04:06:02Z","published":"2024-02-28T03:09:12Z","title":"Diffusion Models as Constrained Samplers for Optimization with Unknown\n  Constraints","summary":"  Addressing real-world optimization problems becomes particularly challenging\nwhen analytic objective functions or constraints are unavailable. While\nnumerous studies have addressed the issue of unknown objectives, limited\nresearch has focused on scenarios where feasibility constraints are not given\nexplicitly. Overlooking these constraints can lead to spurious solutions that\nare unrealistic in practice. To deal with such unknown constraints, we propose\nto perform optimization within the data manifold using diffusion models. To\nconstrain the optimization process to the data manifold, we reformulate the\noriginal optimization problem as a sampling problem from the product of the\nBoltzmann distribution defined by the objective function and the data\ndistribution learned by the diffusion model. Depending on the differentiability\nof the objective function, we propose two different sampling methods. For\ndifferentiable objectives, we propose a two-stage framework that begins with a\nguided diffusion process for warm-up, followed by a Langevin dynamics stage for\nfurther correction. For non-differentiable objectives, we propose an iterative\nimportance sampling strategy using the diffusion model as the proposal\ndistribution. Comprehensive experiments on a synthetic dataset, six real-world\nblack-box optimization datasets, and a multi-objective molecule optimization\ndataset show that our method achieves better or comparable performance with\nprevious state-of-the-art baselines.\n","authors":["Lingkai Kong","Yuanqi Du","Wenhao Mu","Kirill Neklyudov","Valentin De Bortoli","Dongxia Wu","Haorui Wang","Aaron Ferber","Yi-An Ma","Carla P. Gomes","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.18012v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10038v2","updated":"2024-10-21T04:05:25Z","published":"2023-04-20T01:32:32Z","title":"Open-World Continual Learning: Unifying Novelty Detection and Continual\n  Learning","summary":"  As AI agents are increasingly used in the real open world with unknowns or\nnovelties, they need the ability to (1) recognize objects that (a) they have\nlearned before and (b) detect items that they have never seen or learned, and\n(2) learn the new items incrementally to become more and more knowledgeable and\npowerful. (1) is called novelty detection or out-of-distribution (OOD)\ndetection and (2) is called class incremental learning (CIL), which is a\nsetting of continual learning (CL). In existing research, OOD detection and CIL\nare regarded as two completely different problems. This paper first provides a\ntheoretical proof that good OOD detection for each task within the set of\nlearned tasks (called closed-world OOD detection) is necessary for successful\nCIL. We show this by decomposing CIL into two sub-problems: within-task\nprediction (WP) and task-id prediction (TP), and proving that TP is correlated\nwith closed-world OOD detection. The key theoretical result is that regardless\nof whether WP and OOD detection (or TP) are defined explicitly or implicitly by\na CIL algorithm, good WP and good closed-world OOD detection are necessary and\nsufficient conditions for good CIL, which unifies novelty or OOD detection and\ncontinual learning (CIL, in particular). We call this traditional CIL the\nclosed-world CIL as it does not detect future OOD data in the open world. The\npaper then proves that the theory can be generalized or extended to open-world\nCIL, which is the proposed open-world continual learning, that can perform CIL\nin the open world and detect future or open-world OOD data. Based on the\ntheoretical results, new CIL methods are also designed, which outperform strong\nbaselines in CIL accuracy and in continual OOD detection by a large margin.\n","authors":["Gyuhak Kim","Changnan Xiao","Tatsuya Konishi","Zixuan Ke","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2304.10038v2.pdf","comment":"To appear in Artificial Intelligence Journal. arXiv admin note:\n  substantial text overlap with arXiv:2211.02633"},{"id":"http://arxiv.org/abs/2212.10678v3","updated":"2024-10-21T03:55:36Z","published":"2022-12-20T22:41:24Z","title":"Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias","summary":"  Generated texts from large language models (LLMs) have been shown to exhibit\na variety of harmful, human-like biases against various demographics. These\nfindings motivate research efforts aiming to understand and measure such\neffects. This paper introduces a causal formulation for bias measurement in\ngenerative language models. Based on this theoretical foundation, we outline a\nlist of desiderata for designing robust bias benchmarks. We then propose a\nbenchmark called OccuGender, with a bias-measuring procedure to investigate\noccupational gender bias. We test several state-of-the-art open-source LLMs on\nOccuGender, including Llama, Mistral, and their instruction-tuned versions. The\nresults show that these models exhibit substantial occupational gender bias.\nLastly, we discuss prompting strategies for bias mitigation and an extension of\nour causal formulation to illustrate the generalizability of our framework. Our\ncode and data https://github.com/chenyuen0103/gender-bias.\n","authors":["Yuen Chen","Vethavikashini Chithrra Raghuram","Justus Mattern","Rada Mihalcea","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2212.10678v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15618v1","updated":"2024-10-21T03:40:29Z","published":"2024-10-21T03:40:29Z","title":"Erasing Undesirable Concepts in Diffusion Models with Adversarial\n  Preservation","summary":"  Diffusion models excel at generating visually striking content from text but\ncan inadvertently produce undesirable or harmful content when trained on\nunfiltered internet data. A practical solution is to selectively removing\ntarget concepts from the model, but this may impact the remaining concepts.\nPrior approaches have tried to balance this by introducing a loss term to\npreserve neutral content or a regularization term to minimize changes in the\nmodel parameters, yet resolving this trade-off remains challenging. In this\nwork, we propose to identify and preserving concepts most affected by parameter\nchanges, termed as \\textit{adversarial concepts}. This approach ensures stable\nerasure with minimal impact on the other concepts. We demonstrate the\neffectiveness of our method using the Stable Diffusion model, showing that it\noutperforms state-of-the-art erasure methods in eliminating unwanted content\nwhile maintaining the integrity of other unrelated elements. Our code is\navailable at\n\\url{https://github.com/tuananhbui89/Erasing-Adversarial-Preservation}.\n","authors":["Anh Bui","Long Vuong","Khanh Doan","Trung Le","Paul Montague","Tamas Abraham","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2410.15618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15617v1","updated":"2024-10-21T03:36:34Z","published":"2024-10-21T03:36:34Z","title":"Long-time Integration of Nonlinear Wave Equations with Neural Operators","summary":"  Neural operators have shown promise in solving many types of Partial\nDifferential Equations (PDEs). They are significantly faster compared to\ntraditional numerical solvers once they have been trained with a certain amount\nof observed data. However, their numerical performance in solving\ntime-dependent PDEs, particularly in long-time prediction of dynamic systems,\nstill needs improvement. In this paper, we focus on solving the long-time\nintegration of nonlinear wave equations via neural operators by replacing the\ninitial condition with the prediction in a recurrent manner. Given limited\nobserved temporal trajectory data, we utilize some intrinsic features of these\nnonlinear wave equations, such as conservation laws and well-posedness, to\nimprove the algorithm design and reduce accumulated error. Our numerical\nexperiments examine these improvements in the Korteweg-de Vries (KdV) equation,\nthe sine-Gordon equation, and a semilinear wave equation on the irregular\ndomain.\n","authors":["Guanhang Lei","Zhen Lei","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2410.15617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17358v3","updated":"2024-10-21T03:22:02Z","published":"2024-04-26T12:16:08Z","title":"Adversarial Consistency and the Uniqueness of the Adversarial Bayes\n  Classifier","summary":"  Minimizing an adversarial surrogate risk is a common technique for learning\nrobust classifiers. Prior work showed that convex surrogate losses are not\nstatistically consistent in the adversarial context -- or in other words, a\nminimizing sequence of the adversarial surrogate risk will not necessarily\nminimize the adversarial classification error. We connect the consistency of\nadversarial surrogate losses to properties of minimizers to the adversarial\nclassification risk, known as adversarial Bayes classifiers. Specifically,\nunder reasonable distributional assumptions, a convex surrogate loss is\nstatistically consistent for adversarial learning iff the adversarial Bayes\nclassifier satisfies a certain notion of uniqueness.\n","authors":["Natalie S. Frank"],"pdf_url":"https://arxiv.org/pdf/2404.17358v3.pdf","comment":"2 figures, 20 pages, v2: fixed typos, v3: improved organization of\n  paper and added figures"},{"id":"http://arxiv.org/abs/2410.15612v1","updated":"2024-10-21T03:16:32Z","published":"2024-10-21T03:16:32Z","title":"In-Trajectory Inverse Reinforcement Learning: Learn Incrementally From\n  An Ongoing Trajectory","summary":"  Inverse reinforcement learning (IRL) aims to learn a reward function and a\ncorresponding policy that best fit the demonstrated trajectories of an expert.\nHowever, current IRL works cannot learn incrementally from an ongoing\ntrajectory because they have to wait to collect at least one complete\ntrajectory to learn. To bridge the gap, this paper considers the problem of\nlearning a reward function and a corresponding policy while observing the\ninitial state-action pair of an ongoing trajectory and keeping updating the\nlearned reward and policy when new state-action pairs of the ongoing trajectory\nare observed. We formulate this problem as an online bi-level optimization\nproblem where the upper level dynamically adjusts the learned reward according\nto the newly observed state-action pairs with the help of a meta-regularization\nterm, and the lower level learns the corresponding policy. We propose a novel\nalgorithm to solve this problem and guarantee that the algorithm achieves\nsub-linear local regret $O(\\sqrt{T}+\\log T+\\sqrt{T}\\log T)$. If the reward\nfunction is linear, we prove that the proposed algorithm achieves sub-linear\nregret $O(\\log T)$. Experiments are used to validate the proposed algorithm.\n","authors":["Shicheng Liu","Minghui Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.15612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15610v1","updated":"2024-10-21T03:13:35Z","published":"2024-10-21T03:13:35Z","title":"On The Global Convergence Of Online RLHF With Neural Parametrization","summary":"  The importance of Reinforcement Learning from Human Feedback (RLHF) in\naligning large language models (LLMs) with human values cannot be overstated.\nRLHF is a three-stage process that includes supervised fine-tuning (SFT),\nreward learning, and policy learning. Although there are several offline and\nonline approaches to aligning LLMs, they often suffer from distribution shift\nissues. These issues arise from the inability to accurately capture the\ndistributional interdependence between the reward learning and policy learning\nstages. Consequently, this has led to various approximated approaches, but the\ntheoretical insights and motivations remain largely limited to tabular\nsettings, which do not hold in practice. This gap between theoretical insights\nand practical implementations is critical. It is challenging to address this\ngap as it requires analyzing the performance of AI alignment algorithms in\nneural network-parameterized settings. Although bi-level formulations have\nshown promise in addressing distribution shift issues, they suffer from the\nhyper-gradient problem, and current approaches lack efficient algorithms to\nsolve this. In this work, we tackle these challenges employing the bi-level\nformulation laid out in Kwon et al. (2024) along with the assumption \\emph{Weak\nGradient Domination} to demonstrate convergence in an RLHF setup, obtaining a\nsample complexity of $\\epsilon^{-\\frac{7}{2}}$ . Our key contributions are\ntwofold: (i) We propose a bi-level formulation for AI alignment in\nparameterized settings and introduce a first-order approach to solve this\nproblem. (ii) We analyze the theoretical convergence rates of the proposed\nalgorithm and derive state-of-the-art bounds. To the best of our knowledge,\nthis is the first work to establish convergence rate bounds and global\noptimality for the RLHF framework in neural network-parameterized settings.\n","authors":["Mudit Gaur","Amrit Singh Bedi","Raghu Pasupathy","Vaneet Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2410.15610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15608v1","updated":"2024-10-21T03:13:20Z","published":"2024-10-21T03:13:20Z","title":"Moonshine: Speech Recognition for Live Transcription and Voice Commands","summary":"  This paper introduces Moonshine, a family of speech recognition models\noptimized for live transcription and voice command processing. Moonshine is\nbased on an encoder-decoder transformer architecture and employs Rotary\nPosition Embedding (RoPE) instead of traditional absolute position embeddings.\nThe model is trained on speech segments of various lengths, but without using\nzero-padding, leading to greater efficiency for the encoder during inference\ntime. When benchmarked against OpenAI's Whisper tiny.en, Moonshine Tiny\ndemonstrates a 5x reduction in compute requirements for transcribing a\n10-second speech segment while incurring no increase in word error rates across\nstandard evaluation datasets. These results highlight Moonshine's potential for\nreal-time and resource-constrained applications.\n","authors":["Nat Jeffries","Evan King","Manjunath Kudlur","Guy Nicholson","James Wang","Pete Warden"],"pdf_url":"https://arxiv.org/pdf/2410.15608v1.pdf","comment":"7 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2405.08813v3","updated":"2024-10-21T03:08:08Z","published":"2024-05-14T17:59:02Z","title":"CinePile: A Long Video Question Answering Dataset and Benchmark","summary":"  Current datasets for long-form video understanding often fall short of\nproviding genuine long-form comprehension challenges, as many tasks derived\nfrom these datasets can be successfully tackled by analyzing just one or a few\nrandom frames from a video. To address this issue, we present a novel dataset\nand benchmark, CinePile, specifically designed for authentic long-form video\nunderstanding. This paper details our innovative approach for creating a\nquestion-answer dataset, utilizing advanced LLMs with human-in-the-loop and\nbuilding upon human-generated raw data. Our comprehensive dataset comprises\n305,000 multiple-choice questions (MCQs), covering various visual and\nmultimodal aspects, including temporal comprehension, understanding\nhuman-object interactions, and reasoning about events or actions within a\nscene. Additionally, we fine-tuned open-source Video-LLMs on the training split\nand evaluated both open-source and proprietary video-centric LLMs on the test\nsplit of our dataset. The findings indicate that although current models\nunderperform compared to humans, fine-tuning these models can lead to\nsignificant improvements in their performance.\n","authors":["Ruchit Rawal","Khalid Saifullah","Miquel Farré","Ronen Basri","David Jacobs","Gowthami Somepalli","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2405.08813v3.pdf","comment":"Project page with all the artifacts -\n  https://ruchitrawal.github.io/cinepile/. Updated version with adversarial\n  refinement pipeline and more model evaluations"},{"id":"http://arxiv.org/abs/2409.06998v2","updated":"2024-10-21T03:07:06Z","published":"2024-09-11T04:13:39Z","title":"Learning Personalized Scoping for Graph Neural Networks under\n  Heterophily","summary":"  Heterophilous graphs, where dissimilar nodes tend to connect, pose a\nchallenge for graph neural networks (GNNs) as their superior performance\ntypically comes from aggregating homophilous information. Increasing the GNN\ndepth can expand the scope (i.e., receptive field), potentially finding\nhomophily from the higher-order neighborhoods. However, uniformly expanding the\nscope results in subpar performance since real-world web graphs often exhibit\nhomophily disparity between nodes. An ideal way is personalized scopes,\nallowing nodes to have varying scope sizes. Existing methods typically add\nnode-adaptive weights for each hop. Although expressive, they inevitably suffer\nfrom severe overfitting. To address this issue, we formalize personalized\nscoping as a separate scope classification problem that overcomes GNN\noverfitting in node classification. Specifically, we predict the optimal GNN\ndepth for each node. Our theoretical and empirical analysis suggests that\naccurately predicting the depth can significantly enhance generalization. We\nfurther propose Adaptive Scope (AS), a lightweight approach that only\nparticipates in GNN inference. AS encodes structural patterns and predicts the\ndepth to select the best model for each node's prediction. Experimental results\nshow that AS is highly flexible with various GNN architectures across a wide\nrange of datasets while significantly improving accuracy.\n","authors":["Gangda Deng","Hongkuan Zhou","Rajgopal Kannan","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/2409.06998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15601v1","updated":"2024-10-21T02:53:37Z","published":"2024-10-21T02:53:37Z","title":"All You Need is an Improving Column: Enhancing Column Generation for\n  Parallel Machine Scheduling via Transformers","summary":"  We present a neural network-enhanced column generation (CG) approach for a\nparallel machine scheduling problem. The proposed approach utilizes an\nencoder-decoder attention model, namely the transformer and pointer\narchitectures, to develop job sequences with negative reduced cost and thus\ngenerate columns to add to the master problem. By training the neural network\noffline and using it in inference mode to predict negative reduced costs\ncolumns, we achieve significant computational time savings compared to dynamic\nprogramming (DP). Since the exact DP procedure is used to verify that no\nfurther columns with negative reduced cost can be identified at termination,\nthe optimality guarantee of the original CG procedure is preserved. For small\nto medium-sized instances, our approach achieves an average 45% reduction in\ncomputation time compared to solving the subproblems with DP. Furthermore, the\nmodel generalizes not only to unseen, larger problem instances from the same\nprobability distribution but also to instances from different probability\ndistributions than those presented at training time. For large-sized instances,\nthe proposed approach achieves an 80% improvement in the objective value in\nunder 500 seconds, demonstrating both its scalability and efficiency.\n","authors":["Amira Hijazi","Osman Ozaltin","Reha Uzsoy"],"pdf_url":"https://arxiv.org/pdf/2410.15601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15597v1","updated":"2024-10-21T02:44:58Z","published":"2024-10-21T02:44:58Z","title":"A Comprehensive Comparative Study of Individual ML Models and Ensemble\n  Strategies for Network Intrusion Detection Systems","summary":"  The escalating frequency of intrusions in networked systems has spurred the\nexploration of new research avenues in devising artificial intelligence (AI)\ntechniques for intrusion detection systems (IDS). Various AI techniques have\nbeen used to automate network intrusion detection tasks, yet each model\npossesses distinct strengths and weaknesses. Selecting the optimal model for a\ngiven dataset can pose a challenge, necessitating the exploration of ensemble\nmethods to enhance generalization and applicability in network intrusion\ndetection. This paper addresses this gap by conducting a comprehensive\nevaluation of diverse individual models and both simple and advanced ensemble\nmethods for network IDS. We introduce an ensemble learning framework tailored\nfor assessing individual models and ensemble methods in network intrusion\ndetection tasks. Our framework encompasses the loading of input datasets,\ntraining of individual models and ensemble methods, and the generation of\nevaluation metrics. Furthermore, we incorporate all features across individual\nmodels and ensemble techniques. The study presents results for our framework,\nencompassing 14 methods, including various bagging, stacking, blending, and\nboosting techniques applied to multiple base learners such as decision trees,\nneural networks, and among others. We evaluate the framework using two distinct\nnetwork intrusion datasets, RoEduNet-SIMARGL2021 and CICIDS-2017, each\npossessing unique characteristics. Additionally, we categorize AI models based\non their performances on our evaluation metrics and via their confusion\nmatrices. Our assessment demonstrates the efficacy of learning across most\nsetups explored in this study. Furthermore, we contribute to the community by\nreleasing our source codes, providing a foundational ensemble learning\nframework for network intrusion detection.\n","authors":["Ismail Bibers","Osvaldo Arreche","Mustafa Abdallah"],"pdf_url":"https://arxiv.org/pdf/2410.15597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05021v3","updated":"2024-10-21T02:41:21Z","published":"2024-10-07T13:24:24Z","title":"DEPT: Decoupled Embeddings for Pre-training Language Models","summary":"  Language model pre-training benefits from diverse data to enhance performance\nacross domains and languages. However, training on such heterogeneous corpora\nrequires extensive and costly efforts. Since these data sources vary lexically,\nsyntactically, and semantically, they cause negative interference or the\n``curse of multilinguality''. We propose a novel pre-training framework to\nalleviate this curse. Our method, DEPT, decouples embeddings from the\ntransformer body while simultaneously training the latter in multiple contexts.\nDEPT enables training without a shared global vocabulary and: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces\ntoken embedding parameters by up to 80% and the communication costs by 675x for\nbillion-scale models, (3) enhances model generalization and plasticity in\nadapting to new languages and domains, and (4) permits training with custom\noptimized vocabularies per data source. We demonstrate DEPT's potential via the\nfirst vocabulary-agnostic federated multilingual pre-training of a 1.3\nbillion-parameter model, limiting its embedding size to 102.4 million instead\nof 512 million.\n","authors":["Alex Iacob","Lorenzo Sani","Meghdad Kurmanji","William F. Shen","Xinchi Qiu","Dongqi Cai","Yan Gao","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2410.05021v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02949v2","updated":"2024-10-21T02:32:01Z","published":"2024-02-05T12:21:16Z","title":"Kernel PCA for Out-of-Distribution Detection","summary":"  Out-of-Distribution (OoD) detection is vital for the reliability of Deep\nNeural Networks (DNNs). Existing works have shown the insufficiency of\nPrincipal Component Analysis (PCA) straightforwardly applied on the features of\nDNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA\nsuggests that the network features residing in OoD and InD are not well\nseparated by simply proceeding in a linear subspace, which instead can be\nresolved through proper non-linear mappings. In this work, we leverage the\nframework of Kernel PCA (KPCA) for OoD detection, and seek suitable non-linear\nkernels that advocate the separability between InD and OoD data in the subspace\nspanned by the principal components. Besides, explicit feature mappings induced\nfrom the devoted task-specific kernels are adopted so that the KPCA\nreconstruction error for new test samples can be efficiently obtained with\nlarge-scale data. Extensive theoretical and empirical results on multiple OoD\ndata sets and network structures verify the superiority of our KPCA detector in\nefficiency and efficacy with state-of-the-art detection performance.\n","authors":["Kun Fang","Qinghua Tao","Kexin Lv","Mingzhen He","Xiaolin Huang","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2402.02949v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15595v1","updated":"2024-10-21T02:27:24Z","published":"2024-10-21T02:27:24Z","title":"A Comprehensive Survey of Datasets, Theories, Variants, and Applications\n  in Direct Preference Optimization","summary":"  With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community.\n","authors":["Wenyi Xiao","Zechuan Wang","Leilei Gan","Shuai Zhao","Wanggui He","Luu Anh Tuan","Long Chen","Hao Jiang","Zhou Zhao","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06674v3","updated":"2024-10-21T17:19:12Z","published":"2024-02-07T14:23:01Z","title":"Impact of Dataset Properties on Membership Inference Vulnerability of\n  Deep Transfer Learning","summary":"  We analyse the relationship between privacy vulnerability and dataset\nproperties, such as examples per class and number of classes, when applying two\nstate-of-the-art membership inference attacks (MIAs) to fine-tuned neural\nnetworks. We derive per-example MIA vulnerability in terms of score\ndistributions and statistics computed from shadow models. We introduce a\nsimplified model of membership inference and prove that in this model, the\nlogarithm of the difference of true and false positive rates depends linearly\non the logarithm of the number of examples per class. We complement the\ntheoretical analysis with empirical analysis by systematically testing the\npractical privacy vulnerability of fine-tuning large image classification\nmodels and obtain the previously derived power law dependence between the\nnumber of examples per class in the data and the MIA vulnerability, as measured\nby true positive rate of the attack at a low false positive rate. Finally, we\nfit a parametric model of the previously derived form to predict true positive\nrate based on dataset properties and observe good fit for MIA vulnerability on\nunseen fine-tuning scenarios.\n","authors":["Marlon Tobaben","Hibiki Ito","Joonas Jälkö","Gauri Pradhan","Yuan He","Antti Honkela"],"pdf_url":"https://arxiv.org/pdf/2402.06674v3.pdf","comment":"39 pages, 12 figures"},{"id":"http://arxiv.org/abs/2402.11137v3","updated":"2024-10-21T16:48:06Z","published":"2024-02-17T00:02:23Z","title":"TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks","summary":"  While tabular classification has traditionally relied on from-scratch\ntraining, a recent breakthrough called prior-data fitted networks (PFNs)\nchallenges this approach. Similar to large language models, PFNs make use of\npretraining and in-context learning to achieve strong performance on new tasks\nin a single forward pass. However, current PFNs have limitations that prohibit\ntheir widespread adoption. Notably, TabPFN achieves very strong performance on\nsmall tabular datasets but is not designed to make predictions for datasets of\nsize larger than 1000. In this work, we overcome these limitations and\nsubstantially improve the performance of PFNs via context optimization. We\nintroduce TuneTables, a parameter-efficient fine-tuning strategy for PFNs that\ncompresses large datasets into a smaller learned context. We conduct extensive\nexperiments on 19 algorithms over 98 datasets and find that TuneTables achieves\nthe best performance on average, outperforming boosted trees such as CatBoost,\nwhile optimizing fewer than 5% of TabPFN's parameters. Furthermore, we show\nthat TuneTables can be used as an interpretability tool and can even be used to\nmitigate biases by optimizing a fairness objective. We open-source our code and\nraw results at https://github.com/penfever/TuneTables.\n","authors":["Benjamin Feuer","Robin Tibor Schirrmeister","Valeriia Cherepanova","Chinmay Hegde","Frank Hutter","Micah Goldblum","Niv Cohen","Colin White"],"pdf_url":"https://arxiv.org/pdf/2402.11137v3.pdf","comment":"NeurIPS 2024 Poster"},{"id":"http://arxiv.org/abs/2311.05017v2","updated":"2024-10-21T16:30:03Z","published":"2023-11-08T21:03:43Z","title":"Joint Sensing and Semantic Communications with Multi-Task Deep Learning","summary":"  This paper explores the integration of deep learning techniques for joint\nsensing and communications, with an extension to semantic communications. The\nintegrated system comprises a transmitter and receiver operating over a\nwireless channel, subject to noise and fading. The transmitter employs a deep\nneural network (DNN), namely an encoder, for joint operations of source coding,\nchannel coding, and modulation, while the receiver utilizes another DNN, namely\na decoder, for joint operations of demodulation, channel decoding, and source\ndecoding to reconstruct the data samples. The transmitted signal serves a dual\npurpose, supporting communication with the receiver and enabling sensing. When\na target is present, the reflected signal is received, and another DNN decoder\nis utilized for sensing. This decoder is responsible for detecting the target's\npresence and determining its range. All these DNNs, including one encoder and\ntwo decoders, undergo joint training through multi-task learning, considering\ndata and channel characteristics. This paper extends to incorporate semantic\ncommunications by introducing an additional DNN, another decoder at the\nreceiver, operating as a task classifier. This decoder evaluates the fidelity\nof label classification for received signals, enhancing the integration of\nsemantics within the communication process. The study presents results based on\nusing the CIFAR-10 as the input data and accounting for channel effects like\nAdditive White Gaussian Noise (AWGN) and Rayleigh fading. The results\nunderscore the effectiveness of multi-task deep learning in achieving\nhigh-fidelity joint sensing and semantic communications.\n","authors":["Yalin E. Sagduyu","Tugba Erpek","Aylin Yener","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2311.05017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03946v5","updated":"2024-10-21T15:22:54Z","published":"2023-10-05T23:46:45Z","title":"Improved prediction of ligand-protein binding affinities by\n  meta-modeling","summary":"  The accurate screening of candidate drug ligands against target proteins\nthrough computational approaches is of prime interest to drug development\nefforts. Such virtual screening depends in part on methods to predict the\nbinding affinity between ligands and proteins. Many computational models for\nbinding affinity prediction have been developed, but with varying results\nacross targets. Given that ensembling or meta-modeling approaches have shown\ngreat promise in reducing model-specific biases, we develop a framework to\nintegrate published force-field-based empirical docking and sequence-based deep\nlearning models. In building this framework, we evaluate many combinations of\nindividual base models, training databases, and several meta-modeling\napproaches. We show that many of our meta-models significantly improve affinity\npredictions over base models. Our best meta-models achieve comparable\nperformance to state-of-the-art deep learning tools exclusively based on 3D\nstructures, while allowing for improved database scalability and flexibility\nthrough the explicit inclusion of features such as physicochemical properties\nor molecular descriptors. We further demonstrate improved generalization\ncapability by our models using a large-scale benchmark of affinity prediction\nas well as a virtual screening application benchmark. Overall, we demonstrate\nthat diverse modeling approaches can be ensembled together to gain meaningful\nimprovement in binding affinity prediction.\n","authors":["Ho-Joon Lee","Prashant S. Emani","Mark B. Gerstein"],"pdf_url":"https://arxiv.org/pdf/2310.03946v5.pdf","comment":"54 pages, 6 main tables, 6 main figures, 8 supplementary figures, and\n  supporting information. For 11 supplementary tables and code, see\n  https://github.com/Lee1701/Lee2023a"},{"id":"http://arxiv.org/abs/2208.03107v2","updated":"2024-10-21T11:28:18Z","published":"2022-08-05T11:27:55Z","title":"Fixed-Point Automatic Differentiation of Forward--Backward Splitting\n  Algorithms for Partly Smooth Functions","summary":"  A large class of non-smooth practical optimization problems can be written as\nminimization of a sum of smooth and partly smooth functions. We examine such\nstructured problems which also depend on a parameter vector and study the\nproblem of differentiating its solution mapping with respect to the parameter\nwhich has far reaching applications in sensitivity analysis and parameter\nlearning problems. Under partial smoothness and other mild assumptions, we\napply Implicit (ID) and Automatic Differentiation (AD) to the fixed-point\niterations of proximal splitting algorithms. We show that AD of the sequence\ngenerated by these algorithms converges (linearly under further assumptions) to\nthe derivative of the solution mapping. For a variant of automatic\ndifferentiation, which we call Fixed-Point Automatic Differentiation (FPAD), we\nremedy the memory overhead problem of the Reverse Mode AD and moreover provide\nfaster convergence theoretically. We numerically illustrate the convergence and\nconvergence rates of AD and FPAD on Lasso and Group Lasso problems and\ndemonstrate the working of FPAD on prototypical image denoising problems by\nlearning the regularization term.\n","authors":["Sheheryar Mehmood","Peter Ochs"],"pdf_url":"https://arxiv.org/pdf/2208.03107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15859v1","updated":"2024-10-21T10:39:05Z","published":"2024-10-21T10:39:05Z","title":"Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs","summary":"  Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach.\n","authors":["Xin Ma","Yang Liu","Jingjing Liu","Xiaoxu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.15859v1.pdf","comment":"accepted by NeurIPS 2024"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2410.14211v2","updated":"2024-10-21T01:22:16Z","published":"2024-10-18T06:57:19Z","title":"Paths-over-Graph: Knowledge Graph Empowered Large Language Model\n  Reasoning","summary":"  Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.\n","authors":["Xingyu Tan","Xiaoyang Wang","Qing Liu","Xiwei Xu","Xin Yuan","Wenjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14180v2","updated":"2024-10-21T03:19:40Z","published":"2024-10-18T05:16:39Z","title":"XForecast: Evaluating Natural Language Explanations for Time Series\n  Forecasting","summary":"  Time series forecasting aids decision-making, especially for stakeholders who\nrely on accurate predictions, making it very important to understand and\nexplain these models to ensure informed decisions. Traditional explainable AI\n(XAI) methods, which underline feature or temporal importance, often require\nexpert knowledge. In contrast, natural language explanations (NLEs) are more\naccessible to laypeople. However, evaluating forecast NLEs is difficult due to\nthe complex causal relationships in time series data. To address this, we\nintroduce two new performance metrics based on simulatability, assessing how\nwell a human surrogate can predict model forecasts using the explanations.\nExperiments show these metrics differentiate good from poor explanations and\nalign with human judgments. Utilizing these metrics, we further evaluate the\nability of state-of-the-art large language models (LLMs) to generate\nexplanations for time series data, finding that numerical reasoning, rather\nthan model size, is the main factor influencing explanation quality.\n","authors":["Taha Aksu","Chenghao Liu","Amrita Saha","Sarah Tan","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.14180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10270v3","updated":"2024-10-21T08:13:45Z","published":"2024-10-14T08:21:25Z","title":"QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis","summary":"  Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.\n","authors":["Abhijit Manatkar","Ashlesha Akella","Parthivi Gupta","Krishnasuri Narayanam"],"pdf_url":"https://arxiv.org/pdf/2410.10270v3.pdf","comment":"Accepted for EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.16267v1","updated":"2024-10-21T17:59:11Z","published":"2024-10-21T17:59:11Z","title":"xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video\n  Even in VLMs","summary":"  We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html\n","authors":["Michael S. Ryoo","Honglu Zhou","Shrikant Kendre","Can Qin","Le Xue","Manli Shu","Silvio Savarese","Ran Xu","Caiming Xiong","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2410.16267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16256v1","updated":"2024-10-21T17:56:51Z","published":"2024-10-21T17:56:51Z","title":"CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and\n  Evolution","summary":"  Efficient and accurate evaluation is crucial for the continuous improvement\nof large language models (LLMs). Among various assessment methods, subjective\nevaluation has garnered significant attention due to its superior alignment\nwith real-world usage scenarios and human preferences. However, human-based\nevaluations are costly and lack reproducibility, making precise automated\nevaluators (judgers) vital in this process. In this report, we introduce\n\\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM.\nCompassJudger-1 is a general-purpose LLM that demonstrates remarkable\nversatility. It is capable of: 1. Performing unitary scoring and two-model\ncomparisons as a reward model; 2. Conducting evaluations according to specified\nformats; 3. Generating critiques; 4. Executing diverse tasks like a general\nLLM. To assess the evaluation capabilities of different judge models under a\nunified setting, we have also established \\textbf{JudgerBench}, a new benchmark\nthat encompasses various subjective evaluation tasks and covers a wide range of\ntopics. CompassJudger-1 offers a comprehensive solution for various evaluation\ntasks while maintaining the flexibility to adapt to diverse requirements. Both\nCompassJudger and JudgerBench are released and available to the research\ncommunity athttps://github.com/open-compass/CompassJudger. We believe that by\nopen-sourcing these tools, we can foster collaboration and accelerate progress\nin LLM evaluation methodologies.\n","authors":["Maosong Cao","Alexander Lam","Haodong Duan","Hongwei Liu","Songyang Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16256v1.pdf","comment":"Technical Report, Code and Models:\n  https://github.com/open-compass/CompassJudger"},{"id":"http://arxiv.org/abs/2410.16251v1","updated":"2024-10-21T17:55:54Z","published":"2024-10-21T17:55:54Z","title":"Can Knowledge Editing Really Correct Hallucinations?","summary":"  Large Language Models (LLMs) suffer from hallucinations, referring to the\nnon-factual information in generated content, despite their superior capacities\nacross tasks. Meanwhile, knowledge editing has been developed as a new popular\nparadigm to correct the erroneous factual knowledge encoded in LLMs with the\nadvantage of avoiding retraining from scratch. However, one common issue of\nexisting evaluation datasets for knowledge editing is that they do not ensure\nLLMs actually generate hallucinated answers to the evaluation questions before\nediting. When LLMs are evaluated on such datasets after being edited by\ndifferent techniques, it is hard to directly adopt the performance to assess\nthe effectiveness of different knowledge editing methods in correcting\nhallucinations. Thus, the fundamental question remains insufficiently\nvalidated: Can knowledge editing really correct hallucinations in LLMs? We\nproposed HalluEditBench to holistically benchmark knowledge editing methods in\ncorrecting real-world hallucinations. First, we rigorously construct a massive\nhallucination dataset with 9 domains, 26 topics and more than 6,000\nhallucinations. Then, we assess the performance of knowledge editing methods in\na holistic way on five dimensions including Efficacy, Generalization,\nPortability, Locality, and Robustness. Through HalluEditBench, we have provided\nnew insights into the potentials and limitations of different knowledge editing\nmethods in correcting hallucinations, which could inspire future improvements\nand facilitate the progress in the field of knowledge editing.\n","authors":["Baixiang Huang","Canyu Chen","Xiongxiao Xu","Ali Payani","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2410.16251v1.pdf","comment":"The first two authors contributed equally to this work. The main\n  paper is 10 pages long, with 35 pages total. The code, results, dataset, and\n  additional resources are available on the project website:\n  https://llm-editing.github.io/"},{"id":"http://arxiv.org/abs/2410.16246v1","updated":"2024-10-21T17:51:41Z","published":"2024-10-21T17:51:41Z","title":"Analyzing Context Contributions in LLM-based Machine Translation","summary":"  Large language models (LLMs) have achieved state-of-the-art performance in\nmachine translation (MT) and demonstrated the ability to leverage in-context\nlearning through few-shot examples. However, the mechanisms by which LLMs use\ndifferent parts of the input context remain largely unexplored. In this work,\nwe provide a comprehensive analysis of context utilization in MT, studying how\nLLMs use various context parts, such as few-shot examples and the source text,\nwhen generating translations. We highlight several key findings: (1) the source\npart of few-shot examples appears to contribute more than its corresponding\ntargets, irrespective of translation direction; (2) finetuning LLMs with\nparallel data alters the contribution patterns of different context parts; and\n(3) there is a positional bias where earlier few-shot examples have higher\ncontributions to the translated sequence. Finally, we demonstrate that\ninspecting anomalous context contributions can potentially uncover pathological\ntranslations, such as hallucinations. Our findings shed light on the internal\nworkings of LLM-based MT which go beyond those known for standard\nencoder-decoder MT models.\n","authors":["Emmanouil Zaranis","Nuno M. Guerreiro","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2410.16246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16235v1","updated":"2024-10-21T17:41:11Z","published":"2024-10-21T17:41:11Z","title":"ToW: Thoughts of Words Improve Reasoning in Large Language Models","summary":"  We introduce thoughts of words (ToW), a novel training-time data-augmentation\nmethod for next-word prediction. ToW views next-word prediction as a core\nreasoning task and injects fine-grained thoughts explaining what the next word\nshould be and how it is related to the previous contexts in pre-training texts.\nOur formulation addresses two fundamental drawbacks of existing next-word\nprediction learning schemes: they induce factual hallucination and are\ninefficient for models to learn the implicit reasoning processes in raw texts.\nWhile there are many ways to acquire such thoughts of words, we explore the\nfirst step of acquiring ToW annotations through distilling from larger models.\nAfter continual pre-training with only 70K ToW annotations, we effectively\nimprove models' reasoning performances by 7% to 9% on average and reduce model\nhallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks\nand applications, introducing no additional biases on labels or semantics.\n","authors":["Zhikun Xu","Ming Shen","Jacob Dineen","Zhaonan Li","Xiao Ye","Shijie Lu","Aswin RRV","Chitta Baral","Ben Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.16235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16232v1","updated":"2024-10-21T17:39:49Z","published":"2024-10-21T17:39:49Z","title":"Sketch2Code: Evaluating Vision-Language Models for Interactive Web\n  Design Prototyping","summary":"  Sketches are a natural and accessible medium for UI designers to\nconceptualize early-stage ideas. However, existing research on UI/UX automation\noften requires high-fidelity inputs like Figma designs or detailed screenshots,\nlimiting accessibility and impeding efficient design iteration. To bridge this\ngap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art\nVision Language Models (VLMs) on automating the conversion of rudimentary\nsketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code\nsupports interactive agent evaluation that mimics real-world design workflows,\nwhere a VLM-based agent iteratively refines its generations by communicating\nwith a simulated user, either passively receiving feedback instructions or\nproactively asking clarification questions. We comprehensively analyze ten\ncommercial and open-source models, showing that Sketch2Code is challenging for\nexisting VLMs; even the most capable models struggle to accurately interpret\nsketches and formulate effective questions that lead to steady improvement.\nNevertheless, a user study with UI/UX experts reveals a significant preference\nfor proactive question-asking over passive feedback reception, highlighting the\nneed to develop more effective paradigms for multi-turn conversational agents.\n","authors":["Ryan Li","Yanzhe Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16232v1.pdf","comment":"preprint, 9 pages"},{"id":"http://arxiv.org/abs/2407.02273v3","updated":"2024-10-21T17:37:26Z","published":"2024-07-02T14:02:53Z","title":"Language Model Alignment in Multilingual Trolley Problems","summary":"  We evaluate the moral alignment of large language models (LLMs) with human\npreferences in multilingual trolley problems. Building on the Moral Machine\nexperiment, which captures over 40 million human judgments across 200+\ncountries, we develop a cross-lingual corpus of moral dilemma vignettes in over\n100 languages called MultiTP. This dataset enables the assessment of LLMs'\ndecision-making processes in diverse linguistic contexts. Our analysis explores\nthe alignment of 19 different LLMs with human judgments, capturing preferences\nacross six moral dimensions: species, gender, fitness, status, age, and the\nnumber of lives involved. By correlating these preferences with the demographic\ndistribution of language speakers and examining the consistency of LLM\nresponses to various prompt paraphrasings, our findings provide insights into\ncross-lingual and ethical biases of LLMs and their intersection. We discover\nsignificant variance in alignment across languages, challenging the assumption\nof uniform moral reasoning in AI systems and highlighting the importance of\nincorporating diverse perspectives in AI ethics. The results underscore the\nneed for further research on the integration of multilingual dimensions in\nresponsible AI research to ensure fair and equitable AI interactions worldwide.\nOur code and data are at https://github.com/causalNLP/moralmachine\n","authors":["Zhijing Jin","Max Kleiman-Weiner","Giorgio Piatti","Sydney Levine","Jiarui Liu","Fernando Gonzalez","Francesco Ortu","András Strausz","Mrinmaya Sachan","Rada Mihalcea","Yejin Choi","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2407.02273v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16229v1","updated":"2024-10-21T17:34:39Z","published":"2024-10-21T17:34:39Z","title":"Building A Coding Assistant via the Retrieval-Augmented Language Model","summary":"  Pretrained language models have shown strong effectiveness in code-related\ntasks, such as code retrieval, code generation, code summarization, and code\ncompletion tasks. In this paper, we propose COde assistaNt viA\nretrieval-augmeNted language model (CONAN), which aims to build a code\nassistant by mimicking the knowledge-seeking behaviors of humans during coding.\nSpecifically, it consists of a code structure aware retriever (CONAN-R) and a\ndual-view code representation-based retrieval-augmented generation model\n(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and\nMasked Entity Prediction tasks to make language models code structure-aware and\nlearn effective representations for code snippets and documentation. Then\nCONAN-G designs a dual-view code representation mechanism for implementing a\nretrieval-augmented code generation model. CONAN-G regards the code\ndocumentation descriptions as prompts, which help language models better\nunderstand the code semantics. Our experiments show that CONAN achieves\nconvincing performance on different code generation tasks and significantly\noutperforms previous retrieval augmented code generation models. Our further\nanalyses show that CONAN learns tailored representations for both code snippets\nand documentation by aligning code-documentation data pairs and capturing\nstructural semantics by masking and predicting entities in the code data.\nAdditionally, the retrieved code snippets and documentation provide necessary\ninformation from both program language and natural language to assist the code\ngeneration process. CONAN can also be used as an assistant for Large Language\nModels (LLMs), providing LLMs with external knowledge in shorter code document\nlengths to improve their effectiveness on various code tasks. It shows the\nability of CONAN to extract necessary information and help filter out the noise\nfrom retrieved code documents.\n","authors":["Xinze Li","Hanbin Wang","Zhenghao Liu","Shi Yu","Shuo Wang","Shuo Wang","Yukun Yan","Yukai Fu","Yu Gu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2410.16229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16221v1","updated":"2024-10-21T17:25:32Z","published":"2024-10-21T17:25:32Z","title":"On Creating an English-Thai Code-switched Machine Translation in Medical\n  Domain","summary":"  Machine translation (MT) in the medical domain plays a pivotal role in\nenhancing healthcare quality and disseminating medical knowledge. Despite\nadvancements in English-Thai MT technology, common MT approaches often\nunderperform in the medical field due to their inability to precisely translate\nmedical terminologies. Our research prioritizes not merely improving\ntranslation accuracy but also maintaining medical terminology in English within\nthe translated text through code-switched (CS) translation. We developed a\nmethod to produce CS medical translation data, fine-tuned a CS translation\nmodel with this data, and evaluated its performance against strong baselines,\nsuch as Google Neural Machine Translation (NMT) and GPT-3.5/GPT-4. Our model\ndemonstrated competitive performance in automatic metrics and was highly\nfavored in human preference evaluations. Our evaluation result also shows that\nmedical professionals significantly prefer CS translations that maintain\ncritical English terms accurately, even if it slightly compromises fluency. Our\ncode and test set are publicly available\nhttps://github.com/preceptorai-org/NLLB_CS_EM_NLP2024.\n","authors":["Parinthapat Pengpun","Krittamate Tiankanon","Amrest Chinkamol","Jiramet Kinchagawat","Pitchaya Chairuengjitjaras","Pasit Supholkhan","Pubordee Aussavavirojekul","Chiraphat Boonnag","Kanyakorn Veerakanjana","Hirunkul Phimsiri","Boonthicha Sae-jia","Nattawach Sataudom","Piyalitt Ittichaiwong","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2410.16221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16215v1","updated":"2024-10-21T17:16:13Z","published":"2024-10-21T17:16:13Z","title":"Pre-training Distillation for Large Language Models: A Design Space\n  Exploration","summary":"  Knowledge distillation (KD) aims to transfer knowledge from a large teacher\nmodel to a smaller student model. Previous work applying KD in the field of\nlarge language models (LLMs) typically focused on the post-training phase,\nwhere the student LLM learns directly from instructions and corresponding\nresponses generated by the teacher model. In this paper, we extend KD to the\npre-training phase of LLMs, named pre-training distillation (PD). We first\nconduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a\n1.9B parameter student LLM, validating the effectiveness of PD. Considering the\nkey impact factors of distillation, we systematically explore the design space\nof pre-training distillation across four aspects: logits processing, loss\nselection, scaling law, and offline or online logits. We conduct extensive\nexperiments to explore the design space of pre-training distillation and find\nbetter configurations and interesting conclusions, such as larger student LLMs\ngenerally benefiting more from pre-training distillation, while a larger\nteacher LLM does not necessarily guarantee better results. We hope our\nexploration of the design space will inform future practices in pre-training\ndistillation.\n","authors":["Hao Peng","Xin Lv","Yushi Bai","Zijun Yao","Jiajie Zhang","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2410.16215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16208v1","updated":"2024-10-21T17:11:21Z","published":"2024-10-21T17:11:21Z","title":"Compute-Constrained Data Selection","summary":"  Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. These experiments show the\nvalidity of this model in real-world experiments. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective.\n","authors":["Junjie Oscar Yin","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.16208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16207v1","updated":"2024-10-21T17:10:43Z","published":"2024-10-21T17:10:43Z","title":"CoT-TL: Low-Resource Temporal Knowledge Representation of Planning\n  Instructions Using Chain-of-Thought Reasoning","summary":"  Autonomous agents often face the challenge of interpreting uncertain natural\nlanguage instructions for planning tasks. Representing these instructions as\nLinear Temporal Logic (LTL) enables planners to synthesize actionable plans. We\nintroduce CoT-TL, a data-efficient in-context learning framework for\ntranslating natural language specifications into LTL representations. CoT-TL\naddresses the limitations of large language models, which typically rely on\nextensive fine-tuning data, by extending chain-of-thought reasoning and\nsemantic roles to align with the requirements of formal logic creation. This\napproach enhances the transparency and rationale behind LTL generation,\nfostering user trust. CoT-TL achieves state-of-the-art accuracy across three\ndiverse datasets in low-data scenarios, outperforming existing methods without\nfine-tuning or intermediate translations. To improve reliability and minimize\nhallucinations, we incorporate model checking to validate the syntax of the\ngenerated LTL output. We further demonstrate CoT-TL's effectiveness through\nablation studies and evaluations on unseen LTL structures and formulas in a new\ndataset. Finally, we validate CoT-TL's practicality by integrating it into a\nQuadCopter for multi-step drone planning based on natural language\ninstructions.\n","authors":["Kumar Manas","Stefan Zwicklbauer","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2410.16207v1.pdf","comment":"Accepted for publication in Proceedings of the 2024 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2024), Abu\n  Dhabi 14-18 October 2024"},{"id":"http://arxiv.org/abs/2410.16204v1","updated":"2024-10-21T17:05:50Z","published":"2024-10-21T17:05:50Z","title":"Systematic Review: Text Processing Algorithms in Machine Learning and\n  Deep Learning for Mental Health Detection on Social Media","summary":"  The global rise in depression necessitates innovative detection methods for\nearly intervention. Social media provides a unique opportunity to identify\ndepression through user-generated posts. This systematic review evaluates\nmachine learning (ML) models for depression detection on social media, focusing\non biases and methodological challenges throughout the ML lifecycle. A search\nof PubMed, IEEE Xplore, and Google Scholar identified 47 relevant studies\npublished after 2010. The Prediction model Risk Of Bias ASsessment Tool\n(PROBAST) was utilized to assess methodological quality and risk of bias.\nSignificant biases impacting model reliability and generalizability were found.\nThere is a predominant reliance on Twitter (63.8%) and English-language content\n(over 90%), with most studies focusing on users from the United States and\nEurope. Non-probability sampling methods (approximately 80%) limit\nrepresentativeness. Only 23% of studies explicitly addressed linguistic nuances\nlike negations, crucial for accurate sentiment analysis. Inconsistent\nhyperparameter tuning was observed, with only 27.7% properly tuning models.\nAbout 17% did not adequately partition data into training, validation, and test\nsets, risking overfitting. While 74.5% used appropriate evaluation metrics for\nimbalanced data, others relied on accuracy without addressing class imbalance,\npotentially skewing results. Reporting transparency varied, often lacking\ncritical methodological details. These findings highlight the need to diversify\ndata sources, standardize preprocessing protocols, ensure consistent model\ndevelopment practices, address class imbalance, and enhance reporting\ntransparency. By overcoming these challenges, future research can develop more\nrobust and generalizable ML models for depression detection on social media,\ncontributing to improved mental health outcomes globally.\n","authors":["Yuchen Cao","Jianglai Dai","Zhongyan Wang","Yeyubei Zhang","Xiaorui Shen","Yunchong Liu","Yexin Tian"],"pdf_url":"https://arxiv.org/pdf/2410.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19323v2","updated":"2024-10-21T17:05:15Z","published":"2024-05-29T17:54:22Z","title":"Are Large Language Models Chameleons? An Attempt to Simulate Social\n  Surveys","summary":"  Can large language models (LLMs) simulate social surveys? To answer this\nquestion, we conducted millions of simulations in which LLMs were asked to\nanswer subjective questions. A comparison of different LLM responses with the\nEuropean Social Survey (ESS) data suggests that the effect of prompts on bias\nand variability is fundamental, highlighting major cultural, age, and gender\nbiases. We further discussed statistical methods for measuring the difference\nbetween LLM answers and survey data and proposed a novel measure inspired by\nJaccard similarity, as LLM-generated responses are likely to have a smaller\nvariance. Our experiments also reveal that it is important to analyze the\nrobustness and variability of prompts before using LLMs to simulate social\nsurveys, as their imitation abilities are approximate at best.\n","authors":["Mingmeng Geng","Sihong He","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2405.19323v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.16196v1","updated":"2024-10-21T16:59:25Z","published":"2024-10-21T16:59:25Z","title":"Information for Conversation Generation: Proposals Utilising Knowledge\n  Graphs","summary":"  LLMs are frequently used tools for conversational generation. Without\nadditional information LLMs can generate lower quality responses due to lacking\nrelevant content and hallucinations, as well as the perception of poor\nemotional capability, and an inability to maintain a consistent character.\nKnowledge graphs are commonly used forms of external knowledge and may provide\nsolutions to these challenges. This paper introduces three proposals, utilizing\nknowledge graphs to enhance LLM generation. Firstly, dynamic knowledge graph\nembeddings and recommendation could allow for the integration of new\ninformation and the selection of relevant knowledge for response generation.\nSecondly, storing entities with emotional values as additional features may\nprovide knowledge that is better emotionally aligned with the user input.\nThirdly, integrating character information through narrative bubbles would\nmaintain character consistency, as well as introducing a structure that would\nreadily incorporate new information.\n","authors":["Alex Clay","Ernesto Jiménez-Ruiz"],"pdf_url":"https://arxiv.org/pdf/2410.16196v1.pdf","comment":"7 pages with citations, 1 figure, accepted to the ISWC 2024 Special\n  Session"},{"id":"http://arxiv.org/abs/2406.13791v3","updated":"2024-10-21T16:55:31Z","published":"2024-06-19T19:35:14Z","title":"IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards\n  for Better Well-Being","summary":"  Sustainable Development Goals (SDGs) give the UN a road map for development\nwith Agenda 2030 as a target. SDG3 \"Good Health and Well-Being\" ensures healthy\nlives and promotes well-being for all ages. Digital technologies can support\nSDG3. Burnout and even depression could be reduced by encouraging better\npreventive health. Due to the lack of patient knowledge and focus to take care\nof their health, it is necessary to help patients before it is too late. New\ntrends such as positive psychology and mindfulness are highly encouraged in the\nUSA. Digital Twins (DTs) can help with the continuous monitoring of emotion\nusing physiological signals (e.g., collected via wearables). DTs facilitate\nmonitoring and provide constant health insight to improve quality of life and\nwell-being with better personalization. Healthcare DTs challenges are\nstandardizing data formats, communication protocols, and data exchange\nmechanisms. As an example, ISO has the ISO/IEC JTC 1/SC 41 Internet of Things\n(IoT) and DTs Working Group, with standards such as \"ISO/IEC 21823-3:2021 IoT -\nInteroperability for IoT Systems - Part 3 Semantic interoperability\", \"ISO/IEC\nCD 30178 - IoT - Data format, value and coding\". To achieve those data\nintegration and knowledge challenges, we designed the Mental Health Knowledge\nGraph (ontology and dataset) to boost mental health. As an example, explicit\nknowledge is described such as chocolate contains magnesium which is\nrecommended for depression. The Knowledge Graph (KG) acquires knowledge from\nontology-based mental health projects classified within the LOV4IoT ontology\ncatalog (Emotion, Depression, and Mental Health). Furthermore, the KG is mapped\nto standards when possible. Standards from ETSI SmartM2M can be used such as\nSAREF4EHAW to represent medical devices and sensors, but also ITU/WHO, ISO,\nW3C, NIST, and IEEE standards relevant to mental health can be considered.\n","authors":["Amelie Gyrard","Seyedali Mohammadi","Manas Gaur","Antonio Kung"],"pdf_url":"https://arxiv.org/pdf/2406.13791v3.pdf","comment":"20 pages, Book chapter, Smart Technologies for Achieving Good Health\n  and Well-Being: Towards Sustainable Development Goal, Taylor & Francis"},{"id":"http://arxiv.org/abs/2410.16186v1","updated":"2024-10-21T16:49:35Z","published":"2024-10-21T16:49:35Z","title":"Contamination Report for Multilingual Benchmarks","summary":"  Benchmark contamination refers to the presence of test datasets in Large\nLanguage Model (LLM) pre-training or post-training data. Contamination can lead\nto inflated scores on benchmarks, compromising evaluation results and making it\ndifficult to determine the capabilities of models. In this work, we study the\ncontamination of popular multilingual benchmarks in LLMs that support multiple\nlanguages. We use the Black Box test to determine whether $7$ frequently used\nmultilingual benchmarks are contaminated in $7$ popular open and closed LLMs\nand find that almost all models show signs of being contaminated with almost\nall the benchmarks we test. Our findings can help the community determine the\nbest set of benchmarks to use for multilingual evaluation.\n","authors":["Sanchit Ahuja","Varun Gumma","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.16186v1.pdf","comment":"11 pages, 2 tables"},{"id":"http://arxiv.org/abs/2410.16184v1","updated":"2024-10-21T16:48:26Z","published":"2024-10-21T16:48:26Z","title":"RM-Bench: Benchmarking Reward Models of Language Models with Subtlety\n  and Style","summary":"  Reward models are critical in techniques like Reinforcement Learning from\nHuman Feedback (RLHF) and Inference Scaling Laws, where they guide language\nmodel alignment and select optimal responses. Despite their importance,\nexisting reward model benchmarks often evaluate models by asking them to\ndistinguish between responses generated by models of varying power. However,\nthis approach fails to assess reward models on subtle but critical content\nchanges and variations in style, resulting in a low correlation with policy\nmodel performance. To this end, we introduce RM-Bench, a novel benchmark\ndesigned to evaluate reward models based on their sensitivity to subtle content\ndifferences and resistance to style biases. Extensive experiments demonstrate\nthat RM-Bench strongly correlates with policy model performance, making it a\nreliable reference for selecting reward models to align language models\neffectively. We evaluate nearly 40 reward models on RM-Bench. Our results\nreveal that even state-of-the-art models achieve an average performance of only\n46.6%, which falls short of random-level accuracy (50%) when faced with style\nbias interference. These findings highlight the significant room for\nimprovement in current reward models. Related code and data are available at\nhttps://github.com/THU-KEG/RM-Bench.\n","authors":["Yantao Liu","Zijun Yao","Rui Min","Yixin Cao","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2410.16184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16179v1","updated":"2024-10-21T16:44:51Z","published":"2024-10-21T16:44:51Z","title":"MagicPIG: LSH Sampling for Efficient LLM Generation","summary":"  Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.\n","authors":["Zhuoming Chen","Ranajoy Sadhukhan","Zihao Ye","Yang Zhou","Jianyu Zhang","Niklas Nolte","Yuandong Tian","Matthijs Douze","Leon Bottou","Zhihao Jia","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07428v2","updated":"2024-10-21T16:37:02Z","published":"2024-10-09T20:48:03Z","title":"The First VoicePrivacy Attacker Challenge Evaluation Plan","summary":"  The First VoicePrivacy Attacker Challenge is a new kind of challenge\norganized as part of the VoicePrivacy initiative and supported by ICASSP 2025\nas the SP Grand Challenge It focuses on developing attacker systems against\nvoice anonymization, which will be evaluated against a set of anonymization\nsystems submitted to the VoicePrivacy 2024 Challenge. Training, development,\nand evaluation datasets are provided along with a baseline attacker system.\nParticipants shall develop their attacker systems in the form of automatic\nspeaker verification systems and submit their scores on the development and\nevaluation data to the organizers. To do so, they can use any additional\ntraining data and models, provided that they are openly available and declared\nbefore the specified deadline. The metric for evaluation is equal error rate\n(EER). Results will be presented at the ICASSP 2025 special session to which 5\nselected top-ranked participants will be invited to submit and present their\nchallenge systems.\n","authors":["Natalia Tomashenko","Xiaoxiao Miao","Emmanuel Vincent","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2410.07428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16168v1","updated":"2024-10-21T16:33:16Z","published":"2024-10-21T16:33:16Z","title":"Exploring Pretraining via Active Forgetting for Improving Cross Lingual\n  Transfer for Decoder Language Models","summary":"  Large Language Models (LLMs) demonstrate exceptional capabilities in a\nmultitude of NLP tasks. However, the efficacy of such models to languages other\nthan English is often limited. Prior works have shown that encoder-only models\nsuch as BERT or XLM-RoBERTa show impressive cross lingual transfer of their\ncapabilities from English to other languages. In this work, we propose a\npretraining strategy that uses active forgetting to achieve similar cross\nlingual transfer in decoder-only LLMs. We show that LLMs pretrained with active\nforgetting are highly effective when adapting to new and unseen languages.\nThrough extensive experimentation, we find that LLMs pretrained with active\nforgetting are able to learn better multilingual representations which\ntranslates to better performance in many downstream tasks.\n","authors":["Divyanshu Aggarwal","Ashutosh Sathe","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.16168v1.pdf","comment":"12 pages, 11 tables, 12 figures"},{"id":"http://arxiv.org/abs/2410.16166v1","updated":"2024-10-21T16:32:41Z","published":"2024-10-21T16:32:41Z","title":"Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM\n  Pretraining","summary":"  Multimodal large language models (MLLMs) have made significant strides by\nintegrating visual and textual modalities. A critical factor in training MLLMs\nis the quality of image-text pairs within multimodal pretraining datasets.\nHowever, $\\textit {de facto}$ filter-based data quality enhancement paradigms\noften discard a substantial portion of high-quality image data due to\ninadequate semantic alignment between images and texts, leading to\ninefficiencies in data utilization and scalability. In this paper, we propose\nthe Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically\nassesses and enhances the quality of image-text pairs. AITQE employs a text\nrewriting mechanism for low-quality pairs and incorporates a negative sample\nlearning strategy to improve evaluative capabilities by integrating\ndeliberately selected low-quality samples during training. Unlike prior\napproaches that significantly alter text distributions, our method minimally\nadjusts text to preserve data volume while enhancing quality. Experimental\nresults demonstrate that AITQE surpasses existing methods on various benchmark,\neffectively leveraging raw data and scaling efficiently with increasing data\nvolumes. We hope our work will inspire future works. The code and model are\navailable at: https://github.com/hanhuang22/AITQE.\n","authors":["Han Huang","Yuqi Huo","Zijia Zhao","Haoyu Lu","Shu Wu","Bingning Wang","Qiang Liu","Weipeng Chen","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16165v1","updated":"2024-10-21T16:31:23Z","published":"2024-10-21T16:31:23Z","title":"From Tokens to Materials: Leveraging Language Models for Scientific\n  Discovery","summary":"  Exploring the predictive capabilities of language models in material science\nis an ongoing interest. This study investigates the application of language\nmodel embeddings to enhance material property prediction in materials science.\nBy evaluating various contextual embedding methods and pre-trained models,\nincluding Bidirectional Encoder Representations from Transformers (BERT) and\nGenerative Pre-trained Transformers (GPT), we demonstrate that domain-specific\nmodels, particularly MatBERT significantly outperform general-purpose models in\nextracting implicit knowledge from compound names and material properties. Our\nfindings reveal that information-dense embeddings from the third layer of\nMatBERT, combined with a context-averaging approach, offer the most effective\nmethod for capturing material-property relationships from the scientific\nliterature. We also identify a crucial \"tokenizer effect,\" highlighting the\nimportance of specialized text processing techniques that preserve complete\ncompound names while maintaining consistent token counts. These insights\nunderscore the value of domain-specific training and tokenization in materials\nscience applications and offer a promising pathway for accelerating the\ndiscovery and development of new materials through AI-driven approaches.\n","authors":["Yuwei Wan","Tong Xie","Nan Wu","Wenjie Zhang","Chunyu Kit","Bram Hoex"],"pdf_url":"https://arxiv.org/pdf/2410.16165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16162v1","updated":"2024-10-21T16:26:09Z","published":"2024-10-21T16:26:09Z","title":"Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models\n  Elicits Generalization to Composite Spatial Reasoning","summary":"  Vision language models (VLMs) have demonstrated impressive performance across\na wide range of downstream tasks. However, their proficiency in spatial\nreasoning remains limited, despite its crucial role in tasks involving\nnavigation and interaction with physical environments. Specifically, much of\nthe spatial reasoning in these tasks occurs in two-dimensional (2D)\nenvironments, and our evaluation reveals that state-of-the-art VLMs frequently\ngenerate implausible and incorrect responses to composite spatial reasoning\nproblems, including simple pathfinding tasks that humans can solve effortlessly\nat a glance. To address this, we explore an effective approach to enhance 2D\nspatial reasoning within VLMs by training the model on basic spatial\ncapabilities. We begin by disentangling the key components of 2D spatial\nreasoning: direction comprehension, distance estimation, and localization. Our\ncentral hypothesis is that mastering these basic spatial capabilities can\nsignificantly enhance a model's performance on composite spatial tasks\nrequiring advanced spatial understanding and combinatorial problem-solving. To\ninvestigate this hypothesis, we introduce Sparkle, a framework that fine-tunes\nVLMs on these three basic spatial capabilities by synthetic data generation and\ntargeted supervision to form an instruction dataset for each capability. Our\nexperiments demonstrate that VLMs fine-tuned with Sparkle achieve significant\nperformance gains, not only in the basic tasks themselves but also in\ngeneralizing to composite and out-of-distribution spatial reasoning tasks\n(e.g., improving from 13.5% to 40.0% on the shortest path problem). These\nfindings underscore the effectiveness of mastering basic spatial capabilities\nin enhancing composite spatial problem-solving, offering insights for improving\nVLMs' spatial reasoning capabilities.\n","authors":["Yihong Tang","Ao Qu","Zhaokai Wang","Dingyi Zhuang","Zhaofeng Wu","Wei Ma","Shenhao Wang","Yunhan Zheng","Zhan Zhao","Jinhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13116v4","updated":"2024-10-21T16:22:33Z","published":"2024-02-20T16:17:37Z","title":"A Survey on Knowledge Distillation of Large Language Models","summary":"  In the era of Large Language Models (LLMs), Knowledge Distillation (KD)\nemerges as a pivotal methodology for transferring advanced capabilities from\nleading proprietary LLMs, such as GPT-4, to their open-source counterparts like\nLLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a\ncrucial role in both compressing these models, and facilitating their\nself-improvement by employing themselves as teachers. This paper presents a\ncomprehensive survey of KD's role within the realm of LLM, highlighting its\ncritical function in imparting advanced knowledge to smaller models and its\nutility in model compression and self-improvement. Our survey is meticulously\nstructured around three foundational pillars: \\textit{algorithm},\n\\textit{skill}, and \\textit{verticalization} -- providing a comprehensive\nexamination of KD mechanisms, the enhancement of specific cognitive abilities,\nand their practical implications across diverse fields. Crucially, the survey\nnavigates the intricate interplay between data augmentation (DA) and KD,\nillustrating how DA emerges as a powerful paradigm within the KD framework to\nbolster LLMs' performance. By leveraging DA to generate context-rich,\nskill-specific training data, KD transcends traditional boundaries, enabling\nopen-source models to approximate the contextual adeptness, ethical alignment,\nand deep semantic insights characteristic of their proprietary counterparts.\nThis work aims to provide an insightful guide for researchers and\npractitioners, offering a detailed overview of current methodologies in KD and\nproposing future research directions. Importantly, we firmly advocate for\ncompliance with the legal terms that regulate the use of LLMs, ensuring ethical\nand lawful application of KD of LLMs. An associated Github repository is\navailable at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.\n","authors":["Xiaohan Xu","Ming Li","Chongyang Tao","Tao Shen","Reynold Cheng","Jinyang Li","Can Xu","Dacheng Tao","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.13116v4.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2410.16156v1","updated":"2024-10-21T16:21:45Z","published":"2024-10-21T16:21:45Z","title":"Limpeh ga li gong: Challenges in Singlish Annotations","summary":"  Singlish, or Colloquial Singapore English, is a language formed from oral and\nsocial communication within multicultural Singapore. In this work, we work on a\nfundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS)\ntagging of Singlish sentences. For our analysis, we build a parallel Singlish\ndataset containing direct English translations and POS tags, with translation\nand POS annotation done by native Singlish speakers. Our experiments show that\nautomatic transition- and transformer- based taggers perform with only $\\sim\n80\\%$ accuracy when evaluated against human-annotated POS labels, suggesting\nthat there is indeed room for improvement on computation analysis of the\nlanguage. We provide an exposition of challenges in Singlish annotation: its\ninconsistencies in form and semantics, the highly context-dependent particles\nof the language, its structural unique expressions, and the variation of the\nlanguage on different mediums. Our task definition, resultant labels and\nresults reflects the challenges in analysing colloquial languages formulated\nfrom a variety of dialects, and paves the way for future studies beyond POS\ntagging.\n","authors":["Lynnette Hui Xian Ng","Luo Qi Chan"],"pdf_url":"https://arxiv.org/pdf/2410.16156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16155v1","updated":"2024-10-21T16:21:24Z","published":"2024-10-21T16:21:24Z","title":"A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns","summary":"  With the development of large language models, they are widely used as agents\nin various fields. A key component of agents is memory, which stores vital\ninformation but is susceptible to jailbreak attacks. Existing research mainly\nfocuses on single-agent attacks and shared memory attacks. However, real-world\nscenarios often involve independent memory. In this paper, we propose the\nTroublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale,\nmulti-agent, multi-topology text-based attack evaluation framework. TMCHT\ninvolves one attacker agent attempting to mislead an entire society of agents.\nWe identify two major challenges in multi-agent attacks: (1) Non-complete graph\nstructure, (2) Large-scale systems. We attribute these challenges to a\nphenomenon we term toxicity disappearing. To address these issues, we propose\nan Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes\nthe retrieval suffix to make poisoned samples more easily retrieved and\noptimizes the replication suffix to make poisoned samples have contagious\nability. We demonstrate the superiority of our approach in TMCHT, with 23.51%,\n18.95%, and 52.93% improvements in line topology, star topology, and 100-agent\nsettings. Encourage community attention to the security of multi-agent systems.\n","authors":["Tianyi Men","Pengfei Cao","Zhuoran Jin","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16153v1","updated":"2024-10-21T16:19:41Z","published":"2024-10-21T16:19:41Z","title":"Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages","summary":"  Despite recent advances in multimodal large language models (MLLMs), their\ndevelopment has predominantly focused on English- and western-centric datasets\nand tasks, leaving most of the world's languages and diverse cultural contexts\nunderrepresented. This paper introduces Pangea, a multilingual multimodal LLM\ntrained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.\nPangeaIns features: 1) high-quality English instructions, 2) carefully\nmachine-translated instructions, and 3) culturally relevant multimodal tasks to\nensure cross-cultural coverage. To rigorously assess models' capabilities, we\nintroduce PangeaBench, a holistic evaluation suite encompassing 14 datasets\ncovering 47 languages. Results show that Pangea significantly outperforms\nexisting open-source models in multilingual settings and diverse cultural\ncontexts. Ablation studies further reveal the importance of English data\nproportions, language popularity, and the number of multimodal training samples\non overall performance. We fully open-source our data, code, and trained\ncheckpoints, to facilitate the development of inclusive and robust multilingual\nMLLMs, promoting equity and accessibility across a broader linguistic and\ncultural spectrum.\n","authors":["Xiang Yue","Yueqi Song","Akari Asai","Seungone Kim","Jean de Dieu Nyandwi","Simran Khanuja","Anjali Kantharuban","Lintang Sutawika","Sathyanarayanan Ramamoorthy","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16153v1.pdf","comment":"52 pages, 27 figures"},{"id":"http://arxiv.org/abs/2405.18406v2","updated":"2024-10-21T16:18:37Z","published":"2024-05-28T17:46:36Z","title":"RACCooN: A Versatile Instructional Video Editing Framework with\n  Auto-Generated Narratives","summary":"  Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. (3) RACCooN also plans to imagine new objects in a\ngiven video, so users simply prompt the model to receive a detailed video\nediting plan for complex video editing. The proposed framework demonstrates\nimpressive versatile capabilities in video-to-paragraph generation, video\ncontent editing, and can be incorporated into other SoTA video generative\nmodels for further enhancement.\n","authors":["Jaehong Yoon","Shoubin Yu","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2405.18406v2.pdf","comment":"The first two authors contribute equally. Project Page:\n  https://raccoon-mllm-gen.github.io/"},{"id":"http://arxiv.org/abs/2410.16144v1","updated":"2024-10-21T16:14:57Z","published":"2024-10-21T16:14:57Z","title":"1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs","summary":"  Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shaoguang Mao","Shuming Ma","Hongyu Wang","Yan Xia","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.16144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16139v1","updated":"2024-10-21T16:05:58Z","published":"2024-10-21T16:05:58Z","title":"A Psycholinguistic Evaluation of Language Models' Sensitivity to\n  Argument Roles","summary":"  We present a systematic evaluation of large language models' sensitivity to\nargument roles, i.e., who did what to whom, by replicating psycholinguistic\nstudies on human argument role processing. In three experiments, we find that\nlanguage models are able to distinguish verbs that appear in plausible and\nimplausible contexts, where plausibility is determined through the relation\nbetween the verb and its preceding arguments. However, none of the models\ncapture the same selective patterns that human comprehenders exhibit during\nreal-time verb prediction. This indicates that language models' capacity to\ndetect verb plausibility does not arise from the same mechanism that underlies\nhuman real-time sentence processing.\n","authors":["Eun-Kyoung Rosa Lee","Sathvik Nair","Naomi Feldman"],"pdf_url":"https://arxiv.org/pdf/2410.16139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14134v2","updated":"2024-10-21T15:59:18Z","published":"2024-08-26T09:29:56Z","title":"Exploring the Potential of Large Language Models for Heterophilic Graphs","summary":"  Large language models (LLMs) have presented significant opportunities to\nenhance various machine learning applications, including graph neural networks\n(GNNs). By leveraging the vast open-world knowledge within LLMs, we can more\neffectively interpret and utilize textual data to better characterize\nheterophilic graphs, where neighboring nodes often have different labels.\nHowever, existing approaches for heterophilic graphs overlook the rich textual\ndata associated with nodes, which could unlock deeper insights into their\nheterophilic contexts. In this work, we explore the potential of LLMs for\nmodeling heterophilic graphs and propose a novel two-stage framework:\nLLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first\nstage, we fine-tune the LLM to better identify homophilic and heterophilic\nedges based on the textual content of their nodes. In the second stage, we\nadaptively manage message propagation in GNNs for different edge types based on\nnode features, structures, and heterophilic or homophilic characteristics. To\ncope with the computational demands when deploying LLMs in practical scenarios,\nwe further explore model distillation techniques to fine-tune smaller, more\nefficient models that maintain competitive performance. Extensive experiments\nvalidate the effectiveness of our framework, demonstrating the feasibility of\nusing LLMs to enhance node classification on heterophilic graphs.\n","authors":["Yuxia Wu","Shujie Li","Yuan Fang","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.14134v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.13761v2","updated":"2024-10-21T15:59:18Z","published":"2024-09-16T18:46:24Z","title":"Do Large Language Models Need a Content Delivery Network?","summary":"  As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.\n","authors":["Yihua Cheng","Kuntai Du","Jiayi Yao","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.13761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13502v2","updated":"2024-10-21T15:58:30Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to problems that\nare more complex than the ones on which they have been trained. Empirical\ninvestigations of such questions are impeded by two major flaws of current\nevaluations: (i) much of the evaluation data is contaminated, in the sense that\nit has already been seen during training, and (ii) benchmark datasets do not\ncapture how problem proofs may be arbitrarily complex in various ways. As a\nstep towards addressing these issues, we present a framework for evaluating\nLLMs on problems with arbitrarily complex arithmetic proofs, called MathGAP.\nMathGAP generates problems that follow fixed proof specifications -- along with\nchain-of-thought reasoning annotations -- enabling systematic studies on\ngeneralization with respect to arithmetic proof complexity. We apply MathGAP to\nanalyze how in-context learning interacts with generalization to problems that\nhave more complex proofs. We find that among the models tested, most show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for GPT-4o. Surprisingly, providing in-context examples from\nthe same distribution as the test set is not always beneficial for performance.\nIn particular, zero-shot prompting as well as demonstrating a diverse range of\nexamples that are less complex than the test data sometimes yield similar or\nhigher accuracies.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schölkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.16130v1","updated":"2024-10-21T15:55:27Z","published":"2024-10-21T15:55:27Z","title":"Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with\n  Multi-Task Assessment and Stepwise Audio Reasoning","summary":"  Recent advancements in large audio-language models (LALMs) have shown\nimpressive capabilities in understanding and reasoning about audio and speech\ninformation. However, these models still face challenges, including\nhallucinating non-existent sound events, misidentifying the order of sound\nevents, and incorrectly attributing sound sources, which undermine their\nreliability and real-world application. To systematically evaluate these\nissues, we propose three distinct tasks: object existence, temporal order, and\nobject attribute within audio. These tasks assess the models' comprehension of\ncritical audio information aspects. Our experimental results reveal limitations\nin these fundamental tasks, underscoring the need for better models in\nrecognizing specific sound events, determining event sequences, and identifying\nsound sources. To improve performance in these areas, we introduce a multi-turn\nchain-of-thought approach, which demonstrates significantly improved model\nperformance across the proposed tasks.\n","authors":["Chun-Yi Kuan","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2410.16130v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.13073v2","updated":"2024-10-21T15:54:34Z","published":"2024-10-16T22:25:15Z","title":"PromptExp: Multi-granularity Prompt Explanation of Large Language Models","summary":"  Large Language Models excel in tasks like natural language understanding and\ntext generation. Prompt engineering plays a critical role in leveraging LLM\neffectively. However, LLMs black-box nature hinders its interpretability and\neffective prompting engineering. A wide range of model explanation approaches\nhave been developed for deep learning models, However, these local explanations\nare designed for single-output tasks like classification and regression,and\ncannot be directly applied to LLMs, which generate sequences of tokens. Recent\nefforts in LLM explanation focus on natural language explanations, but they are\nprone to hallucinations and inaccuracies. To address this, we introduce\nOurTool, a framework for multi-granularity prompt explanations by aggregating\ntoken-level insights. OurTool introduces two token-level explanation\napproaches: 1.an aggregation-based approach combining local explanation\ntechniques, and 2. a perturbation-based approach with novel techniques to\nevaluate token masking impact. OurTool supports both white-box and black-box\nexplanations and extends explanations to higher granularity levels, enabling\nflexible analysis. We evaluate OurTool in case studies such as sentiment\nanalysis, showing the perturbation-based approach performs best using semantic\nsimilarity to assess perturbation impact. Furthermore, we conducted a user\nstudy to confirm OurTool's accuracy and practical value, and demonstrate its\npotential to enhance LLM interpretability.\n","authors":["Ximing Dong","Shaowei Wang","Dayi Lin","Gopi Krishnan Rajbahadur","Boquan Zhou","Shichao Liu","Ahmed E. Hassan"],"pdf_url":"https://arxiv.org/pdf/2410.13073v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2407.15711v2","updated":"2024-10-21T15:45:31Z","published":"2024-07-22T15:18:45Z","title":"AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?","summary":"  Language agents, built on top of language models (LMs), are systems that can\ninteract with complex environments, such as the open web. In this work, we\nexamine whether such agents can perform realistic and time-consuming tasks on\nthe web, e.g., monitoring real-estate markets or locating relevant nearby\nbusinesses. We introduce AssistantBench, a challenging new benchmark consisting\nof 214 realistic tasks that can be automatically evaluated, covering different\nscenarios and domains. We find that AssistantBench exposes the limitations of\ncurrent systems, including language models and retrieval-augmented language\nmodels, as no model reaches an accuracy of more than 26 points. While\nclosed-book LMs perform well in terms of accuracy, they exhibit low precision\nand tend to hallucinate facts. State-of-the-art web agents reach a score of\nnear zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that\nsignificantly outperforms previous agents, and an ensemble of SPA and\nclosed-book models reaches the best overall performance. Moreover, we analyze\nfailures of current systems and highlight that open web navigation remains a\nmajor challenge.\n","authors":["Ori Yoran","Samuel Joseph Amouyal","Chaitanya Malaviya","Ben Bogin","Ofir Press","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2407.15711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16107v1","updated":"2024-10-21T15:35:44Z","published":"2024-10-21T15:35:44Z","title":"Do LLMs write like humans? Variation in grammatical and rhetorical\n  styles","summary":"  Large language models (LLMs) are capable of writing grammatical text that\nfollows instructions, answers questions, and solves problems. As they have\nadvanced, it has become difficult to distinguish their output from\nhuman-written text. While past research has found some differences in surface\nfeatures such as word choice and punctuation, and developed classifiers to\ndetect LLM output, none has studied the rhetorical styles of LLMs.\n  Using several variants of Llama 3 and GPT-4o, we construct two parallel\ncorpora of human- and LLM-written texts from common prompts. Using Douglas\nBiber's set of lexical, grammatical, and rhetorical features, we identify\nsystematic differences between LLMs and humans and between different LLMs.\nThese differences persist when moving from smaller models to larger ones, and\nare larger for instruction-tuned models than base models. This demonstrates\nthat despite their advanced abilities, LLMs struggle to match human styles, and\nhence more advanced linguistic features can detect patterns in their behavior\nnot previously recognized.\n","authors":["Alex Reinhart","David West Brown","Ben Markey","Michael Laudenbach","Kachatad Pantusen","Ronald Yurko","Gordon Weinberg"],"pdf_url":"https://arxiv.org/pdf/2410.16107v1.pdf","comment":"29 pages, 4 figures, 11 tables"},{"id":"http://arxiv.org/abs/2409.08160v3","updated":"2024-10-21T15:22:58Z","published":"2024-09-12T15:52:22Z","title":"On the Role of Context in Reading Time Prediction","summary":"  We present a new perspective on how readers integrate context during\nreal-time language comprehension. Our proposals build on surprisal theory,\nwhich posits that the processing effort of a linguistic unit (e.g., a word) is\nan affine function of its in-context information content. We first observe that\nsurprisal is only one out of many potential ways that a contextual predictor\ncan be derived from a language model. Another one is the pointwise mutual\ninformation (PMI) between a unit and its context, which turns out to yield the\nsame predictive power as surprisal when controlling for unigram frequency.\nMoreover, both PMI and surprisal are correlated with frequency. This means that\nneither PMI nor surprisal contains information about context alone. In response\nto this, we propose a technique where we project surprisal onto the orthogonal\ncomplement of frequency, yielding a new contextual predictor that is\nuncorrelated with frequency. Our experiments show that the proportion of\nvariance in reading times explained by context is a lot smaller when context is\nrepresented by the orthogonalized predictor. From an interpretability\nstandpoint, this indicates that previous studies may have overstated the role\nthat context has in predicting reading times.\n","authors":["Andreas Opedal","Eleanor Chodroff","Ryan Cotterell","Ethan Gotlieb Wilcox"],"pdf_url":"https://arxiv.org/pdf/2409.08160v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2401.05072v2","updated":"2024-10-21T15:19:41Z","published":"2024-01-10T11:03:53Z","title":"Aligning Translation-Specific Understanding to General Understanding in\n  Large Language Models","summary":"  Large Language models (LLMs) have exhibited remarkable abilities in\nunderstanding complex texts, offering a promising path towards human-like\ntranslation performance. However, this study reveals the misalignment between\nthe translation-specific understanding and the general understanding inside\nLLMs. This understanding misalignment leads to LLMs mistakenly or literally\ntranslating some complicated concepts that they accurately comprehend in the\ngeneral scenarios (e.g., QA). To align the translation-specific understanding\nto the general one, we propose a novel translation process, DUAT (Difficult\nwords Understanding Aligned Translation), explicitly incorporating the general\nunderstanding on the complicated content incurring inconsistent understanding\nto guide the translation. Specifically, DUAT performs cross-lingual\ninterpretation for the difficult-to-translate words and enhances the\ntranslation with the generated interpretations. Furthermore, we reframe the\nexternal tools to improve DUAT in detecting difficult words and generating\nhelpful interpretations. We conduct experiments on the self-constructed\nbenchmark Challenge-WMT, consisting of samples that are prone to\nmistranslation. Human evaluation results on high-resource and low-resource\nlanguage pairs indicate that DUAT significantly facilitates the understanding\nalignment, which improves the translation quality (up to +3.85 COMET) and\nreduces the literality of the translation by -25% to -51%.\n","authors":["Yichong Huang","Baohang Li","Xiaocheng Feng","Chengpeng Fu","Wenshuai Huo","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2401.05072v2.pdf","comment":"EMNLP2024 (Main)"},{"id":"http://arxiv.org/abs/2410.16090v1","updated":"2024-10-21T15:12:51Z","published":"2024-10-21T15:12:51Z","title":"Analysing the Residual Stream of Language Models Under Knowledge\n  Conflicts","summary":"  Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context. Such conflicts can lead to\nundesirable model behaviour, such as reliance on outdated or incorrect\ninformation. In this work, we investigate whether LLMs can identify knowledge\nconflicts and whether it is possible to know which source of knowledge the\nmodel will rely on by analysing the residual stream of the LLM. Through probing\ntasks, we find that LLMs can internally register the signal of knowledge\nconflict in the residual stream, which can be accurately detected by probing\nthe intermediate model activations. This allows us to detect conflicts within\nthe residual stream before generating the answers without modifying the input\nor model parameters. Moreover, we find that the residual stream shows\nsignificantly different patterns when the model relies on contextual knowledge\nversus parametric knowledge to resolve conflicts. This pattern can be employed\nto estimate the behaviour of LLMs when conflict happens and prevent unexpected\nanswers before producing the answers. Our analysis offers insights into how\nLLMs internally manage knowledge conflicts and provides a foundation for\ndeveloping methods to control the knowledge selection processes.\n","authors":["Yu Zhao","Xiaotang Du","Giwon Hong","Aryo Pradipta Gema","Alessio Devoto","Hongru Wang","Xuanli He","Kam-Fai Wong","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2410.16090v1.pdf","comment":"Foundation Model Interventions Workshop @ NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16088v1","updated":"2024-10-21T15:12:20Z","published":"2024-10-21T15:12:20Z","title":"Fine-Tuning LLMs for Reliable Medical Question-Answering Services","summary":"  We present an advanced approach to medical question-answering (QA) services,\nusing fine-tuned Large Language Models (LLMs) to improve the accuracy and\nreliability of healthcare information. Our study focuses on optimizing models\nlike LLaMA-2 and Mistral, which have shown great promise in delivering precise,\nreliable medical answers. By leveraging comprehensive datasets, we applied\nfine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model\nperformance through a combination of decomposed model weights, varied learning\nrates for low-rank matrices, and rank stabilization, leading to improved\nefficiency. ReRAG, which integrates retrieval on demand and question rewriting,\nfurther refines the accuracy of the responses. This approach enables healthcare\nproviders to access fast, dependable information, aiding in more efficient\ndecision-making and fostering greater patient trust. Our work highlights the\npotential of fine-tuned LLMs to significantly improve the quality and\naccessibility of medical information services, ultimately contributing to\nbetter healthcare outcomes for all.\n","authors":["Ali Anaissi","Ali Braytee","Junaid Akram"],"pdf_url":"https://arxiv.org/pdf/2410.16088v1.pdf","comment":"8 pages, 10 figures, accepted and to be published in the proceedings\n  of 2024 IEEE International Conference on Data Mining Workshops (ICDMW)"},{"id":"http://arxiv.org/abs/2405.11459v2","updated":"2024-10-21T15:10:20Z","published":"2024-05-19T06:00:36Z","title":"Du-IN: Discrete units-guided mask modeling for decoding speech from\n  Intracranial Neural signals","summary":"  Invasive brain-computer interfaces with Electrocorticography (ECoG) have\nshown promise for high-performance speech decoding in medical applications, but\nless damaging methods like intracranial stereo-electroencephalography (sEEG)\nremain underexplored. With rapid advances in representation learning,\nleveraging abundant recordings to enhance speech decoding is increasingly\nattractive. However, popular methods often pre-train temporal models based on\nbrain-level tokens, overlooking that brain activities in different regions are\nhighly desynchronized during tasks. Alternatively, they pre-train\nspatial-temporal models based on channel-level tokens but fail to evaluate them\non challenging tasks like speech decoding, which requires intricate processing\nin specific language-related areas. To address this issue, we collected a\nwell-annotated Chinese word-reading sEEG dataset targeting language-related\nbrain networks from 12 subjects. Using this benchmark, we developed the Du-IN\nmodel, which extracts contextual embeddings based on region-level tokens\nthrough discrete codex-guided mask modeling. Our model achieves\nstate-of-the-art performance on the 61-word classification task, surpassing all\nbaselines. Model comparisons and ablation studies reveal that our design\nchoices, including (i) temporal modeling based on region-level tokens by\nutilizing 1D depthwise convolution to fuse channels in the lateral sensorimotor\ncortex (vSMC) and superior temporal gyrus (STG) and (ii) self-supervision\nthrough discrete codex-guided mask modeling, significantly contribute to this\nperformance. Overall, our approach -- inspired by neuroscience findings and\ncapitalizing on region-level representations from specific brain regions -- is\nsuitable for invasive brain modeling and represents a promising neuro-inspired\nAI approach in brain-computer interfaces.\n","authors":["Hui Zheng","Hai-Teng Wang","Wei-Bang Jiang","Zhong-Tao Chen","Li He","Pei-Yang Lin","Peng-Hu Wei","Guo-Guang Zhao","Yun-Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2405.11459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16077v1","updated":"2024-10-21T14:55:59Z","published":"2024-10-21T14:55:59Z","title":"CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian\n  Product Routing in Mixture-of-Experts","summary":"  Large language models (LLM) have been attracting much attention from the\ncommunity recently, due to their remarkable performance in all kinds of\ndownstream tasks. According to the well-known scaling law, scaling up a dense\nLLM enhances its capabilities, but also significantly increases the\ncomputational complexity. Mixture-of-Experts (MoE) models address that by\nallowing the model size to grow without substantially raising training or\ninference costs. Yet MoE models face challenges regarding knowledge sharing\namong experts, making their performance somehow sensitive to routing accuracy.\nTo tackle that, previous works introduced shared experts and combined their\noutputs with those of the top $K$ routed experts in an ``addition'' manner. In\nthis paper, inspired by collective matrix factorization to learn shared\nknowledge among data, we propose CartesianMoE, which implements more effective\nknowledge sharing among experts in more like a ``multiplication'' manner.\nExtensive experimental results indicate that CartesianMoE outperforms previous\nMoE models for building LLMs, in terms of both perplexity and downstream task\nperformance. And we also find that CartesianMoE achieves better expert routing\nrobustness.\n","authors":["Zhenpeng Su","Xing Wu","Zijia Lin","Yizhe Xiong","Minxuan Lv","Guangyuan Ma","Hui Chen","Songlin Hu","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2410.16077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16070v1","updated":"2024-10-21T14:48:35Z","published":"2024-10-21T14:48:35Z","title":"On-Device LLMs for SMEs: Challenges and Opportunities","summary":"  This paper presents a systematic review of the infrastructure requirements\nfor deploying Large Language Models (LLMs) on-device within the context of\nsmall and medium-sized enterprises (SMEs), focusing on both hardware and\nsoftware perspectives. From the hardware viewpoint, we discuss the utilization\nof processing units like GPUs and TPUs, efficient memory and storage solutions,\nand strategies for effective deployment, addressing the challenges of limited\ncomputational resources typical in SME settings. From the software perspective,\nwe explore framework compatibility, operating system optimization, and the use\nof specialized libraries tailored for resource-constrained environments. The\nreview is structured to first identify the unique challenges faced by SMEs in\ndeploying LLMs on-device, followed by an exploration of the opportunities that\nboth hardware innovations and software adaptations offer to overcome these\nobstacles. Such a structured review provides practical insights, contributing\nsignificantly to the community by enhancing the technological resilience of\nSMEs in integrating LLMs.\n","authors":["Jeremy Stephen Gabriel Yee Zhi Wen","Pai Chet Ng","Zhengkui Wang","Ian McLoughlin","Aik Beng Ng","Simon See"],"pdf_url":"https://arxiv.org/pdf/2410.16070v1.pdf","comment":"9 pages, 1 figure. The work is supported by the SIT-NVIDIA Joint AI\n  Centre"},{"id":"http://arxiv.org/abs/2410.16069v1","updated":"2024-10-21T14:47:37Z","published":"2024-10-21T14:47:37Z","title":"Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context","summary":"  Human processing of idioms relies on understanding the contextual sentences\nin which idioms occur, as well as language-intrinsic features such as frequency\nand speaker-intrinsic factors like familiarity. While LLMs have shown high\nperformance on idiomaticity detection tasks, this success may be attributed to\nreasoning shortcuts in existing datasets. To this end, we construct a novel,\ncontrolled contrastive dataset designed to test whether LLMs can effectively\nuse context to disambiguate idiomatic meaning. Additionally, we explore how\ncollocational frequency and sentence probability influence model performance.\nOur findings reveal that LLMs often fail to resolve idiomaticity when it is\nrequired to attend to the surrounding context, and that models perform better\non sentences that have higher likelihood. The collocational frequency of\nexpressions also impacts performance. We make our code and dataset publicly\navailable.\n","authors":["Maggie Mi","Aline Villavicencio","Nafise Sadat Moosavi"],"pdf_url":"https://arxiv.org/pdf/2410.16069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16062v1","updated":"2024-10-21T14:42:37Z","published":"2024-10-21T14:42:37Z","title":"Surprise! Uniform Information Density Isn't the Whole Story: Predicting\n  Surprisal Contours in Long-form Discourse","summary":"  The Uniform Information Density (UID) hypothesis posits that speakers tend to\ndistribute information evenly across linguistic units to achieve efficient\ncommunication. Of course, information rate in texts and discourses is not\nperfectly uniform. While these fluctuations can be viewed as theoretically\nuninteresting noise on top of a uniform target, another explanation is that UID\nis not the only functional pressure regulating information content in a\nlanguage. Speakers may also seek to maintain interest, adhere to writing\nconventions, and build compelling arguments. In this paper, we propose one such\nfunctional pressure; namely that speakers modulate information rate based on\nlocation within a hierarchically-structured model of discourse. We term this\nthe Structured Context Hypothesis and test it by predicting the surprisal\ncontours of naturally occurring discourses extracted from large language models\nusing predictors derived from discourse structure. We find that hierarchical\npredictors are significant predictors of a discourse's information contour and\nthat deeply nested hierarchical predictors are more predictive than shallow\nones. This work takes an initial step beyond UID to propose testable hypotheses\nfor why the information rate fluctuates in predictable ways\n","authors":["Eleftheria Tsipidi","Franz Nowak","Ryan Cotterell","Ethan Wilcox","Mario Giulianelli","Alex Warstadt"],"pdf_url":"https://arxiv.org/pdf/2410.16062v1.pdf","comment":"EMNLP 2024 (main conference)"},{"id":"http://arxiv.org/abs/2404.03881v3","updated":"2024-10-21T14:29:44Z","published":"2024-04-05T04:04:23Z","title":"A Bi-consolidating Model for Joint Relational Triple Extraction","summary":"  Current methods to extract relational triples directly make a prediction\nbased on a possible entity pair in a raw sentence without depending on entity\nrecognition. The task suffers from a serious semantic overlapping problem, in\nwhich several relation triples may share one or two entities in a sentence. In\nthis paper, based on a two-dimensional sentence representation, a\nbi-consolidating model is proposed to address this problem by simultaneously\nreinforcing the local and global semantic features relevant to a relation\ntriple. This model consists of a local consolidation component and a global\nconsolidation component. The first component uses a pixel difference\nconvolution to enhance semantic information of a possible triple representation\nfrom adjacent regions and mitigate noise in neighbouring neighbours. The second\ncomponent strengthens the triple representation based a channel attention and a\nspatial attention, which has the advantage to learn remote semantic\ndependencies in a sentence. They are helpful to improve the performance of both\nentity identification and relation type classification in relation triple\nextraction. After evaluated on several publish datasets, the bi-consolidating\nmodel achieves competitive performance. Analytical experiments demonstrate the\neffectiveness of our model for relational triple extraction and give motivation\nfor other natural language processing tasks.\n","authors":["Xiaocheng Luo","Yanping Chen","Ruixue Tang","Caiwei Yang","Ruizhang Huang","Yongbin Qin"],"pdf_url":"https://arxiv.org/pdf/2404.03881v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04684v2","updated":"2024-10-21T14:21:59Z","published":"2023-12-07T20:36:10Z","title":"Latent Skill Discovery for Chain-of-Thought Reasoning","summary":"  Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks.\n","authors":["Zifan Xu","Haozhu Wang","Dmitriy Bespalov","Xuan Wang","Peter Stone","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2312.04684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16044v1","updated":"2024-10-21T14:20:25Z","published":"2024-10-21T14:20:25Z","title":"Large Language Models Know What To Say But Not When To Speak","summary":"  Turn-taking is a fundamental mechanism in human communication that ensures\nsmooth and coherent verbal interactions. Recent advances in Large Language\nModels (LLMs) have motivated their use in improving the turn-taking\ncapabilities of Spoken Dialogue Systems (SDS), such as their ability to respond\nat appropriate times. However, existing models often struggle to predict\nopportunities for speaking -- called Transition Relevance Places (TRPs) -- in\nnatural, unscripted conversations, focusing only on turn-final TRPs and not\nwithin-turn TRPs. To address these limitations, we introduce a novel dataset of\nparticipant-labeled within-turn TRPs and use it to evaluate the performance of\nstate-of-the-art LLMs in predicting opportunities for speaking. Our experiments\nreveal the current limitations of LLMs in modeling unscripted spoken\ninteractions, highlighting areas for improvement and paving the way for more\nnaturalistic dialogue systems.\n","authors":["Muhammad Umair","Vasanth Sarathy","JP de Ruiter"],"pdf_url":"https://arxiv.org/pdf/2410.16044v1.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2410.16027v1","updated":"2024-10-21T14:02:40Z","published":"2024-10-21T14:02:40Z","title":"ComPO: Community Preferences for Language Model Personalization","summary":"  Conventional algorithms for training language models (LMs) with human\nfeedback rely on preferences that are assumed to account for an \"average\" user,\ndisregarding subjectivity and finer-grained variations. Recent studies have\nraised concerns that aggregating such diverse and often contradictory human\nfeedback to finetune models results in generic models that generate outputs not\npreferred by many user groups, as they tend to average out styles and norms. To\naddress this issue, we draw inspiration from recommendation systems and propose\nComPO, a method to personalize preference optimization in LMs by\ncontextualizing the probability distribution of model outputs with the\npreference provider. Focusing on group-level preferences rather than\nindividuals, we collect and release ComPRed, a question answering dataset with\ncommunity-level preferences from Reddit. This dataset facilitates studying\ndiversity in preferences without incurring privacy concerns associated with\nindividual feedback. Our experiments reveal that conditioning language models\non a community identifier (i.e., subreddit name) during preference tuning\nsubstantially enhances model performance. Conversely, replacing this context\nwith random subreddit identifiers significantly diminishes performance,\nhighlighting the effectiveness of our approach in tailoring responses to\ncommunities' preferences.\n","authors":["Sachin Kumar","Chan Young Park","Yulia Tsvetkov","Noah A. Smith","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2410.16027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15518v2","updated":"2024-10-21T14:02:00Z","published":"2024-02-11T13:41:17Z","title":"Beware of Words: Evaluating the Lexical Diversity of Conversational LLMs\n  using ChatGPT as Case Study","summary":"  The performance of conversational Large Language Models (LLMs) in general,\nand of ChatGPT in particular, is currently being evaluated on many different\ntasks, from logical reasoning or maths to answering questions on a myriad of\ntopics. Instead, much less attention is being devoted to the study of the\nlinguistic features of the texts generated by these LLMs. This is surprising\nsince LLMs are models for language, and understanding how they use the language\nis important. Indeed, conversational LLMs are poised to have a significant\nimpact on the evolution of languages as they may eventually dominate the\ncreation of new text. This means that for example, if conversational LLMs do\nnot use a word it may become less and less frequent and eventually stop being\nused altogether. Therefore, evaluating the linguistic features of the text they\nproduce and how those depend on the model parameters is the first step toward\nunderstanding the potential impact of conversational LLMs on the evolution of\nlanguages. In this paper, we consider the evaluation of the lexical richness of\nthe text generated by LLMs and how it depends on the model parameters. A\nmethodology is presented and used to conduct a comprehensive evaluation of\nlexical richness using ChatGPT as a case study. The results show how lexical\nrichness depends on the version of ChatGPT and some of its parameters, such as\nthe presence penalty, or on the role assigned to the model. The dataset and\ntools used in our analysis are released under open licenses with the goal of\ndrawing the much-needed attention to the evaluation of the linguistic features\nof LLM-generated text.\n","authors":["Gonzalo Martínez","José Alberto Hernández","Javier Conde","Pedro Reviriego","Elena Merino"],"pdf_url":"https://arxiv.org/pdf/2402.15518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16011v1","updated":"2024-10-21T13:42:19Z","published":"2024-10-21T13:42:19Z","title":"CA*: Addressing Evaluation Pitfalls in Computation-Aware Latency for\n  Simultaneous Speech Translation","summary":"  Simultaneous speech translation (SimulST) systems must balance translation\nquality with response time, making latency measurement crucial for evaluating\ntheir real-world performance. However, there has been a longstanding belief\nthat current metrics yield unrealistically high latency measurements in\nunsegmented streaming settings. In this paper, we investigate this phenomenon,\nrevealing its root cause in a fundamental misconception underlying existing\nlatency evaluation approaches. We demonstrate that this issue affects not only\nstreaming but also segment-level latency evaluation across different metrics.\nFurthermore, we propose a modification to correctly measure computation-aware\nlatency for SimulST systems, addressing the limitations present in existing\nmetrics.\n","authors":["Xi Xu","Wenda Xu","Siqi Ouyang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.16011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05770v3","updated":"2024-10-21T13:41:54Z","published":"2024-10-08T07:52:35Z","title":"Efficient Few-shot Learning for Multi-label Classification of Scientific\n  Documents with Many Classes","summary":"  Scientific document classification is a critical task and often involves many\nclasses. However, collecting human-labeled data for many classes is expensive\nand usually leads to label-scarce scenarios. Moreover, recent work has shown\nthat sentence embedding model fine-tuning for few-shot classification is\nefficient, robust, and effective. In this work, we propose FusionSent\n(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free\napproach for few-shot classification of scientific documents with many classes.\nFusionSent uses available training examples and their respective label texts to\ncontrastively fine-tune two different sentence embedding models. Afterward, the\nparameters of both fine-tuned models are fused to combine the complementary\nknowledge from the separate fine-tuning steps into a single model. Finally, the\nresulting sentence embedding model is frozen to embed the training instances,\nwhich are then used as input features to train a classification head. Our\nexperiments show that FusionSent significantly outperforms strong baselines by\nan average of $6.0$ $F_{1}$ points across multiple scientific document\nclassification datasets. In addition, we introduce a new dataset for\nmulti-label classification of scientific documents, which contains 203,961\nscientific articles and 130 classes from the arXiv category taxonomy. Code and\ndata are available at https://github.com/sebischair/FusionSent.\n","authors":["Tim Schopf","Alexander Blatzheim","Nektarios Machner","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2410.05770v3.pdf","comment":"Accepted to the 7th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2024)"},{"id":"http://arxiv.org/abs/2406.10576v2","updated":"2024-10-21T13:39:32Z","published":"2024-06-15T09:31:03Z","title":"Bypass Back-propagation: Optimization-based Structural Pruning for Large\n  Language Models via Policy Gradient","summary":"  In contrast to moderate-size neural network pruning, structural weight\npruning on the Large-Language Models (LLMs) imposes a novel challenge on the\nefficiency of the pruning algorithms, due to the heavy computation/memory\ndemands of the LLMs. Recent efficient LLM pruning methods typically operate at\nthe post-training phase without the expensive weight finetuning, however, their\npruning criteria often rely on heuristically hand-crafted metrics, potentially\nleading to suboptimal performance. We instead propose a novel\noptimization-based structural pruning that learns the pruning masks in a\nprobabilistic space directly by optimizing the loss of the pruned model. To\npreserve the efficiency, our method eliminates the back-propagation through the\nLLM per se during the optimization, requiring only the forward pass of the LLM.\nWe achieve this by learning an underlying Bernoulli distribution to sample\nbinary pruning masks, where we decouple the Bernoulli parameters from the LLM\nloss, thus facilitating an efficient optimization via a policy gradient\nestimator without back-propagation. As a result, our method is able to 1)\noperate at structural granularities of channels, heads, and layers, 2) support\nglobal and heterogeneous pruning (i.e., our method automatically determines\ndifferent redundancy for different layers), and 3) optionally initialize with a\nmetric-based method (for our Bernoulli distributions). Extensive experiments on\nLLaMA, LLaMA-2, LLaMA-3, Vicuna, and Mistral using the C4 and WikiText2\ndatasets demonstrate that our method operates for 2.7 hours with around 35GB\nmemory for the 13B models on a single A100 GPU, and our pruned models\noutperform the state-of-the-arts w.r.t. both perplexity and the majority of\nvarious zero-shot tasks. Codes will be released.\n","authors":["Yuan Gao","Zujing Liu","Weizhong Zhang","Bo Du","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2406.10576v2.pdf","comment":"Initially submitted on June 15, 2024, this version mainly changed the\n  title, and added several experiments: such as 1) experiments on LLaMA-3,\n  Mistral, 2) additional baseline methods (i.e., Bosai -- Everybody Prune Now),\n  and 3) post-pruning finetuned performance (i.e., first prune then finetune)"},{"id":"http://arxiv.org/abs/2410.16006v1","updated":"2024-10-21T13:39:03Z","published":"2024-10-21T13:39:03Z","title":"Exploring Continual Fine-Tuning for Enhancing Language Ability in Large\n  Language Model","summary":"  A common challenge towards the adaptability of Large Language Models (LLMs)\nis their ability to learn new languages over time without hampering the model's\nperformance on languages in which the model is already proficient (usually\nEnglish). Continual fine-tuning (CFT) is the process of sequentially\nfine-tuning an LLM to enable the model to adapt to downstream tasks with\nvarying data distributions and time shifts. This paper focuses on the language\nadaptability of LLMs through CFT. We study a two-phase CFT process in which an\nEnglish-only end-to-end fine-tuned LLM from Phase 1 (predominantly Task\nAbility) is sequentially fine-tuned on a multilingual dataset -- comprising\ntask data in new languages -- in Phase 2 (predominantly Language Ability). We\nobserve that the ``similarity'' of Phase 2 tasks with Phase 1 determines the\nLLM's adaptability. For similar phase-wise datasets, the LLM after Phase 2 does\nnot show deterioration in task ability. In contrast, when the phase-wise\ndatasets are not similar, the LLM's task ability deteriorates. We test our\nhypothesis on the open-source \\mis\\ and \\llm\\ models with multiple phase-wise\ndataset pairs. To address the deterioration, we analyze tailored variants of\ntwo CFT methods: layer freezing and generative replay. Our findings demonstrate\ntheir effectiveness in enhancing the language ability of LLMs while preserving\ntask performance, in comparison to relevant baselines.\n","authors":["Divyanshu Aggarwal","Sankarshan Damle","Navin Goyal","Satya Lokam","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.16006v1.pdf","comment":"19 pages, 6 tables, 4 figures"},{"id":"http://arxiv.org/abs/2410.15999v1","updated":"2024-10-21T13:30:47Z","published":"2024-10-21T13:30:47Z","title":"Steering Knowledge Selection Behaviours in LLMs via SAE-Based\n  Representation Engineering","summary":"  Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context -- this phenomenon, known as\n\\emph{context-memory knowledge conflicts}, can lead to undesirable model\nbehaviour, such as reliance on outdated or incorrect information. Analysing the\ninternal activations of LLMs, we find that they can internally register the\nsignals of knowledge conflict at mid-layers. Such signals allow us to detect\nwhether a knowledge conflict occurs and use \\emph{inference-time} intervention\nstrategies to resolve it. In this work, we propose \\textsc{SpARE}, a\n\\emph{training-free} representation engineering method that uses pre-trained\nsparse auto-encoders (SAEs) to control the knowledge selection behaviour of\nLLMs. \\textsc{SpARE} identifies the functional features that control the\nknowledge selection behaviours and applies them to edit the internal\nactivations of LLMs at inference time. Our experimental results show that\n\\textsc{SpARE} can effectively control the usage of either knowledge source to\nresolve knowledge conflict in open-domain question-answering tasks, surpassing\nexisting representation engineering methods ($+10\\%$) as well as contrastive\ndecoding methods ($+15\\%$).\n","authors":["Yu Zhao","Alessio Devoto","Giwon Hong","Xiaotang Du","Aryo Pradipta Gema","Hongru Wang","Kam-Fai Wong","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2410.15999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15998v1","updated":"2024-10-21T13:29:08Z","published":"2024-10-21T13:29:08Z","title":"1024m at SMM4H 2024: Tasks 3, 5 & 6 -- Ensembles of Transformers and\n  Large Language Models for Medical Text Classification","summary":"  Social media is a great source of data for users reporting information and\nregarding their health and how various things have had an effect on them. This\npaper presents various approaches using Transformers and Large Language Models\nand their ensembles, their performance along with advantages and drawbacks for\nvarious tasks of SMM4H'24 - Classifying texts on impact of nature and outdoor\nspaces on the author's mental health (Task 3), Binary classification of tweets\nreporting their children's health disorders like Asthma, Autism, ADHD and\nSpeech disorder (task 5), Binary classification of users self-reporting their\nage (task 6).\n","authors":["Ram Mohan Rao Kadiyala","M. V. P. Chandra Sekhara Rao"],"pdf_url":"https://arxiv.org/pdf/2410.15998v1.pdf","comment":"short paper , acl 2024"},{"id":"http://arxiv.org/abs/2410.15990v1","updated":"2024-10-21T13:20:15Z","published":"2024-10-21T13:20:15Z","title":"Augmenting Legal Decision Support Systems with LLM-based NLI for\n  Analyzing Social Media Evidence","summary":"  This paper presents our system description and error analysis of our entry\nfor NLLP 2024 shared task on Legal Natural Language Inference (L-NLI)\n\\citep{hagag2024legallenssharedtask2024}. The task required classifying these\nrelationships as entailed, contradicted, or neutral, indicating any association\nbetween the review and the complaint. Our system emerged as the winning\nsubmission, significantly outperforming other entries with a substantial margin\nand demonstrating the effectiveness of our approach in legal text analysis. We\nprovide a detailed analysis of the strengths and limitations of each model and\napproach tested, along with a thorough error analysis and suggestions for\nfuture improvements. This paper aims to contribute to the growing field of\nlegal NLP by offering insights into advanced techniques for natural language\ninference in legal contexts, making it accessible to both experts and newcomers\nin the field.\n","authors":["Ram Mohan Rao Kadiyala","Siddartha Pullakhandam","Kanwal Mehreen","Subhasya Tippareddy","Ashay Srivastava"],"pdf_url":"https://arxiv.org/pdf/2410.15990v1.pdf","comment":"8 pages , accepted to emnlp 2024"},{"id":"http://arxiv.org/abs/2410.11786v2","updated":"2024-10-21T13:11:44Z","published":"2024-10-15T17:05:25Z","title":"Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.\n","authors":["Tsz Ting Chung","Leyang Cui","Lemao Liu","Xinting Huang","Shuming Shi","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.11786v2.pdf","comment":"14 pages, 5 figures, 10 tables, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15974v1","updated":"2024-10-21T13:00:09Z","published":"2024-10-21T13:00:09Z","title":"Large Language Models for Cross-lingual Emotion Detection","summary":"  This paper presents a detailed system description of our entry for the WASSA\n2024 Task 2, focused on cross-lingual emotion detection. We utilized a\ncombination of large language models (LLMs) and their ensembles to effectively\nunderstand and categorize emotions across different languages. Our approach not\nonly outperformed other submissions with a large margin, but also demonstrated\nthe strength of integrating multiple models to enhance performance.\nAdditionally, We conducted a thorough comparison of the benefits and\nlimitations of each model used. An error analysis is included along with\nsuggested areas for future improvement. This paper aims to offer a clear and\ncomprehensive understanding of advanced techniques in emotion detection, making\nit accessible even to those new to the field.\n","authors":["Ram Mohan Rao Kadiyala"],"pdf_url":"https://arxiv.org/pdf/2410.15974v1.pdf","comment":"6 pages , accepted to acl 2024"},{"id":"http://arxiv.org/abs/2410.15970v1","updated":"2024-10-21T12:58:03Z","published":"2024-10-21T12:58:03Z","title":"Policy-driven Knowledge Selection and Response Generation for\n  Document-grounded Dialogue","summary":"  Document-grounded dialogue (DGD) uses documents as external knowledge for\ndialogue generation. Correctly understanding the dialogue context is crucial\nfor selecting knowledge from the document and generating proper responses. In\nthis paper, we propose using a dialogue policy to help the dialogue\nunderstanding in DGD. Our dialogue policy consists of two kinds of guiding\nsignals: utterance function and topic transfer intent. The utterance function\nreflects the purpose and style of an utterance, and the topic transfer intent\nreflects the topic and content of an utterance. We propose a novel framework\nexploiting our dialogue policy for two core tasks in DGD, namely knowledge\nselection (KS) and response generation (RG). The framework consists of two\nmodules: the Policy planner leverages policy-aware dialogue representation to\nselect knowledge and predict the policy of the response; the generator uses\npolicy/knowledge-aware dialogue representation for response generation. Our\npolicy-driven model gets state-of-the-art performance on three public\nbenchmarks and we provide a detailed analysis of the experimental results. Our\ncode/data will be released on GitHub.\n","authors":["Longxuan Ma","Jiapeng Li","Mingda Li","Wei-Nan Zhang","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15970v1.pdf","comment":"29 pages, 9 figures, 14 tables, TOIS 2024"},{"id":"http://arxiv.org/abs/2409.14038v3","updated":"2024-10-21T12:54:33Z","published":"2024-09-21T06:49:34Z","title":"OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching","summary":"  Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.14038v3.pdf","comment":"5 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2410.15966v1","updated":"2024-10-21T12:52:03Z","published":"2024-10-21T12:52:03Z","title":"Self-Explained Keywords Empower Large Language Models for Code\n  Generation","summary":"  Large language models (LLMs) have achieved impressive performance in code\ngeneration. However, due to the long-tail distribution of LLMs' training data,\nlow-frequency terms are typically underrepresented in the training process.\nConsequently, LLMs often misunderstand or overlook problem-specific,\nlow-frequency keywords during code generation, compromising the accuracy of the\ngenerated code. To address this, we propose a novel technique named\nSEK(\\textbf{S}elf-\\textbf{E}xplained \\textbf{K}eywords), which empowers an LLM\nfor better code generation by extracting and explaining the key terms in the\nproblem description with the LLM itself and ranking them based on frequency.\nComprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+),\nand APPS, with five representative LLMs, show that SEK can significantly\nimprove LLMs in code generation, yielding substantial and consistent gains. For\ninstance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\\% to\n93.3\\% on the Humaneval benchmark. Further analysis confirms that SEK enables\nthe LLMs to shift their attention from low-frequency keywords to their\ncorresponding high-frequency counterparts.\n","authors":["Lishui Fan","Mouxiang Chen","Zhongxin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19381v2","updated":"2024-10-21T12:48:58Z","published":"2024-09-28T15:12:55Z","title":"INC-Math: Integrating Natural Language and Code for Enhanced\n  Mathematical Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) are commonly used to generate solutions for\nmathematical reasoning problems in the following formats: natural language,\ncode, or a combination of both. In this paper, we explore fundamental questions\nrelated to solving mathematical reasoning problems using natural language and\ncode with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.\nOur findings show that LLMs are better at reasoning in natural language\ncompared to code. Additionally, although natural language and code serve as\ncomplementary forms of reasoning, they can affect each other in a negative way\nin certain scenarios. These insights motivate our development of a new\nprompting method, INC-Math, which leverages an LLM to dynamically select the\nmost appropriate reasoning form, resulting in improved performance over\ncomparable baselines with GPT-4o-mini.\n","authors":["Xuyuan Xiong","Simeng Han","Ziyue Zhou","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2409.19381v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15962v1","updated":"2024-10-21T12:47:57Z","published":"2024-10-21T12:47:57Z","title":"Systematic Exploration of Dialogue Summarization Approaches for\n  Reproducibility, Comparative Assessment, and Methodological Innovations for\n  Advancing Natural Language Processing in Abstractive Summarization","summary":"  Reproducibility in scientific research, particularly within the realm of\nnatural language processing (NLP), is essential for validating and verifying\nthe robustness of experimental findings. This paper delves into the\nreproduction and evaluation of dialogue summarization models, focusing\nspecifically on the discrepancies observed between original studies and our\nreproduction efforts. Dialogue summarization is a critical aspect of NLP,\naiming to condense conversational content into concise and informative\nsummaries, thus aiding in efficient information retrieval and decision-making\nprocesses. Our research involved a thorough examination of several dialogue\nsummarization models using the AMI (Augmented Multi-party Interaction) dataset.\nThe models assessed include Hierarchical Memory Networks (HMNet) and various\nversions of Pointer-Generator Networks (PGN), namely PGN(DKE), PGN(DRD),\nPGN(DTS), and PGN(DALL). The primary objective was to evaluate the\ninformativeness and quality of the summaries generated by these models through\nhuman assessment, a method that introduces subjectivity and variability in the\nevaluation process. The analysis began with Dataset 1, where the sample\nstandard deviation of 0.656 indicated a moderate dispersion of data points\naround the mean.\n","authors":["Yugandhar Reddy Gogireddy","Jithendra Reddy Gogireddy"],"pdf_url":"https://arxiv.org/pdf/2410.15962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15956v1","updated":"2024-10-21T12:34:17Z","published":"2024-10-21T12:34:17Z","title":"Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs","summary":"  Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.\n","authors":["Yanzhu Guo","Simone Conia","Zelin Zhou","Min Li","Saloni Potdar","Henry Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.15956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15949v1","updated":"2024-10-21T12:30:44Z","published":"2024-10-21T12:30:44Z","title":"Findings of the Third Shared Task on Multilingual Coreference Resolution","summary":"  The paper presents an overview of the third edition of the shared task on\nmultilingual coreference resolution, held as part of the CRAC 2024 workshop.\nSimilarly to the previous two editions, the participants were challenged to\ndevelop systems capable of identifying mentions and clustering them based on\nidentity coreference.\n  This year's edition took another step towards real-world application by not\nproviding participants with gold slots for zero anaphora, increasing the task's\ncomplexity and realism. In addition, the shared task was expanded to include a\nmore diverse set of languages, with a particular focus on historical languages.\nThe training and evaluation data were drawn from version 1.2 of the\nmultilingual collection of harmonized coreference resources CorefUD,\nencompassing 21 datasets across 15 languages. 6 systems competed in this shared\ntask.\n","authors":["Michal Novák","Barbora Dohnalová","Miloslav Konopík","Anna Nedoluzhko","Martin Popel","Ondřej Pražák","Jakub Sido","Milan Straka","Zdeněk Žabokrtský","Daniel Zeman"],"pdf_url":"https://arxiv.org/pdf/2410.15949v1.pdf","comment":"Accepted to CRAC 2024"},{"id":"http://arxiv.org/abs/2410.15939v1","updated":"2024-10-21T12:12:21Z","published":"2024-10-21T12:12:21Z","title":"CausalGraph2LLM: Evaluating LLMs for Causal Queries","summary":"  Causality is essential in scientific research, enabling researchers to\ninterpret true relationships between variables. These causal relationships are\noften represented by causal graphs, which are directed acyclic graphs. With the\nrecent advancements in Large Language Models (LLMs), there is an increasing\ninterest in exploring their capabilities in causal reasoning and their\npotential use to hypothesize causal graphs. These tasks necessitate the LLMs to\nencode the causal graph effectively for subsequent downstream tasks. In this\npaper, we propose a comprehensive benchmark, \\emph{CausalGraph2LLM},\nencompassing a variety of causal graph settings to assess the causal graph\nunderstanding capability of LLMs. We categorize the causal queries into two\ntypes: graph-level and node-level queries. We benchmark both open-sourced and\nclosed models for our study. Our findings reveal that while LLMs show promise\nin this domain, they are highly sensitive to the encoding used. Even capable\nmodels like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with\ndeviations of about $60\\%$. We further demonstrate this sensitivity for\ndownstream causal intervention tasks. Moreover, we observe that LLMs can often\ndisplay biases when presented with contextual information about a causal graph,\npotentially stemming from their parametric memory.\n","authors":["Ivaxi Sheth","Bahare Fatemi","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2410.15939v1.pdf","comment":"Code - https://github.com/ivaxi0s/CausalGraph2LLM"},{"id":"http://arxiv.org/abs/2410.15929v1","updated":"2024-10-21T11:57:56Z","published":"2024-10-21T11:57:56Z","title":"Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with\n  Fine-tuning of Voice Activity Projection","summary":"  In human conversations, short backchannel utterances such as \"yeah\" and \"oh\"\nplay a crucial role in facilitating smooth and engaging dialogue. These\nbackchannels signal attentiveness and understanding without interrupting the\nspeaker, making their accurate prediction essential for creating more natural\nconversational agents. This paper proposes a novel method for real-time,\ncontinuous backchannel prediction using a fine-tuned Voice Activity Projection\n(VAP) model. While existing approaches have relied on turn-based or\nartificially balanced datasets, our approach predicts both the timing and type\nof backchannels in a continuous and frame-wise manner on unbalanced, real-world\ndatasets. We first pre-train the VAP model on a general dialogue corpus to\ncapture conversational dynamics and then fine-tune it on a specialized dataset\nfocused on backchannel behavior. Experimental results demonstrate that our\nmodel outperforms baseline methods in both timing and type prediction tasks,\nachieving robust performance in real-time environments. This research offers a\npromising step toward more responsive and human-like dialogue systems, with\nimplications for interactive spoken dialogue applications such as virtual\nassistants and robots.\n","authors":["Koji Inoue","Divesh Lala","Gabriel Skantze","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2410.15929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15926v1","updated":"2024-10-21T11:54:53Z","published":"2024-10-21T11:54:53Z","title":"Mitigating Object Hallucination via Concentric Causal Attention","summary":"  Recent Large Vision Language Models (LVLMs) present remarkable zero-shot\nconversational and reasoning capabilities given multimodal queries.\nNevertheless, they suffer from object hallucination, a phenomenon where LVLMs\nare prone to generate textual responses not factually aligned with image\ninputs. Our pilot study reveals that object hallucination is closely tied with\nRotary Position Encoding (RoPE), a widely adopted positional dependency\nmodeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs\ntend to hallucinate more when relevant visual cues are distant from instruction\ntokens in the multimodal input sequence. Additionally, we observe a similar\neffect when reversing the sequential order of visual tokens during multimodal\nalignment. Our tests indicate that long-term decay in RoPE poses challenges to\nLVLMs while capturing visual-instruction interactions across long distances. We\npropose Concentric Causal Attention (CCA), a simple yet effective positional\nalignment strategy that mitigates the impact of RoPE long-term decay in LVLMs\nby naturally reducing relative distance between visual and instruction tokens.\nWith CCA, visual tokens can better interact with instruction tokens, thereby\nenhancing model's perception capability and alleviating object hallucination.\nWithout bells and whistles, our positional alignment method surpasses existing\nhallucination mitigation strategies by large margins on multiple object\nhallucination benchmarks.\n","authors":["Yun Xing","Yiheng Li","Ivan Laptev","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2410.15926v1.pdf","comment":"To appear at NeurIPS 2024. Code is available at\n  https://github.com/xing0047/cca-llava"},{"id":"http://arxiv.org/abs/2404.12174v2","updated":"2024-10-21T11:33:33Z","published":"2024-04-18T13:31:05Z","title":"Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation\n  Guidelines?","summary":"  The increasing threat of disinformation calls for automating parts of the\nfact-checking pipeline. Identifying text segments requiring fact-checking is\nknown as claim detection (CD) and claim check-worthiness detection (CW), the\nlatter incorporating complex domain-specific criteria of worthiness and often\nframed as a ranking task. Zero- and few-shot LLM prompting is an attractive\noption for both tasks, as it bypasses the need for labeled datasets and allows\nverbalized claim and worthiness criteria to be directly used for prompting. We\nevaluate the LLMs' predictive and calibration accuracy on five CD/CW datasets\nfrom diverse domains, each utilizing a different worthiness criterion. We\ninvestigate two key aspects: (1) how best to distill factuality and worthiness\ncriteria into a prompt and (2) what amount of context to provide for each\nclaim. To this end, we experiment with varying the level of prompt verbosity\nand the amount of contextual information provided to the model. Our results\nshow that optimal prompt verbosity is domain-dependent, adding context does not\nimprove performance, and confidence scores can be directly used to produce\nreliable check-worthiness rankings.\n","authors":["Laura Majer","Jan Šnajder"],"pdf_url":"https://arxiv.org/pdf/2404.12174v2.pdf","comment":"Accepted to WASSA at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.15911v1","updated":"2024-10-21T11:33:18Z","published":"2024-10-21T11:33:18Z","title":"DefVerify: Do Hate Speech Models Reflect Their Dataset's Definition?","summary":"  When building a predictive model, it is often difficult to ensure that\ndomain-specific requirements are encoded by the model that will eventually be\ndeployed. Consider researchers working on hate speech detection. They will have\nan idea of what is considered hate speech, but building a model that reflects\ntheir view accurately requires preserving those ideals throughout the workflow\nof data set construction and model training. Complications such as sampling\nbias, annotation bias, and model misspecification almost always arise, possibly\nresulting in a gap between the domain specification and the model's actual\nbehavior upon deployment. To address this issue for hate speech detection, we\npropose DefVerify: a 3-step procedure that (i) encodes a user-specified\ndefinition of hate speech, (ii) quantifies to what extent the model reflects\nthe intended definition, and (iii) tries to identify the point of failure in\nthe workflow. We use DefVerify to find gaps between definition and model\nbehavior when applied to six popular hate speech benchmark datasets.\n","authors":["Urja Khurana","Eric Nalisnick","Antske Fokkens"],"pdf_url":"https://arxiv.org/pdf/2410.15911v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.04284v2","updated":"2024-10-21T11:26:20Z","published":"2024-08-08T07:43:17Z","title":"LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection","summary":"  The ease of access to large language models (LLMs) has enabled a widespread\nof machine-generated texts, and now it is often hard to tell whether a piece of\ntext was human-written or machine-generated. This raises concerns about\npotential misuse, particularly within educational and academic domains. Thus,\nit is important to develop practical systems that can automate the process.\nHere, we present one such system, LLM-DetectAIve, designed for fine-grained\ndetection. Unlike most previous work on machine-generated text detection, which\nfocused on binary classification, LLM-DetectAIve supports four categories: (i)\nhuman-written, (ii) machine-generated, (iii) machine-written, then\nmachine-humanized, and (iv) human-written, then machine-polished. Category\n(iii) aims to detect attempts to obfuscate the fact that a text was\nmachine-generated, while category (iv) looks for cases where the LLM was used\nto polish a human-written text, which is typically acceptable in academic\nwriting, but not in education. Our experiments show that LLM-DetectAIve can\neffectively identify the above four categories, which makes it a potentially\nuseful tool in education, academia, and other domains.\n  LLM-DetectAIve is publicly accessible at\nhttps://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system\nis available at https://youtu.be/E8eT_bE7k8c.\n","authors":["Mervat Abassy","Kareem Elozeiri","Alexander Aziz","Minh Ngoc Ta","Raj Vardhan Tomar","Bimarsha Adhikari","Saad El Dine Ahmed","Yuxia Wang","Osama Mohammed Afzal","Zhuohan Xie","Jonibek Mansurov","Ekaterina Artemova","Vladislav Mikhailov","Rui Xing","Jiahui Geng","Hasan Iqbal","Zain Muhammad Mujahid","Tarek Mahmoud","Akim Tsvigun","Alham Fikri Aji","Artem Shelmanov","Nizar Habash","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2408.04284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14859v2","updated":"2024-10-21T11:25:48Z","published":"2024-03-21T22:08:44Z","title":"Log Probabilities Are a Reliable Estimate of Semantic Plausibility in\n  Base and Instruction-Tuned Language Models","summary":"  Semantic plausibility (e.g. knowing that \"the actor won the award\" is more\nlikely than \"the actor won the battle\") serves as an effective proxy for\ngeneral world knowledge. Language models (LMs) capture vast amounts of world\nknowledge by learning distributional patterns in text, accessible via log\nprobabilities (LogProbs) they assign to plausible vs. implausible outputs. The\nnew generation of instruction-tuned LMs can now also provide explicit estimates\nof plausibility via prompting. Here, we evaluate the effectiveness of LogProbs\nand basic prompting to measure semantic plausibility, both in single-sentence\nminimal pairs (Experiment 1) and short context-dependent scenarios (Experiment\n2). We find that (i) in both base and instruction-tuned LMs, LogProbs offers a\nmore reliable measure of semantic plausibility than direct zero-shot prompting,\nwhich yields inconsistent and often poor results; (ii) instruction-tuning\ngenerally does not alter the sensitivity of LogProbs to semantic plausibility\n(although sometimes decreases it); (iii) across models, context mostly\nmodulates LogProbs in expected ways, as measured by three novel metrics of\ncontext-sensitive plausibility and their match to explicit human plausibility\njudgments. We conclude that, even in the era of prompt-based evaluations,\nLogProbs constitute a useful metric of semantic plausibility, both in base and\ninstruction-tuned LMs.\n","authors":["Carina Kauf","Emmanuele Chersoni","Alessandro Lenci","Evelina Fedorenko","Anna A. Ivanova"],"pdf_url":"https://arxiv.org/pdf/2403.14859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13296v2","updated":"2024-10-21T11:10:00Z","published":"2024-08-23T14:48:02Z","title":"The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An\n  Exhaustive Review of Technologies, Research, Best Practices, Applied Research\n  Challenges and Opportunities","summary":"  This report examines the fine-tuning of Large Language Models (LLMs),\nintegrating theoretical insights with practical applications. It outlines the\nhistorical evolution of LLMs from traditional Natural Language Processing (NLP)\nmodels to their pivotal role in AI. A comparison of fine-tuning methodologies,\nincluding supervised, unsupervised, and instruction-based approaches,\nhighlights their applicability to different tasks. The report introduces a\nstructured seven-stage pipeline for fine-tuning LLMs, spanning data\npreparation, model initialization, hyperparameter tuning, and model deployment.\nEmphasis is placed on managing imbalanced datasets and optimization techniques.\nParameter-efficient methods like Low-Rank Adaptation (LoRA) and Half\nFine-Tuning are explored for balancing computational efficiency with\nperformance. Advanced techniques such as memory fine-tuning, Mixture of Experts\n(MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized\nnetworks and multi-agent collaboration. The report also examines novel\napproaches like Proximal Policy Optimization (PPO) and Direct Preference\nOptimization (DPO), which align LLMs with human preferences, alongside pruning\nand routing optimizations to improve efficiency. Further sections cover\nvalidation frameworks, post-deployment monitoring, and inference optimization,\nwith attention to deploying LLMs on distributed and cloud-based platforms.\nEmerging areas such as multimodal LLMs, fine-tuning for audio and speech, and\nchallenges related to scalability, privacy, and accountability are also\naddressed. This report offers actionable insights for researchers and\npractitioners navigating LLM fine-tuning in an evolving landscape.\n","authors":["Venkatesh Balavadhani Parthasarathy","Ahtsham Zafar","Aafaq Khan","Arsalan Shahid"],"pdf_url":"https://arxiv.org/pdf/2408.13296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10621v3","updated":"2024-10-21T11:06:06Z","published":"2024-06-15T12:48:00Z","title":"StrucText-Eval: Evaluating Large Language Model's Reasoning Ability in\n  Structure-Rich Text","summary":"  The effective utilization of structured data, integral to corporate data\nstrategies, has been challenged by the rise of large language models (LLMs)\ncapable of processing unstructured information. This shift prompts the\nquestion: can LLMs interpret structured data directly in its unstructured form?\nWe propose an automatic evaluation data generation method for assessing LLMs'\nreasoning capabilities on structure-rich text to explore this. Our approach\nsupports 8 structured languages and 29 tasks, generating data with adjustable\ncomplexity through controllable nesting and structural width. We introduce\nStrucText-Eval, a benchmark containing 5,800 pre-generated and annotated\nsamples designed to evaluate how well LLMs understand and reason through\nstructured text. StrucText-Eval is divided into two suites: a regular Test\nsuite (3,712 samples) and a Test-Hard suite (2,088 samples), the latter\nemphasizing the gap between human and model performance on more complex tasks.\nExperimental results show that while open-source LLMs achieve a maximum\naccuracy of 74.9\\% on the standard dataset, their performance drops\nsignificantly to 45.8\\% on the harder dataset. In contrast, human participants\nreach an accuracy of 92.6\\% on StrucText-Eval-Hard, highlighting LLMs' current\nlimitations in handling intricate structural information. The benchmark and\ngeneration codes are open sourced in\n\\url{https://github.com/MikeGu721/StrucText-Eval}\n","authors":["Zhouhong Gu","Haoning Ye","Xingzhou Chen","Zeyang Zhou","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.10621v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03615v2","updated":"2024-10-21T11:05:27Z","published":"2024-08-07T08:16:32Z","title":"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in\n  Long-Horizon Tasks","summary":"  Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.\n","authors":["Zaijing Li","Yuquan Xie","Rui Shao","Gongwei Chen","Dongmei Jiang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2408.03615v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15884v1","updated":"2024-10-21T11:02:18Z","published":"2024-10-21T11:02:18Z","title":"Using GPT Models for Qualitative and Quantitative News Analytics in the\n  2024 US Presidental Election Process","summary":"  The paper considers an approach of using Google Search API and GPT-4o model\nfor qualitative and quantitative analyses of news through retrieval-augmented\ngeneration (RAG). This approach was applied to analyze news about the 2024 US\npresidential election process. Different news sources for different time\nperiods have been analyzed. Quantitative scores generated by GPT model have\nbeen analyzed using Bayesian regression to derive trend lines. The\ndistributions found for the regression parameters allow for the analysis of\nuncertainty in the election process. The obtained results demonstrate that\nusing the GPT models for news analysis, one can get informative analytics and\nprovide key insights that can be applied in further analyses of election\nprocesses.\n","authors":["Bohdan M. Pavlyshenko"],"pdf_url":"https://arxiv.org/pdf/2410.15884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07989v3","updated":"2024-10-21T10:54:55Z","published":"2024-04-11T17:59:45Z","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding","summary":"  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n","authors":["Yiwen Tang","Ray Zhang","Jiaming Liu","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Shanghang Zhang","Peng Gao","Hongsheng Li","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07989v3.pdf","comment":"Code and models are released at\n  https://github.com/Ivan-Tang-3D/Any2Point"},{"id":"http://arxiv.org/abs/2410.15865v1","updated":"2024-10-21T10:49:54Z","published":"2024-10-21T10:49:54Z","title":"Principles of semantic and functional efficiency in grammatical\n  patterning","summary":"  Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation, a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints. Our analyses reveal that grammatical organization provably\ninherits from perceptual attributes, but that grammars empirically prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.\n","authors":["Emily Cheng","Francesca Franzon"],"pdf_url":"https://arxiv.org/pdf/2410.15865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12284v2","updated":"2024-10-21T10:11:11Z","published":"2024-10-16T06:43:02Z","title":"Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting","summary":"  The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.\n","authors":["Maxime Kayser","Bayar Menzat","Cornelius Emde","Bogdan Bercean","Alex Novak","Abdala Espinosa","Bartlomiej W. Papiez","Susanne Gaube","Thomas Lukasiewicz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2410.12284v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2408.04167v2","updated":"2024-10-21T09:48:08Z","published":"2024-08-08T02:28:32Z","title":"mbrs: A Library for Minimum Bayes Risk Decoding","summary":"  Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks\nthat outperforms conventional maximum a posterior (MAP) decoding using beam\nsearch by selecting high-quality outputs based on a utility function rather\nthan those with high-probability. Typically, it finds the most suitable\nhypothesis from the set of hypotheses under the sampled pseudo-references. mbrs\nis a library of MBR decoding, which can flexibly combine various metrics,\nalternative expectation estimations, and algorithmic variants. It is designed\nwith a focus on speed measurement and calling count of code blocks,\ntransparency, reproducibility, and extensibility, which are essential for\nresearchers and developers. We published our mbrs as an MIT-licensed\nopen-source project, and the code is available on GitHub.\n  GitHub: https://github.com/naist-nlp/mbrs\n","authors":["Hiroyuki Deguchi","Yusuke Sakai","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2408.04167v2.pdf","comment":"Accepted at EMNLP2024 System Demonstration track"},{"id":"http://arxiv.org/abs/2402.14296v3","updated":"2024-10-21T09:42:55Z","published":"2024-02-22T05:17:49Z","title":"Mitigating Biases of Large Language Models in Stance Detection with\n  Counterfactual Augmented Calibration","summary":"  Stance detection is critical for understanding the underlying position or\nattitude expressed toward a topic. Large language models (LLMs) have\ndemonstrated significant advancements across various natural language\nprocessing tasks including stance detection, however, their performance in\nstance detection is limited by biases and spurious correlations inherent due to\ntheir data-driven nature. Our statistical experiment reveals that LLMs are\nprone to generate biased stances due to sentiment-stance spurious correlations\nand preference towards certain individuals and topics. Furthermore, the results\ndemonstrate a strong negative correlation between stance bias and stance\ndetection performance, underscoring the importance of mitigating bias to\nenhance the utility of LLMs in stance detection. Therefore, in this paper, we\npropose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel\ncalibration network is devised to calibrate potential bias in the stance\nprediction of LLMs. Further, to address the challenge of effectively learning\nbias representations and the difficulty in the generalizability of debiasing,\nwe construct counterfactual augmented data. This approach enhances the\ncalibration network, facilitating the debiasing and out-of-domain\ngeneralization. Experimental results on in-target and zero-shot stance\ndetection tasks show that the proposed FACTUAL can effectively mitigate biases\nof LLMs, achieving state-of-the-art results.\n","authors":["Ang Li","Jingqian Zhao","Bin Liang","Lin Gui","Hui Wang","Xi Zeng","Xingwei Liang","Kam-Fai Wong","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2402.14296v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15825v1","updated":"2024-10-21T09:42:13Z","published":"2024-10-21T09:42:13Z","title":"Did somebody say \"Gest-IT\"? A pilot exploration of multimodal data\n  management","summary":"  The paper presents a pilot exploration of the construction, management and\nanalysis of a multimodal corpus. Through a three-layer annotation that provides\northographic, prosodic, and gestural transcriptions, the Gest-IT resource\nallows to investigate the variation of gesture-making patterns in conversations\nbetween sighted people and people with visual impairment. After discussing the\ntranscription methods and technical procedures employed in our study, we\npropose a unified CoNLL-U corpus and indicate our future steps\n","authors":["Ludovica Pannitto","Lorenzo Albanesi","Laura Marion","Federica Maria Martines","Carmelo Caruso","Claudia S. Bianchini","Francesca Masini","Caterina Mauri"],"pdf_url":"https://arxiv.org/pdf/2410.15825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05846v2","updated":"2024-10-21T09:38:03Z","published":"2024-03-09T09:11:49Z","title":"Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines","summary":"  Text-to-image diffusion models (T2I) use a latent representation of a text\nprompt to guide the image generation process. However, the process by which the\nencoder produces the text representation is unknown. We propose the Diffusion\nLens, a method for analyzing the text encoder of T2I models by generating\nimages from its intermediate representations. Using the Diffusion Lens, we\nperform an extensive analysis of two recent T2I models. Exploring compound\nprompts, we find that complex scenes describing multiple objects are composed\nprogressively and more slowly compared to simple scenes; Exploring knowledge\nretrieval, we find that representation of uncommon concepts requires further\ncomputation compared to common concepts, and that knowledge retrieval is\ngradual across layers. Overall, our findings provide valuable insights into the\ntext encoder component in T2I pipelines.\n","authors":["Michael Toker","Hadas Orgad","Mor Ventura","Dana Arad","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.05846v2.pdf","comment":"Published in: ACL 2024 Project webpage:\n  tokeron.github.io/DiffusionLensWeb"},{"id":"http://arxiv.org/abs/2410.12691v3","updated":"2024-10-21T09:28:12Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15801v1","updated":"2024-10-21T09:18:30Z","published":"2024-10-21T09:18:30Z","title":"Improve Dense Passage Retrieval with Entailment Tuning","summary":"  Retrieval module can be plugged into many downstream NLP tasks to improve\ntheir performance, such as open-domain question answering and\nretrieval-augmented generation. The key to a retrieval system is to calculate\nrelevance scores to query and passage pairs. However, the definition of\nrelevance is often ambiguous. We observed that a major class of relevance\naligns with the concept of entailment in NLI tasks. Based on this observation,\nwe designed a method called entailment tuning to improve the embedding of dense\nretrievers. Specifically, we unify the form of retrieval data and NLI data\nusing existence claim as a bridge. Then, we train retrievers to predict the\nclaims entailed in a passage with a variant task of masked prediction. Our\nmethod can be efficiently plugged into current dense retrieval methods, and\nexperiments show the effectiveness of our method.\n","authors":["Lu Dai","Hao Liu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.15801v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2408.10724v2","updated":"2024-10-21T08:58:39Z","published":"2024-08-20T10:45:36Z","title":"Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian","summary":"  In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.\n","authors":["Cem Üyük","Danica Rovó","Shaghayegh Kolli","Rabia Varol","Georg Groh","Daryna Dementieva"],"pdf_url":"https://arxiv.org/pdf/2408.10724v2.pdf","comment":"EMNLP 2024 NLP4PI Workshop"},{"id":"http://arxiv.org/abs/2407.12831v2","updated":"2024-10-21T08:55:49Z","published":"2024-07-03T13:01:54Z","title":"Truth is Universal: Robust Detection of Lies in LLMs","summary":"  Large Language Models (LLMs) have revolutionised natural language processing,\nexhibiting impressive human-like capabilities. In particular, LLMs are capable\nof \"lying\", knowingly outputting false statements. Hence, it is of interest and\nimportance to develop methods to detect when LLMs lie. Indeed, several authors\ntrained classifiers to detect LLM lies based on their internal model\nactivations. However, other researchers showed that these classifiers may fail\nto generalise, for example to negated statements. In this work, we aim to\ndevelop a robust method to detect when an LLM is lying. To this end, we make\nthe following key contributions: (i) We demonstrate the existence of a\ntwo-dimensional subspace, along which the activation vectors of true and false\nstatements can be separated. Notably, this finding is universal and holds for\nvarious LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our\nanalysis explains the generalisation failures observed in previous studies and\nsets the stage for more robust lie detection; (ii) Building upon (i), we\nconstruct an accurate LLM lie detector. Empirically, our proposed classifier\nachieves state-of-the-art performance, attaining 94% accuracy in both\ndistinguishing true from false factual statements and detecting lies generated\nin real-world scenarios.\n","authors":["Lennart Bürger","Fred A. Hamprecht","Boaz Nadler"],"pdf_url":"https://arxiv.org/pdf/2407.12831v2.pdf","comment":"NeurIPS 2024 poster"},{"id":"http://arxiv.org/abs/2405.20648v2","updated":"2024-10-21T08:52:10Z","published":"2024-05-31T07:30:24Z","title":"Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision\n  Models For Video Captioning and Summarization","summary":"  Video is an increasingly prominent and information-dense medium, yet it poses\nsubstantial challenges for language models. A typical video consists of a\nsequence of shorter segments, or shots, that collectively form a coherent\nnarrative. Each shot is analogous to a word in a sentence where multiple data\nstreams of information (such as visual and auditory data) must be processed\nsimultaneously. Comprehension of the entire video requires not only\nunderstanding the visual-audio information of each shot but also requires that\nthe model links the ideas between each shot to generate a larger,\nall-encompassing story. Despite significant progress in the field, current\nworks often overlook videos' more granular shot-by-shot semantic information.\nIn this project, we propose a family of efficient large language vision models\n(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By\nleveraging better pretraining and data collection strategies, we extend the\nabilities of existing small LLVMs from being able to understand a picture to\nbeing able to understand a sequence of frames. Specifically, we show that\nShotluck Holmes achieves better performance than state-of-the-art results on\nthe Shot2Story video captioning and summary task with significantly smaller and\nmore computationally efficient models.\n","authors":["Richard Luo","Austin Peng","Adithya Vasudev","Rishabh Jain"],"pdf_url":"https://arxiv.org/pdf/2405.20648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08703v2","updated":"2024-10-21T08:49:18Z","published":"2024-10-11T10:47:02Z","title":"On the token distance modeling ability of higher RoPE attention\n  dimension","summary":"  Length extrapolation algorithms based on Rotary position embedding (RoPE)\nhave shown promising results in extending the context length of language\nmodels. However, understanding how position embedding can capture longer-range\ncontextual information remains elusive. Based on the intuition that different\ndimensions correspond to different frequency of changes in RoPE encoding, we\nconducted a dimension-level analysis to investigate the correlation between a\nhidden dimension of an attention head and its contribution to capturing\nlong-distance dependencies. Using our correlation metric, we identified a\nparticular type of attention heads, which we named Positional Heads, from\nvarious length-extrapolated models. These heads exhibit a strong focus on\nlong-range information interaction and play a pivotal role in long input\nprocessing, as evidence by our ablation. We further demonstrate the correlation\nbetween the efficiency of length extrapolation and the extension of the\nhigh-dimensional attention allocation of these heads. The identification of\nPositional Heads provides insights for future research in long-text\ncomprehension.\n","authors":["Xiangyu Hong","Che Jiang","Biqing Qi","Fandong Meng","Mo Yu","Bowen Zhou","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.08703v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2407.18698v2","updated":"2024-10-21T08:43:41Z","published":"2024-07-26T12:23:54Z","title":"Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended\n  Text Generation","summary":"  Decoding from the output distributions of large language models to produce\nhigh-quality text is a complex challenge in language modeling. Various\napproaches, such as beam search, sampling with temperature, $k-$sampling,\nnucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive\nsearch, have been proposed to address this problem, aiming to improve\ncoherence, diversity, as well as resemblance to human-generated text. In this\nstudy, we introduce adaptive contrastive search, a novel decoding strategy\nextending contrastive search by incorporating an adaptive degeneration penalty,\nguided by the estimated uncertainty of the model at each generation step. This\nstrategy is designed to enhance both the creativity and diversity of the\nlanguage modeling process while at the same time producing coherent and\nhigh-quality generated text output. Our findings indicate performance\nenhancement in both aspects, across different model architectures and datasets,\nunderscoring the effectiveness of our method in text generation tasks. Our code\nbase, datasets, and models are publicly available.\n","authors":["Esteban Garces Arias","Julian Rodemann","Meimingwei Li","Christian Heumann","Matthias Aßenmacher"],"pdf_url":"https://arxiv.org/pdf/2407.18698v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15761v1","updated":"2024-10-21T08:21:00Z","published":"2024-10-21T08:21:00Z","title":"Learning-to-Defer for Extractive Question Answering","summary":"  Pre-trained language models have profoundly impacted the field of extractive\nquestion-answering, leveraging large-scale textual corpora to enhance\ncontextual language understanding. Despite their success, these models struggle\nin complex scenarios that demand nuanced interpretation or inferential\nreasoning beyond immediate textual cues. Furthermore, their size poses\ndeployment challenges on resource-constrained devices. Addressing these\nlimitations, we introduce an adapted two-stage Learning-to-Defer mechanism that\nenhances decision-making by enabling selective deference to human experts or\nlarger models without retraining language models in the context of\nquestion-answering. This approach not only maintains computational efficiency\nbut also significantly improves model reliability and accuracy in ambiguous\ncontexts. We establish the theoretical soundness of our methodology by proving\nBayes and $(\\mathcal{H}, \\mathcal{R})$--consistency of our surrogate loss\nfunction, guaranteeing the optimality of the final solution. Empirical\nevaluations on the SQuADv2 dataset illustrate performance gains from\nintegrating human expertise and leveraging larger models. Our results further\ndemonstrate that deferring a minimal number of queries allows the smaller model\nto achieve performance comparable to their larger counterparts while preserving\ncomputing efficiency, thus broadening the applicability of pre-trained language\nmodels in diverse operational environments.\n","authors":["Montreuil Yannis","Carlier Axel","Ng Lai Xing","Ooi Wei Tsang"],"pdf_url":"https://arxiv.org/pdf/2410.15761v1.pdf","comment":"25 pages, 17 main paper"},{"id":"http://arxiv.org/abs/2408.10188v4","updated":"2024-10-21T08:12:42Z","published":"2024-08-19T17:48:08Z","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","summary":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models \\qinghao{by co-designing the\nalgorithm and system. For model training, we upgrade existing VLMs to support\nlong video understanding by incorporating two additional stages, {\\em i.e.},\nlong context extension and long video supervised fine-tuning. However, training\non long video is computationally and memory intensive. We introduce the\nlong-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently\nparallelizes long video training and inference, enabling 2M context length\ntraining on 256 GPUs without any gradient checkpointing. LongVILA efficiently\nextends the number of video frames of VILA from 8 to 2048, improving the long\nvideo captioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy\nin 6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%\nwith subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence\nparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.\n","authors":["Fuzhao Xue","Yukang Chen","Dacheng Li","Qinghao Hu","Ligeng Zhu","Xiuyu Li","Yunhao Fang","Haotian Tang","Shang Yang","Zhijian Liu","Ethan He","Hongxu Yin","Pavlo Molchanov","Jan Kautz","Linxi Fan","Yuke Zhu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2408.10188v4.pdf","comment":"Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md"},{"id":"http://arxiv.org/abs/2410.15753v1","updated":"2024-10-21T08:11:47Z","published":"2024-10-21T08:11:47Z","title":"Natural Language Querying System Through Entity Enrichment","summary":"  This paper focuses on a domain expert querying system over databases. It\npresents a solution designed for a French enterprise interested in offering a\nnatural language interface for its clients. The approach, based on entity\nenrichment, aims at translating natural language queries into database queries.\nIn this paper, the database is treated through a logical paradigm, suggesting\nthe adaptability of our approach to different database models. The good\nprecision of our method is shown through some preliminary experiments.\n","authors":["Joshua Amavi","Mirian Halfeld Ferrari","Nicolas Hiot"],"pdf_url":"https://arxiv.org/pdf/2410.15753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15743v1","updated":"2024-10-21T08:01:46Z","published":"2024-10-21T08:01:46Z","title":"Toeing the Party Line: Election Manifestos as a Key to Understand\n  Political Discourse on Twitter","summary":"  Political discourse on Twitter is a moving target: politicians continuously\nmake statements about their positions. It is therefore crucial to track their\ndiscourse on social media to understand their ideological positions and goals.\nHowever, Twitter data is also challenging to work with since it is ambiguous\nand often dependent on social context, and consequently, recent work on\npolitical positioning has tended to focus strongly on manifestos (parties'\nelectoral programs) rather than social media.\n  In this paper, we extend recently proposed methods to predict pairwise\npositional similarities between parties from the manifesto case to the Twitter\ncase, using hashtags as a signal to fine-tune text representations, without the\nneed for manual annotation. We verify the efficacy of fine-tuning and conduct a\nseries of experiments that assess the robustness of our method for low-resource\nscenarios. We find that our method yields stable positioning reflective of\nmanifesto positioning, both in scenarios with all tweets of candidates across\nyears available and when only smaller subsets from shorter time periods are\navailable. This indicates that it is possible to reliably analyze the relative\npositioning of actors forgoing manual annotation, even in the noisier context\nof social media.\n","authors":["Maximilian Maurer","Tanise Ceron","Sebastian Padó","Gabriella Lapesa"],"pdf_url":"https://arxiv.org/pdf/2410.15743v1.pdf","comment":"9 pages, accepted at EMNLP (Findings) 2024"},{"id":"http://arxiv.org/abs/2410.15737v1","updated":"2024-10-21T07:56:45Z","published":"2024-10-21T07:56:45Z","title":"Who's Who: Large Language Models Meet Knowledge Conflicts in Practice","summary":"  Retrieval-augmented generation (RAG) methods are viable solutions for\naddressing the static memory limits of pre-trained language models.\nNevertheless, encountering conflicting sources of information within the\nretrieval context is an inevitable practical challenge. In such situations, the\nlanguage models are recommended to transparently inform users about the\nconflicts rather than autonomously deciding what to present based on their\ninherent biases. To analyze how current large language models (LLMs) align with\nour recommendation, we introduce WhoQA, a public benchmark dataset to examine\nmodel's behavior in knowledge conflict situations. We induce conflicts by\nasking about a common property among entities having the same name, resulting\nin questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K\nquestions across 13 Wikidata property types and 150K Wikipedia entities. Our\nexperiments show that despite the simplicity of WhoQA questions, knowledge\nconflicts significantly degrades LLMs' performance in RAG settings.\n","authors":["Quang Hieu Pham","Hoang Ngo","Anh Tuan Luu","Dat Quoc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.15737v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15726v1","updated":"2024-10-21T07:44:01Z","published":"2024-10-21T07:44:01Z","title":"Reducing annotator bias by belief elicitation","summary":"  Crowdsourced annotations of data play a substantial role in the development\nof Artificial Intelligence (AI). It is broadly recognised that annotations of\ntext data can contain annotator bias, where systematic disagreement in\nannotations can be traced back to differences in the annotators' backgrounds.\nBeing unaware of such annotator bias can lead to representational bias against\nminority group perspectives and therefore several methods have been proposed\nfor recognising bias or preserving perspectives. These methods typically\nrequire either a substantial number of annotators or annotations per data\ninstance. In this study, we propose a simple method for handling bias in\nannotations without requirements on the number of annotators or instances.\nInstead, we ask annotators about their beliefs of other annotators' judgements\nof an instance, under the hypothesis that these beliefs may provide more\nrepresentative and less biased labels than judgements. The method was examined\nin two controlled, survey-based experiments involving Democrats and Republicans\n(n=1,590) asked to judge statements as arguments and then report beliefs about\nothers' judgements. The results indicate that bias, defined as systematic\ndifferences between the two groups of annotators, is consistently reduced when\nasking for beliefs instead of judgements. Our proposed method therefore has the\npotential to reduce the risk of annotator bias, thereby improving the\ngeneralisability of AI systems and preventing harm to unrepresented\nsocio-demographic groups, and we highlight the need for further studies of this\npotential in other tasks and downstream applications.\n","authors":["Terne Sasha Thorn Jakobsen","Andreas Bjerre-Nielsen","Robert Böhm"],"pdf_url":"https://arxiv.org/pdf/2410.15726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15702v1","updated":"2024-10-21T07:19:19Z","published":"2024-10-21T07:19:19Z","title":"Mitigating Hallucinations of Large Language Models in Medical\n  Information Extraction via Contrastive Decoding","summary":"  The impressive capabilities of large language models (LLMs) have attracted\nextensive interests of applying LLMs to medical field. However, the complex\nnature of clinical environments presents significant hallucination challenges\nfor LLMs, hindering their widespread adoption. In this paper, we address these\nhallucination issues in the context of Medical Information Extraction (MIE)\ntasks by introducing ALternate Contrastive Decoding (ALCD). We begin by\nredefining MIE tasks as an identify-and-classify process. We then separate the\nidentification and classification functions of LLMs by selectively masking the\noptimization of tokens during fine-tuning. During the inference stage, we\nalternately contrast output distributions derived from sub-task models. This\napproach aims to selectively enhance the identification and classification\ncapabilities while minimizing the influence of other inherent abilities in\nLLMs. Additionally, we propose an alternate adaptive constraint strategy to\nmore effectively adjust the scale and scope of contrastive tokens. Through\ncomprehensive experiments on two different backbones and six diverse medical\ninformation extraction tasks, ALCD demonstrates significant improvements in\nresolving hallucination issues compared to conventional decoding methods.\n","authors":["Derong Xu","Ziheng Zhang","Zhihong Zhu","Zhenxi Lin","Qidong Liu","Xian Wu","Tong Xu","Xiangyu Zhao","Yefeng Zheng","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15702v1.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15700v1","updated":"2024-10-21T07:18:23Z","published":"2024-10-21T07:18:23Z","title":"InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert\n  Iteration on Large-Scale LEAN Problems","summary":"  Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. The\nmajor learning paradigm is expert iteration, which necessitates a pre-defined\ndataset comprising numerous mathematical problems. In this process, LLMs\nattempt to prove problems within the dataset and iteratively refine their\ncapabilities through self-training on the proofs they discover. We propose to\nuse large scale LEAN problem datasets Lean-workbook for expert iteration with\nmore than 20,000 CPU days. During expert iteration, we found log-linear trends\nbetween solved problem amount with proof length and CPU usage. We train a\ncritic model to select relatively easy problems for policy models to make\ntrials and guide the model to search for deeper proofs. InternLM2.5-StepProver\nachieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet,\nand Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the\nMiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus\nwhich shows a significant improvement compared to only 9.5% of problems proved\nwhen Lean-Workbook-Plus was released. We open-source our models and searched\nproofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.\n","authors":["Zijian Wu","Suozhi Huang","Zhejian Zhou","Huaiyuan Ying","Jiayu Wang","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19170v2","updated":"2024-10-21T07:17:20Z","published":"2024-06-27T13:44:03Z","title":"The Illusion of Competence: Evaluating the Effect of Explanations on\n  Users' Mental Models of Visual Question Answering Systems","summary":"  We examine how users perceive the limitations of an AI system when it\nencounters a task that it cannot perform perfectly and whether providing\nexplanations alongside its answers aids users in constructing an appropriate\nmental model of the system's capabilities and limitations. We employ a visual\nquestion answer and explanation task where we control the AI system's\nlimitations by manipulating the visual inputs: during inference, the system\neither processes full-color or grayscale images. Our goal is to determine\nwhether participants can perceive the limitations of the system. We hypothesize\nthat explanations will make limited AI capabilities more transparent to users.\nHowever, our results show that explanations do not have this effect. Instead of\nallowing users to more accurately assess the limitations of the AI system,\nexplanations generally increase users' perceptions of the system's competence -\nregardless of its actual performance.\n","authors":["Judith Sieker","Simeon Junker","Ronja Utescher","Nazia Attari","Heiko Wersing","Hendrik Buschmeier","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2406.19170v2.pdf","comment":"17 pages (including Appendix). Accepted at EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2410.13699v2","updated":"2024-10-21T07:12:26Z","published":"2024-10-17T16:04:07Z","title":"Unconstrained Model Merging for Enhanced LLM Reasoning","summary":"  Recent advancements in building domain-specific large language models (LLMs)\nhave shown remarkable success, especially in tasks requiring reasoning\nabilities like logical inference over complex relationships and multi-step\nproblem solving. However, creating a powerful all-in-one LLM remains\nchallenging due to the need for proprietary data and vast computational\nresources. As a resource-friendly alternative, we explore the potential of\nmerging multiple expert models into a single LLM. Existing studies on model\nmerging mainly focus on generalist LLMs instead of domain experts, or the LLMs\nunder the same architecture and size. In this work, we propose an unconstrained\nmodel merging framework that accommodates both homogeneous and heterogeneous\nmodel architectures with a focus on reasoning tasks. A fine-grained layer-wise\nweight merging strategy is designed for homogeneous models merging, while\nheterogeneous model merging is built upon the probabilistic distribution\nknowledge derived from instruction-response fine-tuning data. Across 7\nbenchmarks and 9 reasoning-optimized LLMs, we reveal key findings that\ncombinatorial reasoning emerges from merging which surpasses simple additive\neffects. We propose that unconstrained model merging could serve as a\nfoundation for decentralized LLMs, marking a notable progression from the\nexisting centralized LLM framework. This evolution could enhance wider\nparticipation and stimulate additional advancement in the field of artificial\nintelligence, effectively addressing the constraints posed by centralized\nmodels.\n","authors":["Yiming Zhang","Baoyi He","Shengyu Zhang","Yuhao Fu","Qi Zhou","Zhijie Sang","Zijin Hong","Kejing Yang","Wenjun Wang","Jianbo Yuan","Guanghan Ning","Linyi Li","Chunlin Ji","Fei Wu","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13699v2.pdf","comment":"Under review, correct typos"},{"id":"http://arxiv.org/abs/2410.15696v1","updated":"2024-10-21T07:10:07Z","published":"2024-10-21T07:10:07Z","title":"Tokenization as Finite-State Transduction","summary":"  Tokenization is the first step in modern neural language model pipelines\nwhere an input text is converted to a sequence of subword tokens. We introduce\nfrom first principles a finite-state transduction framework which can\nefficiently encode all possible tokenizations of a regular language. We then\nconstructively show that Byte-Pair Encoding (BPE) and MaxMatch (WordPiece), two\npopular tokenization schemes, fit within this framework. For BPE, this is\nparticularly surprising given its resemblance to context-free grammar and the\nfact that it does not tokenize strings from left to right.\n  An application of this is to guided generation, where the outputs of a\nlanguage model are constrained to match some pattern. Here, patterns are\nencoded at the character level, which creates a mismatch between the\nconstraints and the model's subword vocabulary. While past work has focused\nonly on constraining outputs without regard to the underlying tokenization\nalgorithm, our framework allows for simultaneously constraining the model\noutputs to match a specified pattern while also adhering to the underlying\ntokenizer's canonical tokenization.\n","authors":["Marco Cognetta","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2410.15696v1.pdf","comment":"10 pages + 5 pages in appendix"},{"id":"http://arxiv.org/abs/2406.05392v2","updated":"2024-10-21T07:08:11Z","published":"2024-06-08T07:55:01Z","title":"Deconstructing The Ethics of Large Language Models from Long-standing\n  Issues to New-emerging Dilemmas: A Survey","summary":"  Large Language Models (LLMs) have achieved unparalleled success across\ndiverse language modeling tasks in recent years. However, this progress has\nalso intensified ethical concerns, impacting the deployment of LLMs in everyday\ncontexts. This paper provides a comprehensive survey of ethical challenges\nassociated with LLMs, from longstanding issues such as copyright infringement,\nsystematic bias, and data privacy, to emerging problems like truthfulness and\nsocial norms. We critically analyze existing research aimed at understanding,\nexamining, and mitigating these ethical risks. Our survey underscores\nintegrating ethical standards and societal values into the development of LLMs,\nthereby guiding the development of responsible and ethically aligned language\nmodels.\n","authors":["Chengyuan Deng","Yiqun Duan","Xin Jin","Heng Chang","Yijun Tian","Han Liu","Yichen Wang","Kuofeng Gao","Henry Peng Zou","Yiqiao Jin","Yijia Xiao","Shenghao Wu","Zongxing Xie","Weimin Lyu","Sihong He","Lu Cheng","Haohan Wang","Jun Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.05392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15690v1","updated":"2024-10-21T07:01:25Z","published":"2024-10-21T07:01:25Z","title":"Efficient Terminology Integration for LLM-based Translation in\n  Specialized Domains","summary":"  Traditional machine translation methods typically involve training models\ndirectly on large parallel corpora, with limited emphasis on specialized\nterminology. However, In specialized fields such as patent, finance, or\nbiomedical domains, terminology is crucial for translation, with many terms\nthat needs to be translated following agreed-upon conventions. In this paper we\nintroduce a methodology that efficiently trains models with a smaller amount of\ndata while preserving the accuracy of terminology translation. We achieve this\nthrough a systematic process of term extraction and glossary creation using the\nTrie Tree algorithm, followed by data reconstruction to teach the LLM how to\nintegrate these specialized terms. This methodology enhances the model's\nability to handle specialized terminology and ensures high-quality\ntranslations, particularly in fields where term consistency is crucial. Our\napproach has demonstrated exceptional performance, achieving the highest\ntranslation score among participants in the WMT patent task to date, showcasing\nits effectiveness and broad applicability in specialized translation domains\nwhere general methods often fall short.\n","authors":["Sejoon Kim","Mingi Sung","Jeonghwan Lee","Hyunkuk Lim","Jorge Froilan Gimenez Perez"],"pdf_url":"https://arxiv.org/pdf/2410.15690v1.pdf","comment":"Accepted to WMT 2024"},{"id":"http://arxiv.org/abs/2402.06900v4","updated":"2024-10-21T06:56:26Z","published":"2024-02-10T07:55:27Z","title":"Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric","summary":"  In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.\n","authors":["Hyukhun Koh","Dohyung Kim","Minwoo Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2402.06900v4.pdf","comment":"8 page long"},{"id":"http://arxiv.org/abs/2410.15687v1","updated":"2024-10-21T06:55:35Z","published":"2024-10-21T06:55:35Z","title":"DomainSum: A Hierarchical Benchmark for Fine-Grained Domain Shift in\n  Abstractive Text Summarization","summary":"  Most research on abstractive summarization focuses on single-domain\napplications, often neglecting how domain shifts between documents affect\nperformance and the generalization ability of summarization models. To address\nthis issue, we introduce DomainSum, a hierarchical benchmark designed to\ncapture fine-grained domain shifts in abstractive summarization. We categorize\nthese shifts into three levels: genre, style, and topic, and demonstrate\nthrough comprehensive benchmark analysis that they follow a hierarchical\nstructure. Furthermore, we evaluate the domain generalization capabilities of\ncommonly used pre-trained language models (PLMs) and large language models\n(LLMs) in in-domain and cross-domain settings.\n","authors":["Haohan Yuan","Haopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15678v1","updated":"2024-10-21T06:42:11Z","published":"2024-10-21T06:42:11Z","title":"Revealing and Mitigating the Local Pattern Shortcuts of Mamba","summary":"  Large language models (LLMs) have advanced significantly due to the attention\nmechanism, but their quadratic complexity and linear memory demands limit their\nperformance on long-context tasks. Recently, researchers introduced Mamba, an\nadvanced model built upon State Space Models(SSMs) that offers linear\ncomplexity and constant memory. Although Mamba is reported to match or surpass\nthe performance of attention-based models, our analysis reveals a performance\ngap: Mamba excels in tasks that involve localized key information but faces\nchallenges with tasks that require handling distributed key information. Our\ncontrolled experiments suggest that this inconsistency arises from Mamba's\nreliance on local pattern shortcuts, which enable the model to remember local\nkey information within its limited memory but hinder its ability to retain more\ndispersed information. Therefore, we introduce a global selection module into\nthe Mamba model to address this issue. Experiments on both existing and\nproposed synthetic tasks, as well as real-world tasks, demonstrate the\neffectiveness of our method. Notably, with the introduction of only 4M extra\nparameters, our approach enables the Mamba model(130M) to achieve a significant\nimprovement on tasks with distributed information, increasing its performance\nfrom 0 to 80.54 points.\n","authors":["Wangjie You","Zecheng Tang","Juntao Li","Lili Yao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02050v3","updated":"2024-10-21T06:33:13Z","published":"2024-06-04T07:31:06Z","title":"Analyzing Social Biases in Japanese Large Language Models","summary":"  With the development of Large Language Models (LLMs), social biases in the\nLLMs have become a crucial issue. While various benchmarks for social biases\nhave been provided across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, and analyze social biases in Japanese LLMs. The\nresults show that while current open Japanese LLMs improve their accuracies on\nJBBQ by setting larger parameters, their bias scores become larger. In\naddition, prompts with warnings about social biases and Chain-of-Thought\nprompting reduce the effect of biases in model outputs, but there is room for\nimprovement in the consistency of reasoning.\n","authors":["Hitomi Yanaka","Namgi Han","Ryoma Kumon","Jie Lu","Masashi Takeshita","Ryo Sekizawa","Taisei Kato","Hiromi Arai"],"pdf_url":"https://arxiv.org/pdf/2406.02050v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16756v2","updated":"2024-10-21T06:30:07Z","published":"2024-08-29T17:54:14Z","title":"How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities\n  of Large Language Models","summary":"  The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.\n","authors":["Jiyue Jiang","Pengan Chen","Liheng Chen","Sheng Wang","Qinghang Bao","Lingpeng Kong","Yu Li","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00608v2","updated":"2024-10-21T06:26:03Z","published":"2024-09-01T04:23:48Z","title":"TinyAgent: Function Calling at the Edge","summary":"  Recent large language models (LLMs) have enabled the development of advanced\nagentic systems that can integrate various tools and APIs to fulfill user\nqueries through function calling. However, the deployment of these LLMs on the\nedge has not been explored since they typically require cloud-based\ninfrastructure due to their substantial model size and computational demands.\nTo this end, we present TinyAgent, an end-to-end framework for training and\ndeploying task-specific small language model agents capable of function calling\nfor driving agentic systems at the edge. We first show how to enable accurate\nfunction calling for open-source models via the LLMCompiler framework. We then\nsystematically curate a high-quality dataset for function calling, which we use\nto fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient\ninference, we introduce a novel tool retrieval method to reduce the input\nprompt length and utilize quantization to further accelerate the inference\nspeed. As a driving application, we demonstrate a local Siri-like system for\nApple's MacBook that can execute user commands through text or voice input. Our\nresults show that our models can achieve, and even surpass, the\nfunction-calling capabilities of larger models like GPT-4-Turbo, while being\nfully deployed at the edge. We open-source our dataset, models, and installable\npackage and provide a demo video for our MacBook assistant agent.\n","authors":["Lutfi Eren Erdogan","Nicholas Lee","Siddharth Jha","Sehoon Kim","Ryan Tabrizi","Suhong Moon","Coleman Hooper","Gopala Anumanchipalli","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2409.00608v2.pdf","comment":"EMNLP 2024 Demo"},{"id":"http://arxiv.org/abs/2410.15669v1","updated":"2024-10-21T06:22:51Z","published":"2024-10-21T06:22:51Z","title":"Learning to Generate and Evaluate Fact-checking Explanations with\n  Transformers","summary":"  In an era increasingly dominated by digital platforms, the spread of\nmisinformation poses a significant challenge, highlighting the need for\nsolutions capable of assessing information veracity. Our research contributes\nto the field of Explainable Artificial Antelligence (XAI) by developing\ntransformer-based fact-checking models that contextualise and justify their\ndecisions by generating human-accessible explanations. Importantly, we also\ndevelop models for automatic evaluation of explanations for fact-checking\nverdicts across different dimensions such as \\texttt{(self)-contradiction},\n\\texttt{hallucination}, \\texttt{convincingness} and \\texttt{overall quality}.\nBy introducing human-centred evaluation methods and developing specialised\ndatasets, we emphasise the need for aligning Artificial Intelligence\n(AI)-generated explanations with human judgements. This approach not only\nadvances theoretical knowledge in XAI but also holds practical implications by\nenhancing the transparency, reliability and users' trust in AI-driven\nfact-checking systems. Furthermore, the development of our metric learning\nmodels is a first step towards potentially increasing efficiency and reducing\nreliance on extensive manual assessment. Based on experimental results, our\nbest performing generative model \\textsc{ROUGE-1} score of 47.77, demonstrating\nsuperior performance in generating fact-checking explanations, particularly\nwhen provided with high-quality evidence. Additionally, the best performing\nmetric learning model showed a moderately strong correlation with human\njudgements on objective dimensions such as \\texttt{(self)-contradiction and\n\\texttt{hallucination}, achieving a Matthews Correlation Coefficient (MCC) of\naround 0.7.}\n","authors":["Darius Feher","Abdullah Khered","Hao Zhang","Riza Batista-Navarro","Viktor Schlegel"],"pdf_url":"https://arxiv.org/pdf/2410.15669v1.pdf","comment":"Forthcoming in Engineering Applications of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2407.11417v2","updated":"2024-10-21T06:17:56Z","published":"2024-07-16T06:18:21Z","title":"SPINACH: SPARQL-Based Information Navigation for Challenging Real-World\n  Questions","summary":"  Large Language Models (LLMs) have led to significant improvements in the\nKnowledge Base Question Answering (KBQA) task. However, datasets used in KBQA\nstudies do not capture the true complexity of KBQA tasks. They either have\nsimple questions, use synthetically generated logical forms, or are based on\nsmall knowledge base (KB) schemas.\n  We introduce the SPINACH dataset, an expert-annotated KBQA dataset collected\nfrom discussions on Wikidata's \"Request a Query\" forum with 320\ndecontextualized question-SPARQL pairs. The complexity of these in-the-wild\nqueries calls for a KBQA system that can dynamically explore large and often\nincomplete schemas and reason about them, as it is infeasible to create a\ncomprehensive training dataset.\n  We also introduce an in-context learning KBQA agent, also called SPINACH,\nthat mimics how a human expert would write SPARQLs to handle challenging\nquestions. SPINACH achieves a new state of the art on the QALD-7, QALD-9 Plus\nand QALD-10 datasets by 31.0%, 27.0%, and 10.0% in $F_1$, respectively, and\ncoming within 1.6% of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On\nour new SPINACH dataset, the SPINACH agent outperforms all baselines, including\nthe best GPT-4-based KBQA agent, by at least 38.1% in $F_1$.\n","authors":["Shicheng Liu","Sina J. Semnani","Harold Triedman","Jialiang Xu","Isaac Dan Zhao","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2407.11417v2.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.15667v1","updated":"2024-10-21T06:11:38Z","published":"2024-10-21T06:11:38Z","title":"RAC: Efficient LLM Factuality Correction with Retrieval Augmentation","summary":"  Large Language Models (LLMs) exhibit impressive results across a wide range\nof natural language processing (NLP) tasks, yet they can often produce\nfactually incorrect outputs. This paper introduces a simple but effective\nlow-latency post-correction method, \\textbf{Retrieval Augmented Correction\n(RAC)}, aimed at enhancing the factual performance of LLMs without requiring\nadditional fine-tuning. Our method is general and can be used with any\ninstruction-tuned LLM, and has greatly reduced latency compared to prior\napproaches. RAC decomposes the LLM's output into atomic facts and applies a\nfine-grained verification and correction process with retrieved content to\nverify and correct the LLM-generated output. Our extensive experiments show\nthat RAC yields up to 30\\% improvements over state-of-the-art baselines across\ntwo popular factuality evaluation datasets, validating its efficacy and\nrobustness in both with and without the integration of Retrieval-Augmented\nGeneration (RAG) across different LLMs.\\footnote{Our code is at\n\\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}\n","authors":["Changmao Li","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2410.15667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15661v1","updated":"2024-10-21T06:03:49Z","published":"2024-10-21T06:03:49Z","title":"Scalable Data Ablation Approximations for Language Models through\n  Modular Training and Merging","summary":"  Training data compositions for Large Language Models (LLMs) can significantly\naffect their downstream performance. However, a thorough data ablation study\nexploring large sets of candidate data mixtures is typically prohibitively\nexpensive since the full effect is seen only after training the models; this\ncan lead practitioners to settle for sub-optimal data mixtures. We propose an\nefficient method for approximating data ablations which trains individual\nmodels on subsets of a training corpus and reuses them across evaluations of\ncombinations of subsets. In continued pre-training experiments, we find that,\ngiven an arbitrary evaluation set, the perplexity score of a single model\ntrained on a candidate set of data is strongly correlated with perplexity\nscores of parameter averages of models trained on distinct partitions of that\ndata. From this finding, we posit that researchers and practitioners can\nconduct inexpensive simulations of data ablations by maintaining a pool of\nmodels that were each trained on partitions of a large training corpus, and\nassessing candidate data mixtures by evaluating parameter averages of\ncombinations of these models. This approach allows for substantial improvements\nin amortized training efficiency -- scaling only linearly with respect to new\ndata -- by enabling reuse of previous training computation, opening new avenues\nfor improving model performance through rigorous, incremental data assessment\nand mixing.\n","authors":["Clara Na","Ian Magnusson","Ananya Harsh Jha","Tom Sherborne","Emma Strubell","Jesse Dodge","Pradeep Dasigi"],"pdf_url":"https://arxiv.org/pdf/2410.15661v1.pdf","comment":"EMNLP 2024. 17 pages"},{"id":"http://arxiv.org/abs/2410.15657v1","updated":"2024-10-21T05:51:51Z","published":"2024-10-21T05:51:51Z","title":"CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision\n  Large Language Models","summary":"  Human-object interaction (HOI) detection has seen advancements with Vision\nLanguage Models (VLMs), but these methods often depend on extensive manual\nannotations. Vision Large Language Models (VLLMs) can inherently recognize and\nreason about interactions at the image level but are computationally heavy and\nnot designed for instance-level HOI detection. To overcome these limitations,\nwe propose a Cross-Level HOI distillation (CL-HOI) framework, which distills\ninstance-level HOIs from VLLMs image-level understanding without the need for\nmanual annotations. Our approach involves two stages: context distillation,\nwhere a Visual Linguistic Translator (VLT) converts visual information into\nlinguistic form, and interaction distillation, where an Interaction Cognition\nNetwork (ICN) reasons about spatial, visual, and context relations. We design\ncontrastive distillation losses to transfer image-level context and interaction\nknowledge from the teacher to the student model, enabling instance-level HOI\ndetection. Evaluations on HICO-DET and V-COCO datasets demonstrate that our\nCL-HOI surpasses existing weakly supervised methods and VLLM supervised\nmethods, showing its efficacy in detecting HOIs without manual labels.\n","authors":["Jianjun Gao","Chen Cai","Ruoyu Wang","Wenyang Liu","Kim-Hui Yap","Kratika Garg","Boon-Siew Han"],"pdf_url":"https://arxiv.org/pdf/2410.15657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15642v1","updated":"2024-10-21T05:08:18Z","published":"2024-10-21T05:08:18Z","title":"Resource-Efficient Medical Report Generation using Large Language Models","summary":"  Medical report generation is the task of automatically writing radiology\nreports for chest X-ray images. Manually composing these reports is a\ntime-consuming process that is also prone to human errors. Generating medical\nreports can therefore help reduce the burden on radiologists. In other words,\nwe can promote greater clinical automation in the medical domain. In this work,\nwe propose a new framework leveraging vision-enabled Large Language Models\n(LLM) for the task of medical report generation. We introduce a lightweight\nsolution that achieves better or comparative performance as compared to\nprevious solutions on the task of medical report generation. We conduct\nextensive experiments exploring different model sizes and enhancement\napproaches, such as prefix tuning to improve the text generation abilities of\nthe LLMs. We evaluate our approach on a prominent large-scale radiology report\ndataset - MIMIC-CXR. Our results demonstrate the capability of our\nresource-efficient framework to generate patient-specific reports with strong\nmedical contextual understanding and high precision.\n","authors":[" Abdullah","Ameer Hamza","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.15642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15641v1","updated":"2024-10-21T05:01:50Z","published":"2024-10-21T05:01:50Z","title":"SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical\n  Synthesis","summary":"  The increasing integration of large language models (LLMs) across various\nfields has heightened concerns about their potential to propagate dangerous\ninformation. This paper specifically explores the security vulnerabilities of\nLLMs within the field of chemistry, particularly their capacity to provide\ninstructions for synthesizing hazardous substances. We evaluate the\neffectiveness of several prompt injection attack methods, including\nred-teaming, explicit prompting, and implicit prompting. Additionally, we\nintroduce a novel attack technique named SMILES-prompting, which uses the\nSimplified Molecular-Input Line-Entry System (SMILES) to reference chemical\nsubstances. Our findings reveal that SMILES-prompting can effectively bypass\ncurrent safety mechanisms. These findings highlight the urgent need for\nenhanced domain-specific safeguards in LLMs to prevent misuse and improve their\npotential for positive social impact.\n","authors":["Aidan Wong","He Cao","Zijing Liu","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2410.15641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15639v1","updated":"2024-10-21T04:57:09Z","published":"2024-10-21T04:57:09Z","title":"Can Large Language Models Invent Algorithms to Improve Themselves?","summary":"  Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and evaluates model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. In mathematical reasoning tasks, Self-Developing not only creates\nmodels that surpass the seed model but also consistently outperforms models\ncreated using human-designed algorithms. Additionally, these LLM-discovered\nalgorithms demonstrate strong effectiveness, including transferability to\nout-of-domain models.\n","authors":["Yoichi Ishibashi","Taro Yano","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2410.15639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02355v2","updated":"2024-10-21T04:32:56Z","published":"2024-10-03T10:06:27Z","title":"AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models","summary":"  Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.\n","authors":["Junfeng Fang","Houcheng Jiang","Kun Wang","Yunshan Ma","Xiang Wang","Xiangnan He","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.02355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15633v1","updated":"2024-10-21T04:30:53Z","published":"2024-10-21T04:30:53Z","title":"Selecting Influential Samples for Long Context Alignment via Homologous\n  Models' Guidance and Contextual Awareness Measurement","summary":"  The expansion of large language models to effectively handle instructions\nwith extremely long contexts has yet to be fully investigated. The primary\nobstacle lies in constructing a high-quality long instruction-following dataset\ndevised for long context alignment. Existing studies have attempted to scale up\nthe available data volume by synthesizing long instruction-following samples.\nHowever, indiscriminately increasing the quantity of data without a\nwell-defined strategy for ensuring data quality may introduce low-quality\nsamples and restrict the final performance. To bridge this gap, we aim to\naddress the unique challenge of long-context alignment, i.e., modeling the\nlong-range dependencies for handling instructions and lengthy input contexts.\nWe propose GATEAU, a novel framework designed to identify the influential and\nhigh-quality samples enriched with long-range dependency relations by utilizing\ncrafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement\n(CAM). Specifically, HMG attempts to measure the difficulty of generating\ncorresponding responses due to the long-range dependencies, using the\nperplexity scores of the response from two homologous models with different\ncontext windows. Also, the role of CAM is to measure the difficulty of\nunderstanding the long input contexts due to long-range dependencies by\nevaluating whether the model's attention is focused on important segments.\nBuilt upon both proposed methods, we select the most challenging samples as the\ninfluential data to effectively frame the long-range dependencies, thereby\nachieving better performance of LLMs. Comprehensive experiments indicate that\nGATEAU effectively identifies samples enriched with long-range dependency\nrelations and the model trained on these selected samples exhibits better\ninstruction-following and long-context understanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09603v4","updated":"2024-10-21T04:16:09Z","published":"2023-11-16T06:22:17Z","title":"Self-Contradictory Reasoning Evaluation and Detection","summary":"  In a plethora of recent work, large language models (LLMs) demonstrated\nimpressive reasoning ability, but many proposed downstream reasoning tasks only\nfocus on final answers. Two fundamental questions persist: 1) how consistent is\nthe reasoning, and 2) can models detect unreliable reasoning? In this paper, we\ninvestigate self-contradictory (Self-Contra) reasoning, where the model\nreasoning does not support its answers. To answer 1), we define and assess the\nSelf-Contra rate across three datasets and delve into finer-grained categories\nof Self-Contra reasoning. We find that LLMs often contradict themselves in\nreasoning tasks involving contextual information understanding or commonsense.\nThe model may generate correct answers by taking shortcuts in reasoning or\noverlooking contextual evidence, leading to compromised reasoning. For 2), we\ntask the state-of-the-art model GPT-4 with identifying Self-Contra reasoning\nand finer-grained fallacies. We find that finer-grained categories enhanced\ndetection can improve GPT-4's ability to detect Self-Contra. However, it is\nonly able to detect Self-Contra with a 52.2% F1 score, much lower compared to\n66.7% for humans. Our results indicate that current LLMs lack the robustness\nnecessary for reliable reasoning and we emphasize the urgent need for\nestablishing best practices in comprehensive reasoning evaluations beyond pure\nperformance-based metrics.\n","authors":["Ziyi Liu","Soumya Sanyal","Isabelle Lee","Yongkang Du","Rahul Gupta","Yang Liu","Jieyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.09603v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03744v2","updated":"2024-10-21T04:10:50Z","published":"2024-02-06T06:23:12Z","title":"INSIDE: LLMs' Internal States Retain the Power of Hallucination\n  Detection","summary":"  Knowledge hallucination have raised widespread concerns for the security and\nreliability of deployed LLMs. Previous efforts in detecting hallucinations have\nbeen employed at logit-level uncertainty estimation or language-level\nself-consistency evaluation, where the semantic information is inevitably lost\nduring the token-decoding procedure. Thus, we propose to explore the dense\nsemantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates\nfor halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular,\na simple yet effective \\textbf{EigenScore} metric is proposed to better\nevaluate responses' self-consistency, which exploits the eigenvalues of\nresponses' covariance matrix to measure the semantic consistency/diversity in\nthe dense embedding space. Furthermore, from the perspective of self-consistent\nhallucination detection, a test time feature clipping approach is explored to\ntruncate extreme activations in the internal states, which reduces\noverconfident generations and potentially benefits the detection of\noverconfident hallucinations. Extensive experiments and ablation studies are\nperformed on several popular LLMs and question-answering (QA) benchmarks,\nshowing the effectiveness of our proposal.\n","authors":["Chao Chen","Kai Liu","Ze Chen","Yi Gu","Yue Wu","Mingyuan Tao","Zhihang Fu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2402.03744v2.pdf","comment":"Accepted by ICLR-2024"},{"id":"http://arxiv.org/abs/2406.10786v2","updated":"2024-10-21T04:09:33Z","published":"2024-06-16T02:52:32Z","title":"Exploring the Zero-Shot Capabilities of LLMs Handling Multiple Problems\n  at once","summary":"  Recent studies have proposed placing multiple problems in a single prompt to\nimprove input token utilization for a more efficient LLM inference. We call\nthis MPP, in contrast to conventional SPP that prompts an LLM with a single\nproblem at a time. While MPP has been shown to work comparably well or even\nbetter than SPP under few-shot settings, its zero-shot performance is\nunderexplored, which better reveals the innate multiple problem handling\ncapabilities of LLMs. To address that, we study the zero-shot MPP performance\nof various LLMs on 6 classification and 12 reasoning benchmarks and confirm\nthat LLMs are competent zero-shot multi-problem solvers. We also examine the\nconditions of effectiveness of zero-shot MPP and explore several model-level\nfactors that may enable MPP. We observe that LLMs consistently perform worse\nwith selecting indices of texts of a given class label and with multiple\nmixed-source reasoning problems, indicating a lack of true understanding. We\nalso find that instruction tuning is an important factor than enhances MPP.\n","authors":["Zhengxiang Wang","Jordan Kodner","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2406.10786v2.pdf","comment":"26 pages, 11 figures, 16 tables"},{"id":"http://arxiv.org/abs/2410.15625v1","updated":"2024-10-21T04:08:37Z","published":"2024-10-21T04:08:37Z","title":"Improving Parallel Program Performance Through DSL-Driven Code\n  Generation with LLM Optimizers","summary":"  Mapping computations to processors and assigning data to memory are critical\nfor maximizing performance in parallel programming. These mapping decisions are\nmanaged through the development of specialized low-level system code, called\nmappers, crafted by performance engineers. Each mapper is tailored to a\nspecific application and optimized for the underlying machine architecture, a\nprocess that requires days of refinement and tuning from an expert. Despite\nadvances in system research, automating mapper generation remains a challenge\ndue to the complexity of making millions of decisions to find the optimal\nsolution and generate the solution as code. We introduce an approach that\nleverages recent advances in LLM-based optimizers for mapper design. In under\nten minutes, our method automatically discovers mappers that surpass human\nexpert designs in scientific applications by up to 1.34X speedup. For parallel\nmatrix multiplication algorithms, our mapper achieves up to 1.31X of the\nexpert-designed solution. To achieve this, we simplify the complexity of\nlow-level code generation by introducing a domain-specific language (DSL) that\nabstracts the low-level system programming details and defines a structured\nsearch space for LLMs to explore. To maximize the application performance, we\nuse an LLM optimizer to improve an agentic system that generates the mapper\ncode. As a result, this approach significantly reduces the workload for\nperformance engineers while achieving substantial performance gains across\ndiverse applications. Finally, our results demonstrate the effectiveness of\nLLM-based optimization in system design and suggest its potential for\naddressing other complex system challenges.\n","authors":["Anjiang Wei","Allen Nie","Thiago S. F. X. Teixeira","Rohan Yadav","Wonchan Lee","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2410.15625v1.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.15623v1","updated":"2024-10-21T04:08:16Z","published":"2024-10-21T04:08:16Z","title":"Guardians of Discourse: Evaluating LLMs on Multilingual Offensive\n  Language Detection","summary":"  Identifying offensive language is essential for maintaining safety and\nsustainability in the social media era. Though large language models (LLMs)\nhave demonstrated encouraging potential in social media analytics, they lack\nthorough evaluation when in offensive language detection, particularly in\nmultilingual environments. We for the first time evaluate multilingual\noffensive language detection of LLMs in three languages: English, Spanish, and\nGerman with three LLMs, GPT-3.5, Flan-T5, and Mistral, in both monolingual and\nmultilingual settings. We further examine the impact of different prompt\nlanguages and augmented translation data for the task in non-English contexts.\nFurthermore, we discuss the impact of the inherent bias in LLMs and the\ndatasets in the mispredictions related to sensitive topics.\n","authors":["Jianfei He","Lilin Wang","Jiaying Wang","Zhenyu Liu","Hongbin Na","Zimu Wang","Wei Wang","Qi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15623v1.pdf","comment":"Accepted at UIC 2024 proceedings. Accepted version"},{"id":"http://arxiv.org/abs/2409.08098v2","updated":"2024-10-21T04:02:23Z","published":"2024-09-12T14:51:43Z","title":"The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal","summary":"  This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.\n","authors":["Huiyuan Xie","Felix Steffek","Joana Ribeiro de Faria","Christine Carter","Jonathan Rutherford"],"pdf_url":"https://arxiv.org/pdf/2409.08098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10678v3","updated":"2024-10-21T03:55:36Z","published":"2022-12-20T22:41:24Z","title":"Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias","summary":"  Generated texts from large language models (LLMs) have been shown to exhibit\na variety of harmful, human-like biases against various demographics. These\nfindings motivate research efforts aiming to understand and measure such\neffects. This paper introduces a causal formulation for bias measurement in\ngenerative language models. Based on this theoretical foundation, we outline a\nlist of desiderata for designing robust bias benchmarks. We then propose a\nbenchmark called OccuGender, with a bias-measuring procedure to investigate\noccupational gender bias. We test several state-of-the-art open-source LLMs on\nOccuGender, including Llama, Mistral, and their instruction-tuned versions. The\nresults show that these models exhibit substantial occupational gender bias.\nLastly, we discuss prompting strategies for bias mitigation and an extension of\nour causal formulation to illustrate the generalizability of our framework. Our\ncode and data https://github.com/chenyuen0103/gender-bias.\n","authors":["Yuen Chen","Vethavikashini Chithrra Raghuram","Justus Mattern","Rada Mihalcea","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2212.10678v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15620v1","updated":"2024-10-21T03:48:23Z","published":"2024-10-21T03:48:23Z","title":"Acoustic Model Optimization over Multiple Data Sources: Merging and\n  Valuation","summary":"  Due to the rising awareness of privacy protection and the voluminous scale of\nspeech data, it is becoming infeasible for Automatic Speech Recognition (ASR)\nsystem developers to train the acoustic model with complete data as before. For\nexample, the data may be owned by different curators, and it is not allowed to\nshare with others. In this paper, we propose a novel paradigm to solve salient\nproblems plaguing the ASR field. In the first stage, multiple acoustic models\nare trained based upon different subsets of the complete speech data, while in\nthe second phase, two novel algorithms are utilized to generate a high-quality\nacoustic model based upon those trained on data subsets. We first propose the\nGenetic Merge Algorithm (GMA), which is a highly specialized algorithm for\noptimizing acoustic models but suffers from low efficiency. We further propose\nthe SGD-Based Optimizational Merge Algorithm (SOMA), which effectively\nalleviates the efficiency bottleneck of GMA and maintains superior model\naccuracy. Extensive experiments on public data show that the proposed methods\ncan significantly outperform the state-of-the-art. Furthermore, we introduce\nShapley Value to estimate the contribution score of the trained models, which\nis useful for evaluating the effectiveness of the data and providing fair\nincentives to their curators.\n","authors":["Victor Junqiu Wei","Weicheng Wang","Di Jiang","Conghui Tan","Rongzhong Lian"],"pdf_url":"https://arxiv.org/pdf/2410.15620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13185v2","updated":"2024-10-21T03:36:05Z","published":"2024-10-17T03:26:37Z","title":"Chain of Ideas: Revolutionizing Research in Novel Idea Development with\n  LLM Agents","summary":"  Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.\n","authors":["Long Li","Weiwen Xu","Jiayan Guo","Ruochen Zhao","Xinxuan Li","Yuqian Yuan","Boqiang Zhang","Yuming Jiang","Yifei Xin","Ronghao Dang","Deli Zhao","Yu Rong","Tian Feng","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.13185v2.pdf","comment":"10 pages,5 figures, conference"},{"id":"http://arxiv.org/abs/2410.15609v1","updated":"2024-10-21T03:13:22Z","published":"2024-10-21T03:13:22Z","title":"Interventional Speech Noise Injection for ASR Generalizable Spoken\n  Language Understanding","summary":"  Recently, pre-trained language models (PLMs) have been increasingly adopted\nin spoken language understanding (SLU). However, automatic speech recognition\n(ASR) systems frequently produce inaccurate transcriptions, leading to noisy\ninputs for SLU models, which can significantly degrade their performance. To\naddress this, our objective is to train SLU models to withstand ASR errors by\nexposing them to noises commonly observed in ASR systems, referred to as\nASR-plausible noises. Speech noise injection (SNI) methods have pursued this\nobjective by introducing ASR-plausible noises, but we argue that these methods\nare inherently biased towards specific ASR systems, or ASR-specific noises. In\nthis work, we propose a novel and less biased augmentation method of\nintroducing the noises that are plausible to any ASR system, by cutting off the\nnon-causal effect of noises. Experimental results and analyses demonstrate the\neffectiveness of our proposed methods in enhancing the robustness and\ngeneralizability of SLU models against unseen ASR systems by introducing more\ndiverse and plausible ASR noises in advance.\n","authors":["Yeonjoon Jung","Jaeseong Lee","Seungtaek Choi","Dohyeon Lee","Minsoo Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.15609v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.15608v1","updated":"2024-10-21T03:13:20Z","published":"2024-10-21T03:13:20Z","title":"Moonshine: Speech Recognition for Live Transcription and Voice Commands","summary":"  This paper introduces Moonshine, a family of speech recognition models\noptimized for live transcription and voice command processing. Moonshine is\nbased on an encoder-decoder transformer architecture and employs Rotary\nPosition Embedding (RoPE) instead of traditional absolute position embeddings.\nThe model is trained on speech segments of various lengths, but without using\nzero-padding, leading to greater efficiency for the encoder during inference\ntime. When benchmarked against OpenAI's Whisper tiny.en, Moonshine Tiny\ndemonstrates a 5x reduction in compute requirements for transcribing a\n10-second speech segment while incurring no increase in word error rates across\nstandard evaluation datasets. These results highlight Moonshine's potential for\nreal-time and resource-constrained applications.\n","authors":["Nat Jeffries","Evan King","Manjunath Kudlur","Guy Nicholson","James Wang","Pete Warden"],"pdf_url":"https://arxiv.org/pdf/2410.15608v1.pdf","comment":"7 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.03421v2","updated":"2024-10-21T02:43:50Z","published":"2024-10-04T13:31:09Z","title":"One2set + Large Language Model: Best Partners for Keyphrase Generation","summary":"  Keyphrase generation (KPG) aims to automatically generate a collection of\nphrases representing the core concepts of a given document. The dominant\nparadigms in KPG include one2seq and one2set. Recently, there has been\nincreasing interest in applying large language models (LLMs) to KPG. Our\npreliminary experiments reveal that it is challenging for a single model to\nexcel in both recall and precision. Further analysis shows that: 1) the one2set\nparadigm owns the advantage of high recall, but suffers from improper\nassignments of supervision signals during training; 2) LLMs are powerful in\nkeyphrase selection, but existing selection methods often make redundant\nselections. Given these observations, we introduce a generate-then-select\nframework decomposing KPG into two steps, where we adopt a one2set-based model\nas generator to produce candidates and then use an LLM as selector to select\nkeyphrases from these candidates. Particularly, we make two important\nimprovements on our generator and selector: 1) we design an Optimal\nTransport-based assignment strategy to address the above improper assignments;\n2) we model the keyphrase selection as a sequence labeling task to alleviate\nredundant selections. Experimental results on multiple benchmark datasets show\nthat our framework significantly surpasses state-of-the-art models, especially\nin absent keyphrase prediction.\n","authors":["Liangying Shao","Liang Zhang","Minlong Peng","Guoqi Ma","Hao Yue","Mingming Sun","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2410.03421v2.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.05021v3","updated":"2024-10-21T02:41:21Z","published":"2024-10-07T13:24:24Z","title":"DEPT: Decoupled Embeddings for Pre-training Language Models","summary":"  Language model pre-training benefits from diverse data to enhance performance\nacross domains and languages. However, training on such heterogeneous corpora\nrequires extensive and costly efforts. Since these data sources vary lexically,\nsyntactically, and semantically, they cause negative interference or the\n``curse of multilinguality''. We propose a novel pre-training framework to\nalleviate this curse. Our method, DEPT, decouples embeddings from the\ntransformer body while simultaneously training the latter in multiple contexts.\nDEPT enables training without a shared global vocabulary and: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces\ntoken embedding parameters by up to 80% and the communication costs by 675x for\nbillion-scale models, (3) enhances model generalization and plasticity in\nadapting to new languages and domains, and (4) permits training with custom\noptimized vocabularies per data source. We demonstrate DEPT's potential via the\nfirst vocabulary-agnostic federated multilingual pre-training of a 1.3\nbillion-parameter model, limiting its embedding size to 102.4 million instead\nof 512 million.\n","authors":["Alex Iacob","Lorenzo Sani","Meghdad Kurmanji","William F. Shen","Xinchi Qiu","Dongqi Cai","Yan Gao","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2410.05021v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15595v1","updated":"2024-10-21T02:27:24Z","published":"2024-10-21T02:27:24Z","title":"A Comprehensive Survey of Datasets, Theories, Variants, and Applications\n  in Direct Preference Optimization","summary":"  With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community.\n","authors":["Wenyi Xiao","Zechuan Wang","Leilei Gan","Shuai Zhao","Wanggui He","Luu Anh Tuan","Long Chen","Hao Jiang","Zhou Zhao","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15592v1","updated":"2024-10-21T02:21:56Z","published":"2024-10-21T02:21:56Z","title":"CPE-Pro: A Structure-Sensitive Deep Learning Model for Protein\n  Representation and Origin Evaluation","summary":"  Protein structures are important for understanding their functions and\ninteractions. Currently, many protein structure prediction methods are\nenriching the structure database. Discriminating the origin of structures is\ncrucial for distinguishing between experimentally resolved and computationally\npredicted structures, evaluating the reliability of prediction methods, and\nguiding downstream biological studies. Building on works in structure\nprediction, We developed a structure-sensitive supervised deep learning model,\nCrystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent\nand discriminate the origin of protein structures. CPE-Pro learns the\nstructural information of proteins and captures inter-structural differences to\nachieve accurate traceability on four data classes, and is expected to be\nextended to more. Simultaneously, we utilized Foldseek to encode protein\nstructures into \"structure-sequence\" and trained a protein Structural Sequence\nLanguage Model, SSLM. Preliminary experiments demonstrated that, compared to\nlarge-scale protein language models pre-trained on vast amounts of amino acid\nsequences, the \"structure-sequences\" enable the language model to learn more\ninformative protein features, enhancing and optimizing structural\nrepresentations. We have provided the code, model weights, and all related\nmaterials on https://github.com/GouWenrui/CPE-Pro-main.git.\n","authors":["Wenrui Gou","Wenhui Ge"," YangTan","Guisheng Fan","Mingchen Li","Huiqun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.15592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15591v1","updated":"2024-10-21T02:19:24Z","published":"2024-10-21T02:19:24Z","title":"AMPLE: Emotion-Aware Multimodal Fusion Prompt Learning for Fake News\n  Detection","summary":"  Detecting fake news in large datasets is challenging due to its diversity and\ncomplexity, with traditional approaches often focusing on textual features\nwhile underutilizing semantic and emotional elements. Current methods also rely\nheavily on large annotated datasets, limiting their effectiveness in more\nnuanced analysis. To address these challenges, this paper introduces\nEmotion-\\textbf{A}ware \\textbf{M}ultimodal Fusion \\textbf{P}rompt\n\\textbf{L}\\textbf{E}arning (\\textbf{AMPLE}) framework to address the above\nissue by combining text sentiment analysis with multimodal data and hybrid\nprompt templates. This framework extracts emotional elements from texts by\nleveraging sentiment analysis tools. It then employs Multi-Head Cross-Attention\n(MCA) mechanisms and similarity-aware fusion methods to integrate multimodal\ndata. The proposed AMPLE framework demonstrates strong performance on two\npublic datasets in both few-shot and data-rich settings, with results\nindicating the potential of emotional aspects in fake news detection.\nFurthermore, the study explores the impact of integrating large language models\nwith this method for text sentiment extraction, revealing substantial room for\nfurther improvement. The code can be found at\n:\\url{https://github.com/xxm1215/MMM2025_few-shot/\n","authors":["Xiaoman Xu","Xiangrun Li","Taihang Wang","Ye Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.15591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15580v1","updated":"2024-10-21T01:57:16Z","published":"2024-10-21T01:57:16Z","title":"Language Models are Symbolic Learners in Arithmetic","summary":"  Large Language Models (LLMs) are thought to struggle with arithmetic learning\ndue to the inherent differences between language modeling and numerical\ncomputation, but concrete evidence has been lacking. This work responds to this\nclaim through a two-side experiment. We first investigate whether LLMs leverage\npartial products during arithmetic learning. We find that although LLMs can\nidentify some partial products after learning, they fail to leverage them for\narithmetic tasks, conversely. We then explore how LLMs approach arithmetic\nsymbolically by breaking tasks into subgroups, hypothesizing that difficulties\narise from subgroup complexity and selection. Our results show that when\nsubgroup complexity is fixed, LLMs treat a collection of different arithmetic\noperations similarly. By analyzing position-level accuracy across different\ntraining sizes, we further observe that it follows a U-shaped pattern: LLMs\nquickly learn the easiest patterns at the first and last positions, while\nprogressively learning the more difficult patterns in the middle positions.\nThis suggests that LLMs select subgroup following an easy-to-hard paradigm\nduring learning. Our work confirms that LLMs are pure symbolic learners in\narithmetic tasks and underscores the importance of understanding them deeply\nthrough subgroup-level quantification.\n","authors":["Chunyuan Deng","Zhiqi Li","Roy Xie","Ruidi Chang","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15578v1","updated":"2024-10-21T01:55:52Z","published":"2024-10-21T01:55:52Z","title":"Generalized Probabilistic Attention Mechanism in Transformers","summary":"  The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.\n","authors":["DongNyeong Heo","Heeyoul Choi"],"pdf_url":"https://arxiv.org/pdf/2410.15578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15576v1","updated":"2024-10-21T01:54:46Z","published":"2024-10-21T01:54:46Z","title":"A Survey of Conversational Search","summary":"  As a cornerstone of modern information access, search engines have become\nindispensable in everyday life. With the rapid advancements in AI and natural\nlanguage processing (NLP) technologies, particularly large language models\n(LLMs), search engines have evolved to support more intuitive and intelligent\ninteractions between users and systems. Conversational search, an emerging\nparadigm for next-generation search engines, leverages natural language\ndialogue to facilitate complex and precise information retrieval, thus\nattracting significant attention. Unlike traditional keyword-based search\nengines, conversational search systems enhance user experience by supporting\nintricate queries, maintaining context over multi-turn interactions, and\nproviding robust information integration and processing capabilities. Key\ncomponents such as query reformulation, search clarification, conversational\nretrieval, and response generation work in unison to enable these sophisticated\ninteractions. In this survey, we explore the recent advancements and potential\nfuture directions in conversational search, examining the critical modules that\nconstitute a conversational search system. We highlight the integration of LLMs\nin enhancing these systems and discuss the challenges and opportunities that\nlie ahead in this dynamic field. Additionally, we provide insights into\nreal-world applications and robust evaluations of current conversational search\nsystems, aiming to guide future research and development in conversational\nsearch.\n","authors":["Fengran Mo","Kelong Mao","Ziliang Zhao","Hongjin Qian","Haonan Chen","Yiruo Cheng","Xiaoxi Li","Yutao Zhu","Zhicheng Dou","Jian-Yun Nie"],"pdf_url":"https://arxiv.org/pdf/2410.15576v1.pdf","comment":"35 pages, 8 figures, continue to update"},{"id":"http://arxiv.org/abs/2410.15575v1","updated":"2024-10-21T01:50:59Z","published":"2024-10-21T01:50:59Z","title":"Neural Search Space in Gboard Decoder","summary":"  Gboard Decoder produces suggestions by looking for paths that best match\ninput touch points on the context aware search space, which is backed by the\nlanguage Finite State Transducers (FST). The language FST is currently an\nN-gram language model (LM). However, N-gram LMs, limited in context length, are\nknown to have sparsity problem under device model size constraint. In this\npaper, we propose \\textbf{Neural Search Space} which substitutes the N-gram LM\nwith a Neural Network LM (NN-LM) and dynamically constructs the search space\nduring decoding. Specifically, we integrate the long range context awareness of\nNN-LM into the search space by converting its outputs given context, into the\nlanguage FST at runtime. This involves language FST structure redesign, pruning\nstrategy tuning, and data structure optimizations. Online experiments\ndemonstrate improved quality results, reducing Words Modified Ratio by [0.26\\%,\n1.19\\%] on various locales with acceptable latency increases. This work opens\nnew avenues for further improving keyboard decoding quality by enhancing neural\nLM more directly.\n","authors":["Yanxiang Zhang","Yuanbo Zhang","Haicheng Sun","Yun Wang","Billy Dou","Gary Sivek","Shumin Zhai"],"pdf_url":"https://arxiv.org/pdf/2410.15575v1.pdf","comment":"10 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.15573v1","updated":"2024-10-21T01:36:42Z","published":"2024-10-21T01:36:42Z","title":"OpenMU: Your Swiss Army Knife for Music Understanding","summary":"  We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.\n","authors":["Mengjie Zhao","Zhi Zhong","Zhuoyuan Mao","Shiqi Yang","Wei-Hsiang Liao","Shusuke Takahashi","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.15573v1.pdf","comment":"Resources: https://github.com/mzhaojp22/openmu"},{"id":"http://arxiv.org/abs/2410.15572v1","updated":"2024-10-21T01:36:08Z","published":"2024-10-21T01:36:08Z","title":"Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka\n  Chatbots: Design Insights and User Perceptions","summary":"  In an era where cultural preservation is increasingly intertwined with\ntechnological innovation, this study introduces a groundbreaking approach to\npromoting and safeguarding the rich heritage of Taiwanese Hakka culture through\nthe development of a Retrieval-Augmented Generation (RAG)-enhanced chatbot.\nTraditional large language models (LLMs), while powerful, often fall short in\ndelivering accurate and contextually rich responses, particularly in culturally\nspecific domains. By integrating external databases with generative AI models,\nRAG technology bridges this gap, empowering chatbots to not only provide\nprecise answers but also resonate deeply with the cultural nuances that are\ncrucial for authentic interactions. This study delves into the intricate\nprocess of augmenting the chatbot's knowledge base with targeted cultural data,\nspecifically curated to reflect the unique aspects of Hakka traditions,\nlanguage, and practices. Through dynamic information retrieval, the\nRAG-enhanced chatbot becomes a versatile tool capable of handling complex\ninquiries that demand an in-depth understanding of Hakka cultural context. This\nis particularly significant in an age where digital platforms often dilute\ncultural identities, making the role of culturally aware AI systems more\ncritical than ever. System usability studies conducted as part of our research\nreveal a marked improvement in both user satisfaction and engagement,\nhighlighting the chatbot's effectiveness in fostering a deeper connection with\nHakka culture. The feedback underscores the potential of RAG technology to not\nonly enhance user experience but also to serve as a vital instrument in the\nbroader mission of ethnic mainstreaming and cultural celebration.\n","authors":["Chen-Chi Chang","Han-Pi Chang","Hung-Shin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.15572v1.pdf","comment":"Accepted to IEEE RASSE 2024"},{"id":"http://arxiv.org/abs/2410.15570v1","updated":"2024-10-21T01:27:29Z","published":"2024-10-21T01:27:29Z","title":"Stacking Small Language Models for Generalizability","summary":"  Recent advances show that large language models (LLMs) generalize strong\nperformance across different natural language benchmarks. However, the large\nsize of LLMs makes training and inference expensive and impractical to run in\nresource-limited settings. This paper introduces a new approach called\nfine-tuning stacks of language models (FSLM), which involves stacking small\nlanguage models (SLM) as an alternative to LLMs. By fine-tuning each SLM to\nperform a specific task, this approach breaks down high level reasoning into\nmultiple lower-level steps that specific SLMs are responsible for. As a result,\nFSLM allows for lower training and inference costs, and also improves model\ninterpretability as each SLM communicates with the subsequent one through\nnatural language. By evaluating FSLM on common natural language benchmarks,\nthis paper highlights promising early results toward generalizable performance\nusing FSLM as a cost-effective alternative to LLMs.\n","authors":["Laurence Liang"],"pdf_url":"https://arxiv.org/pdf/2410.15570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15567v1","updated":"2024-10-21T01:23:34Z","published":"2024-10-21T01:23:34Z","title":"Pruning Foundation Models for High Accuracy without Retraining","summary":"  Despite the superior performance, it is challenging to deploy foundation\nmodels or large language models (LLMs) due to their massive parameters and\ncomputations. While pruning is a promising technique to reduce model size and\naccelerate the inference, the traditional pruning techniques can hardly be\napplied for LLMs as they need to finetune the model on the full dataset with\nmultiple epochs consuming massive data and hardware resources. To deal with\nthis problem, post-training pruning methods are proposed to prune LLMs in\none-shot without retraining. However, their accuracy after pruning may suffer\nfrom certain performance degradation due to the lack of retraining with massive\ndata. To address this issue, in this paper, we first formulate the\npost-training problem for layer-wise LLM compression to simultaneously prune\nmultiple weights in LLMs. Next, we provide an optimal solution for this problem\nand design our post-training pruning algorithm for both unstructured and\nsemi-structured sparsity. Our extensive experiments demonstrate the superior\nperformance of the proposed methods in comparison to SOTA baselines across\nvarious LLM families including transformer-based LLMs and Mamba-based LLMs.\nCode link: https://github.com/piuzha/APT\n","authors":["Pu Zhao","Fei Sun","Xuan Shen","Pinrui Yu","Zhenglun Kong","Yanzhi Wang","Xue Lin"],"pdf_url":"https://arxiv.org/pdf/2410.15567v1.pdf","comment":"Accepted by EMNLP 2024 findings"},{"id":"http://arxiv.org/abs/2410.15553v1","updated":"2024-10-21T00:59:47Z","published":"2024-10-21T00:59:47Z","title":"Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions\n  Following","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\nvarious tasks, including instruction following, which is crucial for aligning\nmodel outputs with user expectations. However, evaluating LLMs' ability to\nfollow instructions remains challenging due to the complexity and subjectivity\nof human language. Current benchmarks primarily focus on single-turn,\nmonolingual instructions, which do not adequately reflect the complexities of\nreal-world applications that require handling multi-turn and multilingual\ninteractions. To address this gap, we introduce Multi-IF, a new benchmark\ndesigned to assess LLMs' proficiency in following multi-turn and multilingual\ninstructions. Multi-IF, which utilizes a hybrid framework combining LLM and\nhuman annotators, expands upon the IFEval by incorporating multi-turn sequences\nand translating the English prompts into another 7 languages, resulting in a\ndataset of 4,501 multilingual conversations, where each has three turns. Our\nevaluation of 14 state-of-the-art LLMs on Multi-IF reveals that it presents a\nsignificantly more challenging task than existing benchmarks. All the models\ntested showed a higher rate of failure in executing instructions correctly with\neach additional turn. For example, o1-preview drops from 0.877 at the first\nturn to 0.707 at the third turn in terms of average accuracy over all\nlanguages. Moreover, languages with non-Latin scripts (Hindi, Russian, and\nChinese) generally exhibit higher error rates, suggesting potential limitations\nin the models' multilingual capabilities. We release Multi-IF prompts and the\nevaluation code base to encourage further research in this critical area.\n","authors":["Yun He","Di Jin","Chaoqi Wang","Chloe Bi","Karishma Mandyam","Hejia Zhang","Chen Zhu","Ning Li","Tengyu Xu","Hongjiang Lv","Shruti Bhosale","Chenguang Zhu","Karthik Abinav Sankararaman","Eryk Helenowski","Melanie Kambadur","Aditya Tayade","Hao Ma","Han Fang","Sinong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15551v1","updated":"2024-10-21T00:54:31Z","published":"2024-10-21T00:54:31Z","title":"WHoW: A Cross-domain Approach for Analysing Conversation Moderation","summary":"  We propose WHoW, an evaluation framework for analyzing the facilitation\nstrategies of moderators across different domains/scenarios by examining their\nmotives (Why), dialogue acts (How) and target speaker (Who). Using this\nframework, we annotated 5,657 moderation sentences with human judges and 15,494\nsentences with GPT-4o from two domains: TV debates and radio panel discussions.\nComparative analysis demonstrates the framework's cross-domain generalisability\nand reveals distinct moderation strategies: debate moderators emphasise\ncoordination and facilitate interaction through questions and instructions,\nwhile panel discussion moderators prioritize information provision and actively\nparticipate in discussions. Our analytical framework works for different\nmoderation scenarios, enhances our understanding of moderation behaviour\nthrough automatic large-scale analysis, and facilitates the development of\nmoderator agents.\n","authors":["Ming-Bin Chen","Lea Frermann","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2410.15551v1.pdf","comment":"36 pages(including appendix, 10 pages main text), 8 figures, 16\n  tables"},{"id":"http://arxiv.org/abs/2404.11049v3","updated":"2024-10-21T00:42:51Z","published":"2024-04-17T03:44:58Z","title":"Stepwise Alignment for Constrained Language Model Policy Optimization","summary":"  Safety and trustworthiness are indispensable requirements for real-world\napplications of AI systems using large language models (LLMs). This paper\nformulates human value alignment as an optimization problem of the language\nmodel policy to maximize reward under a safety constraint, and then proposes an\nalgorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One\nkey idea behind SACPO, supported by theory, is that the optimal policy\nincorporating reward and safety can be directly obtained from a reward-aligned\npolicy. Building on this key idea, SACPO aligns LLMs step-wise with each metric\nwhile leveraging simple yet powerful alignment algorithms such as direct\npreference optimization (DPO). SACPO offers several advantages, including\nsimplicity, stability, computational efficiency, and flexibility of algorithms\nand datasets. Under mild assumptions, our theoretical analysis provides the\nupper bounds on optimality and safety constraint violation. Our experimental\nresults show that SACPO can fine-tune Alpaca-7B better than the\nstate-of-the-art method in terms of both helpfulness and harmlessness.\n","authors":["Akifumi Wachi","Thien Q. Tran","Rei Sato","Takumi Tanabe","Youhei Akimoto"],"pdf_url":"https://arxiv.org/pdf/2404.11049v3.pdf","comment":"Accepted at NeurIPS 2024. Code and models are available at\n  https://github.com/line/sacpo"},{"id":"http://arxiv.org/abs/2311.07879v3","updated":"2024-10-21T16:48:18Z","published":"2023-11-14T03:18:28Z","title":"Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting\n  Volunteer Content Moderators","summary":"  Extensive efforts in automated approaches for content moderation have been\nfocused on developing models to identify toxic, offensive, and hateful content\nwith the aim of lightening the load for moderators. Yet, it remains uncertain\nwhether improvements on those tasks have truly addressed moderators' needs in\naccomplishing their work. In this paper, we surface gaps between past research\nefforts that have aimed to provide automation for aspects of content moderation\nand the needs of volunteer content moderators, regarding identifying violations\nof various moderation rules. To do so, we conduct a model review on Hugging\nFace to reveal the availability of models to cover various moderation rules and\nguidelines from three exemplar forums. We further put state-of-the-art LLMs to\nthe test, evaluating how well these models perform in flagging violations of\nplatform rules from one particular forum. Finally, we conduct a user survey\nstudy with volunteer moderators to gain insight into their perspectives on\nuseful moderation models. Overall, we observe a non-trivial gap, as missing\ndeveloped models and LLMs exhibit moderate to low performance on a significant\nportion of the rules. Moderators' reports provide guides for future work on\ndeveloping moderation assistant models.\n","authors":["Yang Trista Cao","Lovely-Frances Domingo","Sarah Ann Gilbert","Michelle Mazurek","Katie Shilton","Hal Daumé III"],"pdf_url":"https://arxiv.org/pdf/2311.07879v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09662v2","updated":"2024-10-21T23:58:45Z","published":"2024-06-14T02:21:53Z","title":"Learning Language Structures through Grounding","summary":"  Language is highly structured, with syntactic and semantic structures, to\nsome extent, agreed upon by speakers of the same language. With implicit or\nexplicit awareness of such structures, humans can learn and use language\nefficiently and generalize to sentences that contain unseen words. Motivated by\nhuman language learning, in this dissertation, we consider a family of machine\nlearning tasks that aim to learn language structures through grounding. We seek\ndistant supervision from other data sources (i.e., grounds), including but not\nlimited to other modalities (e.g., vision), execution results of programs, and\nother languages.\n  We demonstrate the potential of this task formulation and advocate for its\nadoption through three schemes. In Part I, we consider learning syntactic\nparses through visual grounding. We propose the task of visually grounded\ngrammar induction, present the first models to induce syntactic structures from\nvisually grounded text and speech, and find that the visual grounding signals\ncan help improve the parsing quality over language-only models. As a side\ncontribution, we propose a novel evaluation metric that enables the evaluation\nof speech parsing without text or automatic speech recognition systems\ninvolved. In Part II, we propose two execution-aware methods to map sentences\ninto corresponding semantic structures (i.e., programs), significantly\nimproving compositional generalization and few-shot program synthesis. In Part\nIII, we propose methods that learn language structures from annotations in\nother languages. Specifically, we propose a method that sets a new state of the\nart on cross-lingual word alignment. We then leverage the learned word\nalignments to improve the performance of zero-shot cross-lingual dependency\nparsing, by proposing a novel substructure-based projection method that\npreserves structural knowledge learned from the source language.\n","authors":["Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2406.09662v2.pdf","comment":"Ph.D. Thesis"},{"id":"http://arxiv.org/abs/2410.13804v3","updated":"2024-10-21T23:37:48Z","published":"2024-10-17T17:41:15Z","title":"BenTo: Benchmark Task Reduction with In-Context Transferability","summary":"  Evaluating large language models (LLMs) is costly: it requires the generation\nand examination of LLM outputs on a large-scale benchmark of various tasks.\nThis paper investigates how to efficiently reduce the tasks used to benchmark\nLLMs without affecting the evaluation quality. Our study reveals that task\ntransferability and relevance provide critical information to identify the most\nrepresentative subset of tasks via optimizing a facility location function. We\npropose a practically efficient metric for estimating the transferability\nbetween two tasks via in-context learning (ICL). By analyzing the pairwise\ntransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or\nFLAN) to 5% while inducing only a <4% difference to the evaluation on the\noriginal benchmark. Compared to prior works, our method is training-free,\ngradient-free, and highly efficient requiring ICL only.\n","authors":["Hongyu Zhao","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13804v3.pdf","comment":"https://github.com/tianyi-lab/bento"},{"id":"http://arxiv.org/abs/2405.18400v5","updated":"2024-10-21T22:56:06Z","published":"2024-05-28T17:40:48Z","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass","summary":"  Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.\n","authors":["Ethan Shen","Alan Fan","Sarah M. Pratt","Jae Sung Park","Matthew Wallingford","Sham M. Kakade","Ari Holtzman","Ranjay Krishna","Ali Farhadi","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2405.18400v5.pdf","comment":"23 pages, 16 figures, accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16560v1","updated":"2024-10-21T22:39:52Z","published":"2024-10-21T22:39:52Z","title":"Raising the Stakes: Performance Pressure Improves AI-Assisted Decision\n  Making","summary":"  AI systems are used in many domains to assist with decision making, and\nalthough the potential for AI systems to assist with decision making is much\ndiscussed, human-AI collaboration often underperforms. Investigation into why\nthe performance potential is not realized has revealed many factors, including\n(mis)trust in the AI system and mental models of AI capabilities on subjective\ntasks. Performance pressure is known to influence human decision making\nbehavior, yet how it interacts with human-AI decision making is understudied.\nIn this work, we show the effects of performance pressure on AI advice reliance\nwhen laypeople (Amazon Mechanical Turk crowdworkers) complete a common\nAI-assisted task (fake review detection) and thus have inherently low\nperformance pressure. We manipulate performance pressure by leveraging people's\nloss aversion towards potential monetary gains when completing a task. We find\nthat when the stakes are high, people use AI advice more appropriately than\nwhen stakes are lower, regardless of the presence of an AI explanation.\nFurthermore, when the AI system gives incorrect advice, people correctly\ndiscount the poor advice more often when the stakes are higher than when they\nare lower. We conclude by discussing the implications of how performance\npressure influences AI-assisted decision making and encourage future research\nto incorporate performance pressure analysis.\n","authors":["Nikita Haduong","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2410.16560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16540v1","updated":"2024-10-21T22:07:20Z","published":"2024-10-21T22:07:20Z","title":"A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and\n  Error-Aware Demonstration","summary":"  Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance\nin improving the reasoning capabilities of large language models (LLMs). While\ntheoretical investigations have been conducted to understand CoT, the\nunderlying transformer used in these studies isolates the CoT reasoning process\ninto separated in-context learning steps (Stepwise ICL). In this work, we\ntheoretically show that, compared to Stepwise ICL, the transformer gains better\nerror correction ability and more accurate predictions if the reasoning from\nearlier steps (Coherent CoT) is integrated. Given that this coherent reasoning\nchanges the behavior of the transformer, we further investigate the sensitivity\nof the transformer with Coherent CoT when the demonstration examples are\ncorrupted at the inference stage. Our theoretical results indicate that the\ntransformer is more sensitive to errors in intermediate reasoning steps than\nthe final outcome. Building upon this observation, we propose an improvement on\nCoT by incorporating both correct and incorrect reasoning paths in the\ndemonstration. Our experiments validate the effectiveness of the proposed\napproach.\n","authors":["Yingqian Cui","Pengfei He","Xianfeng Tang","Qi He","Chen Luo","Jiliang Tang","Yue Xing"],"pdf_url":"https://arxiv.org/pdf/2410.16540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11627v2","updated":"2024-10-21T21:49:33Z","published":"2024-10-15T14:14:19Z","title":"Tokenization and Morphology in Multilingual Language Models: A\n  Comparative Analysis of mT5 and ByT5","summary":"  Morphology is a crucial factor for multilingual language modeling as it poses\ndirect challenges for tokenization. Here, we seek to understand how\ntokenization influences the morphological knowledge encoded in multilingual\nlanguage models. Specifically, we capture the impact of tokenization by\ncontrasting two multilingual language models: mT5 and ByT5. The two models\nshare the same architecture, training objective, and training data and only\ndiffer in their tokenization strategies: subword tokenization vs.\\@\ncharacter-level tokenization. Probing the morphological knowledge encoded in\nthese models on four tasks and 17 languages, our analyses show that the models\nlearn the morphological systems of some languages better than others and that\nmorphological information is encoded in the middle and late layers. Finally, we\nshow that languages with more irregularities benefit more from having a higher\nshare of the pre-training data.\n","authors":["Thao Anh Dang","Limor Raviv","Lukas Galke"],"pdf_url":"https://arxiv.org/pdf/2410.11627v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.16533v1","updated":"2024-10-21T21:48:24Z","published":"2024-10-21T21:48:24Z","title":"Large Body Language Models","summary":"  As virtual agents become increasingly prevalent in human-computer\ninteraction, generating realistic and contextually appropriate gestures in\nreal-time remains a significant challenge. While neural rendering techniques\nhave made substantial progress with static scripts, their applicability to\nhuman-computer interactions remains limited. To address this, we introduce\nLarge Body Language Models (LBLMs) and present LBLM-AVA, a novel LBLM\narchitecture that combines a Transformer-XL large language model with a\nparallelized diffusion model to generate human-like gestures from multimodal\ninputs (text, audio, and video). LBLM-AVA incorporates several key components\nenhancing its gesture generation capabilities, such as multimodal-to-pose\nembeddings, enhanced sequence-to-sequence mapping with redefined attention\nmechanisms, a temporal smoothing module for gesture sequence coherence, and an\nattention-based refinement module for enhanced realism. The model is trained on\nour large-scale proprietary open-source dataset Allo-AVA. LBLM-AVA achieves\nstate-of-the-art performance in generating lifelike and contextually\nappropriate gestures with a 30% reduction in Fr\\'echet Gesture Distance (FGD),\nand a 25% improvement in Fr\\'echet Inception Distance compared to existing\napproaches.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2410.16533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16531v1","updated":"2024-10-21T21:45:22Z","published":"2024-10-21T21:45:22Z","title":"Bayesian scaling laws for in-context learning","summary":"  In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a family of novel Bayesian scaling laws for ICL. In\nexperiments with \\mbox{GPT-2} models of different sizes, our scaling laws\nexceed or match existing scaling laws in accuracy while also offering\ninterpretable terms for task priors, learning efficiency, and per-example\nprobabilities. To illustrate the analytic power that such interpretable scaling\nlaws provide, we report on controlled synthetic dataset experiments designed to\ninform real-world studies of safety alignment. In our experimental protocol, we\nuse SFT to suppress an unwanted existing model capability and then use ICL to\ntry to bring that capability back (many-shot jailbreaking). We then experiment\non real-world instruction-tuned LLMs using capabilities benchmarks as well as a\nnew many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\naccurately predict the conditions under which ICL will cause the suppressed\nbehavior to reemerge, which sheds light on the ineffectiveness of post-training\nat increasing LLM safety.\n","authors":["Aryaman Arora","Dan Jurafsky","Christopher Potts","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2410.16531v1.pdf","comment":"10 pages main text, 26 pages total"},{"id":"http://arxiv.org/abs/2410.14668v2","updated":"2024-10-21T21:42:46Z","published":"2024-10-18T17:57:40Z","title":"MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image\n  Description and Reasoning Steps","summary":"  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for\nimproving the performance of multimodal large language models (MLLMs) across a\nrange of complex reasoning tasks. Despite its popularity, there is a notable\nabsence of automated methods for evaluating the quality of reasoning steps in\nMCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation\n(MiCEval), a framework designed to assess the correctness of reasoning chains\nby evaluating the quality of both the description and each reasoning step. The\nevaluation of the description component focuses on the accuracy of the image\ndescriptions, while the reasoning step evaluates the quality of each step as it\nis conditionally generated based on the preceding steps. MiCEval is built upon\na fine-grained dataset with annotations that rate each step according to\ncorrectness, relevance, and informativeness. Extensive experiments on four\nstate-of-the-art MLLMs show that step-wise evaluations using MiCEval align more\nclosely with human judgments compared to existing methods based on cosine\nsimilarity or fine-tuning approaches. MiCEval datasets and code can be found in\nhttps://github.com/alenai97/MiCEval.\n","authors":["Xiongtao Zhou","Jie He","Lanyu Chen","Jingyu Li","Haojing Chen","Victor Gutierrez Basulto","Jeff Z. Pan","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14668v2.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2406.08598v2","updated":"2024-10-21T21:32:51Z","published":"2024-06-12T19:05:43Z","title":"Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks","summary":"  As Large Language Models (LLMs) continue to evolve, the search for efficient\nand meaningful evaluation methods is ongoing. Many recent evaluations use LLMs\nas judges to score outputs from other LLMs, often relying on a single large\nmodel like GPT-4o. However, using a single LLM judge is prone to intra-model\nbias, and many tasks - such as those related to emotional intelligence,\ncreative writing, and persuasiveness - may be too subjective for a single model\nto judge fairly. We introduce the Language Model Council (LMC), where a group\nof LLMs collaborate to create tests, respond to them, and evaluate each other's\nresponses to produce a ranking in a democratic fashion. Unlike previous\napproaches that focus on reducing cost or bias by using a panel of smaller\nmodels, our work examines the benefits and nuances of a fully inclusive LLM\nevaluation system. In a detailed case study on emotional intelligence, we\ndeploy a council of 20 recent LLMs to rank each other on open-ended responses\nto interpersonal conflicts. Our results show that the LMC produces rankings\nthat are more separable and more robust, and through a user study, we show that\nthey are more consistent with human evaluations than any individual LLM judge.\nUsing all LLMs for judging can be costly, however, so we use Monte Carlo\nsimulations and hand-curated sub-councils to study hypothetical council\ncompositions and discuss the value of the incremental LLM judge.\n","authors":["Justin Zhao","Flor Miriam Plaza-del-Arco","Benjie Genchel","Amanda Cercas Curry"],"pdf_url":"https://arxiv.org/pdf/2406.08598v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16520v1","updated":"2024-10-21T21:21:29Z","published":"2024-10-21T21:21:29Z","title":"AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context","summary":"  As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.\n","authors":["Naba Rizvi","Harper Strickland","Daniel Gitelman","Tristan Cooper","Alexis Morales-Flores","Michael Golden","Aekta Kallepalli","Akshat Alurkar","Haaset Owens","Saleha Ahmedi","Isha Khirwadkar","Imani Munyaka","Nedjma Ousidhoum"],"pdf_url":"https://arxiv.org/pdf/2410.16520v1.pdf","comment":"9 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2405.13274v2","updated":"2024-10-21T21:09:22Z","published":"2024-05-22T01:10:39Z","title":"DiffNorm: Self-Supervised Normalization for Non-autoregressive\n  Speech-to-speech Translation","summary":"  Non-autoregressive Transformers (NATs) are recently applied in direct\nspeech-to-speech translation systems, which convert speech across different\nlanguages without intermediate text data. Although NATs generate high-quality\noutputs and offer faster inference than autoregressive models, they tend to\nproduce incoherent and repetitive results due to complex data distribution\n(e.g., acoustic and linguistic variations in speech). In this work, we\nintroduce DiffNorm, a diffusion-based normalization strategy that simplifies\ndata distributions for training NAT models. After training with a\nself-supervised noise estimation objective, DiffNorm constructs normalized\ntarget data by denoising synthetically corrupted speech features. Additionally,\nwe propose to regularize NATs with classifier-free guidance, improving model\nrobustness and translation quality by randomly dropping out source information\nduring training. Our strategies result in a notable improvement of about +7\nASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr)\ntranslations on the CVSS benchmark, while attaining over 14x speedup for En-Es\nand 5x speedup for En-Fr translations compared to autoregressive baselines.\n","authors":["Weiting Tan","Jingyu Zhang","Lingfeng Shen","Daniel Khashabi","Philipp Koehn"],"pdf_url":"https://arxiv.org/pdf/2405.13274v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16509v1","updated":"2024-10-21T21:00:47Z","published":"2024-10-21T21:00:47Z","title":"Learning from others' mistakes: Finetuning machine translation models\n  with span-level error annotations","summary":"  Despite growing interest in incorporating feedback to improve language\nmodels, most efforts focus only on sequence-level annotations. In this work, we\nexplore the potential of utilizing fine-grained span-level annotations from\noffline datasets to improve model quality. We develop a simple finetuning\nalgorithm, called Training with Annotations (TWA), to directly train machine\ntranslation models on such annotated data. TWA utilizes targeted span-level\nerror information while also flexibly learning what to penalize within a span.\nMoreover, TWA considers the overall trajectory of a sequence when deciding\nwhich non-error spans to utilize as positive signals. Experiments on\nEnglish-German and Chinese-English machine translation show that TWA\noutperforms baselines such as Supervised FineTuning on sequences filtered for\nquality and Direct Preference Optimization on pairs constructed from the same\ndata.\n","authors":["Lily H. Zhang","Hamid Dadkhahi","Mara Finkelstein","Firas Trabelsi","Jiaming Luo","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2410.16509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11414v2","updated":"2024-10-21T20:58:43Z","published":"2024-02-18T01:03:25Z","title":"Fine-grained and Explainable Factuality Evaluation for Multimodal\n  Summarization","summary":"  Multimodal summarization aims to generate a concise summary based on the\ninput text and image. However, the existing methods potentially suffer from\nunfactual output. To evaluate the factuality of multimodal summarization\nmodels, we propose two fine-grained and explainable evaluation frameworks\n(FALLACIOUS) for different application scenarios, i.e. reference-based\nfactuality evaluation framework and reference-free factuality evaluation\nframework. Notably, the reference-free factuality evaluation framework doesn't\nneed ground truth and hence it has a wider application scenario. To evaluate\nthe effectiveness of the proposed frameworks, we compute the correlation\nbetween our frameworks and the other metrics. The experimental results show the\neffectiveness of our proposed method. We will release our code and dataset via\ngithub.\n","authors":["Yue Zhang","Jingxuan Zuo","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2402.11414v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16503v1","updated":"2024-10-21T20:50:51Z","published":"2024-10-21T20:50:51Z","title":"Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for\n  Allocentric Avatar Gesture Animation","summary":"  The scarcity of high-quality, multimodal training data severely hinders the\ncreation of lifelike avatar animations for conversational AI in virtual\nenvironments. Existing datasets often lack the intricate synchronization\nbetween speech, facial expressions, and body movements that characterize\nnatural human communication. To address this critical gap, we introduce\nAllo-AVA, a large-scale dataset specifically designed for text and audio-driven\navatar gesture animation in an allocentric (third person point-of-view)\ncontext. Allo-AVA consists of $\\sim$1,250 hours of diverse video content,\ncomplete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely\nmaps these keypoints to precise timestamps, enabling accurate replication of\nhuman movements (body and facial gestures) in synchronization with speech. This\ncomprehensive resource enables the development and evaluation of more natural,\ncontext-aware avatar animation models, potentially transforming applications\nranging from virtual reality to digital assistants.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2410.16503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16502v1","updated":"2024-10-21T20:48:16Z","published":"2024-10-21T20:48:16Z","title":"Rulebreakers Challenge: Revealing a Blind Spot in Large Language Models'\n  Reasoning with Formal Logic","summary":"  Formal logic has long been applied to natural language reasoning, but this\napproach can sometimes lead to conclusions that, while logically entailed, are\nfactually inconsistent with the premises or are not typically inferred by\nhumans. This study introduces the concept of \"rulebreakers\", which refers to\ninstances where logical entailment diverges from factually acceptable\ninference. We present RULEBREAKERS, a novel dataset for evaluating Large\nLanguage Models' (LLMs) ability to distinguish between rulebreakers and\nnon-rulebreakers. Focusing on modus tollens and disjunctive syllogism, we\nassess six state-of-the-art LLMs using RULEBREAKERS, measuring their\nperformance in terms of token-level exact accuracy and model confidence. Our\nfindings reveal that while most models perform poorly to moderately in\nrecognizing rulebreakers, they demonstrate a latent ability to distinguish\nrulebreakers when assessed by their confidence levels. Further analysis\nsuggests that the failure to recognize rulebreakers is potentially associated\nwith the models' world knowledge and their attention distribution patterns.\nThis research highlights the limitation of LLMs' reasoning capabilities, and\ncontributes to the ongoing discussion on reasoning in LLMs.\n","authors":["Jason Chan","Robert Gaizauskas","Zhixue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16502v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.16498v1","updated":"2024-10-21T20:41:00Z","published":"2024-10-21T20:41:00Z","title":"Natural Language Processing for Human Resources: A Survey","summary":"  The domain of human resources (HR) includes a broad spectrum of tasks related\nto natural language processing (NLP) techniques. Recent breakthroughs in NLP\nhave generated significant interest in its industrial applications in this\ndomain and potentially alleviate challenges such as the difficulty of resource\nacquisition and the complexity of problems. At the same time, the HR domain can\nalso present unique challenges that drive state-of-the-art in NLP research. To\nsupport this, we provide NLP researchers and practitioners with an overview of\nkey HR tasks from an NLP perspective, illustrating how specific sub-tasks\n(e.g., skill extraction) contribute to broader objectives (e.g., job matching).\nThrough this survey, we identify opportunities in NLP for HR and suggest\ndirections for future exploration.\n","authors":["Naoki Otani","Nikita Bhutani","Estevam Hruschka"],"pdf_url":"https://arxiv.org/pdf/2410.16498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16491v1","updated":"2024-10-21T20:32:27Z","published":"2024-10-21T20:32:27Z","title":"BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded\n  Data","summary":"  In this work, we tackle the challenge of embedding realistic human\npersonality traits into LLMs. Previous approaches have primarily focused on\nprompt-based methods that describe the behavior associated with the desired\npersonality traits, suffering from realism and validity issues. To address\nthese limitations, we introduce BIG5-CHAT, a large-scale dataset containing\n100,000 dialogues designed to ground models in how humans express their\npersonality in text. Leveraging this dataset, we explore Supervised Fine-Tuning\nand Direct Preference Optimization as training-based methods to align LLMs more\nnaturally with human personality patterns. Our methods outperform prompting on\npersonality assessments such as BFI and IPIP-NEO, with trait correlations more\nclosely matching human data. Furthermore, our experiments reveal that models\ntrained to exhibit higher conscientiousness, higher agreeableness, lower\nextraversion, and lower neuroticism display better performance on reasoning\ntasks, aligning with psychological findings on how these traits impact human\ncognitive performance. To our knowledge, this work is the first comprehensive\nstudy to demonstrate how training-based methods can shape LLM personalities\nthrough learning from real human behaviors.\n","authors":["Wenkai Li","Jiarui Liu","Andy Liu","Xuhui Zhou","Mona Diab","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2410.16491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16473v1","updated":"2024-10-21T20:01:06Z","published":"2024-10-21T20:01:06Z","title":"Multi-head Sequence Tagging Model for Grammatical Error Correction","summary":"  To solve the Grammatical Error Correction (GEC) problem , a mapping between a\nsource sequence and a target one is needed, where the two differ only on few\nspans. For this reason, the attention has been shifted to the\nnon-autoregressive or sequence tagging models. In which, the GEC has been\nsimplified from Seq2Seq to labeling the input tokens with edit commands chosen\nfrom a large edit space. Due to this large number of classes and the limitation\nof the available datasets, the current sequence tagging approaches still have\nsome issues handling a broad range of grammatical errors just by being\nlaser-focused on one single task. To this end, we simplified the GEC further by\ndividing it into seven related subtasks: Insertion, Deletion, Merge,\nSubstitution, Transformation, Detection, and Correction, with Correction being\nour primary focus. A distinct classification head is dedicated to each of these\nsubtasks. the novel multi-head and multi-task learning model is proposed to\neffectively utilize training data and harness the information from related task\ntraining signals. To mitigate the limited number of available training samples,\na new denoising autoencoder is used to generate a new synthetic dataset to be\nused for pretraining. Additionally, a new character-level transformation is\nproposed to enhance the sequence-to-edit function and improve the model's\nvocabulary coverage. Our single/ensemble model achieves an F0.5 of 74.4/77.0,\nand 68.6/69.1 on BEA-19 (test) and CoNLL-14 (test) respectively. Moreover,\nevaluated on JFLEG test set, the GLEU scores are 61.6 and 61.7 for the single\nand ensemble models, respectively. It mostly outperforms recently published\nstate-of-the-art results by a considerable margin.\n","authors":["Kamal Al-Sabahi","Kang Yang","Wangwang Liu","Guanyu Jiang","Xian Li","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16472v1","updated":"2024-10-21T19:59:04Z","published":"2024-10-21T19:59:04Z","title":"DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding","summary":"  Document structure editing involves manipulating localized textual, visual,\nand layout components in document images based on the user's requests. Past\nworks have shown that multimodal grounding of user requests in the document\nimage and identifying the accurate structural components and their associated\nattributes remain key challenges for this task. To address these, we introduce\nthe DocEdit-v2, a novel framework that performs end-to-end document editing by\nleveraging Large Multimodal Models (LMMs). It consists of three novel\ncomponents: (1) Doc2Command, which simultaneously localizes edit regions of\ninterest (RoI) and disambiguates user edit requests into edit commands; (2)\nLLM-based Command Reformulation prompting to tailor edit commands originally\nintended for specialized software into edit instructions suitable for\ngeneralist LMMs. (3) Moreover, DocEdit-v2 processes these outputs via Large\nMultimodal Models like GPT-4V and Gemini, to parse the document layout, execute\nedits on grounded Region of Interest (RoI), and generate the edited document\nimage. Extensive experiments on the DocEdit dataset show that DocEdit-v2\nsignificantly outperforms strong baselines on edit command generation (2-33%),\nRoI bounding box detection (12-31%), and overall document editing (1-12\\%)\ntasks.\n","authors":["Manan Suri","Puneet Mathur","Franck Dernoncourt","Rajiv Jain","Vlad I Morariu","Ramit Sawhney","Preslav Nakov","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2410.16472v1.pdf","comment":"EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2409.01344v2","updated":"2024-10-21T19:49:41Z","published":"2024-09-02T15:58:24Z","title":"Pairing Analogy-Augmented Generation with Procedural Memory for\n  Procedural Q&A","summary":"  Large language models struggle to synthesize disparate pieces of information\ninto a coherent plan when approaching a complex procedural task. In this work,\nwe introduce a novel formalism and structure for such procedural knowledge.\nBased on this formalism, we present a novel procedural knowledge dataset called\nLCStep, which we created from LangChain tutorials. To leverage this procedural\nknowledge to solve new tasks, we propose analogy-augmented generation (AAG),\nwhich draws inspiration from the human ability to assimilate past experiences\nto solve unfamiliar problems. AAG uses a custom procedure memory store to\nretrieve and adapt specialized domain knowledge to answer new procedural tasks.\nWe demonstrate that AAG outperforms few-shot and RAG baselines on LCStep,\nRecipeNLG, and CHAMP datasets under a pairwise LLM-based evaluation,\ncorroborated by human evaluation in the case of RecipeNLG.\n","authors":["K Roth","Rushil Gupta","Simon Halle","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16464v1","updated":"2024-10-21T19:46:06Z","published":"2024-10-21T19:46:06Z","title":"Beyond Browsing: API-Based Web Agents","summary":"  Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by browsing agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-based agents outperform web browsing agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n20.0% absolute improvement over web browsing alone, achieving a success rate of\n35.8%, achiving the SOTA performance among task-agnostic agents. These results\nstrongly suggest that when APIs are available, they present an attractive\nalternative to relying on web browsing alone.\n","authors":["Yueqi Song","Frank Xu","Shuyan Zhou","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16464v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16461v1","updated":"2024-10-21T19:40:05Z","published":"2024-10-21T19:40:05Z","title":"Comparative Study of Multilingual Idioms and Similes in Large Language\n  Models","summary":"  This study addresses the gap in the literature concerning the comparative\nperformance of LLMs in interpreting different types of figurative language\nacross multiple languages. By evaluating LLMs using two multilingual datasets\non simile and idiom interpretation, we explore the effectiveness of various\nprompt engineering strategies, including chain-of-thought, few-shot, and\nEnglish translation prompts. We extend the language of these datasets to\nPersian as well by building two new evaluation sets. Our comprehensive\nassessment involves both closed-source (GPT-3.5, GPT-4o mini, Gemini 1.5), and\nopen-source models (Llama 3.1, Qwen2), highlighting significant differences in\nperformance across languages and figurative types. Our findings reveal that\nwhile prompt engineering methods are generally effective, their success varies\nby figurative type, language, and model. We also observe that open-source\nmodels struggle particularly with low-resource languages in similes.\nAdditionally, idiom interpretation is nearing saturation for many languages,\nnecessitating more challenging evaluations.\n","authors":["Paria Khoshtab","Danial Namazifard","Mostafa Masoudi","Ali Akhgary","Samin Mahdizadeh Sani","Yadollah Yaghoobzadeh"],"pdf_url":"https://arxiv.org/pdf/2410.16461v1.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.05094v2","updated":"2024-10-21T19:34:27Z","published":"2023-05-08T23:43:15Z","title":"Interactive Concept Learning for Uncovering Latent Themes in Large Text\n  Collections","summary":"  Experts across diverse disciplines are often interested in making sense of\nlarge text collections. Traditionally, this challenge is approached either by\nnoisy unsupervised techniques such as topic models, or by following a manual\ntheme discovery process. In this paper, we expand the definition of a theme to\naccount for more than just a word distribution, and include generalized\nconcepts deemed relevant by domain experts. Then, we propose an interactive\nframework that receives and encodes expert feedback at different levels of\nabstraction. Our framework strikes a balance between automation and manual\ncoding, allowing experts to maintain control of their study while reducing the\nmanual effort required.\n","authors":["Maria Leonor Pacheco","Tunazzina Islam","Lyle Ungar","Ming Yin","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2305.05094v2.pdf","comment":"Accepted to Findings of ACL: ACL 2023"},{"id":"http://arxiv.org/abs/2410.16456v1","updated":"2024-10-21T19:30:05Z","published":"2024-10-21T19:30:05Z","title":"To the Globe (TTG): Towards Language-Driven Guaranteed Travel Planning","summary":"  Travel planning is a challenging and time-consuming task that aims to find an\nitinerary which satisfies multiple, interdependent constraints regarding\nflights, accommodations, attractions, and other travel arrangements. In this\npaper, we propose To the Globe (TTG), a real-time demo system that takes\nnatural language requests from users, translates it to symbolic form via a\nfine-tuned Large Language Model, and produces optimal travel itineraries with\nMixed Integer Linear Programming solvers. The overall system takes ~5 seconds\nto reply to the user request with guaranteed itineraries. To train TTG, we\ndevelop a synthetic data pipeline that generates user requests, flight and\nhotel information in symbolic form without human annotations, based on the\nstatistics of real-world datasets, and fine-tune an LLM to translate NL user\nrequests to their symbolic form, which is sent to the symbolic solver to\ncompute optimal itineraries. Our NL-symbolic translation achieves ~91% exact\nmatch in a backtranslation metric (i.e., whether the estimated symbolic form of\ngenerated natural language matches the groundtruth), and its returned\nitineraries have a ratio of 0.979 compared to the optimal cost of the ground\ntruth user request. When evaluated by users, TTG achieves consistently high Net\nPromoter Scores (NPS) of 35-40% on generated itinerary.\n","authors":["Da JU","Song Jiang","Andrew Cohen","Aaron Foss","Sasha Mitts","Arman Zharmagambetov","Brandon Amos","Xian Li","Justine T Kao","Maryam Fazel-Zarandi","Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.16456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16454v1","updated":"2024-10-21T19:28:37Z","published":"2024-10-21T19:28:37Z","title":"Does your LLM truly unlearn? An embarrassingly simple approach to\n  recover unlearned knowledge","summary":"  Large language models (LLMs) have shown remarkable proficiency in generating\ntext, benefiting from extensive training on vast textual corpora. However, LLMs\nmay also acquire unwanted behaviors from the diverse and sensitive nature of\ntheir training data, which can include copyrighted and private content. Machine\nunlearning has been introduced as a viable solution to remove the influence of\nsuch problematic content without the need for costly and time-consuming\nretraining. This process aims to erase specific knowledge from LLMs while\npreserving as much model utility as possible. Despite the effectiveness of\ncurrent unlearning methods, little attention has been given to whether existing\nunlearning methods for LLMs truly achieve forgetting or merely hide the\nknowledge, which current unlearning benchmarks fail to detect. This paper\nreveals that applying quantization to models that have undergone unlearning can\nrestore the \"forgotten\" information. To thoroughly evaluate this phenomenon, we\nconduct comprehensive experiments using various quantization techniques across\nmultiple precision levels. We find that for unlearning methods with utility\nconstraints, the unlearned model retains an average of 21\\% of the intended\nforgotten knowledge in full precision, which significantly increases to 83\\%\nafter 4-bit quantization. Based on our empirical findings, we provide a\ntheoretical explanation for the observed phenomenon and propose a\nquantization-robust unlearning strategy to mitigate this intricate issue...\n","authors":["Zhiwei Zhang","Fali Wang","Xiaomin Li","Zongyu Wu","Xianfeng Tang","Hui Liu","Qi He","Wenpeng Yin","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16454v1.pdf","comment":"21 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.16451v1","updated":"2024-10-21T19:25:31Z","published":"2024-10-21T19:25:31Z","title":"Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between\n  Ghana and the U.S","summary":"  Recent work has highlighted the culturally-contingent nature of commonsense\nknowledge. We introduce AMAMMER${\\epsilon}$, a test set of 525 multiple-choice\nquestions designed to evaluate the commonsense knowledge of English LLMs,\nrelative to the cultural contexts of Ghana and the United States. To create\nAMAMMER${\\epsilon}$, we select a set of multiple-choice questions (MCQs) from\nexisting commonsense datasets and rewrite them in a multi-stage process\ninvolving surveys of Ghanaian and U.S. participants. In three rounds of\nsurveys, participants from both pools are solicited to (1) write correct and\nincorrect answer choices, (2) rate individual answer choices on a 5-point\nLikert scale, and (3) select the best answer choice from the newly-constructed\nMCQ items, in a final validation step. By engaging participants at multiple\nstages, our procedure ensures that participant perspectives are incorporated\nboth in the creation and validation of test items, resulting in high levels of\nagreement within each pool. We evaluate several off-the-shelf English LLMs on\nAMAMMER${\\epsilon}$. Uniformly, models prefer answers choices that align with\nthe preferences of U.S. annotators over Ghanaian annotators. Additionally, when\ntest items specify a cultural context (Ghana or the U.S.), models exhibit some\nability to adapt, but performance is consistently better in U.S. contexts than\nGhanaian. As large resources are devoted to the advancement of English LLMs,\nour findings underscore the need for culturally adaptable models and\nevaluations to meet the needs of diverse English-speaking populations around\nthe world.\n","authors":["Christabel Acquaye","Haozhe An","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2410.16451v1.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.16443v1","updated":"2024-10-21T19:12:33Z","published":"2024-10-21T19:12:33Z","title":"Improving Neuron-level Interpretability with White-box Language Models","summary":"  Neurons in auto-regressive language models like GPT-2 can be interpreted by\nanalyzing their activation patterns. Recent studies have shown that techniques\nsuch as dictionary learning, a form of post-hoc sparse coding, enhance this\nneuron-level interpretability. In our research, we are driven by the goal to\nfundamentally improve neural network interpretability by embedding sparse\ncoding directly within the model architecture, rather than applying it as an\nafterthought. In our study, we introduce a white-box transformer-like\narchitecture named Coding RAte TransformEr (CRATE), explicitly engineered to\ncapture sparse, low-dimensional structures within data distributions. Our\ncomprehensive experiments showcase significant improvements (up to 103%\nrelative improvement) in neuron-level interpretability across a variety of\nevaluation metrics. Detailed investigations confirm that this enhanced\ninterpretability is steady across different layers irrespective of the model\nsize, underlining CRATE's robust performance in enhancing neural network\ninterpretability. Further analysis shows that CRATE's increased\ninterpretability comes from its enhanced ability to consistently and\ndistinctively activate on relevant tokens. These findings point towards a\npromising direction for creating white-box foundation models that excel in\nneuron-level interpretation.\n","authors":["Hao Bai","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2410.16443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16407v1","updated":"2024-10-21T18:19:09Z","published":"2024-10-21T18:19:09Z","title":"Enhancing Multimodal Affective Analysis with Learned Live Comment\n  Features","summary":"  Live comments, also known as Danmaku, are user-generated messages that are\nsynchronized with video content. These comments overlay directly onto streaming\nvideos, capturing viewer emotions and reactions in real-time. While prior work\nhas leveraged live comments in affective analysis, its use has been limited due\nto the relative rarity of live comments across different video platforms. To\naddress this, we first construct the Live Comment for Affective Analysis\n(LCAffect) dataset which contains live comments for English and Chinese videos\nspanning diverse genres that elicit a wide spectrum of emotions. Then, using\nthis dataset, we use contrastive learning to train a video encoder to produce\nsynthetic live comment features for enhanced multimodal affective content\nanalysis. Through comprehensive experimentation on a wide range of affective\nanalysis tasks (sentiment, emotion recognition, and sarcasm detection) in both\nEnglish and Chinese, we demonstrate that these synthetic live comment features\nsignificantly improve performance over state-of-the-art methods.\n","authors":["Zhaoyuan Deng","Amith Ananthram","Kathleen McKeown"],"pdf_url":"https://arxiv.org/pdf/2410.16407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19261v2","updated":"2024-10-21T18:12:44Z","published":"2024-05-29T16:55:08Z","title":"Faster Cascades via Speculative Decoding","summary":"  Cascades and speculative decoding are two common approaches to improving\nlanguage models' inference efficiency. Both approaches involve interleaving\nmodels of different sizes, but via fundamentally distinct mechanisms: cascades\nemploy a deferral rule that invokes the larger model only for \"hard\" inputs,\nwhile speculative decoding uses speculative execution to primarily invoke the\nlarger model in parallel verification mode. These mechanisms offer different\nbenefits: empirically, cascades offer better cost-quality trade-offs, often\neven outperforming the large model, while theoretically, speculative decoding\noffers a guarantee of quality-neutrality. In this paper, we leverage the best\nof both these approaches by designing new speculative cascading techniques that\nimplement their deferral rule through speculative execution. We characterize\nthe optimal deferral rule for our speculative cascades, and employ a plug-in\napproximation to the optimal rule. Experiments with Gemma and T5 models on a\nrange of language benchmarks show that our approach yields better cost quality\ntrade-offs than cascading and speculative decoding baselines.\n","authors":["Harikrishna Narasimhan","Wittawat Jitkrittum","Ankit Singh Rawat","Seungyeon Kim","Neha Gupta","Aditya Krishna Menon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2405.19261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16400v1","updated":"2024-10-21T18:10:26Z","published":"2024-10-21T18:10:26Z","title":"VipAct: Visual-Perception Enhancement via Specialized VLM Agent\n  Collaboration and Tool-use","summary":"  While vision-language models (VLMs) have demonstrated remarkable performance\nacross various tasks combining textual and visual information, they continue to\nstruggle with fine-grained visual perception tasks that require detailed\npixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs\non such intricate visual elements remains an open challenge. In this paper, we\npresent VipAct, an agent framework that enhances VLMs by integrating\nmulti-agent collaboration and vision expert models, enabling more precise\nvisual understanding and comprehensive reasoning. VipAct consists of an\norchestrator agent, which manages task requirement analysis, planning, and\ncoordination, along with specialized agents that handle specific tasks such as\nimage captioning and vision expert models that provide high-precision\nperceptual information. This multi-agent approach allows VLMs to better perform\nfine-grained visual perception tasks by synergizing planning, reasoning, and\ntool use. We evaluate VipAct on benchmarks featuring a diverse set of visual\nperception tasks, with experimental results demonstrating significant\nperformance improvements over state-of-the-art baselines across all tasks.\nFurthermore, comprehensive ablation studies reveal the critical role of\nmulti-agent collaboration in eliciting more detailed System-2 reasoning and\nhighlight the importance of image input for task planning. Additionally, our\nerror analysis identifies patterns of VLMs' inherent limitations in visual\nperception, providing insights into potential future improvements. VipAct\noffers a flexible and extensible framework, paving the way for more advanced\nvisual perception systems across various real-world applications.\n","authors":["Zhehao Zhang","Ryan Rossi","Tong Yu","Franck Dernoncourt","Ruiyi Zhang","Jiuxiang Gu","Sungchul Kim","Xiang Chen","Zichao Wang","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2410.16400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16392v1","updated":"2024-10-21T18:06:25Z","published":"2024-10-21T18:06:25Z","title":"LLM-based Optimization of Compound AI Systems: A Survey","summary":"  In a compound AI system, components such as an LLM call, a retriever, a code\ninterpreter, or tools are interconnected. The system's behavior is primarily\ndriven by parameters such as instructions or tool definitions. Recent\nadvancements enable end-to-end optimization of these parameters using an LLM.\nNotably, leveraging an LLM as an optimizer is particularly efficient because it\navoids gradient computation and can generate complex code and instructions.\nThis paper presents a survey of the principles and emerging trends in LLM-based\noptimization of compound AI systems. It covers archetypes of compound AI\nsystems, approaches to LLM-based end-to-end optimization, and insights into\nfuture directions and broader impacts. Importantly, this survey uses concepts\nfrom program analysis to provide a unified view of how an LLM optimizer is\nprompted to optimize a compound AI system. The exhaustive list of paper is\nprovided at\nhttps://github.com/linyuhongg/LLM-based-Optimization-of-Compound-AI-Systems.\n","authors":["Matthieu Lin","Jenny Sheng","Andrew Zhao","Shenzhi Wang","Yang Yue","Yiran Wu","Huan Liu","Jun Liu","Gao Huang","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.16392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16385v1","updated":"2024-10-21T18:01:08Z","published":"2024-10-21T18:01:08Z","title":"KatzBot: Revolutionizing Academic Chatbot for Enhanced Communication","summary":"  Effective communication within universities is crucial for addressing the\ndiverse information needs of students, alumni, and external stakeholders.\nHowever, existing chatbot systems often fail to deliver accurate,\ncontext-specific responses, resulting in poor user experiences. In this paper,\nwe present KatzBot, an innovative chatbot powered by KatzGPT, a custom Large\nLanguage Model (LLM) fine-tuned on domain-specific academic data. KatzGPT is\ntrained on two university-specific datasets: 6,280 sentence-completion pairs\nand 7,330 question-answer pairs. KatzBot outperforms established existing open\nsource LLMs, achieving higher accuracy and domain relevance. KatzBot offers a\nuser-friendly interface, significantly enhancing user satisfaction in\nreal-world applications. The source code is publicly available at\n\\url{https://github.com/AiAI-99/katzbot}.\n","authors":["Sahil Kumar","Deepa Paikar","Kiran Sai Vutukuri","Haider Ali","Shashidhar Reddy Ainala","Aditya Murli Krishnan","Youshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12168v4","updated":"2024-10-21T18:00:54Z","published":"2024-06-18T00:41:40Z","title":"BPO: Staying Close to the Behavior LLM Creates Better Online LLM\n  Alignment","summary":"  Direct alignment from preferences (DAP) has emerged as a promising paradigm\nfor aligning large language models (LLMs) to human desiderata from\npre-collected, offline preference datasets. While recent studies indicate that\nexisting offline DAP methods can directly benefit from online training samples,\nwe highlight the need to develop specific online DAP algorithms to fully\nharness the power of online training. Specifically, we identify that the\nlearned LLM should adhere to the proximity of the behavior LLM, which collects\nthe training samples. To this end, we propose online Preference Optimization in\nproximity to the Behavior LLM (BPO), emphasizing the importance of constructing\na proper trust region for LLM alignment.\n  We conduct extensive experiments to validate the effectiveness and\napplicability of our approach by integrating it with various DAP methods,\nresulting in significant performance improvements across a wide range of tasks\nwhen training with the same amount of preference data. Even when only\nintroducing one additional data collection phase, our online BPO improves its\noffline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on\nAnthropic Helpfulness in terms of win rate against human reference text.\n","authors":["Wenda Xu","Jiachen Li","William Yang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2406.12168v4.pdf","comment":"Wenda Xu and Jiachen Li contributed equally. Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.16164v1","updated":"2024-10-21T16:31:16Z","published":"2024-10-21T16:31:16Z","title":"GenAI Assisting Medical Training","summary":"  Medical procedures such as venipuncture and cannulation are essential for\nnurses and require precise skills. Learning this skill, in turn, is a challenge\nfor educators due to the number of teachers per class and the complexity of the\ntask. The study aims to help students with skill acquisition and alleviate the\neducator's workload by integrating generative AI methods to provide real-time\nfeedback on medical procedures such as venipuncture and cannulation.\n","authors":["Stefan Fritsch","Matthias Tschoepe","Vitor Fortes Rey","Lars Krupp","Agnes Gruenerbl","Eloise Monger","Sarah Travenna"],"pdf_url":"https://arxiv.org/pdf/2410.16164v1.pdf","comment":"2 pages, 2 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.14445v2","updated":"2024-10-21T01:45:47Z","published":"2024-10-18T13:04:35Z","title":"Toward Generalizing Visual Brain Decoding to Unseen Subjects","summary":"  Visual brain decoding aims to decode visual information from human brain\nactivities. Despite the great progress, one critical limitation of current\nbrain decoding research lies in the lack of generalization capability to unseen\nsubjects. Prior works typically focus on decoding brain activity of individuals\nbased on the observation that different subjects exhibit different brain\nactivities, while it remains unclear whether brain decoding can be generalized\nto unseen subjects. This study aims to answer this question. We first\nconsolidate an image-fMRI dataset consisting of stimulus-image and\nfMRI-response pairs, involving 177 subjects in the movie-viewing task of the\nHuman Connectome Project (HCP). This dataset allows us to investigate the brain\ndecoding performance with the increase of participants. We then present a\nlearning paradigm that applies uniform processing across all subjects, instead\nof employing different network heads or tokenizers for individuals as in\nprevious methods, which can accommodate a large number of subjects to explore\nthe generalization capability across different subjects. A series of\nexperiments are conducted and we have the following findings. First, the\nnetwork exhibits clear generalization capabilities with the increase of\ntraining subjects. Second, the generalization capability is common to popular\nnetwork architectures (MLP, CNN and Transformer). Third, the generalization\nperformance is affected by the similarity between subjects. Our findings reveal\nthe inherent similarities in brain activities across individuals. With the\nemerging of larger and more comprehensive datasets, it is possible to train a\nbrain decoding foundation model in the future. Codes and models can be found at\nhttps://github.com/Xiangtaokong/TGBD.\n","authors":["Xiangtao Kong","Kexin Huang","Ping Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13621v3","updated":"2024-10-21T07:29:23Z","published":"2024-10-17T14:55:09Z","title":"Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on\n  Segment Anything","summary":"  This work proposes a novel approach beyond supervised learning for effective\npathological image analysis, addressing the challenge of limited robust labeled\ndata. Pathological diagnosis of diseases like cancer has conventionally relied\non the evaluation of morphological features by physicians and pathologists.\nHowever, recent advancements in compute-aided diagnosis (CAD) systems are\ngaining significant attention as diagnostic support tools. Although the\nadvancement of deep learning has improved CAD significantly, segmentation\nmodels typically require large pixel-level annotated dataset, and such labeling\nis expensive. Existing studies not based on supervised approaches still\nstruggle with limited generalization, and no practical approach has emerged\nyet. To address this issue, we present a weakly supervised semantic\nsegmentation (WSSS) model by combining class activation map and Segment\nAnything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt\nthe SAM-a foundation model that is pretrained on large datasets and operates in\nzero-shot configurations using only coarse prompts. The proposed approach\ntransfer enhanced Attention Dropout Layer's knowledge to SAM, thereby\ngenerating pseudo-labels. To demonstrate the superiority of the proposed\nmethod, experimental studies are conducted on histopathological breast cancer\ndatasets. The proposed method outperformed other WSSS methods across three\ndatasets, demonstrating its efficiency by achieving this with only 12GB of GPU\nmemory during training. Our code is available at :\nhttps://github.com/QI-NemoSong/EPLC-SAM\n","authors":["Joonhyeon Song","Seohwan Yun","Seongho Yoon","Joohyeok Kim","Sangmin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.13621v3.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13861v2","updated":"2024-10-21T15:42:46Z","published":"2024-10-17T17:59:57Z","title":"PUMA: Empowering Unified MLLM with Multi-granular Visual Generation","summary":"  Recent advancements in multimodal foundation models have yielded significant\nprogress in vision-language understanding. Initial attempts have also explored\nthe potential of multimodal large language models (MLLMs) for visual content\ngeneration. However, existing works have insufficiently addressed the varying\ngranularity demands of different image generation tasks within a unified MLLM\nparadigm - from the diversity required in text-to-image generation to the\nprecise controllability needed in image manipulation. In this work, we propose\nPUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA\nunifies multi-granular visual features as both inputs and outputs of MLLMs,\nelegantly addressing the different granularity requirements of various image\ngeneration tasks within a unified MLLM framework. Following multimodal\npretraining and task-specific instruction tuning, PUMA demonstrates proficiency\nin a wide range of multimodal tasks. This work represents a significant step\ntowards a truly unified MLLM capable of adapting to the granularity demands of\nvarious visual tasks. The code and model will be released in\nhttps://github.com/rongyaofang/PUMA.\n","authors":["Rongyao Fang","Chengqi Duan","Kun Wang","Hao Li","Hao Tian","Xingyu Zeng","Rui Zhao","Jifeng Dai","Hongsheng Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13861v2.pdf","comment":"Project page: https://rongyaofang.github.io/puma/"},{"id":"http://arxiv.org/abs/2410.13571v2","updated":"2024-10-21T09:15:37Z","published":"2024-10-17T14:07:46Z","title":"DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving\n  Scene Representation","summary":"  Closed-loop simulation is essential for advancing end-to-end autonomous\ndriving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS,\nrely predominantly on conditions closely aligned with training data\ndistributions, which are largely confined to forward-driving scenarios.\nConsequently, these methods face limitations when rendering complex maneuvers\n(e.g., lane change, acceleration, deceleration). Recent advancements in\nautonomous-driving world models have demonstrated the potential to generate\ndiverse driving videos. However, these approaches remain constrained to 2D\nvideo generation, inherently lacking the spatiotemporal coherence required to\ncapture intricacies of dynamic driving environments. In this paper, we\nintroduce DriveDreamer4D, which enhances 4D driving scene representation\nleveraging world model priors. Specifically, we utilize the world model as a\ndata machine to synthesize novel trajectory videos based on real-world driving\ndata. Notably, we explicitly leverage structured conditions to control the\nspatial-temporal consistency of foreground and background elements, thus the\ngenerated data adheres closely to traffic constraints. To our knowledge,\nDriveDreamer4D is the first to utilize video generation models for improving 4D\nreconstruction in driving scenarios. Experimental results reveal that\nDriveDreamer4D significantly enhances generation quality under novel trajectory\nviews, achieving a relative improvement in FID by 24.5%, 39.0%, and 10.5%\ncompared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D\nmarkedly enhances the spatiotemporal coherence of driving agents, which is\nverified by a comprehensive user study and the relative increases of 20.3%,\n42.0%, and 13.7% in the NTA-IoU metric.\n","authors":["Guosheng Zhao","Chaojun Ni","Xiaofeng Wang","Zheng Zhu","Xueyang Zhang","Yida Wang","Guan Huang","Xinze Chen","Boyuan Wang","Youyi Zhang","Wenjun Mei","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13571v2.pdf","comment":"Project Page: https://drivedreamer4d.github.io"},{"id":"http://arxiv.org/abs/2410.16271v1","updated":"2024-10-21T17:59:53Z","published":"2024-10-21T17:59:53Z","title":"FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without\n  Learned Priors","summary":"  Neural Radiance Fields (NeRF) face significant challenges in few-shot\nscenarios, primarily due to overfitting and long training times for\nhigh-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use\nfrequency regularization or pre-trained priors but struggle with complex\nscheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework\nthat leverages weight-sharing voxels across multiple scales to efficiently\nrepresent scene details. Our key contribution is a cross-scale geometric\nadaptation scheme that selects pseudo ground truth depth based on reprojection\nerrors across scales. This guides training without relying on externally\nlearned priors, enabling full utilization of the training data. It can also\nintegrate pre-trained priors, enhancing quality without slowing convergence.\nExperiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms\nother few-shot NeRF methods while significantly reducing training time, making\nit a practical solution for efficient and accurate 3D scene reconstruction.\n","authors":["Chin-Yang Lin","Chung-Ho Wu","Chang-Han Yeh","Shih-Han Yen","Cheng Sun","Yu-Lun Liu"],"pdf_url":"https://arxiv.org/pdf/2410.16271v1.pdf","comment":"Project page: https://linjohnss.github.io/frugalnerf/"},{"id":"http://arxiv.org/abs/2410.16272v1","updated":"2024-10-21T17:59:53Z","published":"2024-10-21T17:59:53Z","title":"MvDrag3D: Drag-based Creative 3D Editing via Multi-view\n  Generation-Reconstruction Priors","summary":"  Drag-based editing has become popular in 2D content creation, driven by the\ncapabilities of image generative models. However, extending this technique to\n3D remains a challenge. Existing 3D drag-based editing methods, whether\nemploying explicit spatial transformations or relying on implicit latent\noptimization within limited-capacity 3D generative models, fall short in\nhandling significant topology changes or generating new textures across diverse\nobject categories. To overcome these limitations, we introduce MVDrag3D, a\nnovel framework for more flexible and creative drag-based 3D editing that\nleverages multi-view generation and reconstruction priors. At the core of our\napproach is the usage of a multi-view diffusion model as a strong generative\nprior to perform consistent drag editing over multiple rendered views, which is\nfollowed by a reconstruction model that reconstructs 3D Gaussians of the edited\nobject. While the initial 3D Gaussians may suffer from misalignment between\ndifferent views, we address this via view-specific deformation networks that\nadjust the position of Gaussians to be well aligned. In addition, we propose a\nmulti-view score function that distills generative priors from multiple views\nto further enhance the view consistency and visual quality. Extensive\nexperiments demonstrate that MVDrag3D provides a precise, generative, and\nflexible solution for 3D drag-based editing, supporting more versatile editing\neffects across various object categories and 3D representations.\n","authors":["Honghua Chen","Yushi Lan","Yongwei Chen","Yifan Zhou","Xingang Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16272v1.pdf","comment":"16 pages, 10 figures, conference"},{"id":"http://arxiv.org/abs/2410.16268v1","updated":"2024-10-21T17:59:19Z","published":"2024-10-21T17:59:19Z","title":"SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a\n  Training-Free Memory Tree","summary":"  The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation\nmodel for object segmentation in both images and videos, paving the way for\nvarious downstream video applications. The crucial design of SAM 2 for video\nsegmentation is its memory module, which prompts object-aware memories from\nprevious frames for current frame prediction. However, its greedy-selection\nmemory design suffers from the \"error accumulation\" problem, where an errored\nor missed mask will cascade and influence the segmentation of the subsequent\nframes, which limits the performance of SAM 2 toward complex long-term videos.\nTo this end, we introduce SAM2Long, an improved training-free video object\nsegmentation strategy, which considers the segmentation uncertainty within each\nframe and chooses the video-level optimal results from multiple segmentation\npathways in a constrained tree search manner. In practice, we maintain a fixed\nnumber of segmentation pathways throughout the video. For each frame, multiple\nmasks are proposed based on the existing pathways, creating various candidate\nbranches. We then select the same fixed number of branches with higher\ncumulative scores as the new pathways for the next frame. After processing the\nfinal frame, the pathway with the highest cumulative score is chosen as the\nfinal segmentation result. Benefiting from its heuristic search design,\nSAM2Long is robust toward occlusions and object reappearances, and can\neffectively segment and track objects for complex long-term videos. Notably,\nSAM2Long achieves an average improvement of 3.0 points across all 24\nhead-to-head comparisons, with gains of up to 5.3 points in J&F on long-term\nvideo object segmentation benchmarks such as SA-V and LVOS. The code is\nreleased at https://github.com/Mark12Ding/SAM2Long.\n","authors":["Shuangrui Ding","Rui Qian","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Yuwei Guo","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16268v1.pdf","comment":"Project page: https://mark12ding.github.io/project/SAM2Long/"},{"id":"http://arxiv.org/abs/2410.16267v1","updated":"2024-10-21T17:59:11Z","published":"2024-10-21T17:59:11Z","title":"xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video\n  Even in VLMs","summary":"  We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html\n","authors":["Michael S. Ryoo","Honglu Zhou","Shrikant Kendre","Can Qin","Le Xue","Manli Shu","Silvio Savarese","Ran Xu","Caiming Xiong","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2410.16267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16266v1","updated":"2024-10-21T17:59:09Z","published":"2024-10-21T17:59:09Z","title":"3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with\n  View-consistent 2D Diffusion Priors","summary":"  Novel-view synthesis aims to generate novel views of a scene from multiple\ninput images or videos, and recent advancements like 3D Gaussian splatting\n(3DGS) have achieved notable success in producing photorealistic renderings\nwith efficient pipelines. However, generating high-quality novel views under\nchallenging settings, such as sparse input views, remains difficult due to\ninsufficient information in under-sampled areas, often resulting in noticeable\nartifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing\nthe representation quality of 3DGS representations. We leverage 2D video\ndiffusion priors to address the challenging 3D view consistency problem,\nreformulating it as achieving temporal consistency within a video generation\nprocess. 3DGS-Enhancer restores view-consistent latent features of rendered\nnovel views and integrates them with the input views through a spatial-temporal\ndecoder. The enhanced views are then used to fine-tune the initial 3DGS model,\nsignificantly improving its rendering performance. Extensive experiments on\nlarge-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields\nsuperior reconstruction performance and high-fidelity rendering results\ncompared to state-of-the-art methods. The project webpage is\nhttps://xiliu8006.github.io/3DGS-Enhancer-project .\n","authors":["Xi Liu","Chaoyi Zhou","Siyu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.16266v1.pdf","comment":"Accepted by NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.16261v1","updated":"2024-10-21T17:58:20Z","published":"2024-10-21T17:58:20Z","title":"Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5%\n  Parameters and 90% Performance","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\nperformance in vision-language tasks across a broad spectrum of domains.\nHowever, the large model scale and associated high computational costs pose\nsignificant challenges for training and deploying MLLMs on consumer-grade GPUs\nor edge devices, thereby hindering their widespread application. In this work,\nwe introduce Mini-InternVL, a series of MLLMs with parameters ranging from 1B\nto 4B, which achieves 90% of the performance with only 5% of the parameters.\nThis significant improvement in efficiency and effectiveness makes our models\nmore accessible and applicable in various real-world scenarios. To further\npromote the adoption of our models, we develop a unified adaptation framework\nfor Mini-InternVL, which enables our models to transfer and outperform\nspecialized models in downstream tasks, including autonomous driving, medical\nimages, and remote sensing. We believe that our study can provide valuable\ninsights and resources to advance the development of efficient and effective\nMLLMs. Code is available at https://github.com/OpenGVLab/InternVL.\n","authors":["Zhangwei Gao","Zhe Chen","Erfei Cui","Yiming Ren","Weiyun Wang","Jinguo Zhu","Hao Tian","Shenglong Ye","Junjun He","Xizhou Zhu","Lewei Lu","Tong Lu","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16261v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2410.16259v1","updated":"2024-10-21T17:57:50Z","published":"2024-10-21T17:57:50Z","title":"Agent-to-Sim: Learning Interactive Behavior Models from Casual\n  Longitudinal Videos","summary":"  We present Agent-to-Sim (ATS), a framework for learning interactive behavior\nmodels of 3D agents from casual longitudinal video collections. Different from\nprior works that rely on marker-based tracking and multiview cameras, ATS\nlearns natural behaviors of animal and human agents non-invasively through\nvideo observations recorded over a long time-span (e.g., a month) in a single\nenvironment. Modeling 3D behavior of an agent requires persistent 3D tracking\n(e.g., knowing which point corresponds to which) over a long time period. To\nobtain such data, we develop a coarse-to-fine registration method that tracks\nthe agent and the camera over time through a canonical 3D space, resulting in a\ncomplete and persistent spacetime 4D representation. We then train a generative\nmodel of agent behaviors using paired data of perception and motion of an agent\nqueried from the 4D reconstruction. ATS enables real-to-sim transfer from video\nrecordings of an agent to an interactive behavior simulator. We demonstrate\nresults on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos\ncaptured by a smartphone.\n","authors":["Gengshan Yang","Andrea Bajcsy","Shunsuke Saito","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2410.16259v1.pdf","comment":"Project page: https://gengshan-y.github.io/agent2sim-www/"},{"id":"http://arxiv.org/abs/2410.16257v1","updated":"2024-10-21T17:57:04Z","published":"2024-10-21T17:57:04Z","title":"Elucidating the design space of language models for image generation","summary":"  The success of autoregressive (AR) language models in text generation has\ninspired the computer vision community to adopt Large Language Models (LLMs)\nfor image generation. However, considering the essential differences between\ntext and image modalities, the design space of language models for image\ngeneration remains underexplored. We observe that image tokens exhibit greater\nrandomness compared to text tokens, which presents challenges when training\nwith token prediction. Nevertheless, AR models demonstrate their potential by\neffectively learning patterns even from a seemingly suboptimal optimization\nproblem. Our analysis also reveals that while all models successfully grasp the\nimportance of local information in image generation, smaller models struggle to\ncapture the global context. In contrast, larger models showcase improved\ncapabilities in this area, helping to explain the performance gains achieved\nwhen scaling up model size. We further elucidate the design space of language\nmodels for vision generation, including tokenizer choice, model choice, model\nscalability, vocabulary design, and sampling strategy through extensive\ncomparative experiments. Our work is the first to analyze the optimization\nbehavior of language models in vision generation, and we believe it can inspire\nmore effective designs when applying LMs to other domains. Finally, our\nelucidated language model for image generation, termed as ELM, achieves\nstate-of-the-art performance on the ImageNet 256*256 benchmark. The code is\navailable at https://github.com/Pepperlll/LMforImageGeneration.git.\n","authors":["Xuantong Liu","Shaozhe Hao","Xianbiao Qi","Tianyang Hu","Jun Wang","Rong Xiao","Yuan Yao"],"pdf_url":"https://arxiv.org/pdf/2410.16257v1.pdf","comment":"Project page: https://pepper-lll.github.io/LMforImageGeneration/"},{"id":"http://arxiv.org/abs/2410.16255v1","updated":"2024-10-21T17:56:47Z","published":"2024-10-21T17:56:47Z","title":"Revisiting Deep Feature Reconstruction for Logical and Structural\n  Industrial Anomaly Detection","summary":"  Industrial anomaly detection is crucial for quality control and predictive\nmaintenance, but it presents challenges due to limited training data, diverse\nanomaly types, and external factors that alter object appearances. Existing\nmethods commonly detect structural anomalies, such as dents and scratches, by\nleveraging multi-scale features from image patches extracted through deep\npre-trained networks. However, significant memory and computational demands\noften limit their practical application. Additionally, detecting logical\nanomalies-such as images with missing or excess elements-requires an\nunderstanding of spatial relationships that traditional patch-based methods\nfail to capture. In this work, we address these limitations by focusing on Deep\nFeature Reconstruction (DFR), a memory- and compute-efficient approach for\ndetecting structural anomalies. We further enhance DFR into a unified\nframework, called ULSAD, which is capable of detecting both structural and\nlogical anomalies. Specifically, we refine the DFR training objective to\nimprove performance in structural anomaly detection, while introducing an\nattention-based loss mechanism using a global autoencoder-like network to\nhandle logical anomaly detection. Our empirical evaluation across five\nbenchmark datasets demonstrates the performance of ULSAD in detecting and\nlocalizing both structural and logical anomalies, outperforming eight\nstate-of-the-art methods. An extensive ablation study further highlights the\ncontribution of each component to the overall performance improvement. Our code\nis available at https://github.com/sukanyapatra1997/ULSAD-2024.git\n","authors":["Sukanya Patra","Souhaib Ben Taieb"],"pdf_url":"https://arxiv.org/pdf/2410.16255v1.pdf","comment":"Accepted in Transactions on Machine Learning Research (TMLR). Link to\n  OpenReview: https://openreview.net/forum?id=kdTC4ktHPD"},{"id":"http://arxiv.org/abs/2410.16239v1","updated":"2024-10-21T17:42:41Z","published":"2024-10-21T17:42:41Z","title":"MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays,\n  ECGs, and Diagnostic Report","summary":"  In this paper, we introduce a novel Multi-Modal Contrastive Pre-training\nFramework that synergistically combines X-rays, electrocardiograms (ECGs), and\nradiology/cardiology reports. Our approach leverages transformers to encode\nthese diverse modalities into a unified representation space, aiming to enhance\ndiagnostic accuracy and facilitate comprehensive patient assessments. We\nutilize LoRA-Peft to significantly reduce trainable parameters in the LLM and\nincorporate recent linear attention dropping strategy in the Vision\nTransformer(ViT) for smoother attention. Furthermore, we provide novel\nmultimodal attention explanations and retrieval for our model. To the best of\nour knowledge, we are the first to propose an integrated model that combines\nX-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing\ncontrastive loss, MoRE effectively aligns modality-specific features into a\ncoherent embedding, which supports various downstream tasks such as zero-shot\nclassification and multimodal retrieval. Employing our proposed methodology, we\nachieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and\nPtbXl downstream datasets, surpassing existing multimodal approaches. Our\nproposed framework shows significant improvements in capturing intricate\ninter-modal relationships and its robustness in medical diagnosis that\nestablishes a framework for future research in multimodal learning in the\nhealthcare sector.\n","authors":["Samrajya Thapa","Koushik Howlader","Subhankar Bhattacharjee","Wei le"],"pdf_url":"https://arxiv.org/pdf/2410.16239v1.pdf","comment":"10 pages, 5 figures, 9 tables. Supplementary detail in Appendix. Code\n  made available in Github for reproducibility"},{"id":"http://arxiv.org/abs/2410.16238v1","updated":"2024-10-21T17:41:58Z","published":"2024-10-21T17:41:58Z","title":"Deep Radiomics Detection of Clinically Significant Prostate Cancer on\n  Multicenter MRI: Initial Comparison to PI-RADS Assessment","summary":"  Objective: To develop and evaluate a deep radiomics model for clinically\nsignificant prostate cancer (csPCa, grade group >= 2) detection and compare its\nperformance to Prostate Imaging Reporting and Data System (PI-RADS) assessment\nin a multicenter cohort. Materials and Methods: This retrospective study\nanalyzed biparametric (T2W and DW) prostate MRI sequences of 615 patients (mean\nage, 63.1 +/- 7 years) from four datasets acquired between 2010 and 2020:\nPROSTATEx challenge, Prostate158 challenge, PCaMAP trial, and an in-house\n(NTNU/St. Olavs Hospital) dataset. With expert annotations as ground truth, a\ndeep radiomics model was trained, including nnU-Net segmentation of the\nprostate gland, voxel-wise radiomic feature extraction, extreme gradient boost\nclassification, and post-processing of tumor probability maps into csPCa\ndetection maps. Training involved 5-fold cross-validation using the PROSTATEx\n(n=199), Prostate158 (n=138), and PCaMAP (n=78) datasets, and testing on the\nin-house (n=200) dataset. Patient- and lesion-level performance were compared\nto PI-RADS using area under ROC curve (AUROC [95% CI]), sensitivity, and\nspecificity analysis. Results: On the test data, the radiologist achieved a\npatient-level AUROC of 0.94 [0.91-0.98] with 94% (75/80) sensitivity and 77%\n(92/120) specificity at PI-RADS >= 3. The deep radiomics model at a tumor\nprobability cut-off >= 0.76 achieved 0.91 [0.86-0.95] AUROC with 90% (72/80)\nsensitivity and 73% (87/120) specificity, not significantly different (p =\n0.068) from PI-RADS. On the lesion level, PI-RADS cut-off >= 3 had 84% (91/108)\nsensitivity at 0.2 (40/200) false positives per patient, while deep radiomics\nattained 68% (73/108) sensitivity at the same false positive rate. Conclusion:\nDeep radiomics machine learning model achieved comparable performance to\nPI-RADS assessment in csPCa detection at the patient-level but not at the\nlesion-level.\n","authors":["G. A. Nketiah","M. R. Sunoqrot","E. Sandsmark","S. Langørgen","K. M. Selnæs","H. Bertilsson","M. Elschot","T. F. Bathen"],"pdf_url":"https://arxiv.org/pdf/2410.16238v1.pdf","comment":"20 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.16236v1","updated":"2024-10-21T17:41:28Z","published":"2024-10-21T17:41:28Z","title":"LLaVA-KD: A Framework of Distilling Multimodal Large Language Models","summary":"  The success of Large Language Models (LLM) has led researchers to explore\nMultimodal Large Language Models (MLLM) for unified visual and linguistic\nunderstanding. However, the increasing model size and computational complexity\nof MLLM limit their use in resource-constrained environments. Small-scale MLLM\n(s-MLLM) aims to retain the capabilities of the large-scale model (l-MLLM)\nwhile reducing computational demands, but resulting in a significant decline in\nperformance. To address the aforementioned issues, we propose a novel LLaVA-KD\nframework to transfer knowledge from l-MLLM to s-MLLM. Specifically, we\nintroduce Multimodal Distillation (MDist) to minimize the divergence between\nthe visual-textual output distributions of l-MLLM and s-MLLM, and Relation\nDistillation (RDist) to transfer l-MLLM's ability to model correlations between\nvisual features. Additionally, we propose a three-stage training scheme to\nfully exploit the potential of s-MLLM: 1) Distilled Pre-Training to align\nvisual-textual representations, 2) Supervised Fine-Tuning to equip the model\nwith multimodal understanding, and 3) Distilled Fine-Tuning to further transfer\nl-MLLM capabilities. Our approach significantly improves performance without\naltering the small model's architecture. Extensive experiments and ablation\nstudies validate the effectiveness of each proposed component. Code will be\navailable at https://github.com/caiyuxuan1120/LLaVA-KD.\n","authors":["Yuxuan Cai","Jiangning Zhang","Haoyang He","Xinwei He","Ao Tong","Zhenye Gan","Chengjie Wang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2410.16236v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.16227v1","updated":"2024-10-21T17:32:36Z","published":"2024-10-21T17:32:36Z","title":"Managing Bandwidth: The Key to Cloud-Assisted Autonomous Driving","summary":"  Prevailing wisdom asserts that one cannot rely on the cloud for critical\nreal-time control systems like self-driving cars. We argue that we can, and\nmust. Following the trends of increasing model sizes, improvements in hardware,\nand evolving mobile networks, we identify an opportunity to offload parts of\ntime-sensitive and latency-critical compute to the cloud. Doing so requires\ncarefully allocating bandwidth to meet strict latency SLOs, while maximizing\nbenefit to the car.\n","authors":["Alexander Krentsel","Peter Schafhalter","Joseph E. Gonzalez","Sylvia Ratnasamy","Scott Shenker","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.16227v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2406.01583v2","updated":"2024-10-21T17:25:44Z","published":"2024-06-03T17:58:43Z","title":"Decomposing and Interpreting Image Representations via Text in ViTs\n  Beyond CLIP","summary":"  Recent work has explored how individual components of the CLIP-ViT model\ncontribute to the final representation by leveraging the shared image-text\nrepresentation space of CLIP. These components, such as attention heads and\nMLPs, have been shown to capture distinct image features like shape, color or\ntexture. However, understanding the role of these components in arbitrary\nvision transformers (ViTs) is challenging. To this end, we introduce a general\nframework which can identify the roles of various components in ViTs beyond\nCLIP. Specifically, we (a) automate the decomposition of the final\nrepresentation into contributions from different model components, and (b)\nlinearly map these contributions to CLIP space to interpret them via text.\nAdditionally, we introduce a novel scoring function to rank components by their\nimportance with respect to specific features. Applying our framework to various\nViT variants (e.g. DeiT, DINO, DINOv2, Swin, MaxViT), we gain insights into the\nroles of different components concerning particular image features. These\ninsights facilitate applications such as image retrieval using text\ndescriptions or reference images, visualizing token importance heatmaps, and\nmitigating spurious correlations. We release our code to reproduce the\nexperiments at https://github.com/SriramB-98/vit-decompose\n","authors":["Sriram Balasubramanian","Samyadeep Basu","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2406.01583v2.pdf","comment":"NeurIPS 2024, 31 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.16198v1","updated":"2024-10-21T17:00:06Z","published":"2024-10-21T17:00:06Z","title":"Improve Vision Language Model Chain-of-thought Reasoning","summary":"  Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial\nfor improving interpretability and trustworthiness. However, current training\nrecipes lack robust CoT reasoning data, relying on datasets dominated by short\nannotations with minimal rationales. In this work, we show that training VLM on\nshort answers does not generalize well to reasoning tasks that require more\ndetailed responses. To address this, we propose a two-fold approach. First, we\ndistill rationales from GPT-4o model to enrich the training data and fine-tune\nVLMs, boosting their CoT performance. Second, we apply reinforcement learning\nto further calibrate reasoning quality. Specifically, we construct positive\n(correct) and negative (incorrect) pairs of model-generated reasoning chains,\nby comparing their predictions with annotated short answers. Using this\npairwise data, we apply the Direct Preference Optimization algorithm to refine\nthe model's reasoning abilities. Our experiments demonstrate significant\nimprovements in CoT reasoning on benchmark datasets and better generalization\nto direct answer prediction as well. This work emphasizes the importance of\nincorporating detailed rationales in training and leveraging reinforcement\nlearning to strengthen the reasoning capabilities of VLMs.\n","authors":["Ruohong Zhang","Bowen Zhang","Yanghao Li","Haotian Zhang","Zhiqing Sun","Zhe Gan","Yinfei Yang","Ruoming Pang","Yiming Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16198v1.pdf","comment":"10 pages + appendix"},{"id":"http://arxiv.org/abs/2410.16190v1","updated":"2024-10-21T16:52:44Z","published":"2024-10-21T16:52:44Z","title":"Training Better Deep Learning Models Using Human Saliency","summary":"  This work explores how human judgement about salient regions of an image can\nbe introduced into deep convolutional neural network (DCNN) training.\nTraditionally, training of DCNNs is purely data-driven. This often results in\nlearning features of the data that are only coincidentally correlated with\nclass labels. Human saliency can guide network training using our proposed new\ncomponent of the loss function that ConveYs Brain Oversight to Raise\nGeneralization (CYBORG) and penalizes the model for using non-salient regions.\nThis mechanism produces DCNNs achieving higher accuracy and generalization\ncompared to using the same training data without human salience. Experimental\nresults demonstrate that CYBORG applies across multiple network architectures\nand problem domains (detection of synthetic faces, iris presentation attacks\nand anomalies in chest X-rays), while requiring significantly less data than\ntraining without human saliency guidance. Visualizations show that\nCYBORG-trained models' saliency is more consistent across independent training\nruns than traditionally-trained models, and also in better agreement with\nhumans. To lower the cost of collecting human annotations, we also explore\nusing deep learning to provide automated annotations. CYBORG training of CNNs\naddresses important issues such as reducing the appetite for large training\nsets, increasing interpretability, and reducing fragility by generalizing\nbetter to new types of data.\n","authors":["Aidan Boyd","Patrick Tinsley","Kevin W. Bowyer","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2410.16190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16177v1","updated":"2024-10-21T16:43:16Z","published":"2024-10-21T16:43:16Z","title":"A Framework for Evaluating Predictive Models Using Synthetic Image\n  Covariates and Longitudinal Data","summary":"  We present a novel framework for synthesizing patient data with complex\ncovariates (e.g., eye scans) paired with longitudinal observations (e.g.,\nvisual acuity over time), addressing privacy concerns in healthcare research.\nOur approach introduces controlled association in latent spaces generating each\ndata modality, enabling the creation of complex covariate-longitudinal\nobservation pairs. This framework facilitates the development of predictive\nmodels and provides openly available benchmarking datasets for healthcare\nresearch. We demonstrate our framework using optical coherence tomography (OCT)\nscans, though it is applicable across domains. Using 109,309 2D OCT scan\nslices, we trained an image generative model combining a variational\nautoencoder and a diffusion model. Longitudinal observations were simulated\nusing a nonlinear mixed effect (NLME) model from a low-dimensional space of\nrandom effects. We generated 1.1M OCT scan slices paired with five sets of\nlongitudinal observations at controlled association levels (100%, 50%, 10%,\n5.26%, and 2% of between-subject variability). To assess the framework, we\nmodeled synthetic longitudinal observations with another NLME model, computed\nempirical Bayes estimates of random effects, and trained a ResNet to predict\nthese estimates from synthetic OCT scans. We then incorporated ResNet\npredictions into the NLME model for patient-individualized predictions.\nPrediction accuracy on withheld data declined as intended with reduced\nassociation between images and longitudinal measurements. Notably, in all but\nthe 2% case, we achieved within 50% of the theoretical best possible prediction\non withheld data, demonstrating our ability to detect even weak signals. This\nconfirms the effectiveness of our framework in generating synthetic data with\ncontrolled levels of association, providing a valuable tool for healthcare\nresearch.\n","authors":["Simon Deltadahl","Andreu Vall","Vijay Ivaturi","Niklas Korsbo"],"pdf_url":"https://arxiv.org/pdf/2410.16177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16166v1","updated":"2024-10-21T16:32:41Z","published":"2024-10-21T16:32:41Z","title":"Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM\n  Pretraining","summary":"  Multimodal large language models (MLLMs) have made significant strides by\nintegrating visual and textual modalities. A critical factor in training MLLMs\nis the quality of image-text pairs within multimodal pretraining datasets.\nHowever, $\\textit {de facto}$ filter-based data quality enhancement paradigms\noften discard a substantial portion of high-quality image data due to\ninadequate semantic alignment between images and texts, leading to\ninefficiencies in data utilization and scalability. In this paper, we propose\nthe Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically\nassesses and enhances the quality of image-text pairs. AITQE employs a text\nrewriting mechanism for low-quality pairs and incorporates a negative sample\nlearning strategy to improve evaluative capabilities by integrating\ndeliberately selected low-quality samples during training. Unlike prior\napproaches that significantly alter text distributions, our method minimally\nadjusts text to preserve data volume while enhancing quality. Experimental\nresults demonstrate that AITQE surpasses existing methods on various benchmark,\neffectively leveraging raw data and scaling efficiently with increasing data\nvolumes. We hope our work will inspire future works. The code and model are\navailable at: https://github.com/hanhuang22/AITQE.\n","authors":["Han Huang","Yuqi Huo","Zijia Zhao","Haoyu Lu","Shu Wu","Bingning Wang","Qiang Liu","Weipeng Chen","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16163v1","updated":"2024-10-21T16:30:29Z","published":"2024-10-21T16:30:29Z","title":"Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large\n  Multimodal Models","summary":"  Large Multimodal Models (LMMs) have achieved significant breakthroughs in\nvarious vision-language and vision-centric tasks based on auto-regressive\nmodeling. However, these models typically focus on either vision-centric tasks,\nsuch as visual grounding and region description, or vision-language tasks, like\nimage caption and multi-scenario VQAs. None of the LMMs have yet\ncomprehensively unified both types of tasks within a single model, as seen in\nLarge Language Models in the natural language processing field. Furthermore,\neven with abundant multi-task instruction-following data, directly stacking\nthese data for universal capabilities extension remains challenging. To address\nthese issues, we introduce a novel multi-dimension curated and consolidated\nmultimodal dataset, named CCMD-8M, which overcomes the data barriers of\nunifying vision-centric and vision-language tasks through multi-level data\ncuration and multi-task consolidation. More importantly, we present Griffon-G,\na general large multimodal model that addresses both vision-centric and\nvision-language tasks within a single end-to-end paradigm. Griffon-G resolves\nthe training collapse issue encountered during the joint optimization of these\ntasks, achieving better training efficiency. Evaluations across multimodal\nbenchmarks, general Visual Question Answering (VQA) tasks, scene text-centric\nVQA tasks, document-related VQA tasks, Referring Expression Comprehension, and\nobject detection demonstrate that Griffon-G surpasses the advanced LMMs and\nachieves expert-level performance in complicated vision-centric tasks.\n","authors":["Yufei Zhan","Hongyin Zhao","Yousong Zhu","Fan Yang","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16163v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Codes and data will be later released at\n  https://github.com/jefferyZhan/Griffon"},{"id":"http://arxiv.org/abs/2410.16162v1","updated":"2024-10-21T16:26:09Z","published":"2024-10-21T16:26:09Z","title":"Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models\n  Elicits Generalization to Composite Spatial Reasoning","summary":"  Vision language models (VLMs) have demonstrated impressive performance across\na wide range of downstream tasks. However, their proficiency in spatial\nreasoning remains limited, despite its crucial role in tasks involving\nnavigation and interaction with physical environments. Specifically, much of\nthe spatial reasoning in these tasks occurs in two-dimensional (2D)\nenvironments, and our evaluation reveals that state-of-the-art VLMs frequently\ngenerate implausible and incorrect responses to composite spatial reasoning\nproblems, including simple pathfinding tasks that humans can solve effortlessly\nat a glance. To address this, we explore an effective approach to enhance 2D\nspatial reasoning within VLMs by training the model on basic spatial\ncapabilities. We begin by disentangling the key components of 2D spatial\nreasoning: direction comprehension, distance estimation, and localization. Our\ncentral hypothesis is that mastering these basic spatial capabilities can\nsignificantly enhance a model's performance on composite spatial tasks\nrequiring advanced spatial understanding and combinatorial problem-solving. To\ninvestigate this hypothesis, we introduce Sparkle, a framework that fine-tunes\nVLMs on these three basic spatial capabilities by synthetic data generation and\ntargeted supervision to form an instruction dataset for each capability. Our\nexperiments demonstrate that VLMs fine-tuned with Sparkle achieve significant\nperformance gains, not only in the basic tasks themselves but also in\ngeneralizing to composite and out-of-distribution spatial reasoning tasks\n(e.g., improving from 13.5% to 40.0% on the shortest path problem). These\nfindings underscore the effectiveness of mastering basic spatial capabilities\nin enhancing composite spatial problem-solving, offering insights for improving\nVLMs' spatial reasoning capabilities.\n","authors":["Yihong Tang","Ao Qu","Zhaokai Wang","Dingyi Zhuang","Zhaofeng Wu","Wei Ma","Shenhao Wang","Yunhan Zheng","Zhan Zhao","Jinhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16159v1","updated":"2024-10-21T16:22:19Z","published":"2024-10-21T16:22:19Z","title":"Metric as Transform: Exploring beyond Affine Transform for Interpretable\n  Neural Network","summary":"  Artificial Neural Networks of varying architectures are generally paired with\naffine transformation at the core. However, we find dot product neurons with\nglobal influence less interpretable as compared to local influence of euclidean\ndistance (as used in Radial Basis Function Network). In this work, we explore\nthe generalization of dot product neurons to $l^p$-norm, metrics, and beyond.\nWe find that metrics as transform performs similarly to affine transform when\nused in MultiLayer Perceptron or Convolutional Neural Network. Moreover, we\nexplore various properties of Metrics, compare it with Affine, and present\nmultiple cases where metrics seem to provide better interpretability. We\ndevelop an interpretable local dictionary based Neural Networks and use it to\nunderstand and reject adversarial examples.\n","authors":["Suman Sapkota"],"pdf_url":"https://arxiv.org/pdf/2410.16159v1.pdf","comment":"22 pages, 20 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.16153v1","updated":"2024-10-21T16:19:41Z","published":"2024-10-21T16:19:41Z","title":"Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages","summary":"  Despite recent advances in multimodal large language models (MLLMs), their\ndevelopment has predominantly focused on English- and western-centric datasets\nand tasks, leaving most of the world's languages and diverse cultural contexts\nunderrepresented. This paper introduces Pangea, a multilingual multimodal LLM\ntrained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.\nPangeaIns features: 1) high-quality English instructions, 2) carefully\nmachine-translated instructions, and 3) culturally relevant multimodal tasks to\nensure cross-cultural coverage. To rigorously assess models' capabilities, we\nintroduce PangeaBench, a holistic evaluation suite encompassing 14 datasets\ncovering 47 languages. Results show that Pangea significantly outperforms\nexisting open-source models in multilingual settings and diverse cultural\ncontexts. Ablation studies further reveal the importance of English data\nproportions, language popularity, and the number of multimodal training samples\non overall performance. We fully open-source our data, code, and trained\ncheckpoints, to facilitate the development of inclusive and robust multilingual\nMLLMs, promoting equity and accessibility across a broader linguistic and\ncultural spectrum.\n","authors":["Xiang Yue","Yueqi Song","Akari Asai","Seungone Kim","Jean de Dieu Nyandwi","Simran Khanuja","Anjali Kantharuban","Lintang Sutawika","Sathyanarayanan Ramamoorthy","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16153v1.pdf","comment":"52 pages, 27 figures"},{"id":"http://arxiv.org/abs/2410.16152v1","updated":"2024-10-21T16:19:34Z","published":"2024-10-21T16:19:34Z","title":"Warped Diffusion: Solving Video Inverse Problems with Image Diffusion\n  Models","summary":"  Using image models naively for solving inverse video problems often suffers\nfrom flickering, texture-sticking, and temporal inconsistency in generated\nvideos. To tackle these problems, in this paper, we view frames as continuous\nfunctions in the 2D space, and videos as a sequence of continuous warping\ntransformations between different frames. This perspective allows us to train\nfunction space diffusion models only on images and utilize them to solve\ntemporally correlated inverse problems. The function space diffusion models\nneed to be equivariant with respect to the underlying spatial transformations.\nTo ensure temporal consistency, we introduce a simple post-hoc test-time\nguidance towards (self)-equivariant solutions. Our method allows us to deploy\nstate-of-the-art latent diffusion models such as Stable Diffusion XL to solve\nvideo inverse problems. We demonstrate the effectiveness of our method for\nvideo inpainting and $8\\times$ video super-resolution, outperforming existing\ntechniques based on noise transformations. We provide generated video results:\nhttps://giannisdaras.github.io/warped\\_diffusion.github.io/.\n","authors":["Giannis Daras","Weili Nie","Karsten Kreis","Alex Dimakis","Morteza Mardani","Nikola Borislavov Kovachki","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2410.16152v1.pdf","comment":"Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.18406v2","updated":"2024-10-21T16:18:37Z","published":"2024-05-28T17:46:36Z","title":"RACCooN: A Versatile Instructional Video Editing Framework with\n  Auto-Generated Narratives","summary":"  Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. (3) RACCooN also plans to imagine new objects in a\ngiven video, so users simply prompt the model to receive a detailed video\nediting plan for complex video editing. The proposed framework demonstrates\nimpressive versatile capabilities in video-to-paragraph generation, video\ncontent editing, and can be incorporated into other SoTA video generative\nmodels for further enhancement.\n","authors":["Jaehong Yoon","Shoubin Yu","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2405.18406v2.pdf","comment":"The first two authors contribute equally. Project Page:\n  https://raccoon-mllm-gen.github.io/"},{"id":"http://arxiv.org/abs/2410.16146v1","updated":"2024-10-21T16:17:01Z","published":"2024-10-21T16:17:01Z","title":"Towards Combating Frequency Simplicity-biased Learning for Domain\n  Generalization","summary":"  Domain generalization methods aim to learn transferable knowledge from source\ndomains that can generalize well to unseen target domains. Recent studies show\nthat neural networks frequently suffer from a simplicity-biased learning\nbehavior which leads to over-reliance on specific frequency sets, namely as\nfrequency shortcuts, instead of semantic information, resulting in poor\ngeneralization performance. Despite previous data augmentation techniques\nsuccessfully enhancing generalization performances, they intend to apply more\nfrequency shortcuts, thereby causing hallucinations of generalization\nimprovement. In this paper, we aim to prevent such learning behavior of\napplying frequency shortcuts from a data-driven perspective. Given the\ntheoretical justification of models' biased learning behavior on different\nspatial frequency components, which is based on the dataset frequency\nproperties, we argue that the learning behavior on various frequency components\ncould be manipulated by changing the dataset statistical structure in the\nFourier domain. Intuitively, as frequency shortcuts are hidden in the dominant\nand highly dependent frequencies of dataset structure, dynamically perturbating\nthe over-reliance frequency components could prevent the application of\nfrequency shortcuts. To this end, we propose two effective data augmentation\nmodules designed to collaboratively and adaptively adjust the frequency\ncharacteristic of the dataset, aiming to dynamically influence the learning\nbehavior of the model and ultimately serving as a strategy to mitigate shortcut\nlearning. Code is available at AdvFrequency\n(https://github.com/C0notSilly/AdvFrequency).\n","authors":["Xilin He","Jingyu Hu","Qinliang Lin","Cheng Luo","Weicheng Xie","Siyang Song","Muhammad Haris Khan","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2410.16146v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16143v1","updated":"2024-10-21T16:14:50Z","published":"2024-10-21T16:14:50Z","title":"An Explainable Contrastive-based Dilated Convolutional Network with\n  Transformer for Pediatric Pneumonia Detection","summary":"  Pediatric pneumonia remains a significant global threat, posing a larger\nmortality risk than any other communicable disease. According to UNICEF, it is\na leading cause of mortality in children under five and requires prompt\ndiagnosis. Early diagnosis using chest radiographs is the prevalent standard,\nbut limitations include low radiation levels in unprocessed images and data\nimbalance issues. This necessitates the development of efficient,\ncomputer-aided diagnosis techniques. To this end, we propose a novel\nEXplainable Contrastive-based Dilated Convolutional Network with Transformer\n(XCCNet) for pediatric pneumonia detection. XCCNet harnesses the spatial power\nof dilated convolutions and the global insights from contrastive-based\ntransformers for effective feature refinement. A robust chest X-ray processing\nmodule tackles low-intensity radiographs, while adversarial-based data\naugmentation mitigates the skewed distribution of chest X-rays in the dataset.\nFurthermore, we actively integrate an explainability approach through feature\nvisualization, directly aligning it with the attention region that pinpoints\nthe presence of pneumonia or normality in radiographs. The efficacy of XCCNet\nis comprehensively assessed on four publicly available datasets. Extensive\nperformance evaluation demonstrates the superiority of XCCNet compared to\nstate-of-the-art methods.\n","authors":["Chandravardhan Singh Raghaw","Parth Shirish Bhore","Mohammad Zia Ur Rehman","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.16143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00299v4","updated":"2024-10-21T15:56:23Z","published":"2024-06-29T03:37:29Z","title":"Human-Agent Joint Learning for Efficient Robot Manipulation Skill\n  Acquisition","summary":"  Employing a teleoperation system for gathering demonstrations offers the\npotential for more efficient learning of robot manipulation. However,\nteleoperating a robot arm equipped with a dexterous hand or gripper, via a\nteleoperation system presents inherent challenges due to the task's high\ndimensionality, complexity of motion, and differences between physiological\nstructures. In this study, we introduce a novel system for joint learning\nbetween human operators and robots, that enables human operators to share\ncontrol of a robot end-effector with a learned assistive agent, simplifies the\ndata collection process, and facilitates simultaneous human demonstration\ncollection and robot manipulation training. As data accumulates, the assistive\nagent gradually learns. Consequently, less human effort and attention are\nrequired, enhancing the efficiency of the data collection process. It also\nallows the human operator to adjust the control ratio to achieve a trade-off\nbetween manual and automated control. We conducted experiments in both\nsimulated environments and physical real-world settings. Through user studies\nand quantitative evaluations, it is evident that the proposed system could\nenhance data collection efficiency and reduce the need for human adaptation\nwhile ensuring the collected data is of sufficient quality for downstream\ntasks. \\textit{For more details, please refer to our webpage\nhttps://norweig1an.github.io/HAJL.github.io/.\n","authors":["Shengcheng Luo","Quanquan Peng","Jun Lv","Kaiwen Hong","Katherine Rose Driggs-Campbell","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2407.00299v4.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.08797v2","updated":"2024-10-21T15:45:23Z","published":"2024-10-11T13:31:28Z","title":"CoTCoNet: An Optimized Coupled Transformer-Convolutional Network with an\n  Adaptive Graph Reconstruction for Leukemia Detection","summary":"  Swift and accurate blood smear analysis is an effective diagnostic method for\nleukemia and other hematological malignancies. However, manual leukocyte count\nand morphological evaluation using a microscope is time-consuming and prone to\nerrors. Conventional image processing methods also exhibit limitations in\ndifferentiating cells due to the visual similarity between malignant and benign\ncell morphology. This limitation is further compounded by the skewed training\ndata that hinders the extraction of reliable and pertinent features. In\nresponse to these challenges, we propose an optimized Coupled Transformer\nConvolutional Network (CoTCoNet) framework for the classification of leukemia,\nwhich employs a well-designed transformer integrated with a deep convolutional\nnetwork to effectively capture comprehensive global features and scalable\nspatial patterns, enabling the identification of complex and large-scale\nhematological features. Further, the framework incorporates a graph-based\nfeature reconstruction module to reveal the hidden or unobserved hard-to-see\nbiological features of leukocyte cells and employs a Population-based\nMeta-Heuristic Algorithm for feature selection and optimization. To mitigate\ndata imbalance issues, we employ a synthetic leukocyte generator. In the\nevaluation phase, we initially assess CoTCoNet on a dataset containing 16,982\nannotated cells, and it achieves remarkable accuracy and F1-Score rates of\n0.9894 and 0.9893, respectively. To broaden the generalizability of our model,\nwe evaluate it across four publicly available diverse datasets, which include\nthe aforementioned dataset. This evaluation demonstrates that our method\noutperforms current state-of-the-art approaches. We also incorporate an\nexplainability approach in the form of feature visualization closely aligned\nwith cell annotations to provide a deeper understanding of the framework.\n","authors":["Chandravardhan Singh Raghaw","Arnav Sharma","Shubhi Bansal","Mohammad Zia Ur Rehman","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.08797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16116v1","updated":"2024-10-21T15:42:47Z","published":"2024-10-21T15:42:47Z","title":"Multimodal Flare Forecasting with Deep Learning","summary":"  Solar flare forecasting mainly relies on photospheric magnetograms and\nassociated physical features to predict forthcoming flares. However, it is\nbelieved that flare initiation mechanisms often originate in the chromosphere\nand the lower corona. In this study, we employ deep learning as a purely\ndata-driven approach to compare the predictive capabilities of chromospheric\nand coronal UV and EUV emissions across different wavelengths with those of\nphotospheric line-of-sight magnetograms. Our findings indicate that individual\nEUV wavelengths can provide discriminatory power comparable or better to that\nof line-of-sight magnetograms. Moreover, we identify simple multimodal neural\nnetwork architectures that consistently outperform single-input models, showing\ncomplementarity between the flare precursors that can be extracted from the\ndistinct layers of the solar atmosphere. To mitigate potential biases from\nknown misattributions in Active Region flare catalogs, our models are trained\nand evaluated using full-disk images and a comprehensive flare event catalog at\nthe full-disk level. We introduce a deep-learning architecture suited for\nextracting temporal features from full-disk videos.\n","authors":["Grégoire Francisco","Sabrina Guastavino","Teresa Barata","João Fernandes","Dario Del Moro"],"pdf_url":"https://arxiv.org/pdf/2410.16116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16115v1","updated":"2024-10-21T15:42:27Z","published":"2024-10-21T15:42:27Z","title":"Increasing Interpretability of Neural Networks By Approximating Human\n  Visual Saliency","summary":"  Understanding specifically where a model focuses on within an image is\ncritical for human interpretability of the decision-making process. Deep\nlearning-based solutions are prone to learning coincidental correlations in\ntraining datasets, causing over-fitting and reducing the explainability. Recent\nadvances have shown that guiding models to human-defined regions of saliency\nwithin individual images significantly increases performance and\ninterpretability. Human-guided models also exhibit greater generalization\ncapabilities, as coincidental dataset features are avoided. Results show that\nmodels trained with saliency incorporation display an increase in\ninterpretability of up to 30% over models trained without saliency information.\nThe collection of this saliency information, however, can be costly, laborious\nand in some cases infeasible. To address this limitation, we propose a\ncombination strategy of saliency incorporation and active learning to reduce\nthe human annotation data required by 80% while maintaining the\ninterpretability and performance increase from human saliency. Extensive\nexperimentation outlines the effectiveness of the proposed approach across five\npublic datasets and six active learning criteria.\n","authors":["Aidan Boyd","Mohamed Trabelsi","Huseyin Uzunalioglu","Dan Kushnir"],"pdf_url":"https://arxiv.org/pdf/2410.16115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16095v1","updated":"2024-10-21T15:20:02Z","published":"2024-10-21T15:20:02Z","title":"LMHaze: Intensity-aware Image Dehazing with a Large-scale\n  Multi-intensity Real Haze Dataset","summary":"  Image dehazing has drawn a significant attention in recent years.\nLearning-based methods usually require paired hazy and corresponding ground\ntruth (haze-free) images for training. However, it is difficult to collect\nreal-world image pairs, which prevents developments of existing methods.\nAlthough several works partially alleviate this issue by using synthetic\ndatasets or small-scale real datasets. The haze intensity distribution bias and\nscene homogeneity in existing datasets limit the generalization ability of\nthese methods, particularly when encountering images with previously unseen\nhaze intensities. In this work, we present LMHaze, a large-scale, high-quality\nreal-world dataset. LMHaze comprises paired hazy and haze-free images captured\nin diverse indoor and outdoor environments, spanning multiple scenarios and\nhaze intensities. It contains over 5K high-resolution image pairs, surpassing\nthe size of the biggest existing real-world dehazing dataset by over 25 times.\nMeanwhile, to better handle images with different haze intensities, we propose\na mixture-of-experts model based on Mamba (MoE-Mamba) for dehazing, which\ndynamically adjusts the model parameters according to the haze intensity.\nMoreover, with our proposed dataset, we conduct a new large multimodal model\n(LMM)-based benchmark study to simulate human perception for evaluating dehazed\nimages. Experiments demonstrate that LMHaze dataset improves the dehazing\nperformance in real scenarios and our dehazing method provides better results\ncompared to state-of-the-art methods.\n","authors":["Ruikun Zhang","Hao Yang","Yan Yang","Ying Fu","Liyuan Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08381v4","updated":"2024-10-21T15:18:39Z","published":"2024-08-15T18:54:31Z","title":"Pre-processing and Compression: Understanding Hidden Representation\n  Refinement Across Imaging Domains via Intrinsic Dimension","summary":"  In recent years, there has been interest in how geometric properties such as\nintrinsic dimension (ID) of a neural network's hidden representations change\nthrough its layers, and how such properties are predictive of important model\nbehavior such as generalization ability. However, evidence has begun to emerge\nthat such behavior can change significantly depending on the domain of the\nnetwork's training data, such as natural versus medical images. Here, we\nfurther this inquiry by exploring how the ID of a network's learned\nrepresentations changes through its layers, in essence, characterizing how the\nnetwork successively refines the information content of input data to be used\nfor predictions. Analyzing eleven natural and medical image datasets across six\nnetwork architectures, we find that how ID changes through the network differs\nnoticeably between natural and medical image models. Specifically, medical\nimage models peak in representation ID earlier in the network, implying a\ndifference in the image features and their abstractness that are typically used\nfor downstream tasks in these domains. Additionally, we discover a strong\ncorrelation of this peak representation ID with the ID of the data in its input\nspace, implying that the intrinsic information content of a model's learned\nrepresentations is guided by that of the data it was trained on. Overall, our\nfindings emphasize notable discrepancies in network behavior between natural\nand non-natural imaging domains regarding hidden representation information\ncontent, and provide further insights into how a network's learned features are\nshaped by its training data.\n","authors":["Nicholas Konz","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.08381v4.pdf","comment":"Published in NeurIPS 2024 Workshop on Scientific Methods for\n  Understanding Deep Learning (SciForDL)"},{"id":"http://arxiv.org/abs/2410.16093v1","updated":"2024-10-21T15:16:00Z","published":"2024-10-21T15:16:00Z","title":"Final Report for CHESS: Cloud, High-Performance Computing, and Edge for\n  Science and Security","summary":"  Automating the theory-experiment cycle requires effective distributed\nworkflows that utilize a computing continuum spanning lab instruments, edge\nsensors, computing resources at multiple facilities, data sets distributed\nacross multiple information sources, and potentially cloud. Unfortunately, the\nobvious methods for constructing continuum platforms, orchestrating workflow\ntasks, and curating datasets over time fail to achieve scientific requirements\nfor performance, energy, security, and reliability. Furthermore, achieving the\nbest use of continuum resources depends upon the efficient composition and\nexecution of workflow tasks, i.e., combinations of numerical solvers, data\nanalytics, and machine learning. Pacific Northwest National Laboratory's LDRD\n\"Cloud, High-Performance Computing (HPC), and Edge for Science and Security\"\n(CHESS) has developed a set of interrelated capabilities for enabling\ndistributed scientific workflows and curating datasets. This report describes\nthe results and successes of CHESS from the perspective of open science.\n","authors":["Nathan Tallent","Jan Strube","Luanzheng Guo","Hyungro Lee","Jesun Firoz","Sayan Ghosh","Bo Fang","Oceane Bel","Steven Spurgeon","Sarah Akers","Christina Doty","Erol Cromwell"],"pdf_url":"https://arxiv.org/pdf/2410.16093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16063v1","updated":"2024-10-21T14:44:08Z","published":"2024-10-21T14:44:08Z","title":"Integrated Image-Text Based on Semi-supervised Learning for Small Sample\n  Instance Segmentation","summary":"  Small sample instance segmentation is a very challenging task, and many\nexisting methods follow the training strategy of meta-learning which pre-train\nmodels on support set and fine-tune on query set. The pre-training phase, which\nis highly task related, requires a significant amount of additional training\ntime and the selection of datasets with close proximity to ensure\neffectiveness. The article proposes a novel small sample instance segmentation\nsolution from the perspective of maximizing the utilization of existing\ninformation without increasing annotation burden and training costs. The\nproposed method designs two modules to address the problems encountered in\nsmall sample instance segmentation. First, it helps the model fully utilize\nunlabeled data by learning to generate pseudo labels, increasing the number of\navailable samples. Second, by integrating the features of text and image, more\naccurate classification results can be obtained. These two modules are suitable\nfor box-free and box-dependent frameworks. In the way, the proposed method not\nonly improves the performance of small sample instance segmentation, but also\ngreatly reduce reliance on pre-training. We have conducted experiments in three\ndatasets from different scenes: on land, underwater and under microscope. As\nevidenced by our experiments, integrated image-text corrects the confidence of\nclassification, and pseudo labels help the model obtain preciser masks. All the\nresults demonstrate the effectiveness and superiority of our method.\n","authors":["Ruting Chi","Zhiyi Huang","Yuexing Han"],"pdf_url":"https://arxiv.org/pdf/2410.16063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11792v2","updated":"2024-10-21T14:42:16Z","published":"2024-03-18T13:50:35Z","title":"SETA: Semantic-Aware Token Augmentation for Domain Generalization","summary":"  Domain generalization (DG) aims to enhance the model robustness against\ndomain shifts without accessing target domains. A prevalent category of methods\nfor DG is data augmentation, which focuses on generating virtual samples to\nsimulate domain shifts. However, existing augmentation techniques in DG are\nmainly tailored for convolutional neural networks (CNNs), with limited\nexploration in token-based architectures, i.e., vision transformer (ViT) and\nmulti-layer perceptrons (MLP) models. In this paper, we study the impact of\nprior CNN-based augmentation methods on token-based models, revealing their\nperformance is suboptimal due to the lack of incentivizing the model to learn\nholistic shape information. To tackle the issue, we propose the SEmantic-aware\nToken Augmentation (SETA) method. SETA transforms token features by perturbing\nlocal edge cues while preserving global shape features, thereby enhancing the\nmodel learning of shape information. To further enhance the generalization\nability of the model, we introduce two stylized variants of our method combined\nwith two state-of-the-art style augmentation methods in DG. We provide a\ntheoretical insight into our method, demonstrating its effectiveness in\nreducing the generalization risk bound. Comprehensive experiments on five\nbenchmarks prove that our method achieves SOTA performances across various ViT\nand MLP architectures. Our code is available at\nhttps://github.com/lingeringlight/SETA.\n","authors":["Jintao Guo","Lei Qi","Yinghuan Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2403.11792v2.pdf","comment":"Accepted by IEEE TIP 2024. The code is available at\n  https://github.com/lingeringlight/SETA"},{"id":"http://arxiv.org/abs/2410.16057v1","updated":"2024-10-21T14:36:36Z","published":"2024-10-21T14:36:36Z","title":"Label Filling via Mixed Supervision for Medical Image Segmentation from\n  Noisy Annotations","summary":"  The success of medical image segmentation usually requires a large number of\nhigh-quality labels. But since the labeling process is usually affected by the\nraters' varying skill levels and characteristics, the estimated masks provided\nby different raters usually suffer from high inter-rater variability. In this\npaper, we propose a simple yet effective Label Filling framework, termed as\nLF-Net, predicting the groundtruth segmentation label given only noisy\nannotations during training. The fundamental idea of label filling is to\nsupervise the segmentation model by a subset of pixels with trustworthy labels,\nmeanwhile filling labels of other pixels by mixed supervision. More concretely,\nwe propose a qualified majority voting strategy, i.e., a threshold voting\nscheme is designed to model agreement among raters and the majority-voted\nlabels of the selected subset of pixels are regarded as supervision. To fill\nlabels of other pixels, two types of mixed auxiliary supervision are proposed:\na soft label learned from intrinsic structures of noisy annotations, and\nraters' characteristics labels which propagate individual rater's\ncharacteristics information. LF-Net has two main advantages. 1) Training with\ntrustworthy pixels incorporates training with confident supervision, guiding\nthe direction of groundtruth label learning. 2) Two types of mixed supervision\nprevent over-fitting issues when the network is supervised by a subset of\npixels, and guarantee high fidelity with the true label. Results on five\ndatasets of diverse imaging modalities show that our LF-Net boosts segmentation\naccuracy in all datasets compared with state-of-the-art methods, with even a 7%\nimprovement in DSC for MS lesion segmentation.\n","authors":["Ming Li","Wei Shen","Qingli Li","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06446v2","updated":"2024-10-21T14:28:18Z","published":"2024-10-09T01:12:07Z","title":"Machine Unlearning in Forgettability Sequence","summary":"  Machine unlearning (MU) is becoming a promising paradigm to achieve the\n\"right to be forgotten\", where the training trace of any chosen data points\ncould be eliminated, while maintaining the model utility on general testing\nsamples after unlearning. With the advancement of forgetting research, many\nfundamental open questions remain unanswered: do different samples exhibit\nvarying levels of difficulty in being forgotten? Further, does the sequence in\nwhich samples are forgotten, determined by their respective difficulty levels,\ninfluence the performance of forgetting algorithms? In this paper, we identify\nkey factor affecting unlearning difficulty and the performance of unlearning\nalgorithms. We find that samples with higher privacy risks are more likely to\nbe unlearning, indicating that the unlearning difficulty varies among different\nsamples which motives a more precise unlearning mode. Built upon this insight,\nwe propose a general unlearning framework, dubbed RSU, which consists of\nRanking module and SeqUnlearn module.\n","authors":["Junjie Chen","Qian Chen","Jian Lou","Xiaoyu Zhang","Kai Wu","Zilong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06446v2.pdf","comment":"The senior authors of the draft are not fully convinced that the\n  novelty is significant enough for this submission compared to the latest\n  research progress in this area. Additionally, the senior authors have\n  identified writing issues. Based on these two reasons, we have decided to\n  withdraw the draft from arXiv"},{"id":"http://arxiv.org/abs/2409.09478v2","updated":"2024-10-21T14:15:30Z","published":"2024-09-14T16:39:17Z","title":"From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter\n  Lesion Segmentation in PET/CT Imaging","summary":"  Automated lesion segmentation in PET/CT scans is crucial for improving\nclinical workflows and advancing cancer diagnostics. However, the task is\nchallenging due to physiological variability, different tracers used in PET\nimaging, and diverse imaging protocols across medical centers. To address this,\nthe autoPET series was created to challenge researchers to develop algorithms\nthat generalize across diverse PET/CT environments. This paper presents our\nsolution for the autoPET III challenge, targeting multitracer, multicenter\ngeneralization using the nnU-Net framework with the ResEncL architecture. Key\ntechniques include misalignment data augmentation and multi-modal pretraining\nacross CT, MR, and PET datasets to provide an initial anatomical understanding.\nWe incorporate organ supervision as a multitask approach, enabling the model to\ndistinguish between physiological uptake and tracer-specific patterns, which is\nparticularly beneficial in cases where no lesions are present. Compared to the\ndefault nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL\n(65.31) our model significantly improved performance with a Dice score of\n68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative\n(FNvol: 10.35) volumes. These results underscore the effectiveness of combining\nadvanced network design, augmentation, pretraining, and multitask learning for\nPET/CT lesion segmentation. After evaluation on the test set, our approach was\nawarded the first place in the model-centric category (Team LesionTracer). Code\nis publicly available at https://github.com/MIC-DKFZ/autopet-3-submission.\n","authors":["Maximilian Rokuss","Balint Kovacs","Yannick Kirchhoff","Shuhan Xiao","Constantin Ulrich","Klaus H. Maier-Hein","Fabian Isensee"],"pdf_url":"https://arxiv.org/pdf/2409.09478v2.pdf","comment":"Winning method of the autoPET III challenge (model-centric) - Team\n  LesionTracer"},{"id":"http://arxiv.org/abs/2410.06558v4","updated":"2024-10-21T14:11:54Z","published":"2024-10-09T05:28:43Z","title":"Deep Correlated Prompting for Visual Recognition with Missing Modalities","summary":"  Large-scale multimodal models have shown excellent performance over a series\nof tasks powered by the large corpus of paired multimodal training data.\nGenerally, they are always assumed to receive modality-complete inputs.\nHowever, this simple assumption may not always hold in the real world due to\nprivacy constraints or collection difficulty, where models pretrained on\nmodality-complete data easily demonstrate degraded performance on\nmissing-modality cases. To handle this issue, we refer to prompt learning to\nadapt large pretrained multimodal models to handle missing-modality scenarios\nby regarding different missing cases as different types of input. Instead of\nonly prepending independent prompts to the intermediate layers, we present to\nleverage the correlations between prompts and input features and excavate the\nrelationships between different layers of prompts to carefully design the\ninstructions. We also incorporate the complementary semantics of different\nmodalities to guide the prompting design for each modality. Extensive\nexperiments on three commonly-used datasets consistently demonstrate the\nsuperiority of our method compared to the previous approaches upon different\nmissing scenarios. Plentiful ablations are further given to show the\ngeneralizability and reliability of our method upon different modality-missing\nratios and types.\n","authors":["Lianyu Hu","Tongkai Shi","Wei Feng","Fanhua Shang","Liang Wan"],"pdf_url":"https://arxiv.org/pdf/2410.06558v4.pdf","comment":"NeurIPS 2024, add some results"},{"id":"http://arxiv.org/abs/2410.16038v1","updated":"2024-10-21T14:10:18Z","published":"2024-10-21T14:10:18Z","title":"Benchmarking Pathology Foundation Models: Adaptation Strategies and\n  Scenarios","summary":"  In computational pathology, several foundation models have recently emerged\nand demonstrated enhanced learning capability for analyzing pathology images.\nHowever, adapting these models to various downstream tasks remains challenging,\nparticularly when faced with datasets from different sources and acquisition\nconditions, as well as limited data availability. In this study, we benchmark\nfour pathology-specific foundation models across 14 datasets and two\nscenarios-consistency assessment and flexibility assessment-addressing diverse\nadaptation scenarios and downstream tasks. In the consistency assessment\nscenario, involving five fine-tuning methods, we found that the\nparameter-efficient fine-tuning approach was both efficient and effective for\nadapting pathology-specific foundation models to diverse datasets within the\nsame downstream task. In the flexibility assessment scenario under data-limited\nenvironments, utilizing five few-shot learning methods, we observed that the\nfoundation models benefited more from the few-shot learning methods that\ninvolve modification during the testing phase only. These findings provide\ninsights that could guide the deployment of pathology-specific foundation\nmodels in real clinical settings, potentially improving the accuracy and\nreliability of pathology image analysis. The code for this study is available\nat: https://github.com/QuIIL/BenchmarkingPathologyFoundationModels.\n","authors":["Jeaung Lee","Jeewoo Lim","Keunho Byeon","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2410.16038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16037v1","updated":"2024-10-21T14:10:14Z","published":"2024-10-21T14:10:14Z","title":"Improving the Multi-label Atomic Activity Recognition by Robust Visual\n  Feature and Advanced Attention @ ROAD++ Atomic Activity Recognition 2024","summary":"  Road++ Track3 proposes a multi-label atomic activity recognition task in\ntraffic scenarios, which can be standardized as a 64-class multi-label video\naction recognition task. In the multi-label atomic activity recognition task,\nthe robustness of visual feature extraction remains a key challenge, which\ndirectly affects the model performance and generalization ability. To cope with\nthese issues, our team optimized three aspects: data processing, model and\npost-processing. Firstly, the appropriate resolution and video sampling\nstrategy are selected, and a fixed sampling strategy is set on the validation\nand test sets. Secondly, in terms of model training, the team selects a variety\nof visual backbone networks for feature extraction, and then introduces the\naction-slot model, which is trained on the training and validation sets, and\nreasoned on the test set. Finally, for post-processing, the team combined the\nstrengths and weaknesses of different models for weighted fusion, and the final\nmAP on the test set was 58%, which is 4% higher than the challenge baseline.\n","authors":["Jiamin Cao","Lingqi Wang","Kexin Zhang","Yuting Yang","Licheng Jiao","Yuwei Guo"],"pdf_url":"https://arxiv.org/pdf/2410.16037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11545v3","updated":"2024-10-21T14:04:16Z","published":"2024-08-21T11:53:53Z","title":"UNetMamba: An Efficient UNet-Like Mamba for Semantic Segmentation of\n  High-Resolution Remote Sensing Images","summary":"  Semantic segmentation of high-resolution remote sensing images is vital in\ndownstream applications such as land-cover mapping, urban planning and disaster\nassessment.Existing Transformer-based methods suffer from the constraint\nbetween accuracy and efficiency, while the recently proposed Mamba is renowned\nfor being efficient. Therefore, to overcome the dilemma, we propose UNetMamba,\na UNet-like semantic segmentation model based on Mamba. It incorporates a mamba\nsegmentation decoder (MSD) that can efficiently decode the complex information\nwithin high-resolution images, and a local supervision module (LSM), which is\ntrain-only but can significantly enhance the perception of local contents.\nExtensive experiments demonstrate that UNetMamba outperforms the\nstate-of-the-art methods with mIoU increased by 0.87% on LoveDA and 0.39% on\nISPRS Vaihingen, while achieving high efficiency through the lightweight\ndesign, less memory footprint and reduced computational cost. The source code\nis available at https://github.com/EnzeZhu2001/UNetMamba.\n","authors":["Enze Zhu","Zhan Chen","Dingkai Wang","Hanru Shi","Xiaoxuan Liu","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.11545v3.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.16028v1","updated":"2024-10-21T14:03:15Z","published":"2024-10-21T14:03:15Z","title":"Few-shot target-driven instance detection based on open-vocabulary\n  object detection models","summary":"  Current large open vision models could be useful for one and few-shot object\nrecognition. Nevertheless, gradient-based re-training solutions are costly. On\nthe other hand, open-vocabulary object detection models bring closer visual and\ntextual concepts in the same latent space, allowing zero-shot detection via\nprompting at small computational cost. We propose a lightweight method to turn\nthe latter into a one-shot or few-shot object recognition models without\nrequiring textual descriptions. Our experiments on the TEgO dataset using the\nYOLO-World model as a base show that performance increases with the model size,\nthe number of examples and the use of image augmentation.\n","authors":["Ben Crulis","Barthelemy Serres","Cyril De Runz","Gilles Venturini"],"pdf_url":"https://arxiv.org/pdf/2410.16028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16020v1","updated":"2024-10-21T13:50:32Z","published":"2024-10-21T13:50:32Z","title":"START: A Generalized State Space Model with Saliency-Driven Token-Aware\n  Transformation","summary":"  Domain Generalization (DG) aims to enable models to generalize to unseen\ntarget domains by learning from multiple source domains. Existing DG methods\nprimarily rely on convolutional neural networks (CNNs), which inherently learn\ntexture biases due to their limited receptive fields, making them prone to\noverfitting source domains. While some works have introduced transformer-based\nmethods (ViTs) for DG to leverage the global receptive field, these methods\nincur high computational costs due to the quadratic complexity of\nself-attention. Recently, advanced state space models (SSMs), represented by\nMamba, have shown promising results in supervised learning tasks by achieving\nlinear complexity in sequence length during training and fast RNN-like\ncomputation during inference. Inspired by this, we investigate the\ngeneralization ability of the Mamba model under domain shifts and find that\ninput-dependent matrices within SSMs could accumulate and amplify\ndomain-specific features, thus hindering model generalization. To address this\nissue, we propose a novel SSM-based architecture with saliency-based\ntoken-aware transformation (namely START), which achieves state-of-the-art\n(SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our\nSTART can selectively perturb and suppress domain-specific features in salient\ntokens within the input-dependent matrices of SSMs, thus effectively reducing\nthe discrepancy between different domains. Extensive experiments on five\nbenchmarks demonstrate that START outperforms existing SOTA DG methods with\nefficient linear complexity. Our code is available at\nhttps://github.com/lingeringlight/START.\n","authors":["Jintao Guo","Lei Qi","Yinghuan Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2410.16020v1.pdf","comment":"Accepted by NeurIPS2024. The code is available at\n  https://github.com/lingeringlight/START"},{"id":"http://arxiv.org/abs/2410.16019v1","updated":"2024-10-21T13:49:54Z","published":"2024-10-21T13:49:54Z","title":"Multispectral Texture Synthesis using RGB Convolutional Neural Networks","summary":"  State-of-the-art RGB texture synthesis algorithms rely on style distances\nthat are computed through statistics of deep features. These deep features are\nextracted by classification neural networks that have been trained on large\ndatasets of RGB images. Extending such synthesis methods to multispectral\nimages is not straightforward, since the pre-trained networks are designed for\nand have been trained on RGB images. In this work, we propose two solutions to\nextend these methods to multispectral imaging. Neither of them require\nadditional training of the neural network from which the second order neural\nstatistics are extracted. The first one consists in optimizing over batches of\nrandom triplets of spectral bands throughout training. The second one projects\nmultispectral pixels onto a 3 dimensional space. We further explore the benefit\nof a color transfer operation upstream of the projection to avoid the\npotentially abnormal color distributions induced by the projection. Our\nexperiments compare the performances of the various methods through different\nmetrics. We demonstrate that they can be used to perform exemplar-based texture\nsynthesis, achieve good visual quality and comes close to state-of-the art\nmethods on RGB bands.\n","authors":["Sélim Ollivier","Yann Gousseau","Sidonie Lefebvre"],"pdf_url":"https://arxiv.org/pdf/2410.16019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09283v2","updated":"2024-10-21T13:45:54Z","published":"2024-01-17T15:37:00Z","title":"A gradient-based approach to fast and accurate head motion compensation\n  in cone-beam CT","summary":"  Cone-beam computed tomography (CBCT) systems, with their flexibility, present\na promising avenue for direct point-of-care medical imaging, particularly in\ncritical scenarios such as acute stroke assessment. However, the integration of\nCBCT into clinical workflows faces challenges, primarily linked to long scan\nduration resulting in patient motion during scanning and leading to image\nquality degradation in the reconstructed volumes. This paper introduces a novel\napproach to CBCT motion estimation using a gradient-based optimization\nalgorithm, which leverages generalized derivatives of the backprojection\noperator for cone-beam CT geometries. Building on that, a fully differentiable\ntarget function is formulated which grades the quality of the current motion\nestimate in reconstruction space. We drastically accelerate motion estimation\nyielding a 19-fold speed-up compared to existing methods. Additionally, we\ninvestigate the architecture of networks used for quality metric regression and\npropose predicting voxel-wise quality maps, favoring autoencoder-like\narchitectures over contracting ones. This modification improves gradient flow,\nleading to more accurate motion estimation. The presented method is evaluated\nthrough realistic experiments on head anatomy. It achieves a reduction in\nreprojection error from an initial average of 3mm to 0.61mm after motion\ncompensation and consistently demonstrates superior performance compared to\nexisting approaches. The analytic Jacobian for the backprojection operation,\nwhich is at the core of the proposed method, is made publicly available. In\nsummary, this paper contributes to the advancement of CBCT integration into\nclinical workflows by proposing a robust motion estimation approach that\nenhances efficiency and accuracy, addressing critical challenges in\ntime-sensitive scenarios.\n","authors":["Mareike Thies","Fabian Wagner","Noah Maul","Haijun Yu","Manuela Goldmann","Linda-Sophie Schneider","Mingxuan Gu","Siyuan Mei","Lukas Folle","Alexander Preuhs","Michael Manhart","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2401.09283v2.pdf","comment":"\\copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.16012v1","updated":"2024-10-21T13:43:02Z","published":"2024-10-21T13:43:02Z","title":"Massimo: Public Queue Monitoring and Management using Mass-Spring Model","summary":"  An efficient system of a queue control and regulation in public spaces is\nvery important in order to avoid the traffic jams and to improve the customer\nsatisfaction. This article offers a detailed road map based on a merger of\nintelligent systems and creating an efficient systems of queues in public\nplaces. Through the utilization of different technologies i.e. computer vision,\nmachine learning algorithms, deep learning our system provide accurate\ninformation about the place is crowded or not and the necessary efforts to be\ntaken.\n","authors":["Abhijeet Kumar","Unnati Singh","Rajdeep Chatterjee","Tathagata Bandyopadhyay"],"pdf_url":"https://arxiv.org/pdf/2410.16012v1.pdf","comment":"8 pages, 6 figures, 3 algorithms, 3 tables"},{"id":"http://arxiv.org/abs/2410.16009v1","updated":"2024-10-21T13:42:06Z","published":"2024-10-21T13:42:06Z","title":"3D-GANTex: 3D Face Reconstruction with StyleGAN3-based Multi-View Images\n  and 3DDFA based Mesh Generation","summary":"  Geometry and texture estimation from a single face image is an ill-posed\nproblem since there is very little information to work with. The problem\nfurther escalates when the face is rotated at a different angle. This paper\ntries to tackle this problem by introducing a novel method for texture\nestimation from a single image by first using StyleGAN and 3D Morphable Models.\nThe method begins by generating multi-view faces using the latent space of GAN.\nThen 3DDFA trained on 3DMM estimates a 3D face mesh as well as a\nhigh-resolution texture map that is consistent with the estimated face shape.\nThe result shows that the generated mesh is of high quality with near to\naccurate texture representation.\n","authors":["Rohit Das","Tzung-Han Lin","Ko-Chih Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16009v1.pdf","comment":"7 pages, 4 figures, 2 tables, pre-print version"},{"id":"http://arxiv.org/abs/2410.15981v1","updated":"2024-10-21T13:06:38Z","published":"2024-10-21T13:06:38Z","title":"Visual Representation Learning Guided By Multi-modal Prior Knowledge","summary":"  Despite the remarkable success of deep neural networks (DNNs) in computer\nvision, they fail to remain high-performing when facing distribution shifts\nbetween training and testing data. In this paper, we propose Knowledge-Guided\nVisual representation learning (KGV), a distribution-based learning approach\nleveraging multi-modal prior knowledge, to improve generalization under\ndistribution shift. We use prior knowledge from two distinct modalities: 1) a\nknowledge graph (KG) with hierarchical and association relationships; and 2)\ngenerated synthetic images of visual elements semantically represented in the\nKG. The respective embeddings are generated from the given modalities in a\ncommon latent space, i.e., visual embeddings from original and synthetic images\nas well as knowledge graph embeddings (KGEs). These embeddings are aligned via\na novel variant of translation-based KGE methods, where the node and relation\nembeddings of the KG are modeled as Gaussian distributions and translations\nrespectively. We claim that incorporating multi-model prior knowledge enables\nmore regularized learning of image representations. Thus, the models are able\nto better generalize across different data distributions. We evaluate KGV on\ndifferent image classification tasks with major or minor distribution shifts,\nnamely road sign classification across datasets from Germany, China, and\nRussia, image classification with the mini-ImageNet dataset and its variants,\nas well as the DVM-CAR dataset. The results demonstrate that KGV consistently\nexhibits higher accuracy and data efficiency than the baselines across all\nexperiments.\n","authors":["Hongkuan Zhou","Lavdim Halilaj","Sebastian Monka","Stefan Schmid","Yuqicheng Zhu","Bo Xiong","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2410.15981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15980v1","updated":"2024-10-21T13:06:21Z","published":"2024-10-21T13:06:21Z","title":"Granularity Matters in Long-Tail Learning","summary":"  Balancing training on long-tail data distributions remains a long-standing\nchallenge in deep learning. While methods such as re-weighting and re-sampling\nhelp alleviate the imbalance issue, limited sample diversity continues to\nhinder models from learning robust and generalizable feature representations,\nparticularly for tail classes. In contrast to existing methods, we offer a\nnovel perspective on long-tail learning, inspired by an observation: datasets\nwith finer granularity tend to be less affected by data imbalance. In this\npaper, we investigate this phenomenon through both quantitative and qualitative\nstudies, showing that increased granularity enhances the generalization of\nlearned features in tail categories. Motivated by these findings, we propose a\nmethod to increase dataset granularity through category extrapolation.\nSpecifically, we introduce open-set auxiliary classes that are visually similar\nto existing ones, aiming to enhance representation learning for both head and\ntail classes. This forms the core contribution and insight of our approach. To\nautomate the curation of auxiliary data, we leverage large language models\n(LLMs) as knowledge bases to search for auxiliary categories and retrieve\nrelevant images through web crawling. To prevent the overwhelming presence of\nauxiliary classes from disrupting training, we introduce a neighbor-silencing\nloss that encourages the model to focus on class discrimination within the\ntarget dataset. During inference, the classifier weights for auxiliary\ncategories are masked out, leaving only the target class weights for use.\nExtensive experiments and ablation studies on three standard long-tail\nbenchmarks demonstrate the effectiveness of our approach, notably outperforming\nstrong baseline methods that use the same amount of data. The code will be made\npublicly available.\n","authors":["Shizhen Zhao","Xin Wen","Jiahui Liu","Chuofan Ma","Chunfeng Yuan","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2410.15980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15971v1","updated":"2024-10-21T12:58:19Z","published":"2024-10-21T12:58:19Z","title":"Zero-Shot Scene Reconstruction from Single Images with Deep Prior\n  Assembly","summary":"  Large language and vision models have been leading a revolution in visual\ncomputing. By greatly scaling up sizes of data and model parameters, the large\nmodels learn deep priors which lead to remarkable performance in various tasks.\nIn this work, we present deep prior assembly, a novel framework that assembles\ndiverse deep priors from large models for scene reconstruction from single\nimages in a zero-shot manner. We show that this challenging task can be done\nwithout extra knowledge but just simply generalizing one deep prior in one\nsub-task. To this end, we introduce novel methods related to poses, scales, and\nocclusion parsing which are keys to enable deep priors to work together in a\nrobust way. Deep prior assembly does not require any 3D or 2D data-driven\ntraining in the task and demonstrates superior performance in generalizing\npriors to open-world scenes. We conduct evaluations on various datasets, and\nreport analysis, numerical and visual comparisons with the latest methods to\nshow our superiority. Project page:\nhttps://junshengzhou.github.io/DeepPriorAssembly.\n","authors":["Junsheng Zhou","Yu-Shen Liu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2410.15971v1.pdf","comment":"To appear at NeurIPS 2024. Project page:\n  https://junshengzhou.github.io/DeepPriorAssembly"},{"id":"http://arxiv.org/abs/2405.17991v2","updated":"2024-10-21T12:53:21Z","published":"2024-05-28T09:23:14Z","title":"VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections","summary":"  Large language models (LLMs) have recently emerged as powerful tools for\ntackling many language-processing tasks. Despite their success, training and\nfine-tuning these models is still far too computationally and memory intensive.\nIn this paper, we identify and characterise the important components needed for\neffective model convergence using gradient descent. In doing so we find that\nthe intermediate activations used to implement backpropagation can be\nexcessively compressed without incurring any degradation in performance. This\nresult leads us to a cheap and memory-efficient algorithm for both fine-tuning\nand pre-training LLMs. The proposed algorithm simply divides the tokens up into\nsmaller sub-tokens before projecting them onto a fixed 1-dimensional subspace\nduring the forward pass. These features are then coarsely reconstructed during\nthe backward pass to implement the update rules. We confirm the effectiveness\nof our algorithm as being complimentary to many state-of-the-art PEFT methods\non the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for\nfine-tuning LLaMA and show competitive performance against other\nmemory-efficient pre-training methods on the large-scale C4 dataset.\n","authors":["Roy Miles","Pradyumna Reddy","Ismail Elezi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2405.17991v2.pdf","comment":"NeurIPS 2024. Code available at https://github.com/roymiles/VeLoRA"},{"id":"http://arxiv.org/abs/2410.15961v1","updated":"2024-10-21T12:47:36Z","published":"2024-10-21T12:47:36Z","title":"A Paradigm Shift in Mouza Map Vectorization: A Human-Machine\n  Collaboration Approach","summary":"  Efficient vectorization of hand-drawn cadastral maps, such as Mouza maps in\nBangladesh, poses a significant challenge due to their complex structures.\nCurrent manual digitization methods are time-consuming and labor-intensive. Our\nstudy proposes a semi-automated approach to streamline the digitization\nprocess, saving both time and human resources. Our methodology focuses on\nseparating the plot boundaries and plot identifiers and applying our\ndigitization methodology to convert both of them into vectorized format. To\naccomplish full vectorization, Convolutional Neural Network (CNN) models are\nutilized for pre-processing and plot number detection along with our smoothing\nalgorithms based on the diversity of vector maps. The CNN models are trained\nwith our own labeled dataset, generated from the maps, and smoothing algorithms\nare introduced from the various observations of the map's vector formats.\nFurther human intervention remains essential for precision. We have evaluated\nour methods on several maps and provided both quantitative and qualitative\nresults with user study. The result demonstrates that our methodology\noutperforms the existing map digitization processes significantly.\n","authors":["Mahir Shahriar Dhrubo","Samira Akter","Anwarul Bashir Shuaib","Md Toki Tahmid","Zahid Hasan","A. B. M. Alim Al Islam"],"pdf_url":"https://arxiv.org/pdf/2410.15961v1.pdf","comment":"13 pages including reference, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.15959v1","updated":"2024-10-21T12:43:54Z","published":"2024-10-21T12:43:54Z","title":"Diffusion Transformer Policy","summary":"  Recent large visual-language action models pretrained on diverse robot\ndatasets have demonstrated the potential for generalizing to new environments\nwith a few in-domain data. However, those approaches usually predict\ndiscretized or continuous actions by a small action head, which limits the\nability in handling diverse action spaces. In contrast, we model the continuous\naction with a large multi-modal diffusion transformer, dubbed as Diffusion\nTransformer Policy, in which we directly denoise action chunks by a large\ntransformer model rather than a small action head. By leveraging the scaling\ncapability of transformers, the proposed approach can effectively model\ncontinuous end-effector actions across large diverse robot datasets, and\nachieve better generalization performance. Extensive experiments demonstrate\nDiffusion Transformer Policy pretrained on diverse robot data can generalize to\ndifferent embodiments, including simulation environments like Maniskill2 and\nCalvin, as well as the real-world Franka arm. Specifically, without bells and\nwhistles, the proposed approach achieves state-of-the-art performance with only\na single third-view camera stream in the Calvin novel task setting (ABC->D),\nimproving the average number of tasks completed in a row of 5 to 3.6, and the\npretraining stage significantly facilitates the success sequence length on the\nCalvin by over 1.2. The code will be publicly available.\n","authors":["Zhi Hou","Tianyi Zhang","Yuwen Xiong","Hengjun Pu","Chengyang Zhao","Ronglei Tong","Yu Qiao","Jifeng Dai","Yuntao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15959v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.15957v1","updated":"2024-10-21T12:36:27Z","published":"2024-10-21T12:36:27Z","title":"CamI2V: Camera-Controlled Image-to-Video Diffusion Model","summary":"  Recently, camera pose, as a user-friendly and physics-related condition, has\nbeen introduced into text-to-video diffusion model for camera control. However,\nexisting methods simply inject camera conditions through a side input. These\napproaches neglect the inherent physical knowledge of camera pose, resulting in\nimprecise camera control, inconsistencies, and also poor interpretability. In\nthis paper, we emphasize the necessity of integrating explicit physical\nconstraints into model design. Epipolar attention is proposed for modeling all\ncross-frame relationships from a novel perspective of noised condition. This\nensures that features are aggregated from corresponding epipolar lines in all\nnoised frames, overcoming the limitations of current attention mechanisms in\ntracking displaced features across frames, especially when features move\nsignificantly with the camera and become obscured by noise. Additionally, we\nintroduce register tokens to handle cases without intersections between frames,\ncommonly caused by rapid camera movements, dynamic objects, or occlusions. To\nsupport image-to-video, we propose the multiple guidance scale to allow for\nprecise control for image, text, and camera, respectively. Furthermore, we\nestablish a more robust and reproducible evaluation pipeline to solve the\ninaccuracy and instability of existing camera control measurement. We achieve a\n25.5\\% improvement in camera controllability on RealEstate10K while maintaining\nstrong generalization to out-of-domain images. Only 24GB and 12GB are required\nfor training and inference, respectively. We plan to release checkpoints, along\nwith training and evaluation codes. Dynamic videos are best viewed at\n\\url{https://zgctroy.github.io/CamI2V}.\n","authors":["Guangcong Zheng","Teng Li","Rui Jiang","Yehao Lu","Tao Wu","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2410.15957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15947v1","updated":"2024-10-21T12:26:53Z","published":"2024-10-21T12:26:53Z","title":"AI-Driven Approaches for Glaucoma Detection -- A Comprehensive Review","summary":"  The diagnosis of glaucoma plays a critical role in the management and\ntreatment of this vision-threatening disease. Glaucoma is a group of eye\ndiseases that cause blindness by damaging the optic nerve at the back of the\neye. Often called \"silent thief of sight\", it exhibits no symptoms during the\nearly stages. Therefore, early detection is crucial to prevent vision loss.\nWith the rise of Artificial Intelligence (AI), particularly Deep Learning (DL)\ntechniques, Computer-Aided Diagnosis (CADx) systems have emerged as promising\ntools to assist clinicians in accurately diagnosing glaucoma early. This paper\naims to provide a comprehensive overview of AI techniques utilized in CADx\nsystems for glaucoma diagnosis. Through a detailed analysis of current\nliterature, we identify key gaps and challenges in these systems, emphasizing\nthe need for improved safety, reliability, interpretability, and\nexplainability. By identifying research gaps, we aim to advance the field of\nCADx systems especially for the early diagnosis of glaucoma, in order to\nprevent any potential loss of vision.\n","authors":["Yuki Hagiwara","Octavia-Andreaa Ciora","Maureen Monnet","Gino Lancho","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2410.15947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15941v1","updated":"2024-10-21T12:13:43Z","published":"2024-10-21T12:13:43Z","title":"MBPU: A Plug-and-Play State Space Model for Point Cloud Upsamping with\n  Fast Point Rendering","summary":"  The task of point cloud upsampling (PCU) is to generate dense and uniform\npoint clouds from sparse input captured by 3D sensors like LiDAR, holding\npotential applications in real yet is still a challenging task. Existing deep\nlearning-based methods have shown significant achievements in this field.\nHowever, they still face limitations in effectively handling long sequences and\naddressing the issue of shrinkage artifacts around the surface of the point\ncloud. Inspired by the newly proposed Mamba, in this paper, we introduce a\nnetwork named MBPU built on top of the Mamba architecture, which performs well\nin long sequence modeling, especially for large-scale point cloud upsampling,\nand achieves fast convergence speed. Moreover, MBPU is an arbitrary-scale\nupsampling framework as the predictor of point distance in the point refinement\nphase. At the same time, we simultaneously predict the 3D position shift and 1D\npoint-to-point distance as regression quantities to constrain the global\nfeatures while ensuring the accuracy of local details. We also introduce a fast\ndifferentiable renderer to further enhance the fidelity of the upsampled point\ncloud and reduce artifacts. It is noted that, by the merits of our fast point\nrendering, MBPU yields high-quality upsampled point clouds by effectively\neliminating surface noise. Extensive experiments have demonstrated that our\nMBPU outperforms other off-the-shelf methods in terms of point cloud\nupsampling, especially for large-scale point clouds.\n","authors":["Jiayi Song","Weidong Yang","Zhijun Li","Wen-Ming Chen","Ben Fei"],"pdf_url":"https://arxiv.org/pdf/2410.15941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15932v1","updated":"2024-10-21T12:00:52Z","published":"2024-10-21T12:00:52Z","title":"Focus on BEV: Self-calibrated Cycle View Transformation for Monocular\n  Birds-Eye-View Segmentation","summary":"  Birds-Eye-View (BEV) segmentation aims to establish a spatial mapping from\nthe perspective view to the top view and estimate the semantic maps from\nmonocular images. Recent studies have encountered difficulties in view\ntransformation due to the disruption of BEV-agnostic features in image space.\nTo tackle this issue, we propose a novel FocusBEV framework consisting of $(i)$\na self-calibrated cross view transformation module to suppress the BEV-agnostic\nimage areas and focus on the BEV-relevant areas in the view transformation\nstage, $(ii)$ a plug-and-play ego-motion-based temporal fusion module to\nexploit the spatiotemporal structure consistency in BEV space with a memory\nbank, and $(iii)$ an occupancy-agnostic IoU loss to mitigate both semantic and\npositional uncertainties. Experimental evidence demonstrates that our approach\nachieves new state-of-the-art on two popular benchmarks,\\ie, 29.2\\% mIoU on\nnuScenes and 35.2\\% mIoU on Argoverse.\n","authors":["Jiawei Zhao","Qixing Jiang","Xuede Li","Junfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2410.15932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07255v3","updated":"2024-10-21T11:57:29Z","published":"2024-06-11T13:34:57Z","title":"Towards Realistic Data Generation for Real-World Super-Resolution","summary":"  Existing image super-resolution (SR) techniques often fail to generalize\neffectively in complex real-world settings due to the significant divergence\nbetween training data and practical scenarios. To address this challenge,\nprevious efforts have either manually simulated intricate physical-based\ndegradations or utilized learning-based techniques, yet these approaches remain\ninadequate for producing large-scale, realistic, and diverse data\nsimultaneously. In this paper, we introduce a novel Realistic Decoupled Data\nGenerator (RealDGen), an unsupervised learning data generation framework\ndesigned for real-world super-resolution. We meticulously develop content and\ndegradation extraction strategies, which are integrated into a novel\ncontent-degradation decoupled diffusion model to create realistic\nlow-resolution images from unpaired real LR and HR images. Extensive\nexperiments demonstrate that RealDGen excels in generating large-scale,\nhigh-quality paired data that mirrors real-world degradations, significantly\nadvancing the performance of popular SR models on various real-world\nbenchmarks.\n","authors":["Long Peng","Wenbo Li","Renjing Pei","Jingjing Ren","Yang Wang","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2406.07255v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15927v1","updated":"2024-10-21T11:55:06Z","published":"2024-10-21T11:55:06Z","title":"GReFEL: Geometry-Aware Reliable Facial Expression Learning under Bias\n  and Imbalanced Data Distribution","summary":"  Reliable facial expression learning (FEL) involves the effective learning of\ndistinctive facial expression characteristics for more reliable, unbiased and\naccurate predictions in real-life settings. However, current systems struggle\nwith FEL tasks because of the variance in people's facial expressions due to\ntheir unique facial structures, movements, tones, and demographics. Biased and\nimbalanced datasets compound this challenge, leading to wrong and biased\nprediction labels. To tackle these, we introduce GReFEL, leveraging Vision\nTransformers and a facial geometry-aware anchor-based reliability balancing\nmodule to combat imbalanced data distributions, bias, and uncertainty in facial\nexpression learning. Integrating local and global data with anchors that learn\ndifferent facial data points and structural features, our approach adjusts\nbiased and mislabeled emotions caused by intra-class disparity, inter-class\nsimilarity, and scale sensitivity, resulting in comprehensive, accurate, and\nreliable facial expression predictions. Our model outperforms current\nstate-of-the-art methodologies, as demonstrated by extensive experiments on\nvarious datasets.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Karlo Serbetar","Dong Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2410.15927v1.pdf","comment":"ACCV 2024. Extended version of ARBEx (arXiv:2305.01486)"},{"id":"http://arxiv.org/abs/2410.15926v1","updated":"2024-10-21T11:54:53Z","published":"2024-10-21T11:54:53Z","title":"Mitigating Object Hallucination via Concentric Causal Attention","summary":"  Recent Large Vision Language Models (LVLMs) present remarkable zero-shot\nconversational and reasoning capabilities given multimodal queries.\nNevertheless, they suffer from object hallucination, a phenomenon where LVLMs\nare prone to generate textual responses not factually aligned with image\ninputs. Our pilot study reveals that object hallucination is closely tied with\nRotary Position Encoding (RoPE), a widely adopted positional dependency\nmodeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs\ntend to hallucinate more when relevant visual cues are distant from instruction\ntokens in the multimodal input sequence. Additionally, we observe a similar\neffect when reversing the sequential order of visual tokens during multimodal\nalignment. Our tests indicate that long-term decay in RoPE poses challenges to\nLVLMs while capturing visual-instruction interactions across long distances. We\npropose Concentric Causal Attention (CCA), a simple yet effective positional\nalignment strategy that mitigates the impact of RoPE long-term decay in LVLMs\nby naturally reducing relative distance between visual and instruction tokens.\nWith CCA, visual tokens can better interact with instruction tokens, thereby\nenhancing model's perception capability and alleviating object hallucination.\nWithout bells and whistles, our positional alignment method surpasses existing\nhallucination mitigation strategies by large margins on multiple object\nhallucination benchmarks.\n","authors":["Yun Xing","Yiheng Li","Ivan Laptev","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2410.15926v1.pdf","comment":"To appear at NeurIPS 2024. Code is available at\n  https://github.com/xing0047/cca-llava"},{"id":"http://arxiv.org/abs/2408.11958v2","updated":"2024-10-21T11:49:56Z","published":"2024-08-21T19:25:03Z","title":"CARLA Drone: Monocular 3D Object Detection from a Different Perspective","summary":"  Existing techniques for monocular 3D detection have a serious restriction.\nThey tend to perform well only on a limited set of benchmarks, faring well\neither on ego-centric car views or on traffic camera views, but rarely on both.\nTo encourage progress, this work advocates for an extended evaluation of 3D\ndetection frameworks across different camera perspectives. We make two key\ncontributions. First, we introduce the CARLA Drone dataset, CDrone. Simulating\ndrone views, it substantially expands the diversity of camera perspectives in\nexisting benchmarks. Despite its synthetic nature, CDrone represents a\nreal-world challenge. To show this, we confirm that previous techniques\nstruggle to perform well both on CDrone and a real-world 3D drone dataset.\nSecond, we develop an effective data augmentation pipeline called GroundMix.\nIts distinguishing element is the use of the ground for creating 3D-consistent\naugmentation of a training image. GroundMix significantly boosts the detection\naccuracy of a lightweight one-stage detector. In our expanded evaluation, we\nachieve the average precision on par with or substantially higher than the\nprevious state of the art across all tested datasets.\n","authors":["Johannes Meier","Luca Scalerandi","Oussema Dhaouadi","Jacques Kaiser","Nikita Araslanov","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2408.11958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15919v1","updated":"2024-10-21T11:49:10Z","published":"2024-10-21T11:49:10Z","title":"Are Large-scale Soft Labels Necessary for Large-scale Dataset\n  Distillation?","summary":"  In ImageNet-condensation, the storage for auxiliary soft labels exceeds that\nof the condensed dataset by over 30 times. However, are large-scale soft labels\nnecessary for large-scale dataset distillation? In this paper, we first\ndiscover that the high within-class similarity in condensed datasets\nnecessitates the use of large-scale soft labels. This high within-class\nsimilarity can be attributed to the fact that previous methods use samples from\ndifferent classes to construct a single batch for batch normalization (BN)\nmatching. To reduce the within-class similarity, we introduce class-wise\nsupervision during the image synthesizing process by batching the samples\nwithin classes, instead of across classes. As a result, we can increase\nwithin-class diversity and reduce the size of required soft labels. A key\nbenefit of improved image diversity is that soft label compression can be\nachieved through simple random pruning, eliminating the need for complex\nrule-based strategies. Experiments validate our discoveries. For example, when\ncondensing ImageNet-1K to 200 images per class, our approach compresses the\nrequired soft labels from 113 GB to 2.8 GB (40x compression) with a 2.6%\nperformance gain. Code is available at:\nhttps://github.com/he-y/soft-label-pruning-for-dataset-distillation\n","authors":["Lingao Xiao","Yang He"],"pdf_url":"https://arxiv.org/pdf/2410.15919v1.pdf","comment":"Accepted by Neurips 2024"},{"id":"http://arxiv.org/abs/2410.15916v1","updated":"2024-10-21T11:46:28Z","published":"2024-10-21T11:46:28Z","title":"Leveraging CORAL-Correlation Consistency Network for Semi-Supervised\n  Left Atrium MRI Segmentation","summary":"  Semi-supervised learning (SSL) has been widely used to learn from both a few\nlabeled images and many unlabeled images to overcome the scarcity of labeled\nsamples in medical image segmentation. Most current SSL-based segmentation\nmethods use pixel values directly to identify similar features in labeled and\nunlabeled data. They usually fail to accurately capture the intricate\nattachment structures in the left atrium, such as the areas of inconsistent\ndensity or exhibit outward curvatures, adding to the complexity of the task. In\nthis paper, we delve into this issue and introduce an effective solution,\nCORAL(Correlation-Aligned)-Correlation Consistency Network (CORN), to capture\nthe global structure shape and local details of Left Atrium. Diverging from\nprevious methods focused on each local pixel value, the CORAL-Correlation\nConsistency Module (CCM) in the CORN leverages second-order statistical\ninformation to capture global structural features by minimizing the\ndistribution discrepancy between labeled and unlabeled samples in feature\nspace. Yet, direct construction of features from unlabeled data frequently\nresults in ``Sample Selection Bias'', leading to flawed supervision. We thus\nfurther propose the Dynamic Feature Pool (DFP) for the CCM, which utilizes a\nconfidence-based filtering strategy to remove incorrectly selected features and\nregularize both teacher and student models by constraining the similarity\nmatrix to be consistent. Extensive experiments on the Left Atrium dataset have\nshown that the proposed CORN outperforms previous state-of-the-art\nsemi-supervised learning methods.\n","authors":["Xinze Li","Runlin Huang","Zhenghao Wu","Bohan Yang","Wentao Fan","Chengzhang Zhu","Weifeng Su"],"pdf_url":"https://arxiv.org/pdf/2410.15916v1.pdf","comment":"5 pages, 3 figures, Accepted by 2024 IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM 2024)"},{"id":"http://arxiv.org/abs/2403.17633v4","updated":"2024-10-21T11:34:27Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v4.pdf","comment":"Accepted for IEEE RA-L 2024"},{"id":"http://arxiv.org/abs/2410.15909v1","updated":"2024-10-21T11:32:46Z","published":"2024-10-21T11:32:46Z","title":"Hybrid Architecture for Real-Time Video Anomaly Detection: Integrating\n  Spatial and Temporal Analysis","summary":"  We propose a new architecture for real-time anomaly detection in video data,\ninspired by human behavior by combining spatial and temporal analyses. This\napproach uses two distinct models: for temporal analysis, a recurrent\nconvolutional network (CNN + RNN) is employed, associating VGG19 and a GRU to\nprocess video sequences. Regarding spatial analysis, it is performed using\nYOLOv7 to analyze individual images. These two analyses can be carried out\neither in parallel, with a final prediction that combines the results of both\nanalyses, or in series, where the spatial analysis enriches the data before the\ntemporal analysis. In this article, we will compare these two architectural\nconfigurations with each other, to evaluate the effectiveness of our hybrid\napproach in video anomaly detection.\n","authors":["Fabien Poirier"],"pdf_url":"https://arxiv.org/pdf/2410.15909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15907v1","updated":"2024-10-21T11:29:35Z","published":"2024-10-21T11:29:35Z","title":"Seismic Phase Picking","summary":"  Seismic phase picking, which aims to determine the arrival time of P- and\nS-waves according to seismic waveforms, is fundamental to earthquake\nmonitoring. Generally, manual phase picking is trustworthy, but with the\nincreasing number of worldwide stations and seismic monitors, it becomes more\nchallenging for human to complete the task comprehensively. In this work, we\nexplore multiple ways to do automatic phase picking, including traditional and\nlearning-based methods.\n","authors":["Yuchen Wang","Ruihuan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07758v2","updated":"2024-10-21T11:12:38Z","published":"2024-10-10T09:37:33Z","title":"HeightFormer: A Semantic Alignment Monocular 3D Object Detection Method\n  from Roadside Perspective","summary":"  The on-board 3D object detection technology has received extensive attention\nas a critical technology for autonomous driving, while few studies have focused\non applying roadside sensors in 3D traffic object detection. Existing studies\nachieve the projection of 2D image features to 3D features through height\nestimation based on the frustum. However, they did not consider the height\nalignment and the extraction efficiency of bird's-eye-view features. We propose\na novel 3D object detection framework integrating Spatial Former and Voxel\nPooling Former to enhance 2D-to-3D projection based on height estimation.\nExtensive experiments were conducted using the Rope3D and DAIR-V2X-I dataset,\nand the results demonstrated the outperformance of the proposed algorithm in\nthe detection of both vehicles and cyclists. These results indicate that the\nalgorithm is robust and generalized under various detection scenarios.\nImproving the accuracy of 3D object detection on the roadside is conducive to\nbuilding a safe and trustworthy intelligent transportation system of\nvehicle-road coordination and promoting the large-scale application of\nautonomous driving. The code and pre-trained models will be released on\nhttps://anonymous.4open.science/r/HeightFormer.\n","authors":["Pei Liu","Zihao Zhang","Haipeng Liu","Nanfang Zheng","Meixin Zhu","Ziyuan Pu"],"pdf_url":"https://arxiv.org/pdf/2410.07758v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15891v1","updated":"2024-10-21T11:10:07Z","published":"2024-10-21T11:10:07Z","title":"TexPro: Text-guided PBR Texturing with Procedural Material Modeling","summary":"  In this paper, we present TexPro, a novel method for high-fidelity material\ngeneration for input 3D meshes given text prompts. Unlike existing\ntext-conditioned texture generation methods that typically generate RGB\ntextures with baked lighting, TexPro is able to produce diverse texture maps\nvia procedural material modeling, which enables physical-based rendering,\nrelighting, and additional benefits inherent to procedural materials.\nSpecifically, we first generate multi-view reference images given the input\ntextual prompt by employing the latest text-to-image model. We then derive\ntexture maps through a rendering-based optimization with recent differentiable\nprocedural materials. To this end, we design several techniques to handle the\nmisalignment between the generated multi-view images and 3D meshes, and\nintroduce a novel material agent that enhances material classification and\nmatching by exploring both part-level understanding and object-aware material\nreasoning. Experiments demonstrate the superiority of the proposed method over\nexisting SOTAs and its capability of relighting.\n","authors":["Ziqiang Dang","Wenqi Dong","Zesong Yang","Bangbang Yang","Liang Li","Yuewen Ma","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2410.15891v1.pdf","comment":"In submission. Supplementary material is included at the end of the\n  main paper (5 pages, 2 figures)"},{"id":"http://arxiv.org/abs/2410.15886v1","updated":"2024-10-21T11:04:58Z","published":"2024-10-21T11:04:58Z","title":"Foundation Models for Slide-level Cancer Subtyping in Digital Pathology","summary":"  Since the emergence of the ImageNet dataset, the pretraining and fine-tuning\napproach has become widely adopted in computer vision due to the ability of\nImageNet-pretrained models to learn a wide variety of visual features. However,\na significant challenge arises when adapting these models to domain-specific\nfields, such as digital pathology, due to substantial gaps between domains. To\naddress this limitation, foundation models (FM) have been trained on\nlarge-scale in-domain datasets to learn the intricate features of\nhistopathology images. In cancer diagnosis, whole-slide image (WSI) prediction\nis essential for patient prognosis, and multiple instance learning (MIL) has\nbeen implemented to handle the giga-pixel size of WSI. As MIL frameworks rely\non patch-level feature aggregation, this work aims to compare the performance\nof various feature extractors developed under different pretraining strategies\nfor cancer subtyping on WSI under a MIL framework. Results demonstrate the\nability of foundation models to surpass ImageNet-pretrained models for the\nprediction of six skin cancer subtypes\n","authors":["Pablo Meseguer","Rocío del Amor","Adrian Colomer","Valery Naranjo"],"pdf_url":"https://arxiv.org/pdf/2410.15886v1.pdf","comment":"Manuscript accepted for oral presentation at Decision Science\n  Allieance -INternational Summer Conference (DSA-ISC) 2024 held on Valencia,\n  Spain"},{"id":"http://arxiv.org/abs/2408.17433v2","updated":"2024-10-21T11:01:46Z","published":"2024-08-30T17:35:06Z","title":"DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised\n  Vector-LoRA of the Foundation Model","summary":"  Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D\nreconstruction and visualization. While foundation models like Depth Anything\nModels (DAM) show promise, directly applying them to surgery often yields\nsuboptimal results. Fully fine-tuning on limited surgical data can cause\noverfitting and catastrophic forgetting, compromising model robustness and\ngeneralization. Although Low-Rank Adaptation (LoRA) addresses some adaptation\nissues, its uniform parameter distribution neglects the inherent feature\nhierarchy, where earlier layers, learning more general features, require more\nparameters than later ones. To tackle this issue, we introduce Depth Anything\nin Robotic Endoscopic Surgery (DARES), a novel approach that employs a new\nadaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to\nperform self-supervised monocular depth estimation in RAS scenes. To enhance\nlearning efficiency, we introduce Vector-LoRA by integrating more parameters in\nearlier layers and gradually decreasing parameters in later layers. We also\ndesign a reprojection loss based on the multi-scale SSIM error to enhance depth\nperception by better tailoring the foundation model to the specific\nrequirements of the surgical environment. The proposed method is validated on\nthe SCARED dataset and demonstrates superior performance over recent\nstate-of-the-art self-supervised monocular depth estimation techniques,\nachieving an improvement of 13.3% in the absolute relative error metric. The\ncode and pre-trained weights are available at\nhttps://github.com/mobarakol/DARES.\n","authors":["Mona Sheikh Zeinoddin","Chiara Lena","Jiongqi Qu","Luca Carlini","Mattia Magro","Seunghoi Kim","Elena De Momi","Sophia Bano","Matthew Grech-Sollars","Evangelos Mazomenos","Daniel C. Alexander","Danail Stoyanov","Matthew J. Clarkson","Mobarakol Islam"],"pdf_url":"https://arxiv.org/pdf/2408.17433v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2410.15882v1","updated":"2024-10-21T11:01:44Z","published":"2024-10-21T11:01:44Z","title":"Distributed Learning for UAV Swarms","summary":"  Unmanned Aerial Vehicle (UAV) swarms are increasingly deployed in dynamic,\ndata-rich environments for applications such as environmental monitoring and\nsurveillance. These scenarios demand efficient data processing while\nmaintaining privacy and security, making Federated Learning (FL) a promising\nsolution. FL allows UAVs to collaboratively train global models without sharing\nraw data, but challenges arise due to the non-Independent and Identically\nDistributed (non-IID) nature of the data collected by UAVs. In this study, we\nshow an integration of the state-of-the-art FL methods to UAV Swarm application\nand invetigate the performance of multiple aggregation methods (namely FedAvg,\nFedProx, FedOpt, and MOON) with a particular focus on tackling non-IID on a\nvariety of datasets, specifically MNIST for baseline performance, CIFAR10 for\nnatural object classification, EuroSAT for environment monitoring, and CelebA\nfor surveillance. These algorithms were selected to cover improved techniques\non both client-side updates and global aggregation. Results show that while all\nalgorithms perform comparably on IID data, their performance deteriorates\nsignificantly under non-IID conditions. FedProx demonstrated the most stable\noverall performance, emphasising the importance of regularising local updates\nin non-IID environments to mitigate drastic deviations in local models.\n","authors":["Chen Hu","Hanchi Ren","Jingjing Deng","Xianghua Xie"],"pdf_url":"https://arxiv.org/pdf/2410.15882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15881v1","updated":"2024-10-21T11:01:20Z","published":"2024-10-21T11:01:20Z","title":"MI-VisionShot: Few-shot adaptation of vision-language models for\n  slide-level classification of histopathological images","summary":"  Vision-language supervision has made remarkable strides in learning visual\nrepresentations from textual guidance. In digital pathology, vision-language\nmodels (VLM), pre-trained on curated datasets of histological image-captions,\nhave been adapted to downstream tasks, such as region of interest\nclassification. Zero-shot transfer for slide-level prediction has been\nformulated by MI-Zero, but it exhibits high variability depending on the\ntextual prompts. Inspired by prototypical learning, we propose MI-VisionShot, a\ntraining-free adaptation method on top of VLMs to predict slide-level labels in\nfew-shot learning scenarios. Our framework takes advantage of the excellent\nrepresentation learning of VLM to create prototype-based classifiers under a\nmultiple-instance setting by retrieving the most discriminative patches within\neach slide. Experimentation through different settings shows the ability of\nMI-VisionShot to surpass zero-shot transfer with lower variability, even in\nlow-shot scenarios. Code coming soon at\nthttps://github.com/cvblab/MIVisionShot.\n","authors":["Pablo Meseguer","Rocío del Amor","Valery Naranjo"],"pdf_url":"https://arxiv.org/pdf/2410.15881v1.pdf","comment":"Manuscript accepted for oral presentation at KES-InnovationInMedicine\n  2024 held on Madeira, Portugal"},{"id":"http://arxiv.org/abs/2404.07989v3","updated":"2024-10-21T10:54:55Z","published":"2024-04-11T17:59:45Z","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding","summary":"  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n","authors":["Yiwen Tang","Ray Zhang","Jiaming Liu","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Shanghang Zhang","Peng Gao","Hongsheng Li","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07989v3.pdf","comment":"Code and models are released at\n  https://github.com/Ivan-Tang-3D/Any2Point"},{"id":"http://arxiv.org/abs/2410.15866v1","updated":"2024-10-21T10:50:00Z","published":"2024-10-21T10:50:00Z","title":"Visual Motif Identification: Elaboration of a Curated Comparative\n  Dataset and Classification Methods","summary":"  In cinema, visual motifs are recurrent iconographic compositions that carry\nartistic or aesthetic significance. Their use throughout the history of visual\narts and media is interesting to researchers and filmmakers alike. Our goal in\nthis work is to recognise and classify these motifs by proposing a new machine\nlearning model that uses a custom dataset to that end. We show how features\nextracted from a CLIP model can be leveraged by using a shallow network and an\nappropriate loss to classify images into 20 different motifs, with surprisingly\ngood results: an $F_1$-score of 0.91 on our test set. We also present several\nablation studies justifying the input features, architecture and\nhyperparameters used.\n","authors":["Adam Phillips","Daniel Grandes Rodriguez","Miriam Sánchez-Manzano","Alan Salvadó","Manuel Garin","Gloria Haro","Coloma Ballester"],"pdf_url":"https://arxiv.org/pdf/2410.15866v1.pdf","comment":"17 pages, 11 figures, one table, to be published in the conference\n  proceedings of ECCV 2024"},{"id":"http://arxiv.org/abs/2310.03059v8","updated":"2024-10-21T10:49:59Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code is released at\nhttps://github.com/Ivan-Tang-3D/Point-PEFT.\n","authors":["Yiwen Tang","Ray Zhang","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2310.03059v8.pdf","comment":"The specialized PEFT framework for 3D pre-trained models, which\n  achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Ivan-Tang-3D/Point-PEFT"},{"id":"http://arxiv.org/abs/2410.15851v1","updated":"2024-10-21T10:27:57Z","published":"2024-10-21T10:27:57Z","title":"R2I-rPPG: A Robust Region of Interest Selection Method for Remote\n  Photoplethysmography to Extract Heart Rate","summary":"  The COVID-19 pandemic has underscored the need for low-cost, scalable\napproaches to measuring contactless vital signs, either during initial triage\nat a healthcare facility or virtual telemedicine visits. Remote\nphotoplethysmography (rPPG) can accurately estimate heart rate (HR) when\napplied to close-up videos of healthy volunteers in well-lit laboratory\nsettings. However, results from such highly optimized laboratory studies may\nnot be readily translated to healthcare settings. One significant barrier to\nthe practical application of rPPG in health care is the accurate localization\nof the region of interest (ROI). Clinical or telemedicine visits may involve\nsub-optimal lighting, movement artifacts, variable camera angle, and subject\ndistance. This paper presents an rPPG ROI selection method based on 3D facial\nlandmarks and patient head yaw angle. We then demonstrate the robustness of\nthis ROI selection method when coupled to the Plane-Orthogonal-to-Skin (POS)\nrPPG method when applied to videos of patients presenting to an Emergency\nDepartment for respiratory complaints. Our results demonstrate the\neffectiveness of our proposed approach in improving the accuracy and robustness\nof rPPG in a challenging clinical environment.\n","authors":["Sandeep Nagar","Mark Hasegawa-Johnson","David G. Beiser","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2410.15851v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.15847v1","updated":"2024-10-21T10:19:45Z","published":"2024-10-21T10:19:45Z","title":"Random Token Fusion for Multi-View Medical Diagnosis","summary":"  In multi-view medical diagnosis, deep learning-based models often fuse\ninformation from different imaging perspectives to improve diagnostic\nperformance. However, existing approaches are prone to overfitting and rely\nheavily on view-specific features, which can lead to trivial solutions. In this\nwork, we introduce Random Token Fusion (RTF), a novel technique designed to\nenhance multi-view medical image analysis using vision transformers. By\nintegrating randomness into the feature fusion process during training, RTF\naddresses the issue of overfitting and enhances the robustness and accuracy of\ndiagnostic models without incurring any additional cost at inference. We\nvalidate our approach on standard mammography and chest X-ray benchmark\ndatasets. Through extensive experiments, we demonstrate that RTF consistently\nimproves the performance of existing fusion methods, paving the way for a new\ngeneration of multi-view medical foundation models.\n","authors":["Jingyu Guo","Christos Matsoukas","Fredrik Strand","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2410.15847v1.pdf","comment":"Originally published at the NeurIPS 2024 Workshop on Advancements In\n  Medical Foundation Models: Explainability, Robustness, Security, and Beyond\n  (AIM-FM)"},{"id":"http://arxiv.org/abs/2410.12284v2","updated":"2024-10-21T10:11:11Z","published":"2024-10-16T06:43:02Z","title":"Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting","summary":"  The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.\n","authors":["Maxime Kayser","Bayar Menzat","Cornelius Emde","Bogdan Bercean","Alex Novak","Abdala Espinosa","Bartlomiej W. Papiez","Susanne Gaube","Thomas Lukasiewicz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2410.12284v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.15833v1","updated":"2024-10-21T09:50:17Z","published":"2024-10-21T09:50:17Z","title":"LiOn-XA: Unsupervised Domain Adaptation via LiDAR-Only Cross-Modal\n  Adversarial Training","summary":"  In this paper, we propose LiOn-XA, an unsupervised domain adaptation (UDA)\napproach that combines LiDAR-Only Cross-Modal (X) learning with Adversarial\ntraining for 3D LiDAR point cloud semantic segmentation to bridge the domain\ngap arising from environmental and sensor setup changes. Unlike existing works\nthat exploit multiple data modalities like point clouds and RGB image data, we\naddress UDA in scenarios where RGB images might not be available and show that\ntwo distinct LiDAR data representations can learn from each other for UDA. More\nspecifically, we leverage 3D voxelized point clouds to preserve important\ngeometric structure in combination with 2D projection-based range images that\nprovide information such as object orientations or surfaces. To further align\nthe feature space between both domains, we apply adversarial training using\nboth features and predictions of both 2D and 3D neural networks. Our\nexperiments on 3 real-to-real adaptation scenarios demonstrate the\neffectiveness of our approach, achieving new state-of-the-art performance when\ncompared to previous uni- and multi-model UDA methods. Our source code is\npublicly available at https://github.com/JensLe97/lion-xa.\n","authors":["Thomas Kreutz","Jens Lemke","Max Mühlhäuser","Alejandro Sanchez Guinea"],"pdf_url":"https://arxiv.org/pdf/2410.15833v1.pdf","comment":"Preprint, Paper has been accepted at IROS2024"},{"id":"http://arxiv.org/abs/2403.05846v2","updated":"2024-10-21T09:38:03Z","published":"2024-03-09T09:11:49Z","title":"Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines","summary":"  Text-to-image diffusion models (T2I) use a latent representation of a text\nprompt to guide the image generation process. However, the process by which the\nencoder produces the text representation is unknown. We propose the Diffusion\nLens, a method for analyzing the text encoder of T2I models by generating\nimages from its intermediate representations. Using the Diffusion Lens, we\nperform an extensive analysis of two recent T2I models. Exploring compound\nprompts, we find that complex scenes describing multiple objects are composed\nprogressively and more slowly compared to simple scenes; Exploring knowledge\nretrieval, we find that representation of uncommon concepts requires further\ncomputation compared to common concepts, and that knowledge retrieval is\ngradual across layers. Overall, our findings provide valuable insights into the\ntext encoder component in T2I pipelines.\n","authors":["Michael Toker","Hadas Orgad","Mor Ventura","Dana Arad","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.05846v2.pdf","comment":"Published in: ACL 2024 Project webpage:\n  tokeron.github.io/DiffusionLensWeb"},{"id":"http://arxiv.org/abs/2410.15819v1","updated":"2024-10-21T09:35:57Z","published":"2024-10-21T09:35:57Z","title":"LiMTR: Time Series Motion Prediction for Diverse Road Users through\n  Multimodal Feature Integration","summary":"  Predicting the behavior of road users accurately is crucial to enable the\nsafe operation of autonomous vehicles in urban or densely populated areas.\nTherefore, there has been a growing interest in time series motion prediction\nresearch, leading to significant advancements in state-of-the-art techniques in\nrecent years. However, the potential of using LiDAR data to capture more\ndetailed local features, such as a person's gaze or posture, remains largely\nunexplored. To address this, we develop a novel multimodal approach for motion\nprediction based on the PointNet foundation model architecture, incorporating\nlocal LiDAR features. Evaluation on the Waymo Open Dataset shows a performance\nimprovement of 6.20% and 1.58% in minADE and mAP respectively, when integrated\nand compared with the previous state-of-the-art MTR. We open-source the code of\nour LiMTR model.\n","authors":["Camiel Oerlemans","Bram Grooten","Michiel Braat","Alaa Alassi","Emilia Silvas","Decebal Constantin Mocanu"],"pdf_url":"https://arxiv.org/pdf/2410.15819v1.pdf","comment":"Accepted at the NeurIPS 2024 workshop Time Series in the Age of Large\n  Models. Code available at https://github.com/Cing2/LiMTR"},{"id":"http://arxiv.org/abs/2410.15814v1","updated":"2024-10-21T09:28:42Z","published":"2024-10-21T09:28:42Z","title":"Kaninfradet3D:A Road-side Camera-LiDAR Fusion 3D Perception Model based\n  on Nonlinear Feature Extraction and Intrinsic Correlation","summary":"  With the development of AI-assisted driving, numerous methods have emerged\nfor ego-vehicle 3D perception tasks, but there has been limited research on\nroadside perception. With its ability to provide a global view and a broader\nsensing range, the roadside perspective is worth developing. LiDAR provides\nprecise three-dimensional spatial information, while cameras offer semantic\ninformation. These two modalities are complementary in 3D detection. However,\nadding camera data does not increase accuracy in some studies since the\ninformation extraction and fusion procedure is not sufficiently reliable.\nRecently, Kolmogorov-Arnold Networks (KANs) have been proposed as replacements\nfor MLPs, which are better suited for high-dimensional, complex data. Both the\ncamera and the LiDAR provide high-dimensional information, and employing KANs\nshould enhance the extraction of valuable features to produce better fusion\noutcomes. This paper proposes Kaninfradet3D, which optimizes the feature\nextraction and fusion modules. To extract features from complex\nhigh-dimensional data, the model's encoder and fuser modules were improved\nusing KAN Layers. Cross-attention was applied to enhance feature fusion, and\nvisual comparisons verified that camera features were more evenly integrated.\nThis addressed the issue of camera features being abnormally concentrated,\nnegatively impacting fusion. Compared to the benchmark, our approach shows\nimprovements of +9.87 mAP and +10.64 mAP in the two viewpoints of the TUMTraf\nIntersection Dataset and an improvement of +1.40 mAP in the roadside end of the\nTUMTraf V2X Cooperative Perception Dataset. The results indicate that\nKaninfradet3D can effectively fuse features, demonstrating the potential of\napplying KANs in roadside perception tasks.\n","authors":["Pei Liu","Nanfang Zheng","Yiqun Li","Junlan Chen","Ziyuan Pu"],"pdf_url":"https://arxiv.org/pdf/2410.15814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15812v1","updated":"2024-10-21T09:27:51Z","published":"2024-10-21T09:27:51Z","title":"FusionLungNet: Multi-scale Fusion Convolution with Refinement Network\n  for Lung CT Image Segmentation","summary":"  Early detection of lung cancer is crucial as it increases the chances of\nsuccessful treatment. Automatic lung image segmentation assists doctors in\nidentifying diseases such as lung cancer, COVID-19, and respiratory disorders.\nHowever, lung segmentation is challenging due to overlapping features like\nvascular and bronchial structures, along with pixel-level fusion of brightness,\ncolor, and texture. New lung segmentation methods face difficulties in\nidentifying long-range relationships between image components, reliance on\nconvolution operations that may not capture all critical features, and the\ncomplex structures of the lungs. Furthermore, semantic gaps between feature\nmaps can hinder the integration of relevant information, reducing model\naccuracy. Skip connections can also limit the decoder's access to complete\ninformation, resulting in partial information loss during encoding. To overcome\nthese challenges, we propose a hybrid approach using the FusionLungNet network,\nwhich has a multi-level structure with key components, including the ResNet-50\nencoder, Channel-wise Aggregation Attention (CAA) module, Multi-scale Feature\nFusion (MFF) block, self refinement (SR) module, and multiple decoders. The\nrefinement sub-network uses convolutional neural networks for image\npost-processing to improve quality. Our method employs a combination of loss\nfunctions, including SSIM, IOU, and focal loss, to optimize image\nreconstruction quality. We created and publicly released a new dataset for lung\nsegmentation called LungSegDB, including 1800 CT images from the LIDC-IDRI\ndataset (dataset version 1) and 700 images from the Chest CT Cancer Images from\nKaggle dataset (dataset version 2). Our method achieved an IOU score of 98.04,\noutperforming existing methods and demonstrating significant improvements in\nsegmentation accuracy. https://github.com/sadjadrz/FusionLungNet\n","authors":["Sadjad Rezvani","Mansoor Fateh","Yeganeh Jalali","Amirreza Fateh"],"pdf_url":"https://arxiv.org/pdf/2410.15812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15811v1","updated":"2024-10-21T09:25:49Z","published":"2024-10-21T09:25:49Z","title":"Data-Efficient CLIP-Powered Dual-Branch Networks for Source-Free\n  Unsupervised Domain Adaptation","summary":"  Source-Free Unsupervised Domain Adaptation (SF-UDA) aims to transfer a\nmodel's performance from a labeled source domain to an unlabeled target domain\nwithout direct access to source samples, addressing data privacy issues.\nHowever, most existing SF-UDA approaches assume the availability of abundant\nsource domain samples, which is often impractical due to the high cost of data\nannotation. In this paper, we explore a more challenging scenario where direct\naccess to source domain samples is restricted, and the source domain contains\nonly a few samples. To tackle the dual challenges of limited source data and\nprivacy concerns, we introduce a data-efficient, CLIP-powered dual-branch\nnetwork (CDBN in short). We design a cross-modal dual-branch network that\nintegrates source domain class semantics into the unsupervised fine-tuning of\nthe target domain. It preserves the class information from the source domain\nwhile enhancing the model's generalization to the target domain. Additionally,\nwe propose an unsupervised optimization strategy driven by accurate\nclassification and diversity, which aims to retain the classification\ncapability learned from the source domain while producing more confident and\ndiverse predictions in the target domain. Extensive experiments across 31\ntransfer tasks on 7 public datasets demonstrate that our approach achieves\nstate-of-the-art performance compared to existing methods.\n","authors":["Yongguang Li","Yueqi Cao","Jindong Li","Qi Wang","Shengsheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15802v1","updated":"2024-10-21T09:20:33Z","published":"2024-10-21T09:20:33Z","title":"Assisted Physical Interaction: Autonomous Aerial Robots with Neural\n  Network Detection, Navigation, and Safety Layers","summary":"  The paper introduces a novel framework for safe and autonomous aerial\nphysical interaction in industrial settings. It comprises two main components:\na neural network-based target detection system enhanced with edge computing for\nreduced onboard computational load, and a control barrier function (CBF)-based\ncontroller for safe and precise maneuvering. The target detection system is\ntrained on a dataset under challenging visual conditions and evaluated for\naccuracy across various unseen data with changing lighting conditions. Depth\nfeatures are utilized for target pose estimation, with the entire detection\nframework offloaded into low-latency edge computing. The CBF-based controller\nenables the UAV to converge safely to the target for precise contact. Simulated\nevaluations of both the controller and target detection are presented,\nalongside an analysis of real-world detection performance.\n","authors":["Andrea Berra","Viswa Narayanan Sankaranarayanan","Achilleas Santi Seisa","Julien Mellet","Udayanga G. W. K. N. Gamage","Sumeet Gajanan Satpute","Fabio Ruggiero","Vincenzo Lippiello","Silvia Tolu","Matteo Fumagalli","George Nikolakopoulos","Miguel Ángel Trujillo Soto","Guillermo Heredia"],"pdf_url":"https://arxiv.org/pdf/2410.15802v1.pdf","comment":"8 pages,14 figures, ICUAS 2024"},{"id":"http://arxiv.org/abs/2409.07825v3","updated":"2024-10-21T09:14:47Z","published":"2024-09-12T08:15:39Z","title":"Deep Multimodal Learning with Missing Modality: A Survey","summary":"  During multimodal model training and testing, certain data modalities may be\nabsent due to sensor limitations, cost constraints, privacy concerns, or data\nloss, negatively affecting performance. Multimodal learning techniques designed\nto handle missing modalities can mitigate this by ensuring model robustness\neven when some modalities are unavailable. This survey reviews recent progress\nin Multimodal Learning with Missing Modality (MLMM), focusing on deep learning\nmethods. It provides the first comprehensive survey that covers the motivation\nand distinctions between MLMM and standard multimodal learning setups, followed\nby a detailed analysis of current methods, applications, and datasets,\nconcluding with challenges and future directions.\n","authors":["Renjie Wu","Hu Wang","Hsiang-Ting Chen","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2409.07825v3.pdf","comment":"Submitted to ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2410.15794v1","updated":"2024-10-21T09:06:13Z","published":"2024-10-21T09:06:13Z","title":"Habaek: High-performance water segmentation through dataset expansion\n  and inductive bias optimization","summary":"  Water segmentation is critical to disaster response and water resource\nmanagement. Authorities may employ high-resolution photography to monitor\nrivers, lakes, and reservoirs, allowing for more proactive management in\nagriculture, industry, and conservation. Deep learning has improved flood\nmonitoring by allowing models like CNNs, U-Nets, and transformers to handle\nlarge volumes of satellite and aerial data. However, these models usually have\nsignificant processing requirements, limiting their usage in real-time\napplications. This research proposes upgrading the SegFormer model for water\nsegmentation by data augmentation with datasets such as ADE20K and RIWA to\nboost generalization. We examine how inductive bias affects attention-based\nmodels and discover that SegFormer performs better on bigger datasets. To\nfurther demonstrate the function of data augmentation, Low-Rank Adaptation\n(LoRA) is used to lower processing complexity while preserving accuracy. We\nshow that the suggested Habaek model outperforms current models in\nsegmentation, with an Intersection over Union (IoU) ranging from 0.91986 to\n0.94397. In terms of F1-score, recall, accuracy, and precision, Habaek performs\nbetter than rival models, indicating its potential for real-world applications.\nThis study highlights the need to enhance structures and include datasets for\neffective water segmentation.\n","authors":["Hanseon Joo","Eunji Lee","Minjong Cheon"],"pdf_url":"https://arxiv.org/pdf/2410.15794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15792v1","updated":"2024-10-21T09:02:40Z","published":"2024-10-21T09:02:40Z","title":"WildOcc: A Benchmark for Off-Road 3D Semantic Occupancy Prediction","summary":"  3D semantic occupancy prediction is an essential part of autonomous driving,\nfocusing on capturing the geometric details of scenes. Off-road environments\nare rich in geometric information, therefore it is suitable for 3D semantic\noccupancy prediction tasks to reconstruct such scenes. However, most of\nresearches concentrate on on-road environments, and few methods are designed\nfor off-road 3D semantic occupancy prediction due to the lack of relevant\ndatasets and benchmarks. In response to this gap, we introduce WildOcc, to our\nknowledge, the first benchmark to provide dense occupancy annotations for\noff-road 3D semantic occupancy prediction tasks. A ground truth generation\npipeline is proposed in this paper, which employs a coarse-to-fine\nreconstruction to achieve a more realistic result. Moreover, we introduce a\nmulti-modal 3D semantic occupancy prediction framework, which fuses\nspatio-temporal information from multi-frame images and point clouds at voxel\nlevel. In addition, a cross-modality distillation function is introduced, which\ntransfers geometric knowledge from point clouds to image features.\n","authors":["Heng Zhai","Jilin Mei","Chen Min","Liang Chen","Fangzhou Zhao","Yu Hu"],"pdf_url":"https://arxiv.org/pdf/2410.15792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20648v2","updated":"2024-10-21T08:52:10Z","published":"2024-05-31T07:30:24Z","title":"Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision\n  Models For Video Captioning and Summarization","summary":"  Video is an increasingly prominent and information-dense medium, yet it poses\nsubstantial challenges for language models. A typical video consists of a\nsequence of shorter segments, or shots, that collectively form a coherent\nnarrative. Each shot is analogous to a word in a sentence where multiple data\nstreams of information (such as visual and auditory data) must be processed\nsimultaneously. Comprehension of the entire video requires not only\nunderstanding the visual-audio information of each shot but also requires that\nthe model links the ideas between each shot to generate a larger,\nall-encompassing story. Despite significant progress in the field, current\nworks often overlook videos' more granular shot-by-shot semantic information.\nIn this project, we propose a family of efficient large language vision models\n(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By\nleveraging better pretraining and data collection strategies, we extend the\nabilities of existing small LLVMs from being able to understand a picture to\nbeing able to understand a sequence of frames. Specifically, we show that\nShotluck Holmes achieves better performance than state-of-the-art results on\nthe Shot2Story video captioning and summary task with significantly smaller and\nmore computationally efficient models.\n","authors":["Richard Luo","Austin Peng","Adithya Vasudev","Rishabh Jain"],"pdf_url":"https://arxiv.org/pdf/2405.20648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15780v1","updated":"2024-10-21T08:45:26Z","published":"2024-10-21T08:45:26Z","title":"An Efficient System for Automatic Map Storytelling -- A Case Study on\n  Historical Maps","summary":"  Historical maps provide valuable information and knowledge about the past.\nHowever, as they often feature non-standard projections, hand-drawn styles, and\nartistic elements, it is challenging for non-experts to identify and interpret\nthem. While existing image captioning methods have achieved remarkable success\non natural images, their performance on maps is suboptimal as maps are\nunderrepresented in their pre-training process. Despite the recent advance of\nGPT-4 in text recognition and map captioning, it still has a limited\nunderstanding of maps, as its performance wanes when texts (e.g., titles and\nlegends) in maps are missing or inaccurate. Besides, it is inefficient or even\nimpractical to fine-tune the model with users' own datasets. To address these\nproblems, we propose a novel and lightweight map-captioning counterpart.\nSpecifically, we fine-tune the state-of-the-art vision-language model CLIP to\ngenerate captions relevant to historical maps and enrich the captions with\nGPT-3.5 to tell a brief story regarding where, what, when and why of a given\nmap. We propose a novel decision tree architecture to only generate captions\nrelevant to the specified map type. Our system shows invariance to text\nalterations in maps. The system can be easily adapted and extended to other map\ntypes and scaled to a larger map captioning system. The code is open-sourced at\nhttps://github.com/claudaff/automatic-map-storytelling.\n","authors":["Ziyi Liu","Claudio Affolter","Sidi Wu","Yizi Chen","Lorenz Hurni"],"pdf_url":"https://arxiv.org/pdf/2410.15780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15778v1","updated":"2024-10-21T08:42:30Z","published":"2024-10-21T08:42:30Z","title":"Reducing Hallucinations in Vision-Language Models via Latent Space\n  Steering","summary":"  Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.\n","authors":["Sheng Liu","Haotian Ye","James Zou"],"pdf_url":"https://arxiv.org/pdf/2410.15778v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.15774v1","updated":"2024-10-21T08:36:25Z","published":"2024-10-21T08:36:25Z","title":"Generalizing Motion Planners with Mixture of Experts for Autonomous\n  Driving","summary":"  Large real-world driving datasets have sparked significant research into\nvarious aspects of data-driven motion planners for autonomous driving. These\ninclude data augmentation, model architecture, reward design, training\nstrategies, and planner pipelines. These planners promise better\ngeneralizations on complicated and few-shot cases than previous methods.\nHowever, experiment results show that many of these approaches produce limited\ngeneralization abilities in planning performance due to overly complex designs\nor training paradigms. In this paper, we review and benchmark previous methods\nfocusing on generalizations. The experimental results indicate that as models\nare appropriately scaled, many design elements become redundant. We introduce\nStateTransformer-2 (STR2), a scalable, decoder-only motion planner that uses a\nVision Transformer (ViT) encoder and a mixture-of-experts (MoE) causal\nTransformer architecture. The MoE backbone addresses modality collapse and\nreward balancing by expert routing during training. Extensive experiments on\nthe NuPlan dataset show that our method generalizes better than previous\napproaches across different test sets and closed-loop simulations. Furthermore,\nwe assess its scalability on billions of real-world urban driving scenarios,\ndemonstrating consistent accuracy improvements as both data and model size\ngrow.\n","authors":["Qiao Sun","Huimin Wang","Jiahao Zhan","Fan Nie","Xin Wen","Leimeng Xu","Kun Zhan","Peng Jia","Xianpeng Lang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.15774v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.15768v1","updated":"2024-10-21T08:28:11Z","published":"2024-10-21T08:28:11Z","title":"Learning to Synthesize Graphics Programs for Geometric Artworks","summary":"  Creating and understanding art has long been a hallmark of human ability.\nWhen presented with finished digital artwork, professional graphic artists can\nintuitively deconstruct and replicate it using various drawing tools, such as\nthe line tool, paint bucket, and layer features, including opacity and blending\nmodes. While most recent research in this field has focused on art generation,\nproposing a range of methods, these often rely on the concept of artwork being\nrepresented as a final image. To bridge the gap between pixel-level results and\nthe actual drawing process, we present an approach that treats a set of drawing\ntools as executable programs. This method predicts a sequence of steps to\nachieve the final image, allowing for understandable and resolution-independent\nreproductions under the usage of a set of drawing commands. Our experiments\ndemonstrate that our program synthesizer, Art2Prog, can comprehensively\nunderstand complex input images and reproduce them using high-quality\nexecutable programs. The experimental results evidence the potential of\nmachines to grasp higher-level information from images and generate compact\nprogram-level descriptions.\n","authors":["Qi Bing","Chaoyi Zhang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2410.15768v1.pdf","comment":"ICPR 2024"},{"id":"http://arxiv.org/abs/2410.15767v1","updated":"2024-10-21T08:27:13Z","published":"2024-10-21T08:27:13Z","title":"Improving Instance Optimization in Deformable Image Registration with\n  Gradient Projection","summary":"  Deformable image registration is inherently a multi-objective optimization\n(MOO) problem, requiring a delicate balance between image similarity and\ndeformation regularity. These conflicting objectives often lead to poor\noptimization outcomes, such as being trapped in unsatisfactory local minima or\nexperiencing slow convergence. Deep learning methods have recently gained\npopularity in this domain due to their efficiency in processing large datasets\nand achieving high accuracy. However, they often underperform during test time\ncompared to traditional optimization techniques, which further explore\niterative, instance-specific gradient-based optimization. This performance gap\nis more pronounced when a distribution shift between training and test data\nexists. To address this issue, we focus on the instance optimization (IO)\nparadigm, which involves additional optimization for test-time instances based\non a pre-trained model. IO effectively combines the generalization capabilities\nof deep learning with the fine-tuning advantages of instance-specific\noptimization. Within this framework, we emphasize the use of gradient\nprojection to mitigate conflicting updates in MOO. This technique projects\nconflicting gradients into a common space, better aligning the dual objectives\nand enhancing optimization stability. We validate our method using a\nstate-of-the-art foundation model on the 3D Brain inter-subject registration\ntask (LUMIR) from the Learn2Reg 2024 Challenge. Our results show significant\nimprovements over standard gradient descent, leading to more accurate and\nreliable registration results.\n","authors":["Yi Zhang","Yidong Zhao","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2410.15767v1.pdf","comment":"L2R 2024 Challenge Paper"},{"id":"http://arxiv.org/abs/2410.15766v1","updated":"2024-10-21T08:24:46Z","published":"2024-10-21T08:24:46Z","title":"How Important are Data Augmentations to Close the Domain Gap for Object\n  Detection in Orbit?","summary":"  We investigate the efficacy of data augmentations to close the domain gap in\nspaceborne computer vision, crucial for autonomous operations like on-orbit\nservicing. As the use of computer vision in space increases, challenges such as\nhostile illumination and low signal-to-noise ratios significantly hinder\nperformance. While learning-based algorithms show promising results, their\nadoption is limited by the need for extensive annotated training data and the\ndomain gap that arises from differences between synthesized and real-world\nimagery. This study explores domain generalization in terms of data\naugmentations -- classical color and geometric transformations, corruptions,\nand noise -- to enhance model performance across the domain gap. To this end,\nwe conduct an large scale experiment using a hyperparameter optimization\npipeline that samples hundreds of different configurations and searches for the\nbest set to bridge the domain gap. As a reference task, we use 2D object\ndetection and evaluate on the SPEED+ dataset that contains real\nhardware-in-the-loop satellite images in its test set. Moreover, we evaluate\nfour popular object detectors, including Mask R-CNN, Faster R-CNN, YOLO-v7, and\nthe open set detector GroundingDINO, and highlight their trade-offs between\nperformance, inference speed, and training time. Our results underscore the\nvital role of data augmentations in bridging the domain gap, improving model\nperformance, robustness, and reliability for critical space applications. As a\nresult, we propose two novel data augmentations specifically developed to\nemulate the visual effects observed in orbital imagery. We conclude by\nrecommending the most effective augmentations for advancing computer vision in\nchallenging orbital environments. Code for training detectors and\nhyperparameter search will be made publicly available.\n","authors":["Maximilian Ulmer","Leonard Klüpfel","Maximilian Durner","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2410.15766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15760v1","updated":"2024-10-21T08:20:19Z","published":"2024-10-21T08:20:19Z","title":"DeepIcon: A Hierarchical Network for Layer-wise Icon Vectorization","summary":"  In contrast to the well-established technique of rasterization, vectorization\nof images poses a significant challenge in the field of computer graphics.\nRecent learning-based methods for converting raster images to vector formats\nfrequently suffer from incomplete shapes, redundant path prediction, and a lack\nof accuracy in preserving the semantics of the original content. These\nshortcomings severely hinder the utility of these methods for further editing\nand manipulation of images. To address these challenges, we present DeepIcon, a\nnovel hierarchical image vectorization network specifically tailored for\ngenerating variable-length icon vector graphics based on the raster image\ninput. Our experimental results indicate that DeepIcon can efficiently produce\nScalable Vector Graphics (SVGs) directly from raster images, bypassing the need\nfor a differentiable rasterizer while also demonstrating a profound\nunderstanding of the image contents.\n","authors":["Qi Bing","Chaoyi Zhang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2410.15760v1.pdf","comment":"Accepted as Oral Presentation at DICTA 2024"},{"id":"http://arxiv.org/abs/2408.10188v4","updated":"2024-10-21T08:12:42Z","published":"2024-08-19T17:48:08Z","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","summary":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models \\qinghao{by co-designing the\nalgorithm and system. For model training, we upgrade existing VLMs to support\nlong video understanding by incorporating two additional stages, {\\em i.e.},\nlong context extension and long video supervised fine-tuning. However, training\non long video is computationally and memory intensive. We introduce the\nlong-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently\nparallelizes long video training and inference, enabling 2M context length\ntraining on 256 GPUs without any gradient checkpointing. LongVILA efficiently\nextends the number of video frames of VILA from 8 to 2048, improving the long\nvideo captioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy\nin 6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%\nwith subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence\nparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.\n","authors":["Fuzhao Xue","Yukang Chen","Dacheng Li","Qinghao Hu","Ligeng Zhu","Xiuyu Li","Yunhao Fang","Haotian Tang","Shang Yang","Zhijian Liu","Ethan He","Hongxu Yin","Pavlo Molchanov","Jan Kautz","Linxi Fan","Yuke Zhu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2408.10188v4.pdf","comment":"Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md"},{"id":"http://arxiv.org/abs/2410.15744v1","updated":"2024-10-21T08:01:58Z","published":"2024-10-21T08:01:58Z","title":"Unleashing the Potential of Vision-Language Pre-Training for 3D\n  Zero-Shot Lesion Segmentation via Mask-Attribute Alignment","summary":"  Recent advancements in medical vision-language pre-training models have\ndriven significant progress in zero-shot disease recognition. However,\ntransferring image-level knowledge to pixel-level tasks, such as lesion\nsegmentation in 3D CT scans, remains a critical challenge. Due to the\ncomplexity and variability of pathological visual characteristics, existing\nmethods struggle to align fine-grained lesion features not encountered during\ntraining with disease-related textual representations. In this paper, we\npresent Malenia, a novel multi-scale lesion-level mask-attribute alignment\nframework, specifically designed for 3D zero-shot lesion segmentation. Malenia\nimproves the compatibility between mask representations and their associated\nelemental attributes, explicitly linking the visual features of unseen lesions\nwith the extensible knowledge learned from previously seen ones. Furthermore,\nwe design a Cross-Modal Knowledge Injection module to enhance both visual and\ntextual features with mutually beneficial information, effectively guiding the\ngeneration of segmentation results. Comprehensive experiments across three\ndatasets and 12 lesion categories validate the superior performance of Malenia.\nCodes will be publicly available.\n","authors":["Yankai Jiang","Wenhui Lei","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15169v2","updated":"2024-10-21T07:57:56Z","published":"2024-07-21T13:58:43Z","title":"Back-in-Time Diffusion: Unsupervised Detection of Medical Deepfakes","summary":"  Recent progress in generative models has made it easier for a wide audience\nto edit and create image content, raising concerns about the proliferation of\ndeepfakes, especially in healthcare. Despite the availability of numerous\ntechniques for detecting manipulated images captured by conventional cameras,\ntheir applicability to medical images is limited. This limitation stems from\nthe distinctive forensic characteristics of medical images, a result of their\nimaging process.\n  In this work we propose a novel anomaly detector for medical imagery based on\ndiffusion models. Normally, diffusion models are used to generate images.\nHowever, we show how a similar process can be used to detect synthetic content\nby making a model reverse the diffusion on a suspected image. We evaluate our\nmethod on the task of detecting fake tumors injected and removed from CT and\nMRI scans. Our method significantly outperforms other state of the art\nunsupervised detectors with an increased AUC of 0.9 from 0.79 for injection and\nof 0.96 from 0.91 for removal on average. We also explore our hypothesis using\nAI explainability tools and publish our code and new medical deepfake datasets\nto encourage further research into this domain.\n","authors":["Fred Grabovski","Lior Yasur","Guy Amit","Yisroel Mirsky"],"pdf_url":"https://arxiv.org/pdf/2407.15169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15732v1","updated":"2024-10-21T07:51:17Z","published":"2024-10-21T07:51:17Z","title":"ViMoE: An Empirical Study of Designing Vision Mixture-of-Experts","summary":"  Mixture-of-Experts (MoE) models embody the divide-and-conquer concept and are\na promising approach for increasing model capacity, demonstrating excellent\nscalability across multiple domains. In this paper, we integrate the MoE\nstructure into the classic Vision Transformer (ViT), naming it ViMoE, and\nexplore the potential of applying MoE to vision through a comprehensive study\non image classification. However, we observe that the performance is sensitive\nto the configuration of MoE layers, making it challenging to obtain optimal\nresults without careful design. The underlying cause is that inappropriate MoE\nlayers lead to unreliable routing and hinder experts from effectively acquiring\nhelpful knowledge. To address this, we introduce a shared expert to learn and\ncapture common information, serving as an effective way to construct stable\nViMoE. Furthermore, we demonstrate how to analyze expert routing behavior,\nrevealing which MoE layers are capable of specializing in handling specific\ninformation and which are not. This provides guidance for retaining the\ncritical layers while removing redundancies, thereby advancing ViMoE to be more\nefficient without sacrificing accuracy. We aspire for this work to offer new\ninsights into the design of vision MoE models and provide valuable empirical\nguidance for future research.\n","authors":["Xumeng Han","Longhui Wei","Zhiyang Dou","Zipeng Wang","Chenhui Qiang","Xin He","Yingfei Sun","Zhenjun Han","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2410.15732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15728v1","updated":"2024-10-21T07:44:44Z","published":"2024-10-21T07:44:44Z","title":"Object-Centric Temporal Consistency via Conditional Autoregressive\n  Inductive Biases","summary":"  Unsupervised object-centric learning from videos is a promising approach\ntowards learning compositional representations that can be applied to various\ndownstream tasks, such as prediction and reasoning. Recently, it was shown that\npretrained Vision Transformers (ViTs) can be useful to learn object-centric\nrepresentations on real-world video datasets. However, while these approaches\nsucceed at extracting objects from the scenes, the slot-based representations\nfail to maintain temporal consistency across consecutive frames in a video,\ni.e. the mapping of objects to slots changes across the video. To address this,\nwe introduce Conditional Autoregressive Slot Attention (CA-SA), a framework\nthat enhances the temporal consistency of extracted object-centric\nrepresentations in video-centric vision tasks. Leveraging an autoregressive\nprior network to condition representations on previous timesteps and a novel\nconsistency loss function, CA-SA predicts future slot representations and\nimposes consistency across frames. We present qualitative and quantitative\nresults showing that our proposed method outperforms the considered baselines\non downstream tasks, such as video prediction and visual question-answering\ntasks.\n","authors":["Cristian Meo","Akihiro Nakano","Mircea Lică","Aniket Didolkar","Masahiro Suzuki","Anirudh Goyal","Mengmi Zhang","Justin Dauwels","Yutaka Matsuo","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2410.15728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15209v2","updated":"2024-10-21T07:34:04Z","published":"2024-05-24T04:36:13Z","title":"Motion Segmentation for Neuromorphic Aerial Surveillance","summary":"  Aerial surveillance demands rapid and precise detection of moving objects in\ndynamic environments. Event cameras, which draw inspiration from biological\nvision systems, present a promising alternative to frame-based sensors due to\ntheir exceptional temporal resolution, superior dynamic range, and minimal\npower requirements. Unlike traditional frame-based sensors that capture\nredundant information at fixed intervals, event cameras asynchronously record\npixel-level brightness changes, providing a continuous and efficient data\nstream ideal for fast motion segmentation. While these sensors are ideal for\nfast motion segmentation, existing event-based motion segmentation methods\noften suffer from limitations such as the need for per-scene parameter tuning\nor reliance on manual labelling, hindering their scalability and practical\ndeployment. In this paper, we address these challenges by introducing a novel\nmotion segmentation method that leverages self-supervised vision transformers\non both event data and optical flow information. Our approach eliminates the\nneed for human annotations and reduces dependency on scene-specific parameters.\nIn this paper, we used the EVK4-HD Prophesee event camera onboard a highly\ndynamic aerial platform in urban settings. We conduct extensive evaluations of\nour framework across multiple datasets, demonstrating state-of-the-art\nperformance compared to existing benchmarks. Our method can effectively handle\nvarious types of motion and an arbitrary number of moving objects. Code and\ndataset are available at: \\url{https://samiarja.github.io/evairborne/}\n","authors":["Sami Arja","Alexandre Marcireau","Saeed Afshar","Bharath Ramesh","Gregory Cohen"],"pdf_url":"https://arxiv.org/pdf/2405.15209v2.pdf","comment":"17 pages, 11 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.12931v5","updated":"2024-10-21T07:32:04Z","published":"2024-03-19T17:34:27Z","title":"You Only Sample Once: Taming One-Step Text-to-Image Synthesis by\n  Self-Cooperative Diffusion GANs","summary":"  Recently, some works have tried to combine diffusion and Generative\nAdversarial Networks (GANs) to alleviate the computational cost of the\niterative denoising inference in Diffusion Models (DMs). However, existing\nworks in this line suffer from either training instability and mode collapse or\nsubpar one-step generation learning efficiency. To address these issues, we\nintroduce YOSO, a novel generative model designed for rapid, scalable, and\nhigh-fidelity one-step image synthesis with high training stability and mode\ncoverage. Specifically, we smooth the adversarial divergence by the denoising\ngenerator itself, performing self-cooperative learning. We show that our method\ncan serve as a one-step generation model training from scratch with competitive\nperformance. Moreover, we extend our YOSO to one-step text-to-image generation\nbased on pre-trained models by several effective training techniques (i.e.,\nlatent perceptual loss and latent discriminator for efficient training along\nwith the latent DMs; the informative prior initialization (IPI), and the quick\nadaption stage for fixing the flawed noise scheduler). Experimental results\nshow that YOSO achieves the state-of-the-art one-step generation performance\neven with Low-Rank Adaptation (LoRA) fine-tuning. In particular, we show that\nthe YOSO-PixArt-$\\alpha$ can generate images in one step trained on 512\nresolution, with the capability of adapting to 1024 resolution without extra\nexplicit training, requiring only ~10 A800 days for fine-tuning. Our code is\nprovided at https://github.com/Luo-Yihong/YOSO.\n","authors":["Yihong Luo","Xiaolong Chen","Xinghua Qu","Tianyang Hu","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2403.12931v5.pdf","comment":"Revision"},{"id":"http://arxiv.org/abs/2404.12020v3","updated":"2024-10-21T07:23:37Z","published":"2024-04-18T09:16:02Z","title":"Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question\n  Answering","summary":"  Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning\ntask, demanding intelligent systems to accurately respond to natural language\nqueries based on audio-video input pairs. Nevertheless, prevalent AVQA\napproaches are prone to overlearning dataset biases, resulting in poor\nrobustness. Furthermore, current datasets may not provide a precise diagnostic\nfor these methods. To tackle these challenges, firstly, we propose a novel\ndataset, MUSIC-AVQA-R, crafted in two steps: rephrasing questions within the\ntest split of a public dataset (MUSIC-AVQA) and subsequently introducing\ndistribution shifts to split questions. The former leads to a large, diverse\ntest space, while the latter results in a comprehensive robustness evaluation\non rare, frequent, and overall questions. Secondly, we propose a robust\narchitecture that utilizes a multifaceted cycle collaborative debiasing\nstrategy to overcome bias learning. Experimental results show that this\narchitecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably\nobtaining a significant improvement of 9.32%. Extensive ablation experiments\nare conducted on the two datasets mentioned to analyze the component\neffectiveness within the debiasing strategy. Additionally, we highlight the\nlimited robustness of existing multi-modal QA methods through the evaluation on\nour dataset. We also conduct experiments combining various baselines with our\nproposed strategy on two datasets to verify its plug-and-play capability. Our\ndataset and code are available at https://github.com/reml-group/MUSIC-AVQA-R.\n","authors":["Jie Ma","Min Hu","Pinghui Wang","Wangchun Sun","Lingyun Song","Hongbin Pei","Jun Liu","Youtian Du"],"pdf_url":"https://arxiv.org/pdf/2404.12020v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15701v1","updated":"2024-10-21T07:18:24Z","published":"2024-10-21T07:18:24Z","title":"Students Rather Than Experts: A New AI For Education Pipeline To Model\n  More Human-Like And Personalised Early Adolescences","summary":"  The capabilities of large language models (LLMs) have been applied in expert\nsystems across various domains, providing new opportunities for AI in\nEducation. Educational interactions involve a cyclical exchange between\nteachers and students. Current research predominantly focuses on using LLMs to\nsimulate teachers, leveraging their expertise to enhance student learning\noutcomes. However, the simulation of students, which could improve teachers'\ninstructional skills, has received insufficient attention due to the challenges\nof modeling and evaluating virtual students. This research asks: Can LLMs be\nutilized to develop virtual student agents that mimic human-like behavior and\nindividual variability? Unlike expert systems focusing on knowledge delivery,\nvirtual students must replicate learning difficulties, emotional responses, and\nlinguistic uncertainties. These traits present significant challenges in both\nmodeling and evaluation. To address these issues, this study focuses on\nlanguage learning as a context for modeling virtual student agents. We propose\na novel AI4Education framework, called SOE (Scene-Object-Evaluation), to\nsystematically construct LVSA (LLM-based Virtual Student Agents). By curating a\ndataset of personalized teacher-student interactions with various personality\ntraits, question types, and learning stages, and fine-tuning LLMs using LoRA,\nwe conduct multi-dimensional evaluation experiments. Specifically, we: (1)\ndevelop a theoretical framework for generating LVSA; (2) integrate human\nsubjective evaluation metrics into GPT-4 assessments, demonstrating a strong\ncorrelation between human evaluators and GPT-4 in judging LVSA authenticity;\nand (3) validate that LLMs can generate human-like, personalized virtual\nstudent agents in educational contexts, laying a foundation for future\napplications in pre-service teacher training and multi-agent simulation\nenvironments.\n","authors":["Yiping Ma","Shiyu Hu","Xuchen Li","Yipei Wang","Shiqing Liu","Kang Hao Cheong"],"pdf_url":"https://arxiv.org/pdf/2410.15701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15694v1","updated":"2024-10-21T07:05:07Z","published":"2024-10-21T07:05:07Z","title":"PALMS: Plane-based Accessible Indoor Localization Using Mobile\n  Smartphones","summary":"  In this paper, we present PALMS, an innovative indoor global localization and\nrelocalization system for mobile smartphones that utilizes publicly available\nfloor plans. Unlike most vision-based methods that require constant visual\ninput, our system adopts a dynamic form of localization that considers a single\ninstantaneous observation and odometry data. The core contribution of this work\nis the introduction of a particle filter initialization method that leverages\nthe Certainly Empty Space (CES) constraint along with principal orientation\nmatching. This approach creates a spatial probability distribution of the\ndevice's location, significantly improving localization accuracy and reducing\nparticle filter convergence time. Our experimental evaluations demonstrate that\nPALMS outperforms traditional methods with uniformly initialized particle\nfilters, providing a more efficient and accessible approach to indoor\nwayfinding. By eliminating the need for prior environmental fingerprinting,\nPALMS provides a scalable and practical approach to indoor navigation.\n","authors":["Yunqian Cheng","Roberto Manduchi"],"pdf_url":"https://arxiv.org/pdf/2410.15694v1.pdf","comment":"7 pages, 3 figures, accepted to the 14th International Conference on\n  Indoor Positioning and Indoor Navigation (IPIN) 2024, Best Presentation Award"},{"id":"http://arxiv.org/abs/2410.15689v1","updated":"2024-10-21T06:59:04Z","published":"2024-10-21T06:59:04Z","title":"Enhancing SNN-based Spatio-Temporal Learning: A Benchmark Dataset and\n  Cross-Modality Attention Model","summary":"  Spiking Neural Networks (SNNs), renowned for their low power consumption,\nbrain-inspired architecture, and spatio-temporal representation capabilities,\nhave garnered considerable attention in recent years. Similar to Artificial\nNeural Networks (ANNs), high-quality benchmark datasets are of great importance\nto the advances of SNNs. However, our analysis indicates that many prevalent\nneuromorphic datasets lack strong temporal correlation, preventing SNNs from\nfully exploiting their spatio-temporal representation capabilities. Meanwhile,\nthe integration of event and frame modalities offers more comprehensive visual\nspatio-temporal information. Yet, the SNN-based cross-modality fusion remains\nunderexplored.\n  In this work, we present a neuromorphic dataset called DVS-SLR that can\nbetter exploit the inherent spatio-temporal properties of SNNs. Compared to\nexisting datasets, it offers advantages in terms of higher temporal\ncorrelation, larger scale, and more varied scenarios. In addition, our\nneuromorphic dataset contains corresponding frame data, which can be used for\ndeveloping SNN-based fusion methods. By virtue of the dual-modal feature of the\ndataset, we propose a Cross-Modality Attention (CMA) based fusion method. The\nCMA model efficiently utilizes the unique advantages of each modality, allowing\nfor SNNs to learn both temporal and spatial attention scores from the\nspatio-temporal features of event and frame modalities, subsequently allocating\nthese scores across modalities to enhance their synergy. Experimental results\ndemonstrate that our method not only improves recognition accuracy but also\nensures robustness across diverse scenarios.\n","authors":["Shibo Zhou","Bo Yang","Mengwen Yuan","Runhao Jiang","Rui Yan","Gang Pan","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.15689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10532v2","updated":"2024-10-21T06:50:51Z","published":"2024-08-20T04:18:53Z","title":"NutrifyAI: An AI-Powered System for Real-Time Food Detection,\n  Nutritional Analysis, and Personalized Meal Recommendations","summary":"  With diet and nutrition apps reaching 1.4 billion users in 2022 [1], it's not\nsurprise that popular health apps, MyFitnessPal, Noom, and Calorie Counter, are\nsurging in popularity. However, one major setback [2] of nearly all nutrition\napplications is that users must enter food data manually, which is\ntime-consuming and tedious. Thus, there has been an increasing demand for\napplications that can accurately identify food items, analyze their nutritional\ncontent, and offer dietary recommendations in real-time. This paper introduces\na comprehensive system that combines advanced computer vision techniques with\nnutritional analysis, implemented in a versatile mobile and web application.\nThe system is divided into three key concepts: 1) food detection using the\nYOLOv8 model, 2) nutrient analysis via the Edamam Nutrition Analysis API, and\n3) personalized meal recommendations using the Edamam Meal Planning and Recipe\nSearch APIs. Preliminary results showcase the system's effectiveness by\nproviding immediate, accurate dietary insights, with a demonstrated food\nrecognition accuracy of nearly 80%, making it a valuable tool for users to make\ninformed dietary decisions.\n","authors":["Michelle Han","Junyao Chen","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.10532v2.pdf","comment":"4 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.05273v2","updated":"2024-10-21T06:50:05Z","published":"2024-09-12T09:18:09Z","title":"HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers","summary":"  Large Vision-Language-Action (VLA) models, leveraging powerful pre trained\nVision-Language Models (VLMs) backends, have shown promise in robotic control\ndue to their impressive generalization ability. However, the success comes at a\ncost. Their reliance on VLM backends with billions of parameters leads to high\ncomputational costs and inference latency, limiting the testing scenarios to\nmainly quasi-static tasks and hindering performance in dynamic tasks requiring\nrapid interactions. To address these limitations, this paper proposes HiRT, a\nHierarchical Robot Transformer framework that enables flexible frequency and\nperformance trade-off. HiRT keeps VLMs running at low frequencies to capture\ntemporarily invariant features while enabling real-time interaction through a\nhigh-frequency vision-based policy guided by the slowly updated features.\nExperiment results in both simulation and real-world settings demonstrate\nsignificant improvements over baseline methods. Empirically, in static tasks,\nwe double the control frequency and achieve comparable success rates.\nAdditionally, on novel real-world dynamic ma nipulation tasks which are\nchallenging for previous VLA models, HiRT improves the success rate from 48% to\n75%.\n","authors":["Jianke Zhang","Yanjiang Guo","Xiaoyu Chen","Yen-Jen Wang","Yucheng Hu","Chengming Shi","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15682v1","updated":"2024-10-21T06:46:49Z","published":"2024-10-21T06:46:49Z","title":"RANSAC Back to SOTA: A Two-stage Consensus Filtering for Real-time 3D\n  Registration","summary":"  Correspondence-based point cloud registration (PCR) plays a key role in\nrobotics and computer vision. However, challenges like sensor noises, object\nocclusions, and descriptor limitations inevitably result in numerous outliers.\nRANSAC family is the most popular outlier removal solution. However, the\nrequisite iterations escalate exponentially with the outlier ratio, rendering\nit far inferior to existing methods (SC2PCR [1], MAC [2], etc.) in terms of\naccuracy or speed. Thus, we propose a two-stage consensus filtering (TCF) that\nelevates RANSAC to state-of-the-art (SOTA) speed and accuracy. Firstly,\none-point RANSAC obtains a consensus set based on length consistency.\nSubsequently, two-point RANSAC refines the set via angle consistency. Then,\nthree-point RANSAC computes a coarse pose and removes outliers based on\ntransformed correspondence's distances. Drawing on optimizations from one-point\nand two-point RANSAC, three-point RANSAC requires only a few iterations.\nEventually, an iterative reweighted least squares (IRLS) is applied to yield\nthe optimal pose. Experiments on the large-scale KITTI and ETH datasets\ndemonstrate our method achieves up to three-orders-of-magnitude speedup\ncompared to MAC while maintaining registration accuracy and recall. Our code is\navailable at https://github.com/ShiPC-AI/TCF.\n","authors":["Pengcheng Shi","Shaocheng Yan","Yilin Xiao","Xinyi Liu","Yongjun Zhang","Jiayuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.15682v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.06403v4","updated":"2024-10-21T06:44:01Z","published":"2024-03-11T03:28:20Z","title":"PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via\n  Foundation Models","summary":"  Recent success of vision foundation models have shown promising performance\nfor the 2D perception tasks. However, it is difficult to train a 3D foundation\nnetwork directly due to the limited dataset and it remains under explored\nwhether existing foundation models can be lifted to 3D space seamlessly. In\nthis paper, we present PointSeg, a novel training-free paradigm that leverages\noff-the-shelf vision foundation models to address 3D scene perception tasks.\nPointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to\nalign their corresponding pixels across frames. Concretely, we design a\ntwo-branch prompts learning structure to construct the 3D point-box prompts\npairs, combining with the bidirectional matching strategy for accurate point\nand proposal prompts generation. Then, we perform the iterative post-refinement\nadaptively when cooperated with different vision foundation models. Moreover,\nwe design a affinity-aware merging algorithm to improve the final ensemble\nmasks. PointSeg demonstrates impressive segmentation performance across various\ndatasets, all without training. Specifically, our approach significantly\nsurpasses the state-of-the-art specialist training-free model by 14.1$\\%$,\n12.3$\\%$, and 12.6$\\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets,\nrespectively. On top of that, PointSeg can incorporate with various foundation\nmodels and even surpasses the specialist training-based methods by\n3.4$\\%$-5.4$\\%$ mAP across various datasets, serving as an effective generalist\nmodel.\n","authors":["Qingdong He","Jinlong Peng","Zhengkai Jiang","Xiaobin Hu","Jiangning Zhang","Qiang Nie","Yabiao Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06403v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15674v1","updated":"2024-10-21T06:37:13Z","published":"2024-10-21T06:37:13Z","title":"TALoS: Enhancing Semantic Scene Completion via Test-time Adaptation on\n  the Line of Sight","summary":"  Semantic Scene Completion (SSC) aims to perform geometric completion and\nsemantic segmentation simultaneously. Despite the promising results achieved by\nexisting studies, the inherently ill-posed nature of the task presents\nsignificant challenges in diverse driving scenarios. This paper introduces\nTALoS, a novel test-time adaptation approach for SSC that excavates the\ninformation available in driving environments. Specifically, we focus on that\nobservations made at a certain moment can serve as Ground Truth (GT) for scene\ncompletion at another moment. Given the characteristics of the LiDAR sensor, an\nobservation of an object at a certain location confirms both 1) the occupation\nof that location and 2) the absence of obstacles along the line of sight from\nthe LiDAR to that point. TALoS utilizes these observations to obtain\nself-supervision about occupancy and emptiness, guiding the model to adapt to\nthe scene in test time. In a similar manner, we aggregate reliable SSC\npredictions among multiple moments and leverage them as semantic pseudo-GT for\nadaptation. Further, to leverage future observations that are not accessible at\nthe current time, we present a dual optimization scheme using the model in\nwhich the update is delayed until the future observation is available.\nEvaluations on the SemanticKITTI validation and test sets demonstrate that\nTALoS significantly improves the performance of the pre-trained SSC model. Our\ncode is available at https://github.com/blue-531/TALoS.\n","authors":["Hyun-Kurl Jang","Jihun Kim","Hyeokjun Kweon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.15674v1.pdf","comment":"Accepted at NeurIPS 2024. Code is available at\n  https://github.com/blue-531/TALoS"},{"id":"http://arxiv.org/abs/2410.04419v2","updated":"2024-10-21T06:35:08Z","published":"2024-10-06T09:26:07Z","title":"LiteVLoc: Map-Lite Visual Localization for Image Goal Navigation","summary":"  This paper presents LiteVLoc, a hierarchical visual localization framework\nthat uses a lightweight topo-metric map to represent the environment. The\nmethod consists of three sequential modules that estimate camera poses in a\ncoarse-to-fine manner. Unlike mainstream approaches relying on detailed 3D\nrepresentations, LiteVLoc reduces storage overhead by leveraging learning-based\nfeature matching and geometric solvers for metric pose estimation. A novel\ndataset for the map-free relocalization task is also introduced. Extensive\nexperiments including localization and navigation in both simulated and\nreal-world scenarios have validate the system's performance and demonstrated\nits precision and efficiency for large-scale deployment. Code and data will be\nmade publicly available.\n","authors":["Jianhao Jiao","Jinhao He","Changkun Liu","Sebastian Aegidius","Xiangcheng Hu","Tristan Braud","Dimitrios Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2410.04419v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.13165v2","updated":"2024-10-21T06:25:57Z","published":"2024-06-19T02:42:29Z","title":"Cardiac Copilot: Automatic Probe Guidance for Echocardiography with\n  World Model","summary":"  Echocardiography is the only technique capable of real-time imaging of the\nheart and is vital for diagnosing the majority of cardiac diseases. However,\nthere is a severe shortage of experienced cardiac sonographers, due to the\nheart's complex structure and significant operational challenges. To mitigate\nthis situation, we present a Cardiac Copilot system capable of providing\nreal-time probe movement guidance to assist less experienced sonographers in\nconducting freehand echocardiography. This system can enable non-experts,\nespecially in primary departments and medically underserved areas, to perform\ncardiac ultrasound examinations, potentially improving global healthcare\ndelivery. The core innovation lies in proposing a data-driven world model,\nnamed Cardiac Dreamer, for representing cardiac spatial structures. This world\nmodel can provide structure features of any cardiac planes around the current\nprobe position in the latent space, serving as an precise navigation map for\nautonomous plane localization. We train our model with real-world ultrasound\ndata and corresponding probe motion from 110 routine clinical scans with 151K\nsample pairs by three certified sonographers. Evaluations on three standard\nplanes with 37K sample pairs demonstrate that the world model can reduce\nnavigation errors by up to 33\\% and exhibit more stable performance.\n","authors":["Haojun Jiang","Zhenguo Sun","Ning Jia","Meng Li","Yu Sun","Shaqi Luo","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.13165v2.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2410.15670v1","updated":"2024-10-21T06:23:13Z","published":"2024-10-21T06:23:13Z","title":"Transforming Blood Cell Detection and Classification with Advanced Deep\n  Learning Models: A Comparative Study","summary":"  Efficient detection and classification of blood cells are vital for accurate\ndiagnosis and effective treatment of blood disorders. This study utilizes a\nYOLOv10 model trained on Roboflow data with images resized to 640x640 pixels\nacross varying epochs. The results show that increased training epochs\nsignificantly enhance accuracy, precision, and recall, particularly in\nreal-time blood cell detection & classification. The YOLOv10 model outperforms\nMobileNetV2, ShuffleNetV2, and DarkNet in real-time performance, though\nMobileNetV2 and ShuffleNetV2 are more computationally efficient, and DarkNet\nexcels in feature extraction for blood cell classification. This research\nhighlights the potential of integrating deep learning models like YOLOv10,\nMobileNetV2, ShuffleNetV2, and DarkNet into clinical workflows, promising\nimprovements in diagnostic accuracy and efficiency. Additionally, a new,\nwell-annotated blood cell dataset was created and will be open-sourced to\nsupport further advancements in automatic blood cell detection and\nclassification. The findings demonstrate the transformative impact of these\nmodels in revolutionizing medical diagnostics and enhancing blood disorder\nmanagement\n","authors":["Shilpa Choudhary","Sandeep Kumar","Pammi Sri Siddhaarth","Guntu Charitasri"],"pdf_url":"https://arxiv.org/pdf/2410.15670v1.pdf","comment":"26 pages, 4884 Words, 17 Figures, 10 Tables"},{"id":"http://arxiv.org/abs/2305.18512v2","updated":"2024-10-21T05:59:50Z","published":"2023-05-29T17:09:26Z","title":"A Rainbow in Deep Network Black Boxes","summary":"  A central question in deep learning is to understand the functions learned by\ndeep networks. What is their approximation class? Do the learned weights and\nrepresentations depend on initialization? Previous empirical work has evidenced\nthat kernels defined by network activations are similar across initializations.\nFor shallow networks, this has been theoretically studied with random feature\nmodels, but an extension to deep networks has remained elusive. Here, we\nprovide a deep extension of such random feature models, which we call the\nrainbow model. We prove that rainbow networks define deterministic\n(hierarchical) kernels in the infinite-width limit. The resulting functions\nthus belong to a data-dependent RKHS which does not depend on the weight\nrandomness. We also verify numerically our modeling assumptions on deep CNNs\ntrained on image classification tasks, and show that the trained networks\napproximately satisfy the rainbow hypothesis. In particular, rainbow networks\nsampled from the corresponding random feature model achieve similar performance\nas the trained networks. Our results highlight the central role played by the\ncovariances of network weights at each layer, which are observed to be low-rank\nas a result of feature learning.\n","authors":["Florentin Guth","Brice Ménard","Gaspar Rochette","Stéphane Mallat"],"pdf_url":"https://arxiv.org/pdf/2305.18512v2.pdf","comment":"59 pages, 10 figures. To appear at JMLR"},{"id":"http://arxiv.org/abs/2410.15658v1","updated":"2024-10-21T05:56:31Z","published":"2024-10-21T05:56:31Z","title":"Calibration of ordinal regression networks","summary":"  Recent studies have shown that deep neural networks are not well-calibrated\nand produce over-confident predictions. The miscalibration issue primarily\nstems from the minimization of cross-entropy, which aims to align predicted\nsoftmax probabilities with one-hot labels. In ordinal regression tasks, this\nproblem is compounded by an additional challenge: the expectation that softmax\nprobabilities should exhibit unimodal distribution is not met with\ncross-entropy. Rather, the ordinal regression literature has focused on\nunimodality and overlooked calibration. To address these issues, we propose a\nnovel loss function that introduces order-aware calibration, ensuring that\nprediction confidence adheres to ordinal relationships between classes. It\nincorporates soft ordinal encoding and label-smoothing-based regularization to\nenforce both calibration and unimodality. Extensive experiments across three\npopular ordinal regression benchmarks demonstrate that our approach achieves\nstate-of-the-art calibration without compromising accuracy.\n","authors":["Daehwan Kim","Haejun Chung","Ikbeom Jang"],"pdf_url":"https://arxiv.org/pdf/2410.15658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15657v1","updated":"2024-10-21T05:51:51Z","published":"2024-10-21T05:51:51Z","title":"CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision\n  Large Language Models","summary":"  Human-object interaction (HOI) detection has seen advancements with Vision\nLanguage Models (VLMs), but these methods often depend on extensive manual\nannotations. Vision Large Language Models (VLLMs) can inherently recognize and\nreason about interactions at the image level but are computationally heavy and\nnot designed for instance-level HOI detection. To overcome these limitations,\nwe propose a Cross-Level HOI distillation (CL-HOI) framework, which distills\ninstance-level HOIs from VLLMs image-level understanding without the need for\nmanual annotations. Our approach involves two stages: context distillation,\nwhere a Visual Linguistic Translator (VLT) converts visual information into\nlinguistic form, and interaction distillation, where an Interaction Cognition\nNetwork (ICN) reasons about spatial, visual, and context relations. We design\ncontrastive distillation losses to transfer image-level context and interaction\nknowledge from the teacher to the student model, enabling instance-level HOI\ndetection. Evaluations on HICO-DET and V-COCO datasets demonstrate that our\nCL-HOI surpasses existing weakly supervised methods and VLLM supervised\nmethods, showing its efficacy in detecting HOIs without manual labels.\n","authors":["Jianjun Gao","Chen Cai","Ruoyu Wang","Wenyang Liu","Kim-Hui Yap","Kratika Garg","Boon-Siew Han"],"pdf_url":"https://arxiv.org/pdf/2410.15657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15642v1","updated":"2024-10-21T05:08:18Z","published":"2024-10-21T05:08:18Z","title":"Resource-Efficient Medical Report Generation using Large Language Models","summary":"  Medical report generation is the task of automatically writing radiology\nreports for chest X-ray images. Manually composing these reports is a\ntime-consuming process that is also prone to human errors. Generating medical\nreports can therefore help reduce the burden on radiologists. In other words,\nwe can promote greater clinical automation in the medical domain. In this work,\nwe propose a new framework leveraging vision-enabled Large Language Models\n(LLM) for the task of medical report generation. We introduce a lightweight\nsolution that achieves better or comparative performance as compared to\nprevious solutions on the task of medical report generation. We conduct\nextensive experiments exploring different model sizes and enhancement\napproaches, such as prefix tuning to improve the text generation abilities of\nthe LLMs. We evaluate our approach on a prominent large-scale radiology report\ndataset - MIMIC-CXR. Our results demonstrate the capability of our\nresource-efficient framework to generate patient-specific reports with strong\nmedical contextual understanding and high precision.\n","authors":[" Abdullah","Ameer Hamza","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.15642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18292v5","updated":"2024-10-21T05:06:15Z","published":"2024-02-28T12:37:30Z","title":"FSL-Rectifier: Rectify Outliers in Few-Shot Learning via Test-Time\n  Augmentation","summary":"  Few-shot-learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few labeled\nsamples of the new classes (support set) as reference. So far, plenty of\nalgorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, e.g., obtaining a test accuracy improvement proportion of around\n10% (e.g., from 46.86% to 53.28%) for trained FSL models. Importantly, given\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves.\n","authors":["Yunwei Bai","Ying Kiat Tan","Shiming Chen","Yao Shu","Tsuhan Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18292v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15636v1","updated":"2024-10-21T04:47:01Z","published":"2024-10-21T04:47:01Z","title":"LucidFusion: Generating 3D Gaussians with Arbitrary Unposed Images","summary":"  Recent large reconstruction models have made notable progress in generating\nhigh-quality 3D objects from single images. However, these methods often\nstruggle with controllability, as they lack information from multiple views,\nleading to incomplete or inconsistent 3D reconstructions. To address this\nlimitation, we introduce LucidFusion, a flexible end-to-end feed-forward\nframework that leverages the Relative Coordinate Map (RCM). Unlike traditional\nmethods linking images to 3D world thorough pose, LucidFusion utilizes RCM to\nalign geometric features coherently across different views, making it highly\nadaptable for 3D generation from arbitrary, unposed images. Furthermore,\nLucidFusion seamlessly integrates with the original single-image-to-3D\npipeline, producing detailed 3D Gaussians at a resolution of $512 \\times 512$,\nmaking it well-suited for a wide range of applications.\n","authors":["Hao He","Yixun Liang","Luozhou Wang","Yuanhao Cai","Xinli Xu","Hao-Xiang Guo","Xiang Wen","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15636v1.pdf","comment":"17 pages, 12 figures, project page: coming soon"},{"id":"http://arxiv.org/abs/2408.03361v7","updated":"2024-10-21T04:26:41Z","published":"2024-08-06T17:59:21Z","title":"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards\n  General Medical AI","summary":"  Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 284 datasets\nacross 38 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 53.96%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.\n","authors":["Pengcheng Chen","Jin Ye","Guoan Wang","Yanjun Li","Zhongying Deng","Wei Li","Tianbin Li","Haodong Duan","Ziyan Huang","Yanzhou Su","Benyou Wang","Shaoting Zhang","Bin Fu","Jianfei Cai","Bohan Zhuang","Eric J Seibel","Junjun He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.03361v7.pdf","comment":"GitHub: https://github.com/uni-medical/GMAI-MMBench Hugging face:\n  https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench"},{"id":"http://arxiv.org/abs/2410.15629v1","updated":"2024-10-21T04:25:43Z","published":"2024-10-21T04:25:43Z","title":"Fully Explicit Dynamic Gaussian Splatting","summary":"  3D Gaussian Splatting has shown fast and high-quality rendering results in\nstatic scenes by leveraging dense 3D prior and explicit representations.\nUnfortunately, the benefits of the prior and representation do not involve\nnovel view synthesis for dynamic motions. Ironically, this is because the main\nbarrier is the reliance on them, which requires increasing training and\nrendering times to account for dynamic motions. In this paper, we design a\nExplicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate\nstatic and dynamic Gaussians during training, and to explicitly sample\npositions and rotations of the dynamic Gaussians at sparse timestamps. The\nsampled positions and rotations are then interpolated to represent both\nspatially and temporally continuous motions of objects in dynamic scenes as\nwell as reducing computational cost. Additionally, we introduce a progressive\ntraining scheme and a point-backtracking technique that improves Ex4DGS's\nconvergence. We initially train Ex4DGS using short timestamps and progressively\nextend timestamps, which makes it work well with a few point clouds. The\npoint-backtracking is used to quantify the cumulative error of each Gaussian\nover time, enabling the detection and removal of erroneous Gaussians in dynamic\nscenes. Comprehensive experiments on various scenes demonstrate the\nstate-of-the-art rendering quality from our method, achieving fast rendering of\n62 fps on a single 2080Ti GPU.\n","authors":["Junoh Lee","Chang-Yeon Won","Hyunjun Jung","Inhwan Bae","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2410.15629v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15628v1","updated":"2024-10-21T04:24:10Z","published":"2024-10-21T04:24:10Z","title":"Towards Kriging-informed Conditional Diffusion for Regional Sea-Level\n  Data Downscaling","summary":"  Given coarser-resolution projections from global climate models or satellite\ndata, the downscaling problem aims to estimate finer-resolution regional\nclimate data, capturing fine-scale spatial patterns and variability.\nDownscaling is any method to derive high-resolution data from low-resolution\nvariables, often to provide more detailed and local predictions and analyses.\nThis problem is societally crucial for effective adaptation, mitigation, and\nresilience against significant risks from climate change. The challenge arises\nfrom spatial heterogeneity and the need to recover finer-scale features while\nensuring model generalization. Most downscaling methods \\cite{Li2020} fail to\ncapture the spatial dependencies at finer scales and underperform on real-world\nclimate datasets, such as sea-level rise. We propose a novel Kriging-informed\nConditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial\nvariability while preserving fine-scale features. Experimental results on\nclimate data show that our proposed method is more accurate than\nstate-of-the-art downscaling techniques.\n","authors":["Subhankar Ghosh","Arun Sharma","Jayant Gupta","Aneesh Subramanian","Shashi Shekhar"],"pdf_url":"https://arxiv.org/pdf/2410.15628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10038v2","updated":"2024-10-21T04:05:25Z","published":"2023-04-20T01:32:32Z","title":"Open-World Continual Learning: Unifying Novelty Detection and Continual\n  Learning","summary":"  As AI agents are increasingly used in the real open world with unknowns or\nnovelties, they need the ability to (1) recognize objects that (a) they have\nlearned before and (b) detect items that they have never seen or learned, and\n(2) learn the new items incrementally to become more and more knowledgeable and\npowerful. (1) is called novelty detection or out-of-distribution (OOD)\ndetection and (2) is called class incremental learning (CIL), which is a\nsetting of continual learning (CL). In existing research, OOD detection and CIL\nare regarded as two completely different problems. This paper first provides a\ntheoretical proof that good OOD detection for each task within the set of\nlearned tasks (called closed-world OOD detection) is necessary for successful\nCIL. We show this by decomposing CIL into two sub-problems: within-task\nprediction (WP) and task-id prediction (TP), and proving that TP is correlated\nwith closed-world OOD detection. The key theoretical result is that regardless\nof whether WP and OOD detection (or TP) are defined explicitly or implicitly by\na CIL algorithm, good WP and good closed-world OOD detection are necessary and\nsufficient conditions for good CIL, which unifies novelty or OOD detection and\ncontinual learning (CIL, in particular). We call this traditional CIL the\nclosed-world CIL as it does not detect future OOD data in the open world. The\npaper then proves that the theory can be generalized or extended to open-world\nCIL, which is the proposed open-world continual learning, that can perform CIL\nin the open world and detect future or open-world OOD data. Based on the\ntheoretical results, new CIL methods are also designed, which outperform strong\nbaselines in CIL accuracy and in continual OOD detection by a large margin.\n","authors":["Gyuhak Kim","Changnan Xiao","Tatsuya Konishi","Zixuan Ke","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2304.10038v2.pdf","comment":"To appear in Artificial Intelligence Journal. arXiv admin note:\n  substantial text overlap with arXiv:2211.02633"},{"id":"http://arxiv.org/abs/2405.10160v2","updated":"2024-10-21T03:49:58Z","published":"2024-05-16T14:53:45Z","title":"PIR: Remote Sensing Image-Text Retrieval with Prior Instruction\n  Representation Learning","summary":"  Remote sensing image-text retrieval constitutes a foundational aspect of\nremote sensing interpretation tasks, facilitating the alignment of vision and\nlanguage representations. This paper introduces a prior instruction\nrepresentation (PIR) learning paradigm that draws on prior knowledge to\ninstruct adaptive learning of vision and text representations. Based on PIR, a\ndomain-adapted remote sensing image-text retrieval framework PIR-ITR is\ndesigned to address semantic noise issues in vision-language understanding\ntasks. However, with massive additional data for pre-training the\nvision-language foundation model, remote sensing image-text retrieval is\nfurther developed into an open-domain retrieval task. Continuing with the\nabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote\nsensing image-text retrieval, to address semantic noise in remote sensing\nvision-language representations and further improve open-domain retrieval\nperformance. In vision representation, we utilize the prior-guided knowledge of\nthe remote sensing scene recognition by building a belief matrix to select key\nfeatures for reducing the impact of semantic noise. In text representation, we\nuse the previous time step to cyclically activate the current time step to\nenhance text representation capability. A cluster-wise Affiliation Loss (AL) is\nproposed to constrain the inter-classes and to reduce the semantic confusion\nzones in the common subspace. Comprehensive experiments demonstrate that PIR\ncould enhance vision and text representations and outperform the\nstate-of-the-art methods of closed-domain and open-domain retrieval on two\nbenchmark datasets, RSICD and RSITMD.\n","authors":["Jiancheng Pan","Muyuan Ma","Qing Ma","Cong Bai","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.10160v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.15618v1","updated":"2024-10-21T03:40:29Z","published":"2024-10-21T03:40:29Z","title":"Erasing Undesirable Concepts in Diffusion Models with Adversarial\n  Preservation","summary":"  Diffusion models excel at generating visually striking content from text but\ncan inadvertently produce undesirable or harmful content when trained on\nunfiltered internet data. A practical solution is to selectively removing\ntarget concepts from the model, but this may impact the remaining concepts.\nPrior approaches have tried to balance this by introducing a loss term to\npreserve neutral content or a regularization term to minimize changes in the\nmodel parameters, yet resolving this trade-off remains challenging. In this\nwork, we propose to identify and preserving concepts most affected by parameter\nchanges, termed as \\textit{adversarial concepts}. This approach ensures stable\nerasure with minimal impact on the other concepts. We demonstrate the\neffectiveness of our method using the Stable Diffusion model, showing that it\noutperforms state-of-the-art erasure methods in eliminating unwanted content\nwhile maintaining the integrity of other unrelated elements. Our code is\navailable at\n\\url{https://github.com/tuananhbui89/Erasing-Adversarial-Preservation}.\n","authors":["Anh Bui","Long Vuong","Khanh Doan","Trung Le","Paul Montague","Tamas Abraham","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2410.15618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15615v1","updated":"2024-10-21T03:33:13Z","published":"2024-10-21T03:33:13Z","title":"Joint Top-Down and Bottom-Up Frameworks for 3D Visual Grounding","summary":"  This paper tackles the challenging task of 3D visual grounding-locating a\nspecific object in a 3D point cloud scene based on text descriptions. Existing\nmethods fall into two categories: top-down and bottom-up methods. Top-down\nmethods rely on a pre-trained 3D detector to generate and select the best\nbounding box, resulting in time-consuming processes. Bottom-up methods directly\nregress object bounding boxes with coarse-grained features, producing worse\nresults. To combine their strengths while addressing their limitations, we\npropose a joint top-down and bottom-up framework, aiming to enhance the\nperformance while improving the efficiency. Specifically, in the first stage,\nwe propose a bottom-up based proposal generation module, which utilizes\nlightweight neural layers to efficiently regress and cluster several coarse\nobject proposals instead of using a complex 3D detector. Then, in the second\nstage, we introduce a top-down based proposal consolidation module, which\nutilizes graph design to effectively aggregate and propagate the query-related\nobject contexts among the generated proposals for further refinement. By\njointly training these two modules, we can avoid the inherent drawbacks of the\ncomplex proposals in the top-down framework and the coarse proposals in the\nbottom-up framework. Experimental results on the ScanRefer benchmark show that\nour framework is able to achieve the state-of-the-art performance.\n","authors":["Yang Liu","Daizong Liu","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2410.15615v1.pdf","comment":"Accepted by ICPR2024"},{"id":"http://arxiv.org/abs/2410.15614v1","updated":"2024-10-21T03:33:09Z","published":"2024-10-21T03:33:09Z","title":"Topology-Aware Exploration of Circle of Willis for CTA and MRA:\n  Segmentation, Detection, and Classification","summary":"  The Circle of Willis (CoW) vessels is critical to connecting major\ncirculations of the brain. The topology of the vascular structure is clinical\nsignificance to evaluate the risk, severity of the neuro-vascular diseases. The\nCoW has two representative angiographic imaging modalities, computed tomography\nangiography (CTA) and magnetic resonance angiography (MRA). TopCow24 provided\n125 paired CTA-MRA dataset for the analysis of CoW. To explore both CTA and MRA\nimages in a unified framework to learn the inherent topology of Cow, we\nconstruct the universal dataset via independent intensity preprocess, followed\nby joint resampling and normarlization. Then, we utilize the topology-aware\nloss to enhance the topology completeness of the CoW and the discrimination\nbetween different classes. A complementary topology-aware refinement is further\nconducted to enhance the connectivity within the same class. Our method was\nevaluated on all the three tasks and two modalities, achieving competitive\nresults. In the final test phase of TopCow24 Challenge, we achieved the second\nplace in the CTA-Seg-Task, the third palce in the CTA-Box-Task, the first place\nin the CTA-Edg-Task, the second place in the MRA-Seg-Task, the third palce in\nthe MRA-Box-Task, the second place in the MRA-Edg-Task.\n","authors":["Minghui Zhang","Xin You","Hanxiao Zhang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2410.15614v1.pdf","comment":"Participation technical report for TopCoW24 challenge @ MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.15613v1","updated":"2024-10-21T03:17:25Z","published":"2024-10-21T03:17:25Z","title":"Exploring Stronger Transformer Representation Learning for Occluded\n  Person Re-Identificatio","summary":"  Due to some complex factors (e.g., occlusion, pose variation and diverse\ncamera perspectives), extracting stronger feature representation in person\nre-identification remains a challenging task. In this paper, we proposed a\nnovel self-supervision and supervision combining transformer-based person\nre-identification framework, namely SSSC-TransReID. Different from the general\ntransformer-based person re-identification models, we designed a\nself-supervised contrastive learning branch, which can enhance the feature\nrepresentation for person re-identification without negative samples or\nadditional pre-training. In order to train the contrastive learning branch, we\nalso proposed a novel random rectangle mask strategy to simulate the occlusion\nin real scenes, so as to enhance the feature representation for occlusion.\nFinally, we utilized the joint-training loss function to integrate the\nadvantages of supervised learning with ID tags and self-supervised contrastive\nlearning without negative samples, which can reinforce the ability of our model\nto excavate stronger discriminative features, especially for occlusion.\nExtensive experimental results on several benchmark datasets show our proposed\nmodel obtains superior Re-ID performance consistently and outperforms the\nstate-of-the-art ReID methods by large margins on the mean average accuracy\n(mAP) and Rank-1 accuracy.\n","authors":["Zhangjian Ji","Donglin Cheng","Kai Feng"],"pdf_url":"https://arxiv.org/pdf/2410.15613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11507v2","updated":"2024-10-21T03:13:20Z","published":"2024-02-18T08:34:15Z","title":"MAL: Motion-Aware Loss with Temporal and Distillation Hints for\n  Self-Supervised Depth Estimation","summary":"  Depth perception is crucial for a wide range of robotic applications.\nMulti-frame self-supervised depth estimation methods have gained research\ninterest due to their ability to leverage large-scale, unlabeled real-world\ndata. However, the self-supervised methods often rely on the assumption of a\nstatic scene and their performance tends to degrade in dynamic environments. To\naddress this issue, we present Motion-Aware Loss, which leverages the temporal\nrelation among consecutive input frames and a novel distillation scheme between\nthe teacher and student networks in the multi-frame self-supervised depth\nestimation methods. Specifically, we associate the spatial locations of moving\nobjects with the temporal order of input frames to eliminate errors induced by\nobject motion. Meanwhile, we enhance the original distillation scheme in\nmulti-frame methods to better exploit the knowledge from a teacher network. MAL\nis a novel, plug-and-play module designed for seamless integration into\nmulti-frame self-supervised monocular depth estimation methods. Adding MAL into\nprevious state-of-the-art methods leads to a reduction in depth estimation\nerrors by up to 4.2% and 10.8% on KITTI and CityScapes benchmarks,\nrespectively.\n","authors":["Yue-Jiang Dong","Fang-Lue Zhang","Song-Hai Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.11507v2.pdf","comment":"Accepted by ICRA 2024; Project homepage:\n  https://yuejiangdong.github.io/MotionAwareLoss/"},{"id":"http://arxiv.org/abs/2406.01597v2","updated":"2024-10-21T03:11:29Z","published":"2024-04-09T14:37:54Z","title":"End-to-End Rate-Distortion Optimized 3D Gaussian Representation","summary":"  3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable\npotential in 3D representation and image rendering. However, the substantial\nstorage overhead of 3DGS significantly impedes its practical applications. In\nthis work, we formulate the compact 3D Gaussian learning as an end-to-end\nRate-Distortion Optimization (RDO) problem and propose RDO-Gaussian that can\nachieve flexible and continuous rate control. RDO-Gaussian addresses two main\nissues that exist in current schemes: 1) Different from prior endeavors that\nminimize the rate under the fixed distortion, we introduce dynamic pruning and\nentropy-constrained vector quantization (ECVQ) that optimize the rate and\ndistortion at the same time. 2) Previous works treat the colors of each\nGaussian equally, while we model the colors of different regions and materials\nwith learnable numbers of parameters. We verify our method on both real and\nsynthetic scenes, showcasing that RDO-Gaussian greatly reduces the size of 3D\nGaussian over 40x, and surpasses existing methods in rate-distortion\nperformance.\n","authors":["Henan Wang","Hanxin Zhu","Tianyu He","Runsen Feng","Jiajun Deng","Jiang Bian","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.01597v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2405.08813v3","updated":"2024-10-21T03:08:08Z","published":"2024-05-14T17:59:02Z","title":"CinePile: A Long Video Question Answering Dataset and Benchmark","summary":"  Current datasets for long-form video understanding often fall short of\nproviding genuine long-form comprehension challenges, as many tasks derived\nfrom these datasets can be successfully tackled by analyzing just one or a few\nrandom frames from a video. To address this issue, we present a novel dataset\nand benchmark, CinePile, specifically designed for authentic long-form video\nunderstanding. This paper details our innovative approach for creating a\nquestion-answer dataset, utilizing advanced LLMs with human-in-the-loop and\nbuilding upon human-generated raw data. Our comprehensive dataset comprises\n305,000 multiple-choice questions (MCQs), covering various visual and\nmultimodal aspects, including temporal comprehension, understanding\nhuman-object interactions, and reasoning about events or actions within a\nscene. Additionally, we fine-tuned open-source Video-LLMs on the training split\nand evaluated both open-source and proprietary video-centric LLMs on the test\nsplit of our dataset. The findings indicate that although current models\nunderperform compared to humans, fine-tuning these models can lead to\nsignificant improvements in their performance.\n","authors":["Ruchit Rawal","Khalid Saifullah","Miquel Farré","Ronen Basri","David Jacobs","Gowthami Somepalli","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2405.08813v3.pdf","comment":"Project page with all the artifacts -\n  https://ruchitrawal.github.io/cinepile/. Updated version with adversarial\n  refinement pipeline and more model evaluations"},{"id":"http://arxiv.org/abs/2410.15605v1","updated":"2024-10-21T03:04:09Z","published":"2024-10-21T03:04:09Z","title":"Deep Active Learning with Manifold-preserving Trajectory Sampling","summary":"  Active learning (AL) is for optimizing the selection of unlabeled data for\nannotation (labeling), aiming to enhance model performance while minimizing\nlabeling effort. The key question in AL is which unlabeled data should be\nselected for annotation. Existing deep AL methods arguably suffer from bias\nincurred by clabeled data, which takes a much lower percentage than unlabeled\ndata in AL context. We observe that such an issue is severe in different types\nof data, such as vision and non-vision data. To address this issue, we propose\na novel method, namely Manifold-Preserving Trajectory Sampling (MPTS), aiming\nto enforce the feature space learned from labeled data to represent a more\naccurate manifold. By doing so, we expect to effectively correct the bias\nincurred by labeled data, which can cause a biased selection of unlabeled data.\nDespite its focus on manifold, the proposed method can be conveniently\nimplemented by performing distribution mapping with MMD (Maximum Mean\nDiscrepancies). Extensive experiments on various vision and non-vision\nbenchmark datasets demonstrate the superiority of our method. Our source code\ncan be found here.\n","authors":["Yingrui Ji","Vijaya Sindhoori Kaza","Nishanth Artham","Tianyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15602v1","updated":"2024-10-21T02:56:44Z","published":"2024-10-21T02:56:44Z","title":"P-YOLOv8: Efficient and Accurate Real-Time Detection of Distracted\n  Driving","summary":"  Distracted driving is a critical safety issue that leads to numerous\nfatalities and injuries worldwide. This study addresses the urgent need for\nefficient and real-time machine learning models to detect distracted driving\nbehaviors. Leveraging the Pretrained YOLOv8 (P-YOLOv8) model, a real-time\nobject detection system is introduced, optimized for both speed and accuracy.\nThis approach addresses the computational constraints and latency limitations\ncommonly associated with conventional detection models. The study demonstrates\nP-YOLOv8 versatility in both object detection and image classification tasks\nusing the Distracted Driver Detection dataset from State Farm, which includes\n22,424 images across ten behavior categories. Our research explores the\napplication of P-YOLOv8 for image classification, evaluating its performance\ncompared to deep learning models such as VGG16, VGG19, and ResNet. Some\ntraditional models often struggle with low accuracy, while others achieve high\naccuracy but come with high computational costs and slow detection speeds,\nmaking them unsuitable for real-time applications. P-YOLOv8 addresses these\nissues by achieving competitive accuracy with significant computational cost\nand efficiency advantages. In particular, P-YOLOv8 generates a lightweight\nmodel with a size of only 2.84 MB and a lower number of parameters, totaling\n1,451,098, due to its innovative architecture. It achieves a high accuracy of\n99.46 percent with this small model size, opening new directions for deployment\non inexpensive and small embedded devices using Tiny Machine Learning (TinyML).\nThe experimental results show robust performance, making P-YOLOv8 a\ncost-effective solution for real-time deployment. This study provides a\ndetailed analysis of P-YOLOv8's architecture, training, and performance\nbenchmarks, highlighting its potential for real-time use in detecting\ndistracted driving.\n","authors":["Mohamed R. Elshamy","Heba M. Emara","Mohamed R. Shoaib","Abdel-Hameed A. Badawy"],"pdf_url":"https://arxiv.org/pdf/2410.15602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15584v1","updated":"2024-10-21T02:10:49Z","published":"2024-10-21T02:10:49Z","title":"Deep Learning and Machine Learning -- Object Detection and Semantic\n  Segmentation: From Theory to Applications","summary":"  This book offers an in-depth exploration of object detection and semantic\nsegmentation, combining theoretical foundations with practical applications. It\ncovers state-of-the-art advancements in machine learning and deep learning,\nwith a focus on convolutional neural networks (CNNs), YOLO architectures, and\ntransformer-based approaches like DETR. The book also delves into the\nintegration of artificial intelligence (AI) techniques and large language\nmodels for enhanced object detection in complex environments. A thorough\ndiscussion of big data analysis is presented, highlighting the importance of\ndata processing, model optimization, and performance evaluation metrics. By\nbridging the gap between traditional methods and modern deep learning\nframeworks, this book serves as a comprehensive guide for researchers, data\nscientists, and engineers aiming to leverage AI-driven methodologies in\nlarge-scale object detection tasks.\n","authors":["Jintao Ren","Ziqian Bi","Qian Niu","Junyu Liu","Benji Peng","Sen Zhang","Xuanhe Pan","Jinlang Wang","Keyu Chen","Caitlyn Heqi Yin","Pohsun Feng","Yizhu Wen","Tianyang Wang","Silin Chen","Ming Li","Jiawei Xu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15584v1.pdf","comment":"167 pages"},{"id":"http://arxiv.org/abs/2410.15582v1","updated":"2024-10-21T02:06:43Z","published":"2024-10-21T02:06:43Z","title":"ARTS: Semi-Analytical Regressor using Disentangled Skeletal\n  Representations for Human Mesh Recovery from Videos","summary":"  Although existing video-based 3D human mesh recovery methods have made\nsignificant progress, simultaneously estimating human pose and shape from\nlow-resolution image features limits their performance. These image features\nlack sufficient spatial information about the human body and contain various\nnoises (e.g., background, lighting, and clothing), which often results in\ninaccurate pose and inconsistent motion. Inspired by the rapid advance in human\npose estimation, we discover that compared to image features, skeletons\ninherently contain accurate human pose and motion. Therefore, we propose a\nnovel semiAnalytical Regressor using disenTangled Skeletal representations for\nhuman mesh recovery from videos, called ARTS. Specifically, a skeleton\nestimation and disentanglement module is proposed to estimate the 3D skeletons\nfrom a video and decouple them into disentangled skeletal representations\n(i.e., joint position, bone length, and human motion). Then, to fully utilize\nthese representations, we introduce a semi-analytical regressor to estimate the\nparameters of the human mesh model. The regressor consists of three modules:\nTemporal Inverse Kinematics (TIK), Bone-guided Shape Fitting (BSF), and\nMotion-Centric Refinement (MCR). TIK utilizes joint position to estimate\ninitial pose parameters and BSF leverages bone length to regress bone-aligned\nshape parameters. Finally, MCR combines human motion representation with image\nfeatures to refine the initial human model parameters. Extensive experiments\ndemonstrate that our ARTS surpasses existing state-of-the-art video-based\nmethods in both per-frame accuracy and temporal consistency on popular\nbenchmarks: 3DPW, MPI-INF-3DHP, and Human3.6M. Code is available at\nhttps://github.com/TangTao-PKU/ARTS.\n","authors":["Tao Tang","Hong Liu","Yingxuan You","Ti Wang","Wenhao Li"],"pdf_url":"https://arxiv.org/pdf/2410.15582v1.pdf","comment":"Accepted by ACM MM 2024. Project page:\n  https://github.com/TangTao-PKU/ARTS"},{"id":"http://arxiv.org/abs/2410.15581v1","updated":"2024-10-21T01:58:26Z","published":"2024-10-21T01:58:26Z","title":"Multimodal Learning for Embryo Viability Prediction in Clinical IVF","summary":"  In clinical In-Vitro Fertilization (IVF), identifying the most viable embryo\nfor transfer is important to increasing the likelihood of a successful\npregnancy. Traditionally, this process involves embryologists manually\nassessing embryos' static morphological features at specific intervals using\nlight microscopy. This manual evaluation is not only time-intensive and costly,\ndue to the need for expert analysis, but also inherently subjective, leading to\nvariability in the selection process. To address these challenges, we develop a\nmultimodal model that leverages both time-lapse video data and Electronic\nHealth Records (EHRs) to predict embryo viability. One of the primary\nchallenges of our research is to effectively combine time-lapse video and EHR\ndata, owing to their inherent differences in modality. We comprehensively\nanalyze our multimodal model with various modality inputs and integration\napproaches. Our approach will enable fast and automated embryo viability\npredictions in scale for clinical IVF.\n","authors":["Junsik Kim","Zhiyi Shi","Davin Jeong","Johannes Knittel","Helen Y. Yang","Yonghyun Song","Wanhua Li","Yicong Li","Dalit Ben-Yosef","Daniel Needleman","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.15581v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.15569v1","updated":"2024-10-21T01:23:42Z","published":"2024-10-21T01:23:42Z","title":"Online Pseudo-Label Unified Object Detection for Multiple Datasets\n  Training","summary":"  The Unified Object Detection (UOD) task aims to achieve object detection of\nall merged categories through training on multiple datasets, and is of great\nsignificance in comprehensive object detection scenarios. In this paper, we\nconduct a thorough analysis of the cross datasets missing annotations issue,\nand propose an Online Pseudo-Label Unified Object Detection scheme. Our method\nuses a periodically updated teacher model to generate pseudo-labels for the\nunlabelled objects in each sub-dataset. This periodical update strategy could\nbetter ensure that the accuracy of the teacher model reaches the local maxima\nand maximized the quality of pseudo-labels. In addition, we survey the\ninfluence of overlapped region proposals on the accuracy of box regression. We\npropose a category specific box regression and a pseudo-label RPN head to\nimprove the recall rate of the Region Proposal Network (PRN). Our experimental\nresults on common used benchmarks (\\eg COCO, Object365 and OpenImages)\nindicates that our online pseudo-label UOD method achieves higher accuracy than\nexisting SOTA methods.\n","authors":["XiaoJun Tang","Jingru Wang","Zeyu Shangguan","Darun Tang","Yuyu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13147v2","updated":"2024-10-21T00:38:07Z","published":"2024-10-17T02:04:57Z","title":"Utilizing Large Language Models in An Iterative Paradigm with Domain\n  Feedback for Molecule Optimization","summary":"  Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^2$DF. In detail, $\\text{Re}^2$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^2$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^2$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^2$DF enhances Hit\nratio by 6.04% and 5.25%.\n","authors":["Khiem Le","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2410.13147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15549v1","updated":"2024-10-21T00:36:02Z","published":"2024-10-21T00:36:02Z","title":"A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM","summary":"  Vision-Language-Action (VLA) models are receiving increasing attention for\ntheir ability to enable robots to perform complex tasks by integrating visual\ncontext with linguistic commands. However, achieving efficient real-time\nperformance remains challenging due to the high computational demands of\nexisting models. To overcome this, we propose Dual Process VLA (DP-VLA), a\nhierarchical framework inspired by dual-process theory. DP-VLA utilizes a Large\nSystem 2 Model (L-Sys2) for complex reasoning and decision-making, while a\nSmall System 1 Model (S-Sys1) handles real-time motor control and sensory\nprocessing. By leveraging Vision-Language Models (VLMs), the L-Sys2 operates at\nlow frequencies, reducing computational overhead, while the S-Sys1 ensures fast\nand accurate task execution. Experimental results on the RoboCasa dataset\ndemonstrate that DP-VLA achieves faster inference and higher task success\nrates, providing a scalable solution for advanced robotic applications.\n","authors":["ByungOk Han","Jaehong Kim","Jinhyeok Jang"],"pdf_url":"https://arxiv.org/pdf/2410.15549v1.pdf","comment":"10 page"},{"id":"http://arxiv.org/abs/2408.12528v6","updated":"2024-10-21T00:33:23Z","published":"2024-08-22T16:32:32Z","title":"Show-o: One Single Transformer to Unify Multimodal Understanding and\n  Generation","summary":"  We present a unified transformer, i.e., Show-o, that unifies multimodal\nunderstanding and generation. Unlike fully autoregressive models, Show-o\nunifies autoregressive and (discrete) diffusion modeling to adaptively handle\ninputs and outputs of various and mixed modalities. The unified model flexibly\nsupports a wide range of vision-language tasks including visual\nquestion-answering, text-to-image generation, text-guided\ninpainting/extrapolation, and mixed-modality generation. Across various\nbenchmarks, it demonstrates comparable or superior performance to existing\nindividual models with an equivalent or larger number of parameters tailored\nfor understanding or generation. This significantly highlights its potential as\na next-generation foundation model. Code and models are released at\nhttps://github.com/showlab/Show-o.\n","authors":["Jinheng Xie","Weijia Mao","Zechen Bai","David Junhao Zhang","Weihao Wang","Kevin Qinghong Lin","Yuchao Gu","Zhijie Chen","Zhenheng Yang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2408.12528v6.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2211.11571v3","updated":"2024-10-21T13:12:25Z","published":"2022-11-21T15:29:38Z","title":"SLLEN: Semantic-aware Low-light Image Enhancement Network","summary":"  How to effectively explore semantic feature is vital for low-light image\nenhancement (LLE). Existing methods usually utilize the semantic feature that\nis only drawn from the output produced by high-level semantic segmentation (SS)\nnetwork. However, if the output is not accurately estimated, it would affect\nthe high-level semantic feature (HSF) extraction, which accordingly interferes\nwith LLE. To this end, we develop a simple and effective semantic-aware LLE\nnetwork (SSLEN) composed of a LLE main-network (LLEmN) and a SS\nauxiliary-network (SSaN). In SLLEN, LLEmN integrates the random intermediate\nembedding feature (IEF), i.e., the information extracted from the intermediate\nlayer of SSaN, together with the HSF into a unified framework for better LLE.\nSSaN is designed to act as a SS role to provide HSF and IEF. Moreover, thanks\nto a shared encoder between LLEmN and SSaN, we further propose an alternating\ntraining mechanism to facilitate the collaboration between them. Unlike\ncurrently available approaches, the proposed SLLEN is able to fully lever the\nsemantic information, e.g., IEF, HSF, and SS dataset, to assist LLE, thereby\nleading to a more promising enhancement performance. Comparisons between the\nproposed SLLEN and other state-of-the-art techniques demonstrate the\nsuperiority of SLLEN with respect to LLE quality over all the comparable\nalternatives.\n","authors":["Mingye Ju","Chuheng Chen","Charles A. Guo","Jinshan Pan","Jinhui Tang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2211.11571v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16005v2","updated":"2024-10-21T13:07:42Z","published":"2022-11-29T07:59:15Z","title":"Convex Relaxations for Isometric and Equiareal NRSfM","summary":"  Extensible objects form a challenging case for NRSfM, owing to the lack of a\nsufficiently constrained extensible model of the point-cloud. We tackle the\nchallenge by proposing 1) convex relaxations of the isometric model up to\nquasi-isometry, and 2) convex relaxations involving the equiareal deformation\nmodel, which preserves local area and has not been used in NRSfM. The equiareal\nmodel is appealing because it is physically plausible and widely applicable.\nHowever, it has two main difficulties: first, when used on its own, it is\nambiguous, and second, it involves quartic, hence highly nonconvex,\nconstraints. Our approach handles the first difficulty by mixing the equiareal\nwith the isometric model and the second difficulty by new convex relaxations.\nWe validate our methods on multiple real and synthetic data, including\nwell-known benchmarks.\n","authors":["Agniva Sengupta","Adrien Bartoli"],"pdf_url":"https://arxiv.org/pdf/2211.16005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09662v2","updated":"2024-10-21T23:58:45Z","published":"2024-06-14T02:21:53Z","title":"Learning Language Structures through Grounding","summary":"  Language is highly structured, with syntactic and semantic structures, to\nsome extent, agreed upon by speakers of the same language. With implicit or\nexplicit awareness of such structures, humans can learn and use language\nefficiently and generalize to sentences that contain unseen words. Motivated by\nhuman language learning, in this dissertation, we consider a family of machine\nlearning tasks that aim to learn language structures through grounding. We seek\ndistant supervision from other data sources (i.e., grounds), including but not\nlimited to other modalities (e.g., vision), execution results of programs, and\nother languages.\n  We demonstrate the potential of this task formulation and advocate for its\nadoption through three schemes. In Part I, we consider learning syntactic\nparses through visual grounding. We propose the task of visually grounded\ngrammar induction, present the first models to induce syntactic structures from\nvisually grounded text and speech, and find that the visual grounding signals\ncan help improve the parsing quality over language-only models. As a side\ncontribution, we propose a novel evaluation metric that enables the evaluation\nof speech parsing without text or automatic speech recognition systems\ninvolved. In Part II, we propose two execution-aware methods to map sentences\ninto corresponding semantic structures (i.e., programs), significantly\nimproving compositional generalization and few-shot program synthesis. In Part\nIII, we propose methods that learn language structures from annotations in\nother languages. Specifically, we propose a method that sets a new state of the\nart on cross-lingual word alignment. We then leverage the learned word\nalignments to improve the performance of zero-shot cross-lingual dependency\nparsing, by proposing a novel substructure-based projection method that\npreserves structural knowledge learned from the source language.\n","authors":["Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2406.09662v2.pdf","comment":"Ph.D. Thesis"},{"id":"http://arxiv.org/abs/2409.03901v2","updated":"2024-10-21T23:15:51Z","published":"2024-09-05T20:21:49Z","title":"Onboard Satellite Image Classification for Earth Observation: A\n  Comparative Study of ViT Models","summary":"  This study focuses on identifying the most effective pre-trained model for\nland use classification in onboard satellite processing, emphasizing achieving\nhigh accuracy, computational efficiency, and robustness against noisy data\nconditions commonly encountered during satellite-based inference. Through\nextensive experimentation, we compare the performance of traditional CNN-based,\nResNet-based, and various pre-trained vision Transformer models. Our findings\ndemonstrate that pre-trained Vision Transformer (ViT) models, particularly\nMobileViTV2 and EfficientViT-M2, outperform models trained from scratch in\nterms of accuracy and efficiency. These models achieve high performance with\nreduced computational requirements and exhibit greater resilience during\ninference under noisy conditions. While MobileViTV2 has excelled on clean\nvalidation data, EfficientViT-M2 has proved more robust when handling noise,\nmaking it the most suitable model for onboard satellite EO tasks. Our\nexperimental results demonstrate that EfficientViT-M2 is the optimal choice for\nreliable and efficient RS-IC in satellite operations, achieving 98.76 % of\naccuracy, precision, and recall. Precisely, EfficientViT-M2 delivers the\nhighest performance across all metrics, excels in training efficiency (1,000s)\nand inference time (10s), and demonstrates greater robustness (overall\nrobustness score of 0.79). Consequently, EfficientViT-M2 consumes 63.93 % less\npower than MobileViTV2 (79.23 W) and 73.26 % less power than SwinTransformer\n(108.90 W). This highlights its significant advantage in energy efficiency.\n","authors":["Thanh-Dung Le","Vu Nguyen Ha","Ti Ti Nguyen","Geoffrey Eappen","Prabhu Thiruvasagam","Luis M. Garces-Socarras","Hong-fu Chou","Jorge L. Gonzalez-Rios","Juan Carlos Merlano-Duncan","Symeon Chatzinotas"],"pdf_url":"https://arxiv.org/pdf/2409.03901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16545v1","updated":"2024-10-21T22:16:37Z","published":"2024-10-21T22:16:37Z","title":"PlaneSAM: Multimodal Plane Instance Segmentation Using the Segment\n  Anything Model","summary":"  Plane instance segmentation from RGB-D data is a crucial research topic for\nmany downstream tasks. However, most existing deep-learning-based methods\nutilize only information within the RGB bands, neglecting the important role of\nthe depth band in plane instance segmentation. Based on EfficientSAM, a fast\nversion of SAM, we propose a plane instance segmentation network called\nPlaneSAM, which can fully integrate the information of the RGB bands (spectral\nbands) and the D band (geometric band), thereby improving the effectiveness of\nplane instance segmentation in a multimodal manner. Specifically, we use a\ndual-complexity backbone, with primarily the simpler branch learning D-band\nfeatures and primarily the more complex branch learning RGB-band features.\nConsequently, the backbone can effectively learn D-band feature representations\neven when D-band training data is limited in scale, retain the powerful\nRGB-band feature representations of EfficientSAM, and allow the original\nbackbone branch to be fine-tuned for the current task. To enhance the\nadaptability of our PlaneSAM to the RGB-D domain, we pretrain our\ndual-complexity backbone using the segment anything task on large-scale RGB-D\ndata through a self-supervised pretraining strategy based on imperfect\npseudo-labels. To support the segmentation of large planes, we optimize the\nloss function combination ratio of EfficientSAM. In addition, Faster R-CNN is\nused as a plane detector, and its predicted bounding boxes are fed into our\ndual-complexity network as prompts, thereby enabling fully automatic plane\ninstance segmentation. Experimental results show that the proposed PlaneSAM\nsets a new SOTA performance on the ScanNet dataset, and outperforms previous\nSOTA approaches in zero-shot transfer on the 2D-3D-S, Matterport3D, and\nICL-NUIM RGB-D datasets, while only incurring a 10% increase in computational\noverhead compared to EfficientSAM.\n","authors":["Zhongchen Deng","Zhechen Yang","Chi Chen","Cheng Zeng","Yan Meng","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16545v1.pdf","comment":"submitted to Information Fusion"},{"id":"http://arxiv.org/abs/2310.17042v3","updated":"2024-10-21T21:54:46Z","published":"2023-10-25T22:45:31Z","title":"StochGradAdam: Accelerating Neural Networks Training with Stochastic\n  Gradient Sampling","summary":"  In this paper, we introduce StochGradAdam, a novel optimizer designed as an\nextension of the Adam algorithm, incorporating stochastic gradient sampling\ntechniques to improve computational efficiency while maintaining robust\nperformance. StochGradAdam optimizes by selectively sampling a subset of\ngradients during training, reducing the computational cost while preserving the\nadvantages of adaptive learning rates and bias corrections found in Adam. Our\nexperimental results, applied to image classification and segmentation tasks,\ndemonstrate that StochGradAdam can achieve comparable or superior performance\nto Adam, even when using fewer gradient updates per iteration. By focusing on\nkey gradient updates, StochGradAdam offers stable convergence and enhanced\nexploration of the loss landscape, while mitigating the impact of noisy\ngradients. The results suggest that this approach is particularly effective for\nlarge-scale models and datasets, providing a promising alternative to\ntraditional optimization techniques for deep learning applications.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2310.17042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16533v1","updated":"2024-10-21T21:48:24Z","published":"2024-10-21T21:48:24Z","title":"Large Body Language Models","summary":"  As virtual agents become increasingly prevalent in human-computer\ninteraction, generating realistic and contextually appropriate gestures in\nreal-time remains a significant challenge. While neural rendering techniques\nhave made substantial progress with static scripts, their applicability to\nhuman-computer interactions remains limited. To address this, we introduce\nLarge Body Language Models (LBLMs) and present LBLM-AVA, a novel LBLM\narchitecture that combines a Transformer-XL large language model with a\nparallelized diffusion model to generate human-like gestures from multimodal\ninputs (text, audio, and video). LBLM-AVA incorporates several key components\nenhancing its gesture generation capabilities, such as multimodal-to-pose\nembeddings, enhanced sequence-to-sequence mapping with redefined attention\nmechanisms, a temporal smoothing module for gesture sequence coherence, and an\nattention-based refinement module for enhanced realism. The model is trained on\nour large-scale proprietary open-source dataset Allo-AVA. LBLM-AVA achieves\nstate-of-the-art performance in generating lifelike and contextually\nappropriate gestures with a 30% reduction in Fr\\'echet Gesture Distance (FGD),\nand a 25% improvement in Fr\\'echet Inception Distance compared to existing\napproaches.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2410.16533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16524v1","updated":"2024-10-21T21:32:17Z","published":"2024-10-21T21:32:17Z","title":"Gradient-Free Supervised Learning using Spike-Timing-Dependent\n  Plasticity for Image Recognition","summary":"  An approach to supervised learning in spiking neural networks is presented\nusing a gradient-free method combined with spike-timing-dependent plasticity\nfor image recognition. The proposed network architecture is scalable to\nmultiple layers, enabling the development of more complex and deeper SNN\nmodels. The effectiveness of this method is demonstrated by its application to\nthe MNIST dataset, showing good learning accuracy. The proposed method provides\na robust and efficient alternative to the backpropagation-based method in\nsupervised learning.\n","authors":["Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2410.16524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16523v1","updated":"2024-10-21T21:31:12Z","published":"2024-10-21T21:31:12Z","title":"Efficient Neural Network Training via Subset Pretraining","summary":"  In training neural networks, it is common practice to use partial gradients\ncomputed over batches, mostly very small subsets of the training set. This\napproach is motivated by the argument that such a partial gradient is close to\nthe true one, with precision growing only with the square root of the batch\nsize. A theoretical justification is with the help of stochastic approximation\ntheory. However, the conditions for the validity of this theory are not\nsatisfied in the usual learning rate schedules. Batch processing is also\ndifficult to combine with efficient second-order optimization methods. This\nproposal is based on another hypothesis: the loss minimum of the training set\ncan be expected to be well-approximated by the minima of its subsets. Such\nsubset minima can be computed in a fraction of the time necessary for\noptimizing over the whole training set. This hypothesis has been tested with\nthe help of the MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks,\noptionally extended by training data augmentation. The experiments have\nconfirmed that results equivalent to conventional training can be reached. In\nsummary, even small subsets are representative if the overdetermination ratio\nfor the given model parameter set sufficiently exceeds unity. The computing\nexpense can be reduced to a tenth or less.\n","authors":["Jan Spörer","Bernhard Bermeitinger","Tomas Hrycej","Niklas Limacher","Siegfried Handschuh"],"pdf_url":"https://arxiv.org/pdf/2410.16523v1.pdf","comment":"To appear in KDIR 2024"},{"id":"http://arxiv.org/abs/2410.16512v1","updated":"2024-10-21T21:05:04Z","published":"2024-10-21T21:05:04Z","title":"TIPS: Text-Image Pretraining with Spatial Awareness","summary":"  While image-text representation learning has become very popular in recent\nyears, existing models tend to lack spatial awareness and have limited direct\napplicability for dense understanding tasks. For this reason, self-supervised\nimage-only pretraining is still the go-to method for many dense vision\napplications (e.g. depth estimation, semantic segmentation), despite the lack\nof explicit supervisory signals. In this paper, we close this gap between\nimage-text and self-supervised learning, by proposing a novel general-purpose\nimage-text model, which can be effectively used off-the-shelf for dense and\nglobal vision tasks. Our method, which we refer to as Text-Image Pretraining\nwith Spatial awareness (TIPS), leverages two simple and effective insights.\nFirst, on textual supervision: we reveal that replacing noisy web image\ncaptions by synthetically generated textual descriptions boosts dense\nunderstanding performance significantly, due to a much richer signal for\nlearning spatially aware representations. We propose an adapted training method\nthat combines noisy and synthetic captions, resulting in improvements across\nboth dense and global understanding tasks. Second, on the learning technique:\nwe propose to combine contrastive image-text learning with self-supervised\nmasked image modeling, to encourage spatial coherence, unlocking substantial\nenhancements for downstream applications. Building on these two ideas, we scale\nour model using the transformer architecture, trained on a curated set of\npublic images. Our experiments are conducted on 8 tasks involving 16 datasets\nin total, demonstrating strong off-the-shelf performance on both dense and\nglobal understanding, for several image-only and image-text tasks.\n","authors":["Kevis-Kokitsi Maninis","Kaifeng Chen","Soham Ghosh","Arjun Karpur","Koert Chen","Ye Xia","Bingyi Cao","Daniel Salz","Guangxing Han","Jan Dlabal","Dan Gnanapragasam","Mojtaba Seyedhosseini","Howard Zhou","Andre Araujo"],"pdf_url":"https://arxiv.org/pdf/2410.16512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03783v2","updated":"2024-10-21T20:54:04Z","published":"2024-10-03T16:42:23Z","title":"Improving Neural Optimal Transport via Displacement Interpolation","summary":"  Optimal Transport (OT) theory investigates the cost-minimizing transport map\nthat moves a source distribution to a target distribution. Recently, several\napproaches have emerged for learning the optimal transport map for a given cost\nfunction using neural networks. We refer to these approaches as the OT Map. OT\nMap provides a powerful tool for diverse machine learning tasks, such as\ngenerative modeling and unpaired image-to-image translation. However, existing\nmethods that utilize max-min optimization often experience training instability\nand sensitivity to hyperparameters. In this paper, we propose a novel method to\nimprove stability and achieve a better approximation of the OT Map by\nexploiting displacement interpolation, dubbed Displacement Interpolation\nOptimal Transport Model (DIOTM). We derive the dual formulation of displacement\ninterpolation at specific time $t$ and prove how these dual problems are\nrelated across time. This result allows us to utilize the entire trajectory of\ndisplacement interpolation in learning the OT Map. Our method improves the\ntraining stability and achieves superior results in estimating optimal\ntransport maps. We demonstrate that DIOTM outperforms existing OT-based models\non image-to-image translation tasks.\n","authors":["Jaemoo Choi","Yongxin Chen","Jaewoong Choi"],"pdf_url":"https://arxiv.org/pdf/2410.03783v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2410.16503v1","updated":"2024-10-21T20:50:51Z","published":"2024-10-21T20:50:51Z","title":"Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for\n  Allocentric Avatar Gesture Animation","summary":"  The scarcity of high-quality, multimodal training data severely hinders the\ncreation of lifelike avatar animations for conversational AI in virtual\nenvironments. Existing datasets often lack the intricate synchronization\nbetween speech, facial expressions, and body movements that characterize\nnatural human communication. To address this critical gap, we introduce\nAllo-AVA, a large-scale dataset specifically designed for text and audio-driven\navatar gesture animation in an allocentric (third person point-of-view)\ncontext. Allo-AVA consists of $\\sim$1,250 hours of diverse video content,\ncomplete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely\nmaps these keypoints to precise timestamps, enabling accurate replication of\nhuman movements (body and facial gestures) in synchronization with speech. This\ncomprehensive resource enables the development and evaluation of more natural,\ncontext-aware avatar animation models, potentially transforming applications\nranging from virtual reality to digital assistants.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2410.16503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16499v1","updated":"2024-10-21T20:41:32Z","published":"2024-10-21T20:41:32Z","title":"SINGAPO: Single Image Controlled Generation of Articulated Parts in\n  Object","summary":"  We address the challenge of creating 3D assets for household articulated\nobjects from a single image. Prior work on articulated object creation either\nrequires multi-view multi-state input, or only allows coarse control over the\ngeneration process. These limitations hinder the scalability and practicality\nfor articulated object modeling. In this work, we propose a method to generate\narticulated objects from a single image. Observing the object in resting state\nfrom an arbitrary view, our method generates an articulated object that is\nvisually consistent with the input image. To capture the ambiguity in part\nshape and motion posed by a single view of the object, we design a diffusion\nmodel that learns the plausible variations of objects in terms of geometry and\nkinematics. To tackle the complexity of generating structured data with\nattributes in multiple domains, we design a pipeline that produces articulated\nobjects from high-level structure to geometric details in a coarse-to-fine\nmanner, where we use a part connectivity graph and part abstraction as proxies.\nOur experiments show that our method outperforms the state-of-the-art in\narticulated object creation by a large margin in terms of the generated object\nrealism, resemblance to the input image, and reconstruction quality.\n","authors":["Jiayi Liu","Denys Iliash","Angel X. Chang","Manolis Savva","Ali Mahdavi-Amiri"],"pdf_url":"https://arxiv.org/pdf/2410.16499v1.pdf","comment":"Project page: https://3dlg-hcvc.github.io/singapo"},{"id":"http://arxiv.org/abs/2410.16485v1","updated":"2024-10-21T20:21:09Z","published":"2024-10-21T20:21:09Z","title":"GenGMM: Generalized Gaussian-Mixture-based Domain Adaptation Model for\n  Semantic Segmentation","summary":"  Domain adaptive semantic segmentation is the task of generating precise and\ndense predictions for an unlabeled target domain using a model trained on a\nlabeled source domain. While significant efforts have been devoted to improving\nunsupervised domain adaptation for this task, it is crucial to note that many\nmodels rely on a strong assumption that the source data is entirely and\naccurately labeled, while the target data is unlabeled. In real-world\nscenarios, however, we often encounter partially or noisy labeled data in\nsource and target domains, referred to as Generalized Domain Adaptation (GDA).\nIn such cases, we suggest leveraging weak or unlabeled data from both domains\nto narrow the gap between them, resulting in effective adaptation. We introduce\nthe Generalized Gaussian-mixture-based (GenGMM) domain adaptation model, which\nharnesses the underlying data distribution in both domains to refine noisy weak\nand pseudo labels. The experiments demonstrate the effectiveness of our\napproach.\n","authors":["Nazanin Moradinasab","Hassan Jafarzadeh","Donald E. Brown"],"pdf_url":"https://arxiv.org/pdf/2410.16485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10695v2","updated":"2024-10-21T20:01:13Z","published":"2024-09-16T19:52:24Z","title":"Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large\n  Language Models","summary":"  We introduce Playground v3 (PGv3), our latest text-to-image model that\nachieves state-of-the-art (SoTA) performance across multiple testing\nbenchmarks, excels in graphic design abilities and introduces new capabilities.\nUnlike traditional text-to-image generative models that rely on pre-trained\nlanguage models like T5 or CLIP text encoders, our approach fully integrates\nLarge Language Models (LLMs) with a novel structure that leverages text\nconditions exclusively from a decoder-only LLM. Additionally, to enhance image\ncaptioning quality-we developed an in-house captioner, capable of generating\ncaptions with varying levels of detail, enriching the diversity of text\nstructures. We also introduce a new benchmark CapsBench to evaluate detailed\nimage captioning performance. Experimental results demonstrate that PGv3 excels\nin text prompt adherence, complex reasoning, and accurate text rendering. User\npreference studies indicate the super-human graphic design ability of our model\nfor common design applications, such as stickers, posters, and logo designs.\nFurthermore, PGv3 introduces new capabilities, including precise RGB color\ncontrol and robust multilingual understanding.\n","authors":["Bingchen Liu","Ehsan Akhgari","Alexander Visheratin","Aleks Kamko","Linmiao Xu","Shivam Shrirao","Chase Lambert","Joao Souza","Suhail Doshi","Daiqing Li"],"pdf_url":"https://arxiv.org/pdf/2409.10695v2.pdf","comment":"Project page: https://playground.com/pg-v3"},{"id":"http://arxiv.org/abs/2410.16438v1","updated":"2024-10-21T19:02:13Z","published":"2024-10-21T19:02:13Z","title":"AlignVSR: Audio-Visual Cross-Modal Alignment for Visual Speech\n  Recognition","summary":"  Visual Speech Recognition (VSR) aims to recognize corresponding text by\nanalyzing visual information from lip movements. Due to the high variability\nand weak information of lip movements, VSR tasks require effectively utilizing\nany information from any source and at any level. In this paper, we propose a\nVSR method based on audio-visual cross-modal alignment, named AlignVSR. The\nmethod leverages the audio modality as an auxiliary information source and\nutilizes the global and local correspondence between the audio and visual\nmodalities to improve visual-to-text inference. Specifically, the method first\ncaptures global alignment between video and audio through a cross-modal\nattention mechanism from video frames to a bank of audio units. Then, based on\nthe temporal correspondence between audio and video, a frame-level local\nalignment loss is introduced to refine the global alignment, improving the\nutility of the audio information. Experimental results on the LRS2 and\nCNVSRC.Single datasets consistently show that AlignVSR outperforms several\nmainstream VSR methods, demonstrating its superior and robust performance.\n","authors":["Zehua Liu","Xiaolou Li","Chen Chen","Li Guo","Lantian Li","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15512v2","updated":"2024-10-21T18:57:08Z","published":"2024-09-03T06:02:02Z","title":"PixelBytes: Catching Unified Embedding for Multimodal Generation","summary":"  This report introduces PixelBytes Embedding, a novel approach for unified\nmultimodal representation learning. Our method captures diverse inputs in a\nsingle, cohesive representation, enabling emergent properties for multimodal\nsequence generation, particularly for text and pixelated images. Inspired by\nstate-of-the-art sequence models such as Image Transformers, PixelCNN, and\nMamba-Bytes, PixelBytes aims to address the challenges of integrating different\ndata types. We explore various model architectures, including Recurrent Neural\nNetworks (RNNs), State Space Models (SSMs), and Attention-based models,\nfocusing on bidirectional processing and our innovative PxBy embedding\ntechnique. Our experiments, conducted on a specialized PixelBytes Pok{\\'e}mon\ndataset, demonstrate that bidirectional sequence models with PxBy embedding and\nconvolutional layers can generate coherent multimodal sequences. This work\ncontributes to the advancement of integrated AI models capable of understanding\nand generating multimodal data in a unified manner.\n","authors":["Fabien Furfaro"],"pdf_url":"https://arxiv.org/pdf/2409.15512v2.pdf","comment":"This article is an earlier version of my work arXiv:2410.01820\n  \"PixelBytes: Catching Unified Representation for Multimodal Generation.\""},{"id":"http://arxiv.org/abs/2410.16430v1","updated":"2024-10-21T18:50:16Z","published":"2024-10-21T18:50:16Z","title":"HaHeAE: Learning Generalisable Joint Representations of Human Hand and\n  Head Movements in Extended Reality","summary":"  Human hand and head movements are the most pervasive input modalities in\nextended reality (XR) and are significant for a wide range of applications.\nHowever, prior works on hand and head modelling in XR only explored a single\nmodality or focused on specific applications. We present HaHeAE - a novel\nself-supervised method for learning generalisable joint representations of hand\nand head movements in XR. At the core of our method is an autoencoder (AE) that\nuses a graph convolutional network-based semantic encoder and a diffusion-based\nstochastic encoder to learn the joint semantic and stochastic representations\nof hand-head movements. It also features a diffusion-based decoder to\nreconstruct the original signals. Through extensive evaluations on three public\nXR datasets, we show that our method 1) significantly outperforms commonly used\nself-supervised methods by up to 74.0% in terms of reconstruction quality and\nis generalisable across users, activities, and XR environments, 2) enables new\napplications, including interpretable hand-head cluster identification and\nvariable hand-head movement generation, and 3) can serve as an effective\nfeature extractor for downstream tasks. Together, these results demonstrate the\neffectiveness of our method and underline the potential of self-supervised\nmethods for jointly modelling hand-head behaviours in extended reality.\n","authors":["Zhiming Hu","Guanhua Zhang","Zheming Yin","Daniel Haeufle","Syn Schmitt","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2410.16430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12090v2","updated":"2024-10-21T18:45:37Z","published":"2023-12-19T12:10:12Z","title":"GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion\n  Prediction","summary":"  Human motion prediction is important for many virtual and augmented reality\n(VR/AR) applications such as collision avoidance and realistic avatar\ngeneration. Existing methods have synthesised body motion only from observed\npast motion, despite the fact that human eye gaze is known to correlate\nstrongly with body movements and is readily available in recent VR/AR headsets.\nWe present GazeMoDiff - a novel gaze-guided denoising diffusion model to\ngenerate stochastic human motions. Our method first uses a gaze encoder and a\nmotion encoder to extract the gaze and motion features respectively, then\nemploys a graph attention network to fuse these features, and finally injects\nthe gaze-motion features into a noise prediction network via a cross-attention\nmechanism to progressively generate multiple reasonable human motions in the\nfuture. Extensive experiments on the MoGaze and GIMO datasets demonstrate that\nour method outperforms the state-of-the-art methods by a large margin in terms\nof multi-modal final displacement error (17.3% on MoGaze and 13.3% on GIMO). We\nfurther conducted a human study (N=21) and validated that the motions generated\nby our method were perceived as both more precise and more realistic than those\nof prior methods. Taken together, these results reveal the significant\ninformation content available in eye gaze for stochastic human motion\nprediction as well as the effectiveness of our method in exploiting this\ninformation.\n","authors":["Haodong Yan","Zhiming Hu","Syn Schmitt","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2312.12090v2.pdf","comment":"Accepted at PG 2024. Link:\n  https://zhiminghu.net/yan24_gazemodiff.html"},{"id":"http://arxiv.org/abs/2410.16418v1","updated":"2024-10-21T18:36:45Z","published":"2024-10-21T18:36:45Z","title":"AttentionPainter: An Efficient and Adaptive Stroke Predictor for Scene\n  Painting","summary":"  Stroke-based Rendering (SBR) aims to decompose an input image into a sequence\nof parameterized strokes, which can be rendered into a painting that resembles\nthe input image. Recently, Neural Painting methods that utilize deep learning\nand reinforcement learning models to predict the stroke sequences have been\ndeveloped, but suffer from longer inference time or unstable training. To\naddress these issues, we propose AttentionPainter, an efficient and adaptive\nmodel for single-step neural painting. First, we propose a novel scalable\nstroke predictor, which predicts a large number of stroke parameters within a\nsingle forward process, instead of the iterative prediction of previous\nReinforcement Learning or auto-regressive methods, which makes AttentionPainter\nfaster than previous neural painting methods. To further increase the training\nefficiency, we propose a Fast Stroke Stacking algorithm, which brings 13 times\nacceleration for training. Moreover, we propose Stroke-density Loss, which\nencourages the model to use small strokes for detailed information, to help\nimprove the reconstruction quality. Finally, we propose a new stroke diffusion\nmodel for both conditional and unconditional stroke-based generation, which\ndenoises in the stroke parameter space and facilitates stroke-based inpainting\nand editing applications helpful for human artists design. Extensive\nexperiments show that AttentionPainter outperforms the state-of-the-art neural\npainting methods.\n","authors":["Yizhe Tang","Yue Wang","Teng Hu","Ran Yi","Xin Tan","Lizhuang Ma","Yu-Kun Lai","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2410.16418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06111v2","updated":"2024-10-21T18:16:09Z","published":"2024-09-09T23:34:24Z","title":"Competency-Aware Planning for Probabilistically Safe Navigation Under\n  Perception Uncertainty","summary":"  Perception-based navigation systems are useful for unmanned ground vehicle\n(UGV) navigation in complex terrains, where traditional depth-based navigation\nschemes are insufficient. However, these data-driven methods are highly\ndependent on their training data and can fail in surprising and dramatic ways\nwith little warning. To ensure the safety of the vehicle and the surrounding\nenvironment, it is imperative that the navigation system is able to recognize\nthe predictive uncertainty of the perception model and respond safely and\neffectively in the face of uncertainty. In an effort to enable safe navigation\nunder perception uncertainty, we develop a probabilistic and\nreconstruction-based competency estimation (PaRCE) method to estimate the\nmodel's level of familiarity with an input image as a whole and with specific\nregions in the image. We find that the overall competency score can correctly\npredict correctly classified, misclassified, and out-of-distribution (OOD)\nsamples. We also confirm that the regional competency maps can accurately\ndistinguish between familiar and unfamiliar regions across images. We then use\nthis competency information to develop a planning and control scheme that\nenables effective navigation while maintaining a low probability of error. We\nfind that the competency-aware scheme greatly reduces the number of collisions\nwith unfamiliar obstacles, compared to a baseline controller with no competency\nawareness. Furthermore, the regional competency information is very valuable in\nenabling efficient navigation.\n","authors":["Sara Pohland","Claire Tomlin"],"pdf_url":"https://arxiv.org/pdf/2409.06111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16395v1","updated":"2024-10-21T18:07:33Z","published":"2024-10-21T18:07:33Z","title":"Joker: Conditional 3D Head Synthesis with Extreme Facial Expressions","summary":"  We introduce Joker, a new method for the conditional synthesis of 3D human\nheads with extreme expressions. Given a single reference image of a person, we\nsynthesize a volumetric human head with the reference identity and a new\nexpression. We offer control over the expression via a 3D morphable model\n(3DMM) and textual inputs. This multi-modal conditioning signal is essential\nsince 3DMMs alone fail to define subtle emotional changes and extreme\nexpressions, including those involving the mouth cavity and tongue\narticulation. Our method is built upon a 2D diffusion-based prior that\ngeneralizes well to out-of-domain samples, such as sculptures, heavy makeup,\nand paintings while achieving high levels of expressiveness. To improve view\nconsistency, we propose a new 3D distillation technique that converts\npredictions of our 2D prior into a neural radiance field (NeRF). Both the 2D\nprior and our distillation technique produce state-of-the-art results, which\nare confirmed by our extensive evaluations. Also, to the best of our knowledge,\nour method is the first to achieve view-consistent extreme tongue articulation.\n","authors":["Malte Prinzler","Egor Zakharov","Vanessa Sklyarova","Berna Kabadayi","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2410.16395v1.pdf","comment":"Project Page: https://malteprinzler.github.io/projects/joker/"},{"id":"http://arxiv.org/abs/2410.16347v1","updated":"2024-10-21T14:12:39Z","published":"2024-10-21T14:12:39Z","title":"Domain-Adaptive Neural Posterior Estimation for Strong Gravitational\n  Lens Analysis","summary":"  Modeling strong gravitational lenses is prohibitively expensive for modern\nand next-generation cosmic survey data. Neural posterior estimation (NPE), a\nsimulation-based inference (SBI) approach, has been studied as an avenue for\nefficient analysis of strong lensing data. However, NPE has not been\ndemonstrated to perform well on out-of-domain target data -- e.g., when trained\non simulated data and then applied to real, observational data. In this work,\nwe perform the first study of the efficacy of NPE in combination with\nunsupervised domain adaptation (UDA). The source domain is noiseless, and the\ntarget domain has noise mimicking modern cosmology surveys. We find that\ncombining UDA and NPE improves the accuracy of the inference by 1-2 orders of\nmagnitude and significantly improves the posterior coverage over an NPE model\nwithout UDA. We anticipate that this combination of approaches will help enable\nfuture applications of NPE models to real observational data.\n","authors":["Paxson Swierc","Marcos Tamargo-Arizmendi","Aleksandra Ćiprijanović","Brian D. Nord"],"pdf_url":"https://arxiv.org/pdf/2410.16347v1.pdf","comment":"20 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2210.04561v4","updated":"2024-10-21T14:11:42Z","published":"2022-10-10T11:01:57Z","title":"A Comprehensive Survey of Data Augmentation in Visual Reinforcement\n  Learning","summary":"  Visual reinforcement learning (RL), which makes decisions directly from\nhigh-dimensional visual inputs, has demonstrated significant potential in\nvarious domains. However, deploying visual RL techniques in the real world\nremains challenging due to their low sample efficiency and large generalization\ngaps. To tackle these obstacles, data augmentation (DA) has become a widely\nused technique in visual RL for acquiring sample-efficient and generalizable\npolicies by diversifying the training data. This survey aims to provide a\ntimely and essential review of DA techniques in visual RL in recognition of the\nthriving development in this field. In particular, we propose a unified\nframework for analyzing visual RL and understanding the role of DA in it. We\nthen present a principled taxonomy of the existing augmentation techniques used\nin visual RL and conduct an in-depth discussion on how to better leverage\naugmented data in different scenarios. Moreover, we report a systematic\nempirical evaluation of DA-based techniques in visual RL and conclude by\nhighlighting the directions for future research. As the first comprehensive\nsurvey of DA in visual RL, this work is expected to offer valuable guidance to\nthis emerging field.\n","authors":["Guozheng Ma","Zhen Wang","Zhecheng Yuan","Xueqian Wang","Bo Yuan","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2210.04561v4.pdf","comment":"A well-classified paper list that will be continuously updated can be\n  found at https://github.com/Guozheng-Ma/DA-in-visualRL"}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2410.14205v2","updated":"2024-10-21T17:53:04Z","published":"2024-10-18T06:38:34Z","title":"Modelling 1/f Noise in TRNGs via Fractional Brownian Motion","summary":"  Building upon the foundational work of atomic clock physicists Barnes and\nAllan, this paper presents a highly scalable and numerically exact framework\nfor modeling \\(1/f\\) noise in oscillatory True Random Number Generators (TRNGs)\nand assessing their cryptographic security. By employing Fractional Brownian\nMotion, the framework constructs Gaussian non-stationary processes that\nrepresent these noise spectra accurately and in a mathematically sound way.\nFurthermore, it establishes several critical properties, including optimal\nbounds on the achievable generation rate of cryptographically secure bits.\n","authors":["Maciej Skorski"],"pdf_url":"https://arxiv.org/pdf/2410.14205v2.pdf","comment":"correcting typos"},{"id":"http://arxiv.org/abs/2410.13995v2","updated":"2024-10-21T16:27:48Z","published":"2024-10-17T19:50:28Z","title":"Adversarial Inception for Bounded Backdoor Poisoning in Deep\n  Reinforcement Learning","summary":"  Recent works have demonstrated the vulnerability of Deep Reinforcement\nLearning (DRL) algorithms against training-time, backdoor poisoning attacks.\nThese attacks induce pre-determined, adversarial behavior in the agent upon\nobserving a fixed trigger during deployment while allowing the agent to solve\nits intended task during training. Prior attacks rely on arbitrarily large\nperturbations to the agent's rewards to achieve both of these objectives -\nleaving them open to detection. Thus, in this work, we propose a new class of\nbackdoor attacks against DRL which achieve state of the art performance while\nminimally altering the agent's rewards. These \"inception\" attacks train the\nagent to associate the targeted adversarial behavior with high returns by\ninducing a disjunction between the agent's chosen action and the true action\nexecuted in the environment during training. We formally define these attacks\nand prove they can achieve both adversarial objectives. We then devise an\nonline inception attack which significantly out-performs prior attacks under\nbounded reward constraints.\n","authors":["Ethan Rathbun","Christopher Amato","Alina Oprea"],"pdf_url":"https://arxiv.org/pdf/2410.13995v2.pdf","comment":"10 pages, 5 figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2403.20059v2","updated":"2024-10-21T16:51:59Z","published":"2024-03-29T08:48:04Z","title":"Optimal s-boxes against alternative operations and linear propagation","summary":"  Civino et al.~(2019) have shown how some diffusion layers can expose a\nSubstitution-Permutation Network to vulnerability from differential\ncryptanalysis when employing alternative operations coming from groups\nisomorphic to the translation group on the message space. In this study, we\npresent a classification of diffusion layers that exhibit linearity in parallel\nalternative operations for ciphers with 4-bit s-boxes, enabling the possibility\nof an alternative differential attack simultaneously targeting all the s-boxes\nwithin the block. Furthermore, we investigate the differential behaviour with\nrespect to alternative operations for all classes of optimal 4-bit s-boxes, as\ndefined by Leander and Poschmann (2007). Our examination reveals that certain\nclasses contain weak permutations w.r.t. alternative differential attacks, and\nwe leverage these vulnerabilities to execute a series of experiments.\n","authors":["Marco Calderini","Roberto Civino","Riccardo Invernizzi"],"pdf_url":"https://arxiv.org/pdf/2403.20059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18169v3","updated":"2024-10-21T16:51:22Z","published":"2024-09-26T17:55:22Z","title":"Harmful Fine-tuning Attacks and Defenses for Large Language Models: A\n  Survey","summary":"  Recent research demonstrates that the nascent fine-tuning-as-a-service\nbusiness model exposes serious safety concerns -- fine-tuning over a few\nharmful data uploaded by the users can compromise the safety alignment of the\nmodel. The attack, known as harmful fine-tuning, has raised a broad research\ninterest among the community. However, as the attack is still new, \\textbf{we\nobserve from our miserable submission experience that there are general\nmisunderstandings within the research community.} We in this paper aim to clear\nsome common concerns for the attack setting, and formally establish the\nresearch problem. Specifically, we first present the threat model of the\nproblem, and introduce the harmful fine-tuning attack and its variants. Then we\nsystematically survey the existing literature on attacks/defenses/mechanical\nanalysis of the problem. Finally, we outline future research directions that\nmight contribute to the development of the field. Additionally, we present a\nlist of questions of interest, which might be useful to refer to when reviewers\nin the peer review process question the realism of the\nexperiment/attack/defense setting. A curated list of relevant papers is\nmaintained and made accessible at:\n\\url{https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers}.\n","authors":["Tiansheng Huang","Sihao Hu","Fatih Ilhan","Selim Furkan Tekin","Ling Liu"],"pdf_url":"https://arxiv.org/pdf/2409.18169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20539v2","updated":"2024-10-21T16:44:58Z","published":"2024-05-30T23:31:25Z","title":"SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement\n  Learning Agents","summary":"  Reinforcement learning (RL) is an actively growing field that is seeing\nincreased usage in real-world, safety-critical applications -- making it\nparamount to ensure the robustness of RL algorithms against adversarial\nattacks. In this work we explore a particularly stealthy form of training-time\nattacks against RL -- backdoor poisoning. Here the adversary intercepts the\ntraining of an RL agent with the goal of reliably inducing a particular action\nwhen the agent observes a pre-determined trigger at inference time. We uncover\ntheoretical limitations of prior work by proving their inability to generalize\nacross domains and MDPs. Motivated by this, we formulate a novel poisoning\nattack framework which interlinks the adversary's objectives with those of\nfinding an optimal policy -- guaranteeing attack success in the limit. Using\ninsights from our theoretical analysis we develop ``SleeperNets'' as a\nuniversal backdoor attack which exploits a newly proposed threat model and\nleverages dynamic reward poisoning techniques. We evaluate our attack in 6\nenvironments spanning multiple domains and demonstrate significant improvements\nin attack success over existing methods, while preserving benign episodic\nreturn.\n","authors":["Ethan Rathbun","Christopher Amato","Alina Oprea"],"pdf_url":"https://arxiv.org/pdf/2405.20539v2.pdf","comment":"23 pages, 14 figures, NeurIPS"},{"id":"http://arxiv.org/abs/2410.07428v2","updated":"2024-10-21T16:37:02Z","published":"2024-10-09T20:48:03Z","title":"The First VoicePrivacy Attacker Challenge Evaluation Plan","summary":"  The First VoicePrivacy Attacker Challenge is a new kind of challenge\norganized as part of the VoicePrivacy initiative and supported by ICASSP 2025\nas the SP Grand Challenge It focuses on developing attacker systems against\nvoice anonymization, which will be evaluated against a set of anonymization\nsystems submitted to the VoicePrivacy 2024 Challenge. Training, development,\nand evaluation datasets are provided along with a baseline attacker system.\nParticipants shall develop their attacker systems in the form of automatic\nspeaker verification systems and submit their scores on the development and\nevaluation data to the organizers. To do so, they can use any additional\ntraining data and models, provided that they are openly available and declared\nbefore the specified deadline. The metric for evaluation is equal error rate\n(EER). Results will be presented at the ICASSP 2025 special session to which 5\nselected top-ranked participants will be invited to submit and present their\nchallenge systems.\n","authors":["Natalia Tomashenko","Xiaoxiao Miao","Emmanuel Vincent","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2410.07428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16161v1","updated":"2024-10-21T16:25:14Z","published":"2024-10-21T16:25:14Z","title":"DMM: Distributed Matrix Mechanism for Differentially-Private Federated\n  Learning using Packed Secret Sharing","summary":"  Federated Learning (FL) has gained lots of traction recently, both in\nindustry and academia. In FL, a machine learning model is trained using data\nfrom various end-users arranged in committees across several rounds. Since such\ndata can often be sensitive, a primary challenge in FL is providing privacy\nwhile still retaining utility of the model. Differential Privacy (DP) has\nbecome the main measure of privacy in the FL setting. DP comes in two flavors:\ncentral and local. In the former, a centralized server is trusted to receive\nthe users' raw gradients from a training step, and then perturb their\naggregation with some noise before releasing the next version of the model. In\nthe latter (more private) setting, noise is applied on users' local devices,\nand only the aggregation of users' noisy gradients is revealed even to the\nserver. Great strides have been made in increasing the privacy-utility\ntrade-off in the central DP setting, by utilizing the so-called matrix\nmechanism. However, progress has been mostly stalled in the local DP setting.\nIn this work, we introduce the distributed matrix mechanism to achieve the\nbest-of-both-worlds; local DP and also better privacy-utility trade-off from\nthe matrix mechanism. We accomplish this by proposing a cryptographic protocol\nthat securely transfers sensitive values across rounds, which makes use of\npacked secret sharing. This protocol accommodates the dynamic participation of\nusers per training round required by FL, including those that may drop out from\nthe computation. We provide experiments which show that our mechanism indeed\nsignificantly improves the privacy-utility trade-off of FL models compared to\nprevious local DP mechanisms, with little added overhead.\n","authors":["Alexander Bienstock","Ujjwal Kumar","Antigoni Polychroniadou"],"pdf_url":"https://arxiv.org/pdf/2410.16161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16141v1","updated":"2024-10-21T16:08:20Z","published":"2024-10-21T16:08:20Z","title":"AdChain: Decentralized Header Bidding","summary":"  Due to the involvement of multiple intermediaries without trusted parties,\nlack of proper regulations, and a complicated supply chain, ad impression\ndiscrepancy affects online advertising. This issue causes up to $82 billion\nannual revenue loss for honest parties. The loss can be significantly reduced\nwith a precise and trusted decentralized mechanism. This paper presents\nAdChain, a decentralized, distributed, and verifiable solution that detects and\nminimizes online advertisement impression discrepancies. AdChain establishes\ntrust by employing multiple independent agents to receive and record log-level\ndata, along with a consensus protocol to validate each ad data. AdChain is\nscalable, efficient, and compatible with the current infrastructure. Our\nexperimental evaluation, using over half a million ad data points, identifies\nsystem parameters that achieve 98% accuracy, reducing the ad discrepancy rate\nfrom 20% to 2%. Our cost analysis shows that active nodes on AdChain can\ngenerate profits comparable to miners on major blockchain networks like\nBitcoin.\n","authors":["Behkish Nassirzadeh","Albert Heinle","Stefanos Leonardos","Anwar Hasan","Vijay Ganesh"],"pdf_url":"https://arxiv.org/pdf/2410.16141v1.pdf","comment":"Being published at MARBLE 2024 (The 5th International Conference on\n  Mathematical Research for Blockchain Economy)"},{"id":"http://arxiv.org/abs/2410.16121v1","updated":"2024-10-21T15:48:34Z","published":"2024-10-21T15:48:34Z","title":"Extracting Spatiotemporal Data from Gradients with Large Language Models","summary":"  Recent works show that sensitive user data can be reconstructed from gradient\nupdates, breaking the key privacy promise of federated learning. While success\nwas demonstrated primarily on image data, these methods do not directly\ntransfer to other domains, such as spatiotemporal data. To understand privacy\nrisks in spatiotemporal federated learning, we first propose Spatiotemporal\nGradient Inversion Attack (ST-GIA), a gradient attack algorithm tailored to\nspatiotemporal data that successfully reconstructs the original location from\ngradients. Furthermore, the absence of priors in attacks on spatiotemporal data\nhas hindered the accurate reconstruction of real client data. To address this\nlimitation, we propose ST-GIA+, which utilizes an auxiliary language model to\nguide the search for potential locations, thereby successfully reconstructing\nthe original data from gradients. In addition, we design an adaptive defense\nstrategy to mitigate gradient inversion attacks in spatiotemporal federated\nlearning. By dynamically adjusting the perturbation levels, we can offer\ntailored protection for varying rounds of training data, thereby achieving a\nbetter trade-off between privacy and utility than current state-of-the-art\nmethods. Through intensive experimental analysis on three real-world datasets,\nwe reveal that the proposed defense strategy can well preserve the utility of\nspatiotemporal federated learning with effective security protection.\n","authors":["Lele Zheng","Yang Cao","Renhe Jiang","Kenjiro Taura","Yulong Shen","Sheng Li","Masatoshi Yoshikawa"],"pdf_url":"https://arxiv.org/pdf/2410.16121v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.08529"},{"id":"http://arxiv.org/abs/2404.16271v3","updated":"2024-10-21T15:39:36Z","published":"2024-04-25T01:01:06Z","title":"Harnessing physical entropy noise in structurally metastable 1T'\n  molybdenum ditelluride for true random number generation","summary":"  True random numbers are essential in various research and engineering\nproblems. Their generation depends upon a robust physical entropy noise. Here,\nwe present true random number generation by harnessing the conductance noise\nprobed in structurally metastable 1T' molybdenum ditelluride (MoTe2). The\nnoise, well-fitting a Poisson process, is proved a robust physical entropy\nnoise at low and even cryogenic temperatures. Noise characteristic analysis\nsuggests the noise may originate from the polarization variations of the\nunderlying ferroelectric dipoles in 1T' MoTe2. We demonstrate the noise allows\nfor true random number generation, enabling their use as seed for generating\nhigh-throughput secure random numbers exceeding 1 Mbit/s, appealing for\npractical applications in, for instance, cryptography where data security is\nnow a severe issue. As an example, we show biometric information safeguarding\nin neural networks by using the random numbers as mask, proving a promising\ndata security measure in big data and artificial intelligence.\n","authors":["Yang Liu","Pengyu Liu","Yingyi Wen","Zihan Liang","Songwei Liu","Lekai Song","Jingfang Pei","Xiaoyue Fan","Teng Ma","Gang Wang","Shuo Gao","Kong-Pang Pun","Xiaolong Chen","Guohua Hu"],"pdf_url":"https://arxiv.org/pdf/2404.16271v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16098v1","updated":"2024-10-21T15:23:58Z","published":"2024-10-21T15:23:58Z","title":"Defending Against Attack on the Cloned: In-Band Active Man-in-the-Middle\n  Detection for the Signal Protocol","summary":"  With Signal's position as one of the most popular secure messaging protocols\nin use today, the threat of government coercion and mass surveillance, i.e.,\nactive Man-in-the-Middle (MitM) attacks, are more relevant than ever. On the\nother hand, studies [29, 33, 37, 38] have shown that user awareness is very\npoor when it comes to authenticating keys in instant messaging applications,\ne.g., comparing key fingerprints out-of-band. The ideal solution to this\nproblem should not require the active participation of the users. Our solution\nto active MitM attacks builds directly on Signal. We automate the process of\nkey confirmation without relying on the intervention of users, and without\nusing an out-of-band communication channel, at the cost of slightly altered\ntrust assumptions on the server. We consider a powerful active MitM that not\nonly controls the communication channel, but also has (one time) access to all\nsecrets on one of the clients, i.e., can perform a key compromise attack. Our\nsolution utilises the server to keep track of the changes in the clients key\nfingerprint as ratcheting is performed. Given that the server can keep a\nmessage log already, we find that any impact on deniability is minimal in\npractice. We present our detailed modifications to Signal, and document the new\nsecurity guarantees while preserving the existing security guarantees of\nSignal. Our proof-of-concept implementation, which is based on the open-source\nSignal library used in real-world instant messaging applications, shows that\nour solution is practical and integrates well with the library. Our\nexperimental results further show that our solution only has a tiny performance\noverhead when compared to Signal.\n","authors":["Wil Liam Teng","Kasper Rasmussen"],"pdf_url":"https://arxiv.org/pdf/2410.16098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14739v2","updated":"2024-10-21T15:01:12Z","published":"2024-03-21T17:28:35Z","title":"Improving Galileo OSNMA Time To First Authenticated Fix","summary":"  Galileo is the first global navigation satellite system to authenticate their\ncivilian signals through the Open Service Galileo Message Authentication\n(OSNMA) protocol. However, OSNMA delays the time to obtain a first position and\ntime fix, the Time To First Authentication Fix (TTFAF). Reducing the TTFAF as\nmuch as possible is crucial to integrate the technology seamlessly into the\ncurrent products. In the cases where the receiver already has cryptographic\ndata available, the so-called hot start mode and focus of this article, the\ncurrently available implementations achieve an average TTFAF of around 100\nseconds in ideal environments. In this work, we explore the TTFAF optimizations\navailable to general OSNMA capable receivers and to receivers with a tighter\ntime synchronization than the required by the OSNMA guidelines. We dissect the\nTTFAF process, describe the optimizations, and benchmark them in three distinct\nscenarios with recorded real data (open-sky, soft urban, and hard urban) and\nthe official OSNMA test vectors. The first block of optimizations centers on\nextracting as much information as possible from broken sub-frames by processing\nthem at page level and combining redundant data from multiple satellites. The\nsecond block of optimizations aims to reconstruct missed navigation data by the\nintelligent use of fields in the authentication tags belonging to the same\nsub-frame as the authentication key. Combining both optimization ideas improves\nthe TTFAF substantially for all considered scenarios. We obtain an average\nTTFAF of 60.9 and 68.8 seconds for the test vectors and the open-sky scenario,\nrespectively, with a lowest TTFAF of 44.0 seconds in both. Likewise, the urban\nscenarios see a drastic reduction of the average TTFAF between the\nnon-optimized and optimized cases. These optimizations have been made available\nas part of the open-source OSNMAlib library on GitHub.\n","authors":["Aleix Galan","Ignacio Fernandez-Hernandez","Wim De Wilde","Sofie Pollin","Gonzalo Seco-Granados"],"pdf_url":"https://arxiv.org/pdf/2403.14739v2.pdf","comment":"This work has been submitted to the IEEE for possible publication. 15\n  pages, 17 figures. Updated version with major changes. New sections II-B,\n  IV-A, IV-C. Updated sections IV-B, VI-D. Acronym changes and fixed typos"},{"id":"http://arxiv.org/abs/2410.16049v1","updated":"2024-10-21T14:24:12Z","published":"2024-10-21T14:24:12Z","title":"Dirty-Waters: Detecting Software Supply Chain Smells","summary":"  Using open-source dependencies is essential in modern software development.\nHowever, this practice implies significant trust in third-party code, while\nthere is little support for developers to assess this trust. As a consequence,\nattacks have been increasingly occurring through third-party dependencies.\nThese are called software supply chain attacks. In this paper, we target the\nproblem of projects that use dependencies while unaware of the potential risks\nposed by their software supply chain. We define the novel concept of software\nsupply chain smell and present Dirty-Waters, a novel tool for detecting\nsoftware supply chain smells. We evaluate Dirty-Waters on three JavaScript\nprojects across nine versions and demonstrate the prevalence of all proposed\nsoftware supply chain smells. Not only are there smells in all projects, but\nthere are many of them, which immediately reveal potential risks and provide\nclear indicators for developers to act on the security of their supply chain.\n","authors":["Raphina Liu","Sofia Bobadilla","Benoit Baudry","Martin Monperrus"],"pdf_url":"https://arxiv.org/pdf/2410.16049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16016v1","updated":"2024-10-21T13:49:35Z","published":"2024-10-21T13:49:35Z","title":"Proactive security defense: cyber threat intelligence modeling for\n  connected autonomous vehicles","summary":"  Cybersecurity has become a crucial concern in the field of connected\nautonomous vehicles. Cyber threat intelligence (CTI), as the collection of\ncyber threat information, offers an ideal way for responding to emerging cyber\nthreats and realizing proactive security defense. However, instant analysis and\nmodeling of vehicle cybersecurity data is a fundamental challenge since its\ncomplex and professional context. In this paper, we suggest an automotive CTI\nmodeling framework, Actim, to extract and analyse the interrelated\nrelationships among cyber threat elements. Specifically, we first design a\nvehicle security-safety conceptual ontology model to depict various threat\nentity classes and their relations. Then, we manually annotate the first\nautomobile CTI corpus by using real cybersecurity data, which comprises 908\nthreat intelligence texts, including 8195 entities and 4852 relationships. To\neffectively extract cyber threat entities and their relations, we propose an\nautomotive CTI mining model based on cross-sentence context. Experiment results\nshow that the proposed BERT-DocHiatt-BiLSTM-LSTM model exceeds the performance\nof existing methods. Finally, we define entity-relation matching rules and\ncreate a CTI knowledge graph that structurally fuses various elements of cyber\nthreats. The Actim framework enables mining the intrinsic connections among\nthreat entities, providing valuable insight on the evolving cyber threat\nlandscape.\n","authors":["Yinghui Wang","Yilong Ren","Zhiyong Cui","Haiyang Yu"],"pdf_url":"https://arxiv.org/pdf/2410.16016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15942v1","updated":"2024-10-21T12:15:03Z","published":"2024-10-21T12:15:03Z","title":"A Low-Cost Privacy-Preserving Digital Wallet for Humanitarian Aid\n  Distribution","summary":"  Humanitarian organizations distribute aid to people affected by armed\nconflicts or natural disasters. Digitalization has the potential to increase\nthe efficiency and fairness of aid-distribution systems, and recent work by\nWang et al. has shown that these benefits are possible without creating privacy\nharms for aid recipients. However, their work only provides a solution for one\nparticular aid-distribution scenario in which aid recipients receive a\npre-defined set of goods. Yet, in many situations it is desirable to enable\nrecipients to decide which items they need at each moment to satisfy their\nspecific needs. We formalize these needs into functional, deployment, security,\nand privacy requirements, and design a privacy-preserving digital wallet for\naid distribution. Our smart-card-based solution enables aid recipients to spend\na pre-defined budget at different vendors to obtain the items that they need.\nWe prove our solution's security and privacy properties, and show it is\npractical at scale.\n","authors":["Eva Luvison","Sylvain Chatel","Justinas Sukaitis","Vincent Graf Narbel","Carmela Troncoso","Wouter Lueks"],"pdf_url":"https://arxiv.org/pdf/2410.15942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06223v3","updated":"2024-10-21T12:11:52Z","published":"2023-09-12T13:42:20Z","title":"Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack\n  Surfaces in DNN Executables","summary":"  Bit-flip attacks (BFAs) can manipulate deep neural networks (DNNs). For\nhigh-level DNN models running on deep learning (DL) frameworks like PyTorch,\nextensive BFAs have been used to flip bits in model weights and shown\neffective. Defenses have also been proposed to guard model weights. However,\nDNNs are increasingly compiled into DNN executables by DL compilers to leverage\nhardware primitives. These executables manifest distinct computation paradigms;\nexisting research fails to accurately capture and expose the BFA surfaces on\nDNN executables.\n  To this end, we launch the first systematic study of BFAs on DNN executables.\nPrior BFAs are limited to attacking model weights and assume a strong whitebox\nattacker with full knowledge of victim model weights, which is unrealistic as\nweights are often confidential. In contrast, we find that BFAs on DNN\nexecutables can achieve high effectiveness by exploiting the model structure\n(usually stored in the executable code), which only requires knowing the (often\npublic) model structure. Importantly, such structure-based BFAs are pervasive,\ntransferable, and more severe in DNN executables. They also slip past existing\ndefenses.\n  To demonstrate the new attack surfaces, we assume a weak and more realistic\nattacker with no knowledge of victim model weights. We design an automated tool\nto identify vulnerable bits in victim executables with high confidence (70% vs.\nbaseline 2%). We show on DDR4 DRAM that only 1.4 flips on average are needed to\nfully downgrade the accuracy of victim models, including quantized ones which\ncould require 23x more flips previously, to random guesses. We comprehensively\nevaluate 16 DNN executables, covering large-scale models trained on\ncommonly-used datasets compiled by the two most popular DL compilers. Our\nfinding calls for incorporating security mechanisms in future DNN compilation\ntoolchains.\n","authors":["Yanzuo Chen","Zhibo Liu","Yuanyuan Yuan","Sihang Hu","Tianxiang Li","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2309.06223v3.pdf","comment":"Accepted by NDSS 2025"},{"id":"http://arxiv.org/abs/2410.15840v1","updated":"2024-10-21T10:03:03Z","published":"2024-10-21T10:03:03Z","title":"Private, Efficient and Scalable Kernel Learning for Medical Image\n  Analysis","summary":"  Medical imaging is key in modern medicine. From magnetic resonance imaging\n(MRI) to microscopic imaging for blood cell detection, diagnostic medical\nimaging reveals vital insights into patient health. To predict diseases or\nprovide individualized therapies, machine learning techniques like kernel\nmethods have been widely used. Nevertheless, there are multiple challenges for\nimplementing kernel methods. Medical image data often originates from various\nhospitals and cannot be combined due to privacy concerns, and the high\ndimensionality of image data presents another significant obstacle. While\nrandomised encoding offers a promising direction, existing methods often\nstruggle with a trade-off between accuracy and efficiency. Addressing the need\nfor efficient privacy-preserving methods on distributed image data, we\nintroduce OKRA (Orthonormal K-fRAmes), a novel randomized encoding-based\napproach for kernel-based machine learning. This technique, tailored for widely\nused kernel functions, significantly enhances scalability and speed compared to\ncurrent state-of-the-art solutions. Through experiments conducted on various\nclinical image datasets, we evaluated model quality, computational performance,\nand resource overhead. Additionally, our method outperforms comparable\napproaches\n","authors":["Anika Hannemann","Arjhun Swaminathan","Ali Burak Ünal","Mete Akgün"],"pdf_url":"https://arxiv.org/pdf/2410.15840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15758v1","updated":"2024-10-21T08:18:52Z","published":"2024-10-21T08:18:52Z","title":"Digital Product Passport Management with Decentralised Identifiers and\n  Verifiable Credentials","summary":"  Digital product passports (DPP) have been proposed in the European Ecodesign\nfor Sustainable Products Regulation (ESPR) as a means to keep and provide\nproduct information that facilitates product reusage, reparation, and\nrecycling. Thus, DPPs should provide a positive effect on the environmental\nimpact of future manufactured products, preventing waste and promoting a\ncircular economy (CE) model. ESPR settles a set of requirements in collecting\nand administering product-related data. Decentralised identifiers (DID) and\nverifiable credentials (VC) are two self-sovereign-identity-related elements\nthat may help in that DPP management since they introduce a decentralised\nadministration of identity that may enhance the overall scalability of the\nresulting system, improving also its reliability. This paper analyses the ESPR\nrequirements and describes how they may be achieved using DIDs and VCs,\nassessing their performance in some scenarios.\n","authors":["Ismael Illán García","Francesc D. Muñoz-Escoí","Jordi Arjona Aroca","F. Javier Fernández-Bravo Peñuela"],"pdf_url":"https://arxiv.org/pdf/2410.15758v1.pdf","comment":"22 pages, 8 images"},{"id":"http://arxiv.org/abs/2405.12751v2","updated":"2024-10-21T08:00:04Z","published":"2024-05-21T13:03:06Z","title":"Dullahan: Stealthy Backdoor Attack against Without-Label-Sharing Split\n  Learning","summary":"  As a novel privacy-preserving paradigm aimed at reducing client computational\ncosts and achieving data utility, split learning has garnered extensive\nattention and proliferated widespread applications across various fields,\nincluding smart health and smart transportation, among others. While recent\nstudies have primarily concentrated on addressing privacy leakage concerns in\nsplit learning, such as inference attacks and data reconstruction, the\nexploration of security issues (e.g., backdoor attacks) within the framework of\nsplit learning has been comparatively limited. Nonetheless, the security\nvulnerability within the context of split learning is highly posing a threat\nand can give rise to grave security implications, such as the illegal\nimpersonation in the face recognition model. Therefore, in this paper, we\npropose a stealthy backdoor attack strategy (namely SBAT) tailored to the\nwithout-label-sharing split learning architecture, which unveils the inherent\nsecurity vulnerability of split learning. We posit the existence of a potential\nattacker on the server side aiming to introduce a backdoor into the training\nmodel, while exploring two scenarios: one with known client network\narchitecture and the other with unknown architecture. Diverging from\ntraditional backdoor attack methods that manipulate the training data and\nlabels, we constructively conduct the backdoor attack by injecting the trigger\nembedding into the server network. Specifically, our SBAT achieves a higher\nlevel of attack stealthiness by refraining from modifying any intermediate\nparameters (e.g., gradients) during training and instead executing all\nmalicious operations post-training.\n","authors":["Yuwen Pu","Zhuoyuan Ding","Jiahao Chen","Chunyi Zhou","Qingming Li","Chunqiang Hu","Shouling Ji"],"pdf_url":"https://arxiv.org/pdf/2405.12751v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2407.15169v2","updated":"2024-10-21T07:57:56Z","published":"2024-07-21T13:58:43Z","title":"Back-in-Time Diffusion: Unsupervised Detection of Medical Deepfakes","summary":"  Recent progress in generative models has made it easier for a wide audience\nto edit and create image content, raising concerns about the proliferation of\ndeepfakes, especially in healthcare. Despite the availability of numerous\ntechniques for detecting manipulated images captured by conventional cameras,\ntheir applicability to medical images is limited. This limitation stems from\nthe distinctive forensic characteristics of medical images, a result of their\nimaging process.\n  In this work we propose a novel anomaly detector for medical imagery based on\ndiffusion models. Normally, diffusion models are used to generate images.\nHowever, we show how a similar process can be used to detect synthetic content\nby making a model reverse the diffusion on a suspected image. We evaluate our\nmethod on the task of detecting fake tumors injected and removed from CT and\nMRI scans. Our method significantly outperforms other state of the art\nunsupervised detectors with an increased AUC of 0.9 from 0.79 for injection and\nof 0.96 from 0.91 for removal on average. We also explore our hypothesis using\nAI explainability tools and publish our code and new medical deepfake datasets\nto encourage further research into this domain.\n","authors":["Fred Grabovski","Lior Yasur","Guy Amit","Yisroel Mirsky"],"pdf_url":"https://arxiv.org/pdf/2407.15169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12719v2","updated":"2024-10-21T07:52:01Z","published":"2024-05-21T12:20:19Z","title":"Mellivora Capensis: A Backdoor-Free Training Framework on the Poisoned\n  Dataset without Auxiliary Data","summary":"  The efficacy of deep learning models is profoundly influenced by the quality\nof their training data. Given the considerations of data diversity, data scale,\nand annotation expenses, model trainers frequently resort to sourcing and\nacquiring datasets from online repositories. Although economically pragmatic,\nthis strategy exposes the models to substantial security vulnerabilities.\nUntrusted entities can clandestinely embed triggers within the dataset,\nfacilitating the hijacking of the trained model on the poisoned dataset through\nbackdoor attacks, which constitutes a grave security concern. Despite the\nproliferation of countermeasure research, their inherent limitations constrain\ntheir effectiveness in practical applications. These include the requirement\nfor substantial quantities of clean samples, inconsistent defense performance\nacross varying attack scenarios, and inadequate resilience against adaptive\nattacks, among others. Therefore, in this paper, we endeavor to address the\nchallenges of backdoor attack countermeasures in real-world scenarios, thereby\nfortifying the security of training paradigm under the data-collection manner.\nConcretely, we first explore the inherent relationship between the potential\nperturbations and the backdoor trigger, and demonstrate the key observation\nthat the poisoned samples perform more robustness to perturbation than the\nclean ones through the theoretical analysis and experiments. Then, based on our\nkey explorations, we propose a robust and clean-data-free backdoor defense\nframework, namely Mellivora Capensis (\\texttt{MeCa}), which enables the model\ntrainer to train a clean model on the poisoned dataset.\n","authors":["Yuwen Pu","Jiahao Chen","Chunyi Zhou","Zhou Feng","Qingming Li","Chunqiang Hu","Shouling Ji"],"pdf_url":"https://arxiv.org/pdf/2405.12719v2.pdf","comment":"12 pages, under review"},{"id":"http://arxiv.org/abs/2410.15724v1","updated":"2024-10-21T07:43:20Z","published":"2024-10-21T07:43:20Z","title":"Efficient and Universally Accessible Cross-Chain Options without Upfront\n  Holder Collateral","summary":"  Options are fundamental to blockchain-based financial markets, offering\nessential tools for risk management and price speculation, which enhance\nliquidity, flexibility, and market efficiency in decentralized finance (DeFi).\nDespite the growing interest in options for blockchain-resident assets, such as\ncryptocurrencies, current option mechanisms face significant challenges,\nincluding limited asset support, high trading delays, and the requirement for\noption holders to provide upfront collateral.\n  In this paper, we present a protocol that addresses the aforementioned issues\nby facilitating efficient and universally accessible option trading without\nrequiring holders to post collateral when establishing options. Our protocol's\nuniversality allows for cross-chain options involving nearly $\\textit{any}$\nassets on $\\textit{any}$ two different blockchains, provided the chains'\nprogramming languages can enforce and execute the necessary contract logic. A\nkey innovation in our approach is the use of Double-Authentication-Preventing\nSignatures (DAPS), which significantly reduces trading latency. Additionally,\nby introducing a guarantee from the option writer, our protocol removes the\nneed of upfront collateral from holders. Our evaluation demonstrates that the\nproposed scheme reduces option transfer latency to less than half of that in\nexisting methods. Rigorous security analysis proves that our protocol achieves\nsecure option trading, even when facing adversarial behaviors.\n","authors":["Zifan Peng","Yingjie Xue","Jingyu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15724v1.pdf","comment":"19 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.08464v3","updated":"2024-10-21T06:05:37Z","published":"2024-08-16T00:18:23Z","title":"$\\textit{MMJ-Bench}$: A Comprehensive Study on Jailbreak Attacks and\n  Defenses for Vision Language Models","summary":"  As deep learning advances, Large Language Models (LLMs) and their multimodal\ncounterparts, Multimodal Language Models (MLLMs), have shown exceptional\nperformance in many real-world tasks. However, MLLMs face significant security\nchallenges, such as jailbreak attacks, where attackers attempt to bypass the\nmodel's safety alignment to elicit harmful responses. The threat of jailbreak\nattacks on MLLMs arises from both the inherent vulnerabilities of LLMs and the\nmultiple information channels that MLLMs process. While various attacks and\ndefenses have been proposed, there is a notable gap in unified and\ncomprehensive evaluations, as each method is evaluated on different dataset and\nmetrics, making it impossible to compare the effectiveness of each method. To\naddress this gap, we introduce \\textit{MMJ-Bench}, a unified pipeline for\nevaluating jailbreak attacks and defense techniques for MLLMs. Through\nextensive experiments, we assess the effectiveness of various attack methods\nagainst SoTA MLLMs and evaluate the impact of defense mechanisms on both\ndefense effectiveness and model utility for normal tasks. Our comprehensive\nevaluation contribute to the field by offering a unified and systematic\nevaluation framework and the first public-available benchmark for MLLM\njailbreak research. We also demonstrate several insightful findings that\nhighlights directions for future studies.\n","authors":["Fenghua Weng","Yue Xu","Chengyan Fu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08464v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15631v1","updated":"2024-10-21T04:27:41Z","published":"2024-10-21T04:27:41Z","title":"Security of Language Models for Code: A Systematic Literature Review","summary":"  Language models for code (CodeLMs) have emerged as powerful tools for\ncode-related tasks, outperforming traditional methods and standard machine\nlearning approaches. However, these models are susceptible to security\nvulnerabilities, drawing increasing research attention from domains such as\nsoftware engineering, artificial intelligence, and cybersecurity. Despite the\ngrowing body of research focused on the security of CodeLMs, a comprehensive\nsurvey in this area remains absent. To address this gap, we systematically\nreview 67 relevant papers, organizing them based on attack and defense\nstrategies. Furthermore, we provide an overview of commonly used language\nmodels, datasets, and evaluation metrics, and highlight open-source tools and\npromising directions for future research in securing CodeLMs.\n","authors":["Yuchen Chen","Weisong Sun","Chunrong Fang","Zhenpeng Chen","Yifei Ge","Tingxu Han","Quanjun Zhang","Yang Liu","Zhenyu Chen","Baowen Xu"],"pdf_url":"https://arxiv.org/pdf/2410.15631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15597v1","updated":"2024-10-21T02:44:58Z","published":"2024-10-21T02:44:58Z","title":"A Comprehensive Comparative Study of Individual ML Models and Ensemble\n  Strategies for Network Intrusion Detection Systems","summary":"  The escalating frequency of intrusions in networked systems has spurred the\nexploration of new research avenues in devising artificial intelligence (AI)\ntechniques for intrusion detection systems (IDS). Various AI techniques have\nbeen used to automate network intrusion detection tasks, yet each model\npossesses distinct strengths and weaknesses. Selecting the optimal model for a\ngiven dataset can pose a challenge, necessitating the exploration of ensemble\nmethods to enhance generalization and applicability in network intrusion\ndetection. This paper addresses this gap by conducting a comprehensive\nevaluation of diverse individual models and both simple and advanced ensemble\nmethods for network IDS. We introduce an ensemble learning framework tailored\nfor assessing individual models and ensemble methods in network intrusion\ndetection tasks. Our framework encompasses the loading of input datasets,\ntraining of individual models and ensemble methods, and the generation of\nevaluation metrics. Furthermore, we incorporate all features across individual\nmodels and ensemble techniques. The study presents results for our framework,\nencompassing 14 methods, including various bagging, stacking, blending, and\nboosting techniques applied to multiple base learners such as decision trees,\nneural networks, and among others. We evaluate the framework using two distinct\nnetwork intrusion datasets, RoEduNet-SIMARGL2021 and CICIDS-2017, each\npossessing unique characteristics. Additionally, we categorize AI models based\non their performances on our evaluation metrics and via their confusion\nmatrices. Our assessment demonstrates the efficacy of learning across most\nsetups explored in this study. Furthermore, we contribute to the community by\nreleasing our source codes, providing a foundational ensemble learning\nframework for network intrusion detection.\n","authors":["Ismail Bibers","Osvaldo Arreche","Mustafa Abdallah"],"pdf_url":"https://arxiv.org/pdf/2410.15597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15568v1","updated":"2024-10-21T01:23:37Z","published":"2024-10-21T01:23:37Z","title":"ZK-DPPS: A Zero-Knowledge Decentralised Data Sharing and Processing\n  Middleware","summary":"  In the current digital landscape, supply chains have transformed into complex\nnetworks driven by the Internet of Things (IoT), necessitating enhanced data\nsharing and processing capabilities to ensure traceability and transparency.\nLeveraging Blockchain technology in IoT applications advances reliability and\ntransparency in near-real-time insight extraction processes. However, it raises\nsignificant concerns regarding data privacy. Existing privacy-preserving\napproaches often rely on Smart Contracts for automation and Zero Knowledge\nProofs (ZKP) for privacy. However, apart from being inflexible in adopting\nsystem changes while effectively protecting data confidentiality, these\napproaches introduce significant computational expenses and overheads that make\nthem impractical for dynamic supply chain environments. To address these\nchallenges, we propose ZK-DPPS, a framework that ensures zero-knowledge\ncommunications without the need for traditional ZKPs. In ZK-DPPS, privacy is\npreserved through a combination of Fully Homomorphic Encryption (FHE) for\ncomputations and Secure Multi-Party Computations (SMPC) for key reconstruction.\nTo ensure that the raw data remains private throughout the entire process, we\nuse FHE to execute computations directly on encrypted data. The\n\"zero-knowledge\" aspect of ZK-DPPS refers to the system's ability to process\nand share data insights without exposing sensitive information, thus offering a\npractical and efficient alternative to ZKP-based methods. We demonstrate the\nefficacy of ZK-DPPS through a simulated supply chain scenario, showcasing its\nability to tackle the dual challenges of privacy preservation and computational\ntrust in decentralised environments.\n","authors":["Amir Jabbari","Gowri Ramachandran","Sidra Malik","Raja Jurdak"],"pdf_url":"https://arxiv.org/pdf/2410.15568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15565v1","updated":"2024-10-21T01:22:59Z","published":"2024-10-21T01:22:59Z","title":"Does quantum lattice sieving require quantum RAM?","summary":"  In this paper, we study the requirement for quantum random access memory\n(QRAM) in quantum lattice sieving, a fundamental algorithm for lattice-based\ncryptanalysis.\n  First, we obtain a lower bound on the cost of quantum lattice sieving with a\nbounded size QRAM. We do so in a new query model encompassing a wide range of\nlattice sieving algorithms similar to those in the classical sieving lower\nbound by Kirshanova and Laarhoven [CRYPTO 21]. This implies that, under\nreasonable assumptions, quantum speedups in lattice sieving require the use of\nQRAM. In particular, no quantum speedup is possible without QRAM.\n  Second, we investigate the trade-off between the size of QRAM and the quantum\nspeedup. We obtain a new interpolation between classical and quantum lattice\nsieving. Moreover, we show that further improvements require a novel way to use\nthe QRAM by proving the optimality of some subroutines. An important caveat is\nthat this trade-off requires a strong assumption on the efficient replacement\nof QRAM data, indicating that even speedups with a small QRAM are already\nchallenging.\n  Finally, we provide a circuit for quantum lattice sieving without using QRAM.\nOur circuit has a better depth complexity than the best classical algorithms\nbut requires an exponential amount of qubits. To the best of our knowledge,\nthis is the first quantum speedup for lattice sieving without QRAM in the\nstandard quantum circuit model. We explain why this circuit does not contradict\nour lower bound, which considers the query complexity.\n","authors":["Beomgeun Cho","Minki Hhan","Taehyun Kim","Jeonghoon Lee","Yixin Shen"],"pdf_url":"https://arxiv.org/pdf/2410.15565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15184v2","updated":"2024-10-21T01:09:29Z","published":"2024-05-24T03:37:32Z","title":"TrojanForge: Generating Adversarial Hardware Trojan Examples with\n  Reinforcement Learning","summary":"  The Hardware Trojan (HT) problem can be thought of as a continuous game\nbetween attackers and defenders, each striving to outsmart the other by\nleveraging any available means for an advantage. Machine Learning (ML) has\nrecently played a key role in advancing HT research. Various novel techniques,\nsuch as Reinforcement Learning (RL) and Graph Neural Networks (GNNs), have\nshown HT insertion and detection capabilities. HT insertion with ML techniques,\nspecifically, has seen a spike in research activity due to the shortcomings of\nconventional HT benchmarks and the inherent human design bias that occurs when\nwe create them. This work continues this innovation by presenting a tool called\nTrojanForge, capable of generating HT adversarial examples that defeat HT\ndetectors; demonstrating the capabilities of GAN-like adversarial tools for\nautomatic HT insertion. We introduce an RL environment where the RL insertion\nagent interacts with HT detectors in an insertion-detection loop where the\nagent collects rewards based on its success in bypassing HT detectors. Our\nresults show that this process helps inserted HTs evade various HT detectors,\nachieving high attack success percentages. This tool provides insight into why\nHT insertion fails in some instances and how we can leverage this knowledge in\ndefense.\n","authors":["Amin Sarihi","Peter Jamieson","Ahmad Patooghy","Abdel-Hameed A. Badawy"],"pdf_url":"https://arxiv.org/pdf/2405.15184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15550v1","updated":"2024-10-21T00:45:20Z","published":"2024-10-21T00:45:20Z","title":"Hiding in Plain Sight: Reframing Hardware Trojan Benchmarking as a\n  Hide&Seek Modification","summary":"  This work focuses on advancing security research in the hardware design space\nby formally defining the realistic problem of Hardware Trojan (HT) detection.\nThe goal is to model HT detection more closely to the real world, i.e.,\ndescribing the problem as The Seeker's Dilemma where a detecting agent is\nunaware of whether circuits are infected by HTs or not. Using this theoretical\nproblem formulation, we create a benchmark that consists of a mixture of\nHT-free and HT-infected restructured circuits while preserving their original\nfunctionalities. The restructured circuits are randomly infected by HTs,\ncausing a situation where the defender is uncertain if a circuit is infected or\nnot. We believe that our innovative benchmark and methodology of creating\nbenchmarks will help the community judge the detection quality of different\nmethods by comparing their success rates in circuit classification. We use our\ndeveloped benchmark to evaluate three state-of-the-art HT detection tools to\nshow baseline results for this approach. We use Principal Component Analysis to\nassess the strength of our benchmark, where we observe that some restructured\nHT-infected circuits are mapped closely to HT-free circuits, leading to\nsignificant label misclassification by detectors.\n","authors":["Amin Sarihi","Ahmad Patooghy","Peter Jamieson","Abdel-Hameed A. Badawy"],"pdf_url":"https://arxiv.org/pdf/2410.15550v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.17918"},{"id":"http://arxiv.org/abs/2402.06674v3","updated":"2024-10-21T17:19:12Z","published":"2024-02-07T14:23:01Z","title":"Impact of Dataset Properties on Membership Inference Vulnerability of\n  Deep Transfer Learning","summary":"  We analyse the relationship between privacy vulnerability and dataset\nproperties, such as examples per class and number of classes, when applying two\nstate-of-the-art membership inference attacks (MIAs) to fine-tuned neural\nnetworks. We derive per-example MIA vulnerability in terms of score\ndistributions and statistics computed from shadow models. We introduce a\nsimplified model of membership inference and prove that in this model, the\nlogarithm of the difference of true and false positive rates depends linearly\non the logarithm of the number of examples per class. We complement the\ntheoretical analysis with empirical analysis by systematically testing the\npractical privacy vulnerability of fine-tuning large image classification\nmodels and obtain the previously derived power law dependence between the\nnumber of examples per class in the data and the MIA vulnerability, as measured\nby true positive rate of the attack at a low false positive rate. Finally, we\nfit a parametric model of the previously derived form to predict true positive\nrate based on dataset properties and observe good fit for MIA vulnerability on\nunseen fine-tuning scenarios.\n","authors":["Marlon Tobaben","Hibiki Ito","Joonas Jälkö","Gauri Pradhan","Yuan He","Antti Honkela"],"pdf_url":"https://arxiv.org/pdf/2402.06674v3.pdf","comment":"39 pages, 12 figures"},{"id":"http://arxiv.org/abs/2408.10200v3","updated":"2024-10-21T22:34:41Z","published":"2024-08-19T17:57:01Z","title":"SoK: Runtime Integrity","summary":"  This paper provides a systematic exploration of Control Flow Integrity (CFI)\nand Control Flow Attestation (CFA) mechanisms, examining their differences and\nrelationships. It addresses crucial questions about the goals, assumptions,\nfeatures, and design spaces of CFI and CFA, including their potential\ncoexistence on the same platform. Through a comprehensive review of existing\ndefenses, this paper positions CFI and CFA within the broader landscape of\nruntime defenses, critically evaluating their strengths, limitations, and\ntrade-offs. The findings emphasize the importance of further research to bridge\nthe gaps in CFI and CFA and thus advance the field of runtime defenses.\n","authors":["Mahmoud Ammar","Adam Caulfield","Ivan De Oliveira Nunes"],"pdf_url":"https://arxiv.org/pdf/2408.10200v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16527v1","updated":"2024-10-21T21:36:03Z","published":"2024-10-21T21:36:03Z","title":"Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A\n  Comparative Analysis","summary":"  This report presents a comparative analysis of open-source vulnerability\nscanners for conversational large language models (LLMs). As LLMs become\nintegral to various applications, they also present potential attack surfaces,\nexposed to security risks such as information leakage and jailbreak attacks.\nOur study evaluates prominent scanners - Garak, Giskard, PyRIT, and\nCyberSecEval - that adapt red-teaming practices to expose these\nvulnerabilities. We detail the distinctive features and practical use of these\nscanners, outline unifying principles of their design and perform quantitative\nevaluations to compare them. These evaluations uncover significant reliability\nissues in detecting successful attacks, highlighting a fundamental gap for\nfuture development. Additionally, we contribute a preliminary labelled dataset,\nwhich serves as an initial step to bridge this gap. Based on the above, we\nprovide strategic recommendations to assist organizations choose the most\nsuitable scanner for their red-teaming needs, accounting for customizability,\ntest suite comprehensiveness, and industry-specific use cases.\n","authors":["Jonathan Brokman","Omer Hofman","Oren Rachmil","Inderjeet Singh","Rathina Sabapathy","Aishvariya Priya","Vikas Pahuja","Amit Giloni","Roman Vainshtein","Hisashi Kojima"],"pdf_url":"https://arxiv.org/pdf/2410.16527v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.16497v1","updated":"2024-10-21T20:40:02Z","published":"2024-10-21T20:40:02Z","title":"Hacking the Fabric: Targeting Partial Reconfiguration for Fault\n  Injection in FPGA Fabrics","summary":"  FPGAs are now ubiquitous in cloud computing infrastructures and\nreconfigurable system-on-chip, particularly for AI acceleration. Major cloud\nservice providers such as Amazon and Microsoft are increasingly incorporating\nFPGAs for specialized compute-intensive tasks within their data centers. The\navailability of FPGAs in cloud data centers has opened up new opportunities for\nusers to improve application performance by implementing customizable hardware\naccelerators directly on the FPGA fabric. However, the virtualization and\nsharing of FPGA resources among multiple users open up new security risks and\nthreats. We present a novel fault attack methodology capable of causing\npersistent fault injections in partial bitstreams during the process of FPGA\nreconfiguration. This attack leverages power-wasters and is timed to inject\nfaults into bitstreams as they are being loaded onto the FPGA through the\nreconfiguration manager, without needing to remain active throughout the entire\nreconfiguration process. Our experiments, conducted on a Pynq FPGA setup,\ndemonstrate the feasibility of this attack on various partial application\nbitstreams, such as a neural network accelerator unit and a signal processing\naccelerator unit.\n","authors":["Jayeeta Chaudhuri","Hassan Nassar","Dennis R. E. Gnad","Jorg Henkel","Mehdi B. Tahoori","Krishnendu Chakrabarty"],"pdf_url":"https://arxiv.org/pdf/2410.16497v1.pdf","comment":"Accepted for presentation in the 2024 IEEE Asian Test Symposium (ATS)"},{"id":"http://arxiv.org/abs/2410.16459v1","updated":"2024-10-21T19:37:35Z","published":"2024-10-21T19:37:35Z","title":"Rényi divergence-based uniformity guarantees for $k$-universal hash\n  functions","summary":"  Universal hash functions map the output of a source to random strings over a\nfinite alphabet, aiming to approximate the uniform distribution on the set of\nstrings. A classic result on these functions, called the Leftover Hash Lemma,\ngives an estimate of the distance from uniformity based on the assumptions\nabout the min-entropy of the source. We prove several results concerning\nextensions of this lemma to a class of functions that are $k^\\ast$-universal,\ni.e., $l$-universal for all $2\\le l\\le k$. As a common distinctive feature, our\nresults provide estimates of closeness to uniformity in terms of the\n$\\alpha$-R\\'enyi divergence for all $\\alpha\\in (1,\\infty]$. For $1\\le \\alpha\\le\nk$ we show that it is possible to convert all the randomness of the source\nmeasured in $\\alpha$-R\\'enyi entropy into approximately uniform bits with\nnearly the same amount of randomness. For large enough $k$ we show that it is\npossible to distill random bits that are nearly uniform, as measured by\nmin-entropy. We also extend these results to hashing with side information.\n","authors":["Madhura Pathegama","Alexander Barg"],"pdf_url":"https://arxiv.org/pdf/2410.16459v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2410.11708v2","updated":"2024-10-21T19:33:10Z","published":"2024-10-15T15:45:58Z","title":"The Age of DDoScovery: An Empirical Comparison of Industry and Academic\n  DDoS Assessments","summary":"  Motivated by the impressive but diffuse scope of DDoS research and reporting,\nwe undertake a multistakeholder (joint industry-academic) analysis to seek\nconvergence across the best available macroscopic views of the relative trends\nin two dominant classes of attacks - direct-path attacks and\nreflection-amplification attacks. We first analyze 24 industry reports to\nextract trends and (in)consistencies across observations by commercial\nstakeholders in 2022. We then analyze ten data sets spanning industry and\nacademic sources, across four years (2019-2023), to find and explain\ndiscrepancies based on data sources, vantage points, methods, and parameters.\nOur method includes a new approach: we share an aggregated list of DDoS targets\nwith industry players who return the results of joining this list with their\nproprietary data sources to reveal gaps in visibility of the academic data\nsources. We use academic data sources to explore an industry-reported relative\ndrop in spoofed reflection-amplification attacks in 2021-2022. Our study\nillustrates the value, but also the challenge, in independent validation of\nsecurity-related properties of Internet infrastructure. Finally, we reflect on\nopportunities to facilitate greater common understanding of the DDoS landscape.\nWe hope our results inform not only future academic and industry pursuits but\nalso emerging policy efforts to reduce systemic Internet security\nvulnerabilities.\n","authors":["Raphael Hiesgen","Marcin Nawrocki","Marinho Barcellos","Daniel Kopp","Oliver Hohlfeld","Echo Chan","Roland Dobbins","Christian Doerr","Christian Rossow","Daniel R. Thomas","Mattijs Jonker","Ricky Mok","Xiapu Luo","John Kristoff","Thomas C. Schmidt","Matthias Wählisch","kc claffy"],"pdf_url":"https://arxiv.org/pdf/2410.11708v2.pdf","comment":"camera-ready"},{"id":"http://arxiv.org/abs/2410.16442v1","updated":"2024-10-21T19:10:53Z","published":"2024-10-21T19:10:53Z","title":"Secure Computation and Trustless Data Intermediaries in Data Spaces","summary":"  This paper explores the integration of advanced cryptographic techniques for\nsecure computation in data spaces to enable secure and trusted data sharing,\nwhich is essential for the evolving data economy. In addition, the paper\nexamines the role of data intermediaries, as outlined in the EU Data Governance\nAct, in data spaces and specifically introduces the idea of trustless\nintermediaries that do not have access to their users' data. Therefore, we\nexploit the introduced secure computation methods, i.e. Secure Multi-Party\nComputation (MPC) and Fully Homomorphic Encryption (FHE), and discuss the\nsecurity benefits. Overall, we identify and address key challenges for\nintegration, focusing on areas such as identity management, policy enforcement,\nnode selection, and access control, and present solutions through real-world\nuse cases, including air traffic management, manufacturing, and secondary data\nuse. Furthermore, through the analysis of practical applications, this work\nproposes a comprehensive framework for the implementation and standardization\nof secure computing technologies in dynamic, trustless data environments,\npaving the way for future research and development of a secure and\ninteroperable data ecosystem.\n","authors":["Christoph Fabianek","Stephan Krenn","Thomas Loruenser","Veronika Siska"],"pdf_url":"https://arxiv.org/pdf/2410.16442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16423v1","updated":"2024-10-21T18:46:05Z","published":"2024-10-21T18:46:05Z","title":"Position: Challenges and Opportunities for Differential Privacy in the\n  U.S. Federal Government","summary":"  In this article, we seek to elucidate challenges and opportunities for\ndifferential privacy within the federal government setting, as seen by a team\nof differential privacy researchers, privacy lawyers, and data scientists\nworking closely with the U.S. government. After introducing differential\nprivacy, we highlight three significant challenges which currently restrict the\nuse of differential privacy in the U.S. government. We then provide two\nexamples where differential privacy can enhance the capabilities of government\nagencies. The first example highlights how the quantitative nature of\ndifferential privacy allows policy security officers to release multiple\nversions of analyses with different levels of privacy. The second example,\nwhich we believe is a novel realization, indicates that differential privacy\ncan be used to improve staffing efficiency in classified applications. We hope\nthat this article can serve as a nontechnical resource which can help frame\nfuture action from the differential privacy community, privacy regulators,\nsecurity officers, and lawmakers.\n","authors":["Amol Khanna","Adam McCormick","Andre Nguyen","Chris Aguirre","Edward Raff"],"pdf_url":"https://arxiv.org/pdf/2410.16423v1.pdf","comment":"2nd Workshop on Regulatable ML at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2306.00934v3","updated":"2024-10-21T18:24:01Z","published":"2023-06-01T17:36:24Z","title":"Explaining Provenance-Based GNN Detectors with Graph Structural Features","summary":"  The opaqueness of ML-based security models hinders their broad adoption and\nconsequently restricts transparent security operations due to their limited\nverifiability and explainability. To enhance the explainability of ML-based\nsecurity models, we introduce PROVEXPLAINER, a framework offering\nsecurity-aware explanations by translating an ML model's decision boundary onto\nthe interpretable feature space of a surrogate DT. Our PROVEXPLAINER framework\nprimarily focuses on explaining security models that are built using GNNs since\nrecent studies employ GNNs to comprehensively digest system provenance graphs\nfor security critical tasks. PROVEXPLAINER uses graph structural features based\non security domain knowledge gained from extensive data analysis, utilizing\npublic and private system provenance datasets.\n  PROVEXPLAINER's interpretable feature space can be directly mapped to the\nsystem provenance problem space, making the explanations human understandable.\nBecause the security landscape is constantly changing, PROVEXPLAINER can be\neasily extended with new explanatory features as they are identified in the\nwild. By considering prominent GNN architectures (e.g., GAT and GraphSAGE) for\nprogram classification and anomaly detection tasks, we show how PROVEXPLAINER\nsynergizes with current SOTA GNN explainers to deliver domain-specific\nexplanations. On malware and APT datasets, PROVEXPLAINER achieves up to 9.14%\nand 6.97% higher precision and recall, respectively, compared to SOTA GNN\nexplainers. When combined with a general-purpose SOTA GNN explainer,\nPROVEXPLAINER shows a further improvement of 7.22% and 4.86% precision and\nrecall over the best individual explainer.\n","authors":["Kunal Mukherjee","Joshua Wiedemeier","Tianhao Wang","Muhyun Kim","Feng Chen","Murat Kantarcioglu","Kangkook Jee"],"pdf_url":"https://arxiv.org/pdf/2306.00934v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13886v2","updated":"2024-10-21T18:06:58Z","published":"2024-10-11T06:54:12Z","title":"Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents","summary":"  For safety reasons, large language models (LLMs) are trained to refuse\nharmful user instructions, such as assisting dangerous activities. We study an\nopen question in this work: does the desired safety refusal, typically enforced\nin chat contexts, generalize to non-chat and agentic use cases? Unlike\nchatbots, LLM agents equipped with general-purpose tools, such as web browsers\nand mobile devices, can directly influence the real world, making it even more\ncrucial to refuse harmful instructions. In this work, we primarily focus on\nred-teaming browser agents, LLMs that manipulate information via web browsers.\nTo this end, we introduce Browser Agent Red teaming Toolkit (BrowserART), a\ncomprehensive test suite designed specifically for red-teaming browser agents.\nBrowserART is consist of 100 diverse browser-related harmful behaviors\n(including original behaviors and ones sourced from HarmBench [Mazeika et al.,\n2024] and AirBench 2024 [Zeng et al., 2024b]) across both synthetic and real\nwebsites. Our empirical study on state-of-the-art browser agents reveals that,\nwhile the backbone LLM refuses harmful instructions as a chatbot, the\ncorresponding agent does not. Moreover, attack methods designed to jailbreak\nrefusal-trained LLMs in the chat settings transfer effectively to browser\nagents. With human rewrites, GPT-4o and o1-preview-based browser agents\nattempted 98 and 63 harmful behaviors (out of 100), respectively. We publicly\nrelease BrowserART and call on LLM developers, policymakers, and agent\ndevelopers to collaborate on improving agent safety\n","authors":["Priyanshu Kumar","Elaine Lau","Saranya Vijayakumar","Tu Trinh","Scale Red Team","Elaine Chang","Vaughn Robinson","Sean Hendryx","Shuyan Zhou","Matt Fredrikson","Summer Yue","Zifan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16355v1","updated":"2024-10-21T18:00:00Z","published":"2024-10-21T18:00:00Z","title":"Quantum inspired factorization up to 100-bit RSA number in polynomial\n  time","summary":"  Classical public-key cryptography standards rely on the Rivest-Shamir-Adleman\n(RSA) encryption protocol. The security of this protocol is based on the\nexponential computational complexity of the most efficient classical algorithms\nfor factoring large semiprime numbers into their two prime components. Here, we\nattack RSA factorization building on Schnorr's mathematical framework where\nfactorization translates into a combinatorial optimization problem. We solve\nthe optimization task via tensor network methods, a quantum-inspired classical\nnumerical technique. This tensor network Schnorr's sieving algorithm displays\nnumerical evidence of a polynomial scaling of the resources with the bit-length\nof the semiprime. We factorize RSA numbers up to 100 bits encoding the\noptimization problem in quantum systems with up to 256 qubits. Only the\nhigh-order polynomial scaling of the required resources limits the\nfactorization of larger numbers. Although these results do not currently\nundermine the security of the present communication infrastructure, they\nstrongly highlight the urgency of implementing post-quantum cryptography or\nquantum key distribution.\n","authors":["Marco Tesoro","Ilaria Siloi","Daniel Jaschke","Giuseppe Magnifico","Simone Montangero"],"pdf_url":"https://arxiv.org/pdf/2410.16355v1.pdf","comment":"6 + 9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.03982v2","updated":"2024-10-21T17:57:34Z","published":"2024-10-04T23:49:38Z","title":"Certified Randomness implies Secure Classical Position-Verification","summary":"  Liu et al. (ITCS22) initiated the study of designing a secure position\nverification protocol based on a specific proof of quantumness protocol and\nclassical communication. In this paper, we study this interesting topic further\nand answer some of the open questions that are left in that paper. We provide a\nnew generic compiler that can convert any single round proof of\nquantumness-based certified randomness protocol to a secure classical\ncommunication-based position verification scheme. Later, we extend our compiler\nto different kinds of multi-round proof of quantumness-based certified\nrandomness protocols. Moreover, we instantiate our compiler with a random\ncircuit sampling (RCS)-based certified randomness protocol proposed by Aaronson\nand Hung (STOC 23). RCS-based techniques are within reach of today's NISQ\ndevices; therefore, our design overcomes the limitation of the Liu et al.\nprotocol that would require a fault-tolerant quantum computer to realize.\nMoreover, this is one of the first cryptographic applications of RCS-based\ntechniques other than certified randomness.\n","authors":["Omar Amer","Kaushik Chakraborty","David Cui","Fatih Kaleoglu","Charles Lim","Minzhao Liu","Marco Pistoia"],"pdf_url":"https://arxiv.org/pdf/2410.03982v2.pdf","comment":"v2: minor changes to related work and addition of acknowledgements.\n  54 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.16341v1","updated":"2024-10-21T10:14:44Z","published":"2024-10-21T10:14:44Z","title":"Vulnerabilities in Machine Learning-Based Voice Disorder Detection\n  Systems","summary":"  The impact of voice disorders is becoming more widely acknowledged as a\npublic health issue. Several machine learning-based classifiers with the\npotential to identify disorders have been used in recent studies to\ndifferentiate between normal and pathological voices and sounds. In this paper,\nwe focus on analyzing the vulnerabilities of these systems by exploring the\npossibility of attacks that can reverse classification and compromise their\nreliability. Given the critical nature of personal health information,\nunderstanding which types of attacks are effective is a necessary first step\ntoward improving the security of such systems. Starting from the original\naudios, we implement various attack methods, including adversarial, evasion,\nand pitching techniques, and evaluate how state-of-the-art disorder detection\nmodels respond to them. Our findings identify the most effective attack\nstrategies, underscoring the need to address these vulnerabilities in\nmachine-learning systems used in the healthcare domain.\n","authors":["Gianpaolo Perelli","Andrea Panzino","Roberto Casula","Marco Micheletto","Giulia Orrù","Gian Luca Marcialis"],"pdf_url":"https://arxiv.org/pdf/2410.16341v1.pdf","comment":"7 pages, 17 figures, accepted for 16th IEEE INTERNATIONAL WORKSHOP ON\n  INFORMATION FORENSICS AND SECURITY (WIFS) 2024"}],"Networking and Internet Architecture":[{"id":"http://arxiv.org/abs/2410.14228v2","updated":"2024-10-21T04:10:26Z","published":"2024-10-18T07:19:28Z","title":"Towards High-Speed Passive Visible Light Communication with Event\n  Cameras and Digital Micro-Mirrors","summary":"  Passive visible light communication (VLC) modulates light propagation or\nreflection to transmit data without directly modulating the light source. Thus,\npassive VLC provides an alternative to conventional VLC, enabling communication\nwhere the light source cannot be directly controlled. There have been ongoing\nefforts to explore new methods and devices for modulating light propagation or\nreflection. The state-of-the-art has broken the 100 kbps data rate barrier for\npassive VLC by using a digital micro-mirror device (DMD) as the light\nmodulating platform, or transmitter, and a photo-diode as the receiver. We\nsignificantly extend this work by proposing a massive spatial data channel\nframework for DMDs, where individual channels can be decoded in parallel using\nan event camera at the receiver. For the event camera, we introduce event\nprocessing algorithms to detect numerous channels and decode bits from\nindividual channels with high reliability. Our prototype, built with\noff-the-shelf event cameras and DMDs, can decode up to $\\sim$2,000 parallel\nchannels, achieving a data transmission rate of 1.6 Mbps, markedly surpassing\ncurrent benchmarks by 16x.\n","authors":["Yanxiang Wang","Yiran Shen","Kenuo Xu","Guangrong Zhao","Mahbub Hassan","Chenren Xu","Wen Hu"],"pdf_url":"https://arxiv.org/pdf/2410.14228v2.pdf","comment":"14 pages, 21 figures, nonacm"},{"id":"http://arxiv.org/abs/2404.04162v5","updated":"2024-10-21T15:26:38Z","published":"2024-04-05T15:13:09Z","title":"Wireless Resource Optimization in Hybrid Semantic/Bit Communication\n  Networks","summary":"  Recently, semantic communication (SemCom) has shown great potential in\nsignificant resource savings and efficient information exchanges, thus\nnaturally introducing a novel and practical cellular network paradigm where two\nmodes of SemCom and conventional bit communication (BitCom) coexist.\nNevertheless, the involved wireless resource management becomes rather\ncomplicated and challenging, given the unique background knowledge matching and\ntime-consuming semantic coding requirements in SemCom. To this end, this paper\njointly investigates user association (UA), mode selection (MS), and bandwidth\nallocation (BA) problems in a hybrid semantic/bit communication network\n(HSB-Net). Concretely, we first identify a unified performance metric of\nmessage throughput for both SemCom and BitCom links. Next, we specially develop\na knowledge matching-aware two-stage tandem packet queuing model and\ntheoretically derive the average packet loss ratio and queuing latency.\nCombined with practical constraints, we then formulate a joint optimization\nproblem for UA, MS, and BA to maximize the overall message throughput of\nHSB-Net. Afterward, we propose an optimal resource management strategy by\nutilizing a Lagrange primal-dual transformation method and a preference\nlist-based heuristic algorithm with polynomial-time complexity. Numerical\nresults not only demonstrate the accuracy of our analytical queuing model, but\nalso validate the performance superiority of our proposed strategy compared\nwith different benchmarks.\n","authors":["Le Xia","Yao Sun","Dusit Niyato","Lan Zhang","Muhammad Ali Imran"],"pdf_url":"https://arxiv.org/pdf/2404.04162v5.pdf","comment":"This paper has been accepted for publication by the IEEE Transactions\n  on Communications"},{"id":"http://arxiv.org/abs/2410.16227v1","updated":"2024-10-21T17:32:36Z","published":"2024-10-21T17:32:36Z","title":"Managing Bandwidth: The Key to Cloud-Assisted Autonomous Driving","summary":"  Prevailing wisdom asserts that one cannot rely on the cloud for critical\nreal-time control systems like self-driving cars. We argue that we can, and\nmust. Following the trends of increasing model sizes, improvements in hardware,\nand evolving mobile networks, we identify an opportunity to offload parts of\ntime-sensitive and latency-critical compute to the cloud. Doing so requires\ncarefully allocating bandwidth to meet strict latency SLOs, while maximizing\nbenefit to the car.\n","authors":["Alexander Krentsel","Peter Schafhalter","Joseph E. Gonzalez","Sylvia Ratnasamy","Scott Shenker","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.16227v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2406.15819v2","updated":"2024-10-21T15:07:13Z","published":"2024-06-22T11:17:50Z","title":"Automatic AI Model Selection for Wireless Systems: Online Learning via\n  Digital Twinning","summary":"  In modern wireless network architectures, such as O-RAN, artificial\nintelligence (AI)-based applications are deployed at intelligent controllers to\ncarry out functionalities like scheduling or power control. The AI \"apps\" are\nselected on the basis of contextual information such as network conditions,\ntopology, traffic statistics, and design goals. The mapping between context and\nAI model parameters is ideally done in a zero-shot fashion via an automatic\nmodel selection (AMS) mapping that leverages only contextual information\nwithout requiring any current data. This paper introduces a general methodology\nfor the online optimization of AMS mappings. Optimizing an AMS mapping is\nchallenging, as it requires exposure to data collected from many different\ncontexts. Therefore, if carried out online, this initial optimization phase\nwould be extremely time consuming. A possible solution is to leverage a digital\ntwin of the physical system to generate synthetic data from multiple simulated\ncontexts. However, given that the simulator at the digital twin is imperfect, a\ndirect use of simulated data for the optimization of the AMS mapping would\nyield poor performance when tested in the real system. This paper proposes a\nnovel method for the online optimization of AMS mapping that corrects for the\nbias of the simulator by means of limited real data collected from the physical\nsystem. Experimental results for a graph neural network-based power control app\ndemonstrate the significant advantages of the proposed approach.\n","authors":["Qiushuo Hou","Matteo Zecchin","Sangwoo Park","Yunlong Cai","Guanding Yu","Kaushik Chowdhury","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2406.15819v2.pdf","comment":"submitted for a journal publication"},{"id":"http://arxiv.org/abs/2410.15868v1","updated":"2024-10-21T10:51:28Z","published":"2024-10-21T10:51:28Z","title":"Enabling Hexa-X 6G Vision: An End-to-End Architecture","summary":"  The end-to-end (E2E) architecture for the 6th generation of mobile network\n(6G) necessitates a comprehensive design, considering emerging use cases (UCs),\nrequirements, and key value Indicators (KVIs). These UCs collectively share\nstringent requirements of extreme connectivity, inclusivity, and flexibility\nimposed on the architecture and its enablers. Furthermore, the trustworthiness\nand security of the 6G architecture must be enhanced compared to previous\ngenerations, owning to the expected increase in security threats and more\ncomplex UCs that may expose new security vulnerabilities. Additionally,\nsustainability emerges as a critical design consideration in the 6G\narchitecture. In light of these new set of values and requirements for 6G, this\npaper aims to describe an architecture proposed within the Hexa-X, the European\n6G flagship project, capable of enabling the above-mentioned 6G vision for the\n2030s and beyond.\n","authors":["Bahare M. Khorsandi","Mohammad Asif Habibi"],"pdf_url":"https://arxiv.org/pdf/2410.15868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15846v1","updated":"2024-10-21T10:16:56Z","published":"2024-10-21T10:16:56Z","title":"Modelling Concurrent RTP Flows for End-to-end Predictions of QoS in Real\n  Time Communications","summary":"  The Real-time Transport Protocol (RTP)-based real-time communications (RTC)\napplications, exemplified by video conferencing, have experienced an\nunparalleled surge in popularity and development in recent years. In pursuit of\noptimizing their performance, the prediction of Quality of Service (QoS)\nmetrics emerges as a pivotal endeavor, bolstering network monitoring and\nproactive solutions. However, contemporary approaches are confined to\nindividual RTP flows and metrics, falling short in relationship capture and\ncomputational efficiency. To this end, we propose Packet-to-Prediction (P2P), a\nnovel deep learning (DL) framework that hinges on raw packets to simultaneously\nprocess concurrent RTP flows and perform end-to-end prediction of multiple QoS\nmetrics. Specifically, we implement a streamlined architecture, namely\nlength-free Transformer with cross and neighbourhood attention, capable of\nhandling an unlimited number of RTP flows, and employ a multi-task learning\nparadigm to forecast four key metrics in a single shot. Our work is based on\nextensive traffic collected during real video calls, and conclusively, P2P\nexcels comparative models in both prediction performance and temporal\nefficiency.\n","authors":["Tailai Song","Paolo Garza","Michela Meo","Maurizio Matteo Munafò"],"pdf_url":"https://arxiv.org/pdf/2410.15846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15820v1","updated":"2024-10-21T09:36:53Z","published":"2024-10-21T09:36:53Z","title":"MAC Revivo: Artificial Intelligence Paves the Way","summary":"  The vast adoption of Wi-Fi and/or Bluetooth capabilities in Internet of\nThings (IoT) devices, along with the rapid growth of deployed smart devices,\nhas caused significant interference and congestion in the industrial,\nscientific, and medical (ISM) bands. Traditional Wi-Fi Medium Access Control\n(MAC) design faces significant challenges in managing increasingly complex\nwireless environments while ensuring network Quality of Service (QoS)\nperformance. This paper explores the potential integration of advanced\nArtificial Intelligence (AI) methods into the design of Wi-Fi MAC protocols. We\npropose AI-MAC, an innovative approach that employs machine learning algorithms\nto dynamically adapt to changing network conditions, optimize channel access,\nmitigate interference, and ensure deterministic latency. By intelligently\npredicting and managing interference, AI-MAC aims to provide a robust solution\nfor next generation of Wi-Fi networks, enabling seamless connectivity and\nenhanced QoS. Our experimental results demonstrate that AI-MAC significantly\nreduces both interference and latency, paving the way for more reliable and\nefficient wireless communications in the increasingly crowded ISM band.\n","authors":["Jinzhe Pan","Jingqing Wang","Zelin Yun","Zhiyong Xiao","Yuehui Ouyang","Wenchi Cheng","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15716v1","updated":"2024-10-21T07:34:17Z","published":"2024-10-21T07:34:17Z","title":"Traffic Matrix Estimation based on Denoising Diffusion Probabilistic\n  Model","summary":"  The traffic matrix estimation (TME) problem has been widely researched for\ndecades of years. Recent progresses in deep generative models offer new\nopportunities to tackle TME problems in a more advanced way. In this paper, we\nleverage the powerful ability of denoising diffusion probabilistic models\n(DDPMs) on distribution learning, and for the first time adopt DDPM to address\nthe TME problem. To ensure a good performance of DDPM on learning the\ndistributions of TMs, we design a preprocessing module to reduce the dimensions\nof TMs while keeping the data variety of each OD flow. To improve the\nestimation accuracy, we parameterize the noise factors in DDPM and transform\nthe TME problem into a gradient-descent optimization problem. Finally, we\ncompared our method with the state-of-the-art TME methods using two real-world\nTM datasets, the experimental results strongly demonstrate the superiority of\nour method on both TM synthesis and TM estimation.\n","authors":["Xinyu Yuan","Yan Qiao","Pei Zhao","Rongyao Hu","Benchu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15693v1","updated":"2024-10-21T07:03:15Z","published":"2024-10-21T07:03:15Z","title":"Geographical Node Clustering and Grouping to Guarantee Data IIDness in\n  Federated Learning","summary":"  Federated learning (FL) is a decentralized AI mechanism suitable for a large\nnumber of devices like in smart IoT. A major challenge of FL is the non-IID\ndataset problem, originating from the heterogeneous data collected by FL\nparticipants, leading to performance deterioration of the trained global model.\nThere have been various attempts to rectify non-IID dataset, mostly focusing on\nmanipulating the collected data. This paper, however, proposes a novel approach\nto ensure data IIDness by properly clustering and grouping mobile IoT nodes\nexploiting their geographical characteristics, so that each FL group can\nachieve IID dataset. We first provide an experimental evidence for the\nindependence and identicalness features of IoT data according to the\ninter-device distance, and then propose Dynamic Clustering and Partial-Steady\nGrouping algorithms that partition FL participants to achieve near-IIDness in\ntheir dataset while considering device mobility. Our mechanism significantly\noutperforms benchmark grouping algorithms at least by 110 times in terms of the\njoint cost between the number of dropout devices and the evenness in per-group\ndevice count, with a mild increase in the number of groups only by up to 0.93\ngroups.\n","authors":["Minkwon Lee","Hyoil Kim","Changhee Joo"],"pdf_url":"https://arxiv.org/pdf/2410.15693v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.15603v1","updated":"2024-10-21T03:02:26Z","published":"2024-10-21T03:02:26Z","title":"Trace-Distance based End-to-End Entanglement Fidelity with Information\n  Preservation in Quantum Networks","summary":"  Quantum networks hold the potential to revolutionize a variety of fields by\nsurpassing the capabilities of their classical counterparts. Many of these\napplications necessitate the sharing of high-fidelity entangled pairs among\ncommunicating parties. However, the inherent nature of entanglement leads to an\nexponential decrease in fidelity as the distance between quantum nodes\nincreases. This phenomenon makes it challenging to generate high-fidelity\nentangled pairs and preserve information in quantum networks. To tackle this\nproblem, we utilized two strategies to ensure high-fidelity entangled pairs and\ninformation preservation within a quantum network. First, we use closeness\ncentrality as a metric to identify the closest nodes in the network. Second, we\nintroduced the trace-distance based path purification (TDPP) algorithm,\nspecifically designed to enable information preservation and path purification\nentanglement routing. This algorithm identifies the shortest path within\nquantum networks using closeness centrality and integrates trace-distance\ncomputations for distinguishing quantum states and maintaining end-to-end (E2E)\nentanglement fidelity. Simulation results demonstrate that the proposed\nalgorithm improves network throughput and E2E fidelity while preserving\ninformation compared to existing methods.\n","authors":["Pankaj Kumar","Binayak Kar","Shan-Hsiang Shen"],"pdf_url":"https://arxiv.org/pdf/2410.15603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15546v1","updated":"2024-10-21T00:19:17Z","published":"2024-10-21T00:19:17Z","title":"Improved Contact Graph Routing in Delay Tolerant Networks with Capacity\n  and Buffer Constraints","summary":"  Satellite communications present challenging characteristics. Continuous\nend-to-end connectivity may not be available due to the large distances between\nsatellites. Moreover, resources such as link capacity and buffer memory may be\nlimited. Routing in satellite networks is therefore both complex and crucial to\navoid packet losses and long delays. The Delay Tolerant Network (DTN) paradigm\nhas emerged as an efficient solution for managing these challenging networks.\nContact Graph Routing (CGR), a deterministic routing algorithm, is one of the\nmost popular DTN algorithms. CGR is compatible with the ``store, carry, and\nforward\" principle, whereby a node receives a message and stores it in its\nbuffer until a transmission opportunity becomes available. However, CGR relies\non simplified models to incorporate potential constraints in the route search.\nFor instance, the linear volume assumption is often used to consider capacity\nconstraints. Moreover, capacity management and buffer management are mostly\nperformed during the forwarding phase, once an issue has occurred. In this\npaper, we propose to take measures before or during the route search in order\nto find routes that respect both contact-capacity limits and node-buffer\nlimits. We introduce the contact splitting and edge pruning operations to\neffectively account for the routing constraints. This ensures that CGR outputs\nthe optimal solution among the subset of valid solutions. The proposed approach\ncan also be used to book resources to be used in case of issues during the\nforwarding step.\n","authors":["Tania Alhajj","Vincent Corlay"],"pdf_url":"https://arxiv.org/pdf/2410.15546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15483v5","updated":"2024-10-21T17:51:30Z","published":"2023-08-03T19:33:43Z","title":"Generative AI for Semantic Communication: Architecture, Challenges, and\n  Outlook","summary":"  Semantic communication (SemCom) is expected to be a core paradigm in future\ncommunication networks, yielding significant benefits in terms of spectrum\nresource saving and information interaction efficiency. However, the existing\nSemCom structure is limited by the lack of context-reasoning ability and\nbackground knowledge provisioning, which, therefore, motivates us to seek the\npotential of incorporating generative artificial intelligence (GAI)\ntechnologies with SemCom. Recognizing GAI's powerful capability in automating\nand creating valuable, diverse, and personalized multimodal content, this\narticle first highlights the principal characteristics of the combination of\nGAI and SemCom along with their pertinent benefits and challenges. To tackle\nthese challenges, we further propose a novel GAI-integrated SemCom network\n(GAI-SCN) framework in a cloud-edge-mobile design. Specifically, by employing\nglobal and local GAI models, our GAI-SCN enables multimodal semantic content\nprovisioning, semantic-level joint-source-channel coding, and AIGC acquisition\nto maximize the efficiency and reliability of semantic reasoning and resource\nutilization. Afterward, we present a detailed implementation workflow of\nGAI-SCN, followed by corresponding initial simulations for performance\nevaluation in comparison with two benchmarks. Finally, we discuss several open\nissues and offer feasible solutions to unlock the full potential of GAI-SCN.\n","authors":["Le Xia","Yao Sun","Chengsi Liang","Lei Zhang","Muhammad Ali Imran","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2308.15483v5.pdf","comment":"This magazine article has been accepted for publication by IEEE\n  Wireless Communications"},{"id":"http://arxiv.org/abs/2311.05017v2","updated":"2024-10-21T16:30:03Z","published":"2023-11-08T21:03:43Z","title":"Joint Sensing and Semantic Communications with Multi-Task Deep Learning","summary":"  This paper explores the integration of deep learning techniques for joint\nsensing and communications, with an extension to semantic communications. The\nintegrated system comprises a transmitter and receiver operating over a\nwireless channel, subject to noise and fading. The transmitter employs a deep\nneural network (DNN), namely an encoder, for joint operations of source coding,\nchannel coding, and modulation, while the receiver utilizes another DNN, namely\na decoder, for joint operations of demodulation, channel decoding, and source\ndecoding to reconstruct the data samples. The transmitted signal serves a dual\npurpose, supporting communication with the receiver and enabling sensing. When\na target is present, the reflected signal is received, and another DNN decoder\nis utilized for sensing. This decoder is responsible for detecting the target's\npresence and determining its range. All these DNNs, including one encoder and\ntwo decoders, undergo joint training through multi-task learning, considering\ndata and channel characteristics. This paper extends to incorporate semantic\ncommunications by introducing an additional DNN, another decoder at the\nreceiver, operating as a task classifier. This decoder evaluates the fidelity\nof label classification for received signals, enhancing the integration of\nsemantics within the communication process. The study presents results based on\nusing the CIFAR-10 as the input data and accounting for channel effects like\nAdditive White Gaussian Noise (AWGN) and Rayleigh fading. The results\nunderscore the effectiveness of multi-task deep learning in achieving\nhigh-fidelity joint sensing and semantic communications.\n","authors":["Yalin E. Sagduyu","Tugba Erpek","Aylin Yener","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2311.05017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.11109v3","updated":"2024-10-21T09:50:31Z","published":"2021-12-21T11:22:38Z","title":"Network Anomaly Detection in Cars: A Case for Time-Sensitive Stream\n  Filtering and Policing","summary":"  Connected vehicles are threatened by cyber-attacks as in-vehicle networks\ntechnologically approach (mobile) LANs with several wireless interconnects to\nthe outside world. Malware that infiltrates a car today faces potential victims\nof constrained, barely shielded Electronic Control Units (ECUs). Many ECUs\nperform critical driving functions, which stresses the need for hardening\nsecurity and resilience of in-vehicle networks in a multifaceted way. Future\nvehicles will comprise Ethernet backbones that differentiate services via\nTime-Sensitive Networking (TSN). The well-known vehicular control flows will\nfollow predefined schedules and TSN traffic classifications. In this paper, we\nexploit this traffic classification to build a network anomaly detection\nsystem. We show how filters and policies of TSN can identify misbehaving\ntraffic and thereby serve as distributed guards on the data link layer. On this\nlowest possible layer, our approach derives a highly efficient network\nprotection directly from TSN. We classify link layer anomalies and\nmicro-benchmark the detection accuracy in each class. Based on a topology\nderived from a real-world car and its traffic definitions we evaluate the\ndetection system in realistic macro-benchmarks based on recorded attack traces.\nOur results show that the detection accuracy depends on how exact the\nspecifications of in-vehicle communication are configured. Most notably for a\nfully specified communication matrix, our anomaly detection remains free of\nfalse-positive alarms, which is a significant benefit for implementing\nautomated countermeasures in future vehicles.\n","authors":["Philipp Meyer","Timo Häckel","Sandra Reider","Franz Korf","Thomas C. Schmidt"],"pdf_url":"https://arxiv.org/pdf/2112.11109v3.pdf","comment":null}],"Operating Systems":[{"id":"http://arxiv.org/abs/2410.15894v1","updated":"2024-10-21T11:14:55Z","published":"2024-10-21T11:14:55Z","title":"Transparent and Efficient Live Migration across Heterogeneous Hosts with\n  Wharf","summary":"  Live migration allows a user to move a running application from one machine\n(a source) to another (a destination) without restarting it. The technique has\nproven useful for diverse tasks including load balancing, managing system\nupdates, improving data locality, and improving system resilience.\nUnfortunately, current live migration solutions fail to meet today's computing\nneeds. First, most techniques do not support heterogeneous source and\ndestination hosts, as they require the two machines to have the same\ninstruction set architecture (ISA) or use the same operating system (OS), which\nhampers numerous live migration usecases. Second, many techniques are not\ntransparent, as they require that applications be written in a specific\nhigh-level language or call specific library functions, which imposes barriers\nto entry for many users. We present a new lightweight abstraction, called a\nvessel, that supports transparent heterogeneous live migration. A vessel\nmaintains a machine-independent encoding of a process's state, using\nWebAssembly abstractions, allowing it to be executed on nearly-arbitrary ISAs.\nA vessel virtualizes all of its OS state, using the WebAssembly System\nInterface (WASI), allowing it to execute on nearly arbitrary OS. We introduce\ndocks and software systems that execute and migrate vessels. Docks face two key\nchallenges: First, maintaining a machine-independent encoding at all points in\na process is extremely expensive. So, docks instead ensure that a vessel is\nguaranteed to eventually reach a machine-independent point and delay the\ninitiation of vessel migration until the vessel reaches such a point. Second, a\ndock may receive a vessel migration that originates from a dock executing on a\ndifferent OS.\n","authors":["Yiwei Yang","Aibo Hu","Yusheng Zheng","Brian Zhao","Xinqi Zhang","Andrew Quinn"],"pdf_url":"https://arxiv.org/pdf/2410.15894v1.pdf","comment":null}],"Neural and Evolutionary Computing":[{"id":"http://arxiv.org/abs/2410.16175v1","updated":"2024-10-21T16:41:35Z","published":"2024-10-21T16:41:35Z","title":"Spiking Neural Networks as a Controller for Emergent Swarm Agents","summary":"  Drones which can swarm and loiter in a certain area cost hundreds of dollars,\nbut mosquitos can do the same and are essentially worthless. To control swarms\nof low-cost robots, researchers may end up spending countless hours\nbrainstorming robot configurations and policies to ``organically\" create\nbehaviors which do not need expensive sensors and perception. Existing research\nexplores the possible emergent behaviors in swarms of robots with only a binary\nsensor and a simple but hand-picked controller structure. Even agents in this\nhighly limited sensing, actuation, and computational capability class can\nexhibit relatively complex global behaviors such as aggregation, milling, and\ndispersal, but finding the local interaction rules that enable more collective\nbehaviors remains a significant challenge. This paper investigates the\nfeasibility of training spiking neural networks to find those local interaction\nrules that result in particular emergent behaviors. In this paper, we focus on\nsimulating a specific milling behavior already known to be producible using\nvery simple binary sensing and acting agents. To do this, we use evolutionary\nalgorithms to evolve not only the parameters (the weights, biases, and delays)\nof a spiking neural network, but also its structure. To create a baseline, we\nalso show an evolutionary search strategy over the parameters for the incumbent\nhand-picked binary controller structure. Our simulations show that spiking\nneural networks can be evolved in binary sensing agents to form a mill.\n","authors":["Kevin Zhu","Connor Mattson","Shay Snyder","Ricardo Vega","Daniel S. Brown","Maryam Parsa","Cameron Nowzari"],"pdf_url":"https://arxiv.org/pdf/2410.16175v1.pdf","comment":"8 pages, 7 figures, presented at the 2024 International Conference on\n  Neuromorphic Systems"},{"id":"http://arxiv.org/abs/2410.16159v1","updated":"2024-10-21T16:22:19Z","published":"2024-10-21T16:22:19Z","title":"Metric as Transform: Exploring beyond Affine Transform for Interpretable\n  Neural Network","summary":"  Artificial Neural Networks of varying architectures are generally paired with\naffine transformation at the core. However, we find dot product neurons with\nglobal influence less interpretable as compared to local influence of euclidean\ndistance (as used in Radial Basis Function Network). In this work, we explore\nthe generalization of dot product neurons to $l^p$-norm, metrics, and beyond.\nWe find that metrics as transform performs similarly to affine transform when\nused in MultiLayer Perceptron or Convolutional Neural Network. Moreover, we\nexplore various properties of Metrics, compare it with Affine, and present\nmultiple cases where metrics seem to provide better interpretability. We\ndevelop an interpretable local dictionary based Neural Networks and use it to\nunderstand and reject adversarial examples.\n","authors":["Suman Sapkota"],"pdf_url":"https://arxiv.org/pdf/2410.16159v1.pdf","comment":"22 pages, 20 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.16151v1","updated":"2024-10-21T16:18:31Z","published":"2024-10-21T16:18:31Z","title":"Small Contributions, Small Networks: Efficient Neural Network Pruning\n  Based on Relative Importance","summary":"  Recent advancements have scaled neural networks to unprecedented sizes,\nachieving remarkable performance across a wide range of tasks. However,\ndeploying these large-scale models on resource-constrained devices poses\nsignificant challenges due to substantial storage and computational\nrequirements. Neural network pruning has emerged as an effective technique to\nmitigate these limitations by reducing model size and complexity. In this\npaper, we introduce an intuitive and interpretable pruning method based on\nactivation statistics, rooted in information theory and statistical analysis.\nOur approach leverages the statistical properties of neuron activations to\nidentify and remove weights with minimal contributions to neuron outputs.\nSpecifically, we build a distribution of weight contributions across the\ndataset and utilize its parameters to guide the pruning process. Furthermore,\nwe propose a Pruning-aware Training strategy that incorporates an additional\nregularization term to enhance the effectiveness of our pruning method.\nExtensive experiments on multiple datasets and network architectures\ndemonstrate that our method consistently outperforms several baseline and\nstate-of-the-art pruning techniques.\n","authors":["Mostafa Hussien","Mahmoud Afifi","Kim Khoa Nguyen","Mohamed Cheriet"],"pdf_url":"https://arxiv.org/pdf/2410.16151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15973v1","updated":"2024-10-21T12:59:58Z","published":"2024-10-21T12:59:58Z","title":"Karush-Kuhn-Tucker Condition-Trained Neural Networks (KKT Nets)","summary":"  This paper presents a novel approach to solving convex optimization problems\nby leveraging the fact that, under certain regularity conditions, any set of\nprimal or dual variables satisfying the Karush-Kuhn-Tucker (KKT) conditions is\nnecessary and sufficient for optimality. Similar to Theory-Trained Neural\nNetworks (TTNNs), the parameters of the convex optimization problem are input\nto the neural network, and the expected outputs are the optimal primal and dual\nvariables. A choice for the loss function in this case is a loss, which we\nrefer to as the KKT Loss, that measures how well the network's outputs satisfy\nthe KKT conditions. We demonstrate the effectiveness of this approach using a\nlinear program as an example. For this problem, we observe that minimizing the\nKKT Loss alone outperforms training the network with a weighted sum of the KKT\nLoss and a Data Loss (the mean-squared error between the ground truth optimal\nsolutions and the network's output). Moreover, minimizing only the Data Loss\nyields inferior results compared to those obtained by minimizing the KKT Loss.\nWhile the approach is promising, the obtained primal and dual solutions are not\nsufficiently close to the ground truth optimal solutions. In the future, we aim\nto develop improved models to obtain solutions closer to the ground truth and\nextend the approach to other problem classes.\n","authors":["Shreya Arvind","Rishabh Pomaje","Rajshekhar V Bhat"],"pdf_url":"https://arxiv.org/pdf/2410.15973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15854v1","updated":"2024-10-21T10:30:24Z","published":"2024-10-21T10:30:24Z","title":"TEXEL: A neuromorphic processor with on-chip learning for beyond-CMOS\n  device integration","summary":"  Recent advances in memory technologies, devices and materials have shown\ngreat potential for integration into neuromorphic electronic systems. However,\na significant gap remains between the development of these materials and the\nrealization of large-scale, fully functional systems. One key challenge is\ndetermining which devices and materials are best suited for specific functions\nand how they can be paired with CMOS circuitry. To address this, we introduce\nTEXEL, a mixed-signal neuromorphic architecture designed to explore the\nintegration of on-chip learning circuits and novel two- and three-terminal\ndevices. TEXEL serves as an accessible platform to bridge the gap between\nCMOS-based neuromorphic computation and the latest advancements in emerging\ndevices. In this paper, we demonstrate the readiness of TEXEL for device\nintegration through comprehensive chip measurements and simulations. TEXEL\nprovides a practical system for testing bio-inspired learning algorithms\nalongside emerging devices, establishing a tangible link between brain-inspired\ncomputation and cutting-edge device research.\n","authors":["Hugh Greatorex","Ole Richter","Michele Mastella","Madison Cotteret","Philipp Klein","Maxime Fabre","Arianna Rubino","Willian Soares Girão","Junren Chen","Martin Ziegler","Laura Bégon-Lours","Giacomo Indiveri","Elisabetta Chicca"],"pdf_url":"https://arxiv.org/pdf/2410.15854v1.pdf","comment":"17 pages, 7 figures. Supplementary material: 8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.15689v1","updated":"2024-10-21T06:59:04Z","published":"2024-10-21T06:59:04Z","title":"Enhancing SNN-based Spatio-Temporal Learning: A Benchmark Dataset and\n  Cross-Modality Attention Model","summary":"  Spiking Neural Networks (SNNs), renowned for their low power consumption,\nbrain-inspired architecture, and spatio-temporal representation capabilities,\nhave garnered considerable attention in recent years. Similar to Artificial\nNeural Networks (ANNs), high-quality benchmark datasets are of great importance\nto the advances of SNNs. However, our analysis indicates that many prevalent\nneuromorphic datasets lack strong temporal correlation, preventing SNNs from\nfully exploiting their spatio-temporal representation capabilities. Meanwhile,\nthe integration of event and frame modalities offers more comprehensive visual\nspatio-temporal information. Yet, the SNN-based cross-modality fusion remains\nunderexplored.\n  In this work, we present a neuromorphic dataset called DVS-SLR that can\nbetter exploit the inherent spatio-temporal properties of SNNs. Compared to\nexisting datasets, it offers advantages in terms of higher temporal\ncorrelation, larger scale, and more varied scenarios. In addition, our\nneuromorphic dataset contains corresponding frame data, which can be used for\ndeveloping SNN-based fusion methods. By virtue of the dual-modal feature of the\ndataset, we propose a Cross-Modality Attention (CMA) based fusion method. The\nCMA model efficiently utilizes the unique advantages of each modality, allowing\nfor SNNs to learn both temporal and spatial attention scores from the\nspatio-temporal features of event and frame modalities, subsequently allocating\nthese scores across modalities to enhance their synergy. Experimental results\ndemonstrate that our method not only improves recognition accuracy but also\nensures robustness across diverse scenarios.\n","authors":["Shibo Zhou","Bo Yang","Mengwen Yuan","Runhao Jiang","Rui Yan","Gang Pan","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.15689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13147v3","updated":"2024-10-21T02:15:42Z","published":"2024-06-19T01:51:15Z","title":"A Simulation Environment for the Neuroevolution of Ant Colony Dynamics","summary":"  We introduce a simulation environment to facilitate research into emergent\ncollective behaviour, with a focus on replicating the dynamics of ant colonies.\nBy leveraging real-world data, the environment simulates a target ant trail\nthat a controllable agent must learn to replicate, using sensory data observed\nby the target ant. This work aims to contribute to the neuroevolution of models\nfor collective behaviour, focusing on evolving neural architectures that encode\ndomain-specific behaviours in the network topology. By evolving models that can\nbe modified and studied in a controlled environment, we can uncover the\nnecessary conditions required for collective behaviours to emerge. We hope this\nenvironment will be useful to those studying the role of interactions in\nemergent behaviour within collective systems.\n","authors":["Michael Crosscombe","Ilya Horiguchi","Norihiro Maruyama","Shigeto Dobata","Takashi Ikegami"],"pdf_url":"https://arxiv.org/pdf/2406.13147v3.pdf","comment":"Accepted for publication at The 2024 Conference on Artificial Life. 2\n  page extended abstract"},{"id":"http://arxiv.org/abs/2410.12211v2","updated":"2024-10-21T17:47:17Z","published":"2024-10-16T04:08:22Z","title":"Increasing the clock speed of a thermodynamic computer by adding noise","summary":"  We describe a proposal for increasing the effective clock speed of a\nthermodynamic computer, by altering the interaction scale of the units within\nthe computer and introducing to the computer an additional source of noise. The\nresulting thermodynamic computer program is equivalent to the original computer\nprogram, but runs at a higher clock speed. This approach offers a way of\nincreasing the speed of thermodynamic computing while preserving the fidelity\nof computation.\n","authors":["Stephen Whitelam"],"pdf_url":"https://arxiv.org/pdf/2410.12211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14571v2","updated":"2024-10-21T23:13:24Z","published":"2024-04-22T20:34:46Z","title":"A Survey of Decomposition-Based Evolutionary Multi-Objective\n  Optimization: Part I-Past and Future","summary":"  Decomposition has been the mainstream approach in classic mathematical\nprogramming for multi-objective optimization and multi-criterion\ndecision-making. However, it was not properly studied in the context of\nevolutionary multi-objective optimization (EMO) until the development of\nmulti-objective evolutionary algorithm based on decomposition (MOEA/D). In this\ntwo-part survey series, we use MOEA/D as the representative of\ndecomposition-based EMO to review the up-to-date development in this area, and\nsystematically and comprehensively analyze its research landscape. In the first\npart, we present a comprehensive survey of the development of MOEA/D from its\norigin to the current state-of-the-art approaches. In order to be\nself-contained, we start with a step-by-step tutorial that aims to help a\nnovice quickly get onto the working mechanism of MOEA/D. Then, selected major\ndevelopments of MOEA/D are reviewed according to its core design components\nincluding weight vector settings, subproblem formulations, selection mechanisms\nand reproduction operators. Besides, we also overview some selected advanced\ntopics for constraint handling, optimization in dynamic and uncertain\nenvironments, computationally expensive objective functions, and preference\nincorporation. In the final part, we shed some light on emerging directions for\nfuture developments.\n","authors":["Ke Li"],"pdf_url":"https://arxiv.org/pdf/2404.14571v2.pdf","comment":"40 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:2108.09588"},{"id":"http://arxiv.org/abs/2310.17042v3","updated":"2024-10-21T21:54:46Z","published":"2023-10-25T22:45:31Z","title":"StochGradAdam: Accelerating Neural Networks Training with Stochastic\n  Gradient Sampling","summary":"  In this paper, we introduce StochGradAdam, a novel optimizer designed as an\nextension of the Adam algorithm, incorporating stochastic gradient sampling\ntechniques to improve computational efficiency while maintaining robust\nperformance. StochGradAdam optimizes by selectively sampling a subset of\ngradients during training, reducing the computational cost while preserving the\nadvantages of adaptive learning rates and bias corrections found in Adam. Our\nexperimental results, applied to image classification and segmentation tasks,\ndemonstrate that StochGradAdam can achieve comparable or superior performance\nto Adam, even when using fewer gradient updates per iteration. By focusing on\nkey gradient updates, StochGradAdam offers stable convergence and enhanced\nexploration of the loss landscape, while mitigating the impact of noisy\ngradients. The results suggest that this approach is particularly effective for\nlarge-scale models and datasets, providing a promising alternative to\ntraditional optimization techniques for deep learning applications.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2310.17042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10075v2","updated":"2024-10-21T21:21:51Z","published":"2024-09-16T08:26:06Z","title":"Steinmetz Neural Networks for Complex-Valued Data","summary":"  In this work, we introduce a new approach to processing complex-valued data\nusing DNNs consisting of parallel real-valued subnetworks with coupled outputs.\nOur proposed class of architectures, referred to as Steinmetz Neural Networks,\nleverages multi-view learning to construct more interpretable representations\nwithin the latent space. Moreover, we present the Analytic Neural Network,\nwhich incorporates a consistency penalty that encourages analytic signal\nrepresentations in the latent space of the Steinmetz neural network. This\npenalty enforces a deterministic and orthogonal relationship between the real\nand imaginary components. Utilizing an information-theoretic construction, we\ndemonstrate that the generalization error upper bound posited by the analytic\nneural network is lower than that of the general class of Steinmetz neural\nnetworks. Our numerical experiments depict the improved performance and\nrobustness to additive noise, afforded by these networks on benchmark datasets\nand synthetic examples.\n","authors":["Shyam Venkatasubramanian","Ali Pezeshki","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2409.10075v2.pdf","comment":null}]},"2024-10-22T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2410.03314v2","updated":"2024-10-22T10:34:47Z","published":"2024-10-04T10:56:36Z","title":"Evaluation of Study Plans using Partial Orders","summary":"  In higher education, data is collected that indicate the term(s) that a\ncourse is taken and when it is passed. Often, study plans propose a suggested\ncourse order to students. Study planners can adjust these based on detected\ndeviations between the proposed and actual order of the courses being taken. In\nthis work, we detect deviations by combining (1) the deviation between the\nproposed and actual course order with (2) the temporal difference between the\nexpected and actual course-taking term(s). Partially ordered alignments\nidentify the deviations between the proposed and actual order. We compute a\npartial order alignment by modeling a study plan as a process model and a\nstudent's course-taking behavior as a partial order. Using partial orders in\nsuch use cases allows one to relax the constraints of strictly ordered traces.\nThis makes our approach less prone to the order in which courses are offered.\nFurther, when modeling course-taking behavior as partial orders, we propose\ndistinguishing intended course-taking behavior from actual course-passing\nbehavior of students by including either all terms in which a course is\nattempted or only the term that a course is passed, respectively. This provides\nmore perspectives when comparing the proposed and actual course-taking\nbehavior. The proposed deviation measuring approach is evaluated on real-life\ndata from RWTH Aachen University.\n","authors":["Christian Rennert","Mahsa Pourbafrani","Wil van der Aalst"],"pdf_url":"https://arxiv.org/pdf/2410.03314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03851v4","updated":"2024-10-22T10:21:59Z","published":"2024-05-06T20:58:36Z","title":"Querying in Constant Expected Time with Learned Indexes","summary":"  Learned indexes leverage machine learning models to accelerate query\nanswering in databases, showing impressive practical performance. However,\ntheoretical understanding of these methods remains incomplete. Existing\nresearch suggests that learned indexes have superior asymptotic complexity\ncompared to their non-learned counterparts, but these findings have been\nestablished under restrictive probabilistic assumptions. Specifically, for a\nsorted array with $n$ elements, it has been shown that learned indexes can find\na key in $O(\\log(\\log n))$ expected time using at most linear space, compared\nwith $O(\\log n)$ for non-learned methods.\n  In this work, we prove $O(1)$ expected time can be achieved with at most\nlinear space, thereby establishing the tightest upper bound so far for the time\ncomplexity of an asymptotically optimal learned index. Notably, we use weaker\nprobabilistic assumptions than prior research, meaning our work generalizes\nprevious results. Furthermore, we introduce a new measure of statistical\ncomplexity for data. This metric exhibits an information-theoretical\ninterpretation and can be estimated in practice. This characterization provides\nfurther theoretical understanding of learned indexes, by helping to explain why\nsome datasets seem to be particularly challenging for these methods.\n","authors":["Luis Croquevielle","Guang Yang","Liang Liang","Ali Hadian","Thomas Heinis"],"pdf_url":"https://arxiv.org/pdf/2405.03851v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17134v1","updated":"2024-10-22T16:06:33Z","published":"2024-10-22T16:06:33Z","title":"TELII: Temporal Event Level Inverted Indexing for Cohort Discovery on a\n  Large Covid-19 EHR Dataset","summary":"  Cohort discovery is a crucial step in clinical research on Electronic Health\nRecord (EHR) data. Temporal queries, which are common in cohort discovery, can\nbe time-consuming and prone to errors when processed on large EHR datasets. In\nthis work, we introduce TELII, a temporal event level inverted indexing method\ndesigned for cohort discovery on large EHR datasets. TELII is engineered to\npre-compute and store the relations along with the time difference between\nevents, thereby providing fast and accurate temporal query capabilities. We\nimplemented TELII for the OPTUM de-identified COVID-19 EHR dataset, which\ncontains data from 8.87 million patients. We demonstrate four common temporal\nquery tasks and their implementation using TELII with a MongoDB backend. Our\nresults show that the temporal query speed for TELII is up to 2000 times faster\nthan that of existing non-temporal inverted indexes. TELII achieves\nmillisecond-level response times, enabling users to quickly explore event\nrelations and find preliminary evidence for their research questions. Not only\nis TELII practical and straightforward to implement, but it also offers easy\nadaptability to other EHR datasets. These advantages underscore TELII's\npotential to serve as the query engine for EHR-based applications, ensuring\nfast, accurate, and user-friendly query responses.\n","authors":["Yan Huang"],"pdf_url":"https://arxiv.org/pdf/2410.17134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07063v4","updated":"2024-10-22T12:28:13Z","published":"2023-04-14T11:35:35Z","title":"Rethinking Complex Queries on Knowledge Graphs with Neural Link\n  Predictors","summary":"  Reasoning on knowledge graphs is a challenging task because it utilizes\nobserved information to predict the missing one. Particularly, answering\ncomplex queries based on first-order logic is one of the crucial tasks to\nverify learning to reason abilities for generalization and composition.\nRecently, the prevailing method is query embedding which learns the embedding\nof a set of entities and treats logic operations as set operations and has\nshown great empirical success. Though there has been much research following\nthe same formulation, many of its claims lack a formal and systematic\ninspection. In this paper, we rethink this formulation and justify many of the\nprevious claims by characterizing the scope of queries investigated previously\nand precisely identifying the gap between its formulation and its goal, as well\nas providing complexity analysis for the currently investigated queries.\nMoreover, we develop a new dataset containing ten new types of queries with\nfeatures that have never been considered and therefore can provide a thorough\ninvestigation of complex queries. Finally, we propose a new neural-symbolic\nmethod, Fuzzy Inference with Truth value (FIT), where we equip the neural link\npredictors with fuzzy logic theory to support end-to-end learning using complex\nqueries with provable reasoning capability. Empirical results show that our\nmethod outperforms previous methods significantly in the new dataset and also\nsurpasses previous methods in the existing dataset at the same time.\n","authors":["Hang Yin","Zihao Wang","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2304.07063v4.pdf","comment":"Received in ICLR 2024"},{"id":"http://arxiv.org/abs/2410.16929v1","updated":"2024-10-22T12:00:15Z","published":"2024-10-22T12:00:15Z","title":"CUBIT: Concurrent Updatable Bitmap Indexing","summary":"  Bitmap indexes are widely used for read-intensive analytical workloads\nbecause they are clustered and offer efficient reads with a small memory\nfootprint. However, they are notoriously inefficient to update. As analytical\napplications are increasingly fused with transactional applications, leading to\nthe emergence of hybrid transactional/analytical processing (HTAP), it is\ndesirable that bitmap indexes support efficient concurrent real-time updates.\nIn this paper, we propose Concurrent Updatable Bitmap indexing (CUBIT) that\noffers efficient real-time updates that scale with the number of CPU cores used\nand do not interfere with queries. Our design relies on three principles.\nFirst, we employ a horizontal bitwise representation of updated bits, which\nenables efficient atomic updates without locking entire bitvectors. Second, we\npropose a lightweight snapshotting mechanism that allows queries (including\nrange queries) to run on separate snapshots and provides a wait-free progress\nguarantee. Third, we consolidate updates in a latch-free manner, providing a\nstrong progress guarantee. Our evaluation shows that CUBIT offers 3x - 16x\nhigher throughput and 3x - 220x lower latency than state-of-the-art updatable\nbitmap indexes.\n  CUBIT's update-friendly nature widens the applicability of bitmap indexing.\nExperimenting with OLAP workloads with standard, batched updates shows that\nCUBIT overcomes the maintenance downtime and outperforms DuckDB by 1.2x - 2.7x\non TPC-H. For HTAP workloads with real-time updates, CUBIT achieves 2x - 11x\nperformance improvement over the state-of-the-art approaches.\n","authors":["Junchang Wang","Manos Athanassoulis"],"pdf_url":"https://arxiv.org/pdf/2410.16929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16720v1","updated":"2024-10-22T06:00:04Z","published":"2024-10-22T06:00:04Z","title":"NodeOP: Optimizing Node Management for Decentralized Networks","summary":"  We present NodeOP, a novel framework designed to optimize the management of\nGeneral Node Operators in decentralized networks. By integrating Agent-Based\nModeling (ABM) with a Tendermint Byzantine Fault Tolerance (BFT)-based\nconsensus mechanism, NodeOP addresses key challenges in task allocation,\nconsensus formation, and system stability. Through rigorous mathematical\nmodeling and formal optimization, NodeOP ensures stable equilibrium in node\ntask distribution. We validate the framework via convergence analysis and\nperformance metrics such as transaction throughput, system latency, and fault\ntolerance. We further demonstrate NodeOP's practical utility through two use\ncases: decentralized sequencer management in Layer 2 networks and off-chain\npayment validation. These examples underscore how NodeOP enhances validation\nefficiency and unlocks new revenue opportunities in large-scale decentralized\nenvironments. Our results position NodeOP as a scalable and flexible solution,\nsignificantly improving operational efficiency and economic sustainability in\ndecentralized systems.\n","authors":["Angela Tsang","Jiankai Sun","Boo Xie","Azeem Khan","Ender Lu","Fletcher Fan","Maggie Wu","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2410.16720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16603v1","updated":"2024-10-22T01:20:32Z","published":"2024-10-22T01:20:32Z","title":"Efficient and Effective Algorithms for A Family of Influence\n  Maximization Problems with A Matroid Constraint","summary":"  Influence maximization (IM) is a classic problem that aims to identify a\nsmall group of critical individuals, known as seeds, who can influence the\nlargest number of users in a social network through word-of-mouth. This problem\nfinds important applications including viral marketing, infection detection,\nand misinformation containment. The conventional IM problem is typically\nstudied with the oversimplified goal of selecting a single seed set. Many\nreal-world scenarios call for multiple sets of seeds, particularly on social\nmedia platforms where various viral marketing campaigns need different sets of\nseeds to propagate effectively. To this end, previous works have formulated\nvarious IM variants, central to which is the requirement of multiple seed sets,\nnaturally modeled as a matroid constraint. However, the current best-known\nsolutions for these variants either offer a weak\n$(1/2-\\epsilon)$-approximation, or offer a $(1-1/e-\\epsilon)$-approximation\nalgorithm that is very expensive. We propose an efficient seed selection method\ncalled AMP, an algorithm with a $(1-1/e-\\epsilon)$-approximation guarantee for\nthis family of IM variants. To further improve efficiency, we also devise a\nfast implementation, called RAMP. We extensively evaluate the performance of\nour proposal against 6 competitors across 4 IM variants and on 7 real-world\nnetworks, demonstrating that our proposal outperforms all competitors in terms\nof result quality, running time, and memory usage. We have also deployed RAMP\nin a real industry strength application involving online gaming, where we show\nthat our deployed solution significantly improves upon the baselines.\n","authors":["Yiqian Huang","Shiqi Zhang","Laks V. S. Lakshmanan","Wenqing Lin","Xiaokui Xiao","Bo Tang"],"pdf_url":"https://arxiv.org/pdf/2410.16603v1.pdf","comment":"The technical report of the paper entitled 'Efficient and Effective\n  Algorithms for A Family of Influence Maximization Problems with A Matroid\n  Constraint' in PVLDB'25"},{"id":"http://arxiv.org/abs/2410.17465v1","updated":"2024-10-22T22:49:01Z","published":"2024-10-22T22:49:01Z","title":"Bauplan: zero-copy, scale-up FaaS for data pipelines","summary":"  Chaining functions for longer workloads is a key use case for FaaS platforms\nin data applications. However, modern data pipelines differ significantly from\ntypical serverless use cases (e.g., webhooks and microservices); this makes it\ndifficult to retrofit existing pipeline frameworks due to structural\nconstraints. In this paper, we describe these limitations in detail and\nintroduce bauplan, a novel FaaS programming model and serverless runtime\ndesigned for data practitioners. bauplan enables users to declaratively define\nfunctional Directed Acyclic Graphs (DAGs) along with their runtime\nenvironments, which are then efficiently executed on cloud-based workers. We\nshow that bauplan achieves both better performance and a superior developer\nexperience for data workloads by making the trade-off of reducing generality in\nfavor of data-awareness\n","authors":["Jacopo Tagliabue","Tyler Caraza-Harter","Ciro Greco"],"pdf_url":"https://arxiv.org/pdf/2410.17465v1.pdf","comment":"Accepted for the 10th International Workshop on Serverless Computing\n  (pre-print)"}],"Operating Systems":[{"id":"http://arxiv.org/abs/2410.14381v2","updated":"2024-10-22T11:27:06Z","published":"2024-10-18T11:08:15Z","title":"Optimizing over FP/EDF Execution Times: Known Results and Open Problems","summary":"  In many use cases the execution time of tasks is unknown and can be chosen by\nthe designer to increase or decrease the application features depending on the\navailability of processing capacity. If the application has real-time\nconstraints, such as deadlines, then the necessary and sufficient\nschedulability test must allow the execution times to be left unspecified. By\ndoing so, the designer can then perform optimization of the execution times by\npicking the schedulable values that minimize any given cost.\n  In this paper, we review existing results on the formulation of both the\nFixed Priority and Earliest Deadline First exact schedulability constraints.\nThe reviewed formulations are expressed by a combination of linear constraints,\nwhich enables then optimization routines.\n","authors":["Enrico Bini"],"pdf_url":"https://arxiv.org/pdf/2410.14381v2.pdf","comment":"Presented at OPERA 2024 (https://opera24.di.unito.it/) This work is\n  partially supported by the project \"Trustworthy Cyber-Physical Pipelines\",\n  funded by the MAECI Italy-Sweden co-operation id. PGR02086, and the spoke\n  \"FutureHPC and BigData\" of the ICSC - Centro Nazionale di Ricerca in\n  High-Performance Computing, Big Data and Quantum Computing funded by European\n  Union -NextGenerationEU"},{"id":"http://arxiv.org/abs/2410.17465v1","updated":"2024-10-22T22:49:01Z","published":"2024-10-22T22:49:01Z","title":"Bauplan: zero-copy, scale-up FaaS for data pipelines","summary":"  Chaining functions for longer workloads is a key use case for FaaS platforms\nin data applications. However, modern data pipelines differ significantly from\ntypical serverless use cases (e.g., webhooks and microservices); this makes it\ndifficult to retrofit existing pipeline frameworks due to structural\nconstraints. In this paper, we describe these limitations in detail and\nintroduce bauplan, a novel FaaS programming model and serverless runtime\ndesigned for data practitioners. bauplan enables users to declaratively define\nfunctional Directed Acyclic Graphs (DAGs) along with their runtime\nenvironments, which are then efficiently executed on cloud-based workers. We\nshow that bauplan achieves both better performance and a superior developer\nexperience for data workloads by making the trade-off of reducing generality in\nfavor of data-awareness\n","authors":["Jacopo Tagliabue","Tyler Caraza-Harter","Ciro Greco"],"pdf_url":"https://arxiv.org/pdf/2410.17465v1.pdf","comment":"Accepted for the 10th International Workshop on Serverless Computing\n  (pre-print)"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2410.16152v2","updated":"2024-10-22T03:37:37Z","published":"2024-10-21T16:19:34Z","title":"Warped Diffusion: Solving Video Inverse Problems with Image Diffusion\n  Models","summary":"  Using image models naively for solving inverse video problems often suffers\nfrom flickering, texture-sticking, and temporal inconsistency in generated\nvideos. To tackle these problems, in this paper, we view frames as continuous\nfunctions in the 2D space, and videos as a sequence of continuous warping\ntransformations between different frames. This perspective allows us to train\nfunction space diffusion models only on images and utilize them to solve\ntemporally correlated inverse problems. The function space diffusion models\nneed to be equivariant with respect to the underlying spatial transformations.\nTo ensure temporal consistency, we introduce a simple post-hoc test-time\nguidance towards (self)-equivariant solutions. Our method allows us to deploy\nstate-of-the-art latent diffusion models such as Stable Diffusion XL to solve\nvideo inverse problems. We demonstrate the effectiveness of our method for\nvideo inpainting and $8\\times$ video super-resolution, outperforming existing\ntechniques based on noise transformations. We provide generated video results:\nhttps://giannisdaras.github.io/warped_diffusion.github.io/.\n","authors":["Giannis Daras","Weili Nie","Karsten Kreis","Alex Dimakis","Morteza Mardani","Nikola Borislavov Kovachki","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2410.16152v2.pdf","comment":"Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16070v2","updated":"2024-10-22T13:40:18Z","published":"2024-10-21T14:48:35Z","title":"On-Device LLMs for SMEs: Challenges and Opportunities","summary":"  This paper presents a systematic review of the infrastructure requirements\nfor deploying Large Language Models (LLMs) on-device within the context of\nsmall and medium-sized enterprises (SMEs), focusing on both hardware and\nsoftware perspectives. From the hardware viewpoint, we discuss the utilization\nof processing units like GPUs and TPUs, efficient memory and storage solutions,\nand strategies for effective deployment, addressing the challenges of limited\ncomputational resources typical in SME settings. From the software perspective,\nwe explore framework compatibility, operating system optimization, and the use\nof specialized libraries tailored for resource-constrained environments. The\nreview is structured to first identify the unique challenges faced by SMEs in\ndeploying LLMs on-device, followed by an exploration of the opportunities that\nboth hardware innovations and software adaptations offer to overcome these\nobstacles. Such a structured review provides practical insights, contributing\nsignificantly to the community by enhancing the technological resilience of\nSMEs in integrating LLMs.\n","authors":["Jeremy Stephen Gabriel Yee","Pai Chet Ng","Zhengkui Wang","Ian McLoughlin","Aik Beng Ng","Simon See"],"pdf_url":"https://arxiv.org/pdf/2410.16070v2.pdf","comment":"9 pages, 1 figure. The work is supported by the SIT-NVIDIA Joint AI\n  Centre"},{"id":"http://arxiv.org/abs/2410.15978v2","updated":"2024-10-22T10:56:35Z","published":"2024-10-21T13:05:33Z","title":"PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs","summary":"  The growing volume of academic publications poses significant challenges for\nresearchers conducting timely and accurate Systematic Literature Reviews,\nparticularly in fast-evolving fields like artificial intelligence. This growth\nof academic literature also makes it increasingly difficult for lay people to\naccess scientific knowledge effectively, meaning academic literature is often\nmisrepresented in the popular press and, more broadly, in society. Traditional\nSLR methods are labor-intensive and error-prone, and they struggle to keep up\nwith the rapid pace of new research. To address these issues, we developed\n\\textit{PROMPTHEUS}: an AI-driven pipeline solution that automates the SLR\nprocess using Large Language Models. We aimed to enhance efficiency by reducing\nthe manual workload while maintaining the precision and coherence required for\ncomprehensive literature synthesis. PROMPTHEUS automates key stages of the SLR\nprocess, including systematic search, data extraction, topic modeling using\nBERTopic, and summarization with transformer models. Evaluations conducted\nacross five research domains demonstrate that PROMPTHEUS reduces review time,\nachieves high precision, and provides coherent topic organization, offering a\nscalable and effective solution for conducting literature reviews in an\nincreasingly crowded research landscape. In addition, such tools may reduce the\nincreasing mistrust in science by making summarization more accessible to\nlaypeople.\n  The code for this project can be found on the GitHub repository at\nhttps://github.com/joaopftorres/PROMPTHEUS.git\n","authors":["João Pedro Fernandes Torres","Catherine Mulligan","Joaquim Jorge","Catarina Moreira"],"pdf_url":"https://arxiv.org/pdf/2410.15978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15910v2","updated":"2024-10-22T05:06:36Z","published":"2024-10-21T11:33:14Z","title":"Diverse Policies Recovering via Pointwise Mutual Information Weighted\n  Imitation Learning","summary":"  Recovering a spectrum of diverse policies from a set of expert trajectories\nis an important research topic in imitation learning. After determining a\nlatent style for a trajectory, previous diverse policies recovering methods\nusually employ a vanilla behavioral cloning learning objective conditioned on\nthe latent style, treating each state-action pair in the trajectory with equal\nimportance. Based on an observation that in many scenarios, behavioral styles\nare often highly relevant with only a subset of state-action pairs, this paper\npresents a new principled method in diverse polices recovery. In particular,\nafter inferring or assigning a latent style for a trajectory, we enhance the\nvanilla behavioral cloning by incorporating a weighting mechanism based on\npointwise mutual information. This additional weighting reflects the\nsignificance of each state-action pair's contribution to learning the style,\nthus allowing our method to focus on state-action pairs most representative of\nthat style. We provide theoretical justifications for our new objective, and\nextensive empirical evaluations confirm the effectiveness of our method in\nrecovering diverse policies from expert data.\n","authors":["Hanlin Yang","Jian Yao","Weiming Liu","Qing Wang","Hanmin Qin","Hansheng Kong","Kirk Tang","Jiechao Xiong","Chao Yu","Kai Li","Junliang Xing","Hongwu Chen","Juchao Zhuo","Qiang Fu","Yang Wei","Haobo Fu"],"pdf_url":"https://arxiv.org/pdf/2410.15910v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.15859v2","updated":"2024-10-22T08:00:00Z","published":"2024-10-21T10:39:05Z","title":"Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs","summary":"  Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach.\n","authors":["Xin Ma","Yang Liu","Jingjing Liu","Xiaoxu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.15859v2.pdf","comment":"Accepted by NeurIPS 2024; 13 pages and 30 pages appendix"},{"id":"http://arxiv.org/abs/2410.15778v2","updated":"2024-10-22T05:01:28Z","published":"2024-10-21T08:42:30Z","title":"Reducing Hallucinations in Vision-Language Models via Latent Space\n  Steering","summary":"  Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.\n","authors":["Sheng Liu","Haotian Ye","Lei Xing","James Zou"],"pdf_url":"https://arxiv.org/pdf/2410.15778v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.15490v2","updated":"2024-10-22T07:46:35Z","published":"2024-10-20T20:07:36Z","title":"Dynamic Intelligence Assessment: Benchmarking LLMs on the Road to AGI\n  with a Focus on Model Confidence","summary":"  As machine intelligence evolves, the need to test and compare the\nproblem-solving abilities of different AI models grows. However, current\nbenchmarks are often overly simplistic, allowing models to perform uniformly\nwell, making it difficult to distinguish their capabilities. Additionally,\nbenchmarks typically rely on static question-answer pairs, which models might\nmemorize or guess. To address these limitations, we introduce the Dynamic\nIntelligence Assessment (DIA), a novel methodology for testing AI models using\ndynamic question templates and improved metrics across multiple disciplines\nsuch as mathematics, cryptography, cybersecurity, and computer science. The\naccompanying DIA-Bench dataset, which includes 150 diverse and challenging task\ntemplates with mutable parameters, is presented in various formats such as\ntext, PDFs, compiled binaries, and visual puzzles. Our framework introduces\nfour new metrics to assess a model's reliability and confidence across multiple\nattempts. These metrics revealed that even simple questions are frequently\nanswered incorrectly when posed in varying forms, highlighting significant gaps\nin models' reliability. Notably, models like GPT-4o tended to overestimate\ntheir mathematical abilities, while ChatGPT-4o demonstrated better\ndecision-making and performance through effective tool usage. We evaluated\neight state-of-the-art large language models (LLMs) using DIA-Bench, showing\nthat current models struggle with complex tasks and often display unexpectedly\nlow confidence, even with simpler questions. The DIA framework sets a new\nstandard for assessing not only problem-solving but also a model's adaptive\nintelligence and ability to assess its own limitations. The dataset is publicly\navailable on our project's website.\n","authors":["Norbert Tihanyi","Tamas Bisztray","Richard A. Dubniczky","Rebeka Toth","Bertalan Borsos","Bilel Cherif","Mohamed Amine Ferrag","Lajos Muzsai","Ridhi Jain","Ryan Marinelli","Lucas C. Cordeiro","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2410.15490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15472v2","updated":"2024-10-22T02:59:51Z","published":"2024-10-20T19:02:41Z","title":"Multi-Layer Feature Fusion with Cross-Channel Attention-Based U-Net for\n  Kidney Tumor Segmentation","summary":"  Renal tumors, especially renal cell carcinoma (RCC), show significant\nheterogeneity, posing challenges for diagnosis using radiology images such as\nMRI, echocardiograms, and CT scans. U-Net based deep learning techniques are\nemerging as a promising approach for automated medical image segmentation for\nminimally invasive diagnosis of renal tumors. However, current techniques need\nfurther improvements in accuracy to become clinically useful to radiologists.\nIn this study, we present an improved U-Net based model for end-to-end\nautomated semantic segmentation of CT scan images to identify renal tumors. The\nmodel uses residual connections across convolution layers, integrates a\nmulti-layer feature fusion (MFF) and cross-channel attention (CCA) within\nencoder blocks, and incorporates skip connections augmented with additional\ninformation derived using MFF and CCA. We evaluated our model on the KiTS19\ndataset, which contains data from 210 patients. For kidney segmentation, our\nmodel achieves a Dice Similarity Coefficient (DSC) of 0.97 and a Jaccard index\n(JI) of 0.95. For renal tumor segmentation, our model achieves a DSC of 0.96\nand a JI of 0.91. Based on a comparison of available DSC scores, our model\noutperforms the current leading models.\n","authors":["Fnu Neha","Arvind K. Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.15472v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.17250v1","updated":"2024-10-22T17:59:56Z","published":"2024-10-22T17:59:56Z","title":"JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding\n  Benchmark for Culture-aware Evaluation","summary":"  Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.\n","authors":["Shota Onohara","Atsuyuki Miyai","Yuki Imajuku","Kazuki Egashira","Jeonghun Baek","Xiang Yue","Graham Neubig","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2410.17250v1.pdf","comment":"Project page: https://mmmu-japanese-benchmark.github.io/JMMMU/"},{"id":"http://arxiv.org/abs/2410.17248v1","updated":"2024-10-22T17:59:55Z","published":"2024-10-22T17:59:55Z","title":"HyperspectralViTs: Fast and Accurate methane detection on-board\n  satellites","summary":"  On-board processing of hyperspectral data with machine learning models would\nenable unprecedented amount of autonomy for a wide range of tasks, for example\nmethane detection or mineral identification. Methane is the second most\nimportant greenhouse gas contributor to climate change, and it's automated\ndetection on-board of satellites using machine learning models would allow for\nearly warning system and could enable new capabilities such as automated\nscheduling inside constellations of satellites. Classical methods for methane\ndetection suffer from high false positive rates and previous deep learning\nmodels exhibit prohibitive computational requirements. We propose fast and\naccurate machine learning architectures which support end-to-end training with\ndata of high spectral dimension. We evaluate our models on two tasks related to\nhyperspectral data processing - methane leak detection and mineral\nidentification. With our proposed general architectures, we improve the F1\nscore of the previous methane detection state-of-the-art models by more than\n27% on a newly created synthetic dataset and by almost 13% on the previously\nreleased large benchmark dataset. We also demonstrate that training models on\nthe synthetic dataset improves performance of models finetuned on the dataset\nof real events by 6.9% in F1 score in contrast with training from scratch. On a\nnewly created dataset for mineral identification, our models provide 3.5%\nimprovement in the F1 score in contrast to the default versions of the models.\nWith our proposed models we improve the inference speed by 85.19% in contrast\nto previous classical and deep learning approaches by removing the dependency\non classically computed features. Namely, one capture from the EMIT sensor can\nbe processed in only 30 seconds on a realistic proxy hardware used on the\nION-SCV 004 satellite.\n","authors":["Vít Růžička","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2410.17248v1.pdf","comment":"13 pages, This work has been submitted for possible publication"},{"id":"http://arxiv.org/abs/2410.17246v1","updated":"2024-10-22T17:59:49Z","published":"2024-10-22T17:59:49Z","title":"Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile\n  Skins","summary":"  While visuomotor policy learning has advanced robotic manipulation, precisely\nexecuting contact-rich tasks remains challenging due to the limitations of\nvision in reasoning about physical interactions. To address this, recent work\nhas sought to integrate tactile sensing into policy learning. However, many\nexisting approaches rely on optical tactile sensors that are either restricted\nto recognition tasks or require complex dimensionality reduction steps for\npolicy learning. In this work, we explore learning policies with magnetic skin\nsensors, which are inherently low-dimensional, highly sensitive, and\ninexpensive to integrate with robotic platforms. To leverage these sensors\neffectively, we present the Visuo-Skin (ViSk) framework, a simple approach that\nuses a transformer-based policy and treats skin sensor data as additional\ntokens alongside visual information. Evaluated on four complex real-world tasks\ninvolving credit card swiping, plug insertion, USB insertion, and bookshelf\nretrieval, ViSk significantly outperforms both vision-only and optical tactile\nsensing based policies. Further analysis reveals that combining tactile and\nvisual modalities enhances policy performance and spatial generalization,\nachieving an average improvement of 27.5% across tasks.\nhttps://visuoskin.github.io/\n","authors":["Venkatesh Pattabiraman","Yifeng Cao","Siddhant Haldar","Lerrel Pinto","Raunaq Bhirangi"],"pdf_url":"https://arxiv.org/pdf/2410.17246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17245v1","updated":"2024-10-22T17:59:39Z","published":"2024-10-22T17:59:39Z","title":"Towards Reliable Evaluation of Behavior Steering Interventions in LLMs","summary":"  Representation engineering methods have recently shown promise for enabling\nefficient steering of model behavior. However, evaluation pipelines for these\nmethods have primarily relied on subjective demonstrations, instead of\nquantitative, objective metrics. We aim to take a step towards addressing this\nissue by advocating for four properties missing from current evaluations: (i)\ncontexts sufficiently similar to downstream tasks should be used for assessing\nintervention quality; (ii) model likelihoods should be accounted for; (iii)\nevaluations should allow for standardized comparisons across different target\nbehaviors; and (iv) baseline comparisons should be offered. We introduce an\nevaluation pipeline grounded in these criteria, offering both a quantitative\nand visual analysis of how effectively a given method works. We use this\npipeline to evaluate two representation engineering methods on how effectively\nthey can steer behaviors such as truthfulness and corrigibility, finding that\nsome interventions are less effective than previously reported.\n","authors":["Itamar Pres","Laura Ruis","Ekdeep Singh Lubana","David Krueger"],"pdf_url":"https://arxiv.org/pdf/2410.17245v1.pdf","comment":"Accepted to the NeurIPS 2024 - Workshop on Foundation Model\n  Interventions"},{"id":"http://arxiv.org/abs/2410.17238v1","updated":"2024-10-22T17:56:08Z","published":"2024-10-22T17:56:08Z","title":"SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning","summary":"  Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.\n","authors":["Yizhou Chi","Yizhang Lin","Sirui Hong","Duyi Pan","Yaying Fei","Guanghao Mei","Bangbang Liu","Tianqi Pang","Jacky Kwok","Ceyao Zhang","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17238v1.pdf","comment":"The code is available at https://github.com/geekan/MetaGPT"},{"id":"http://arxiv.org/abs/2410.17236v1","updated":"2024-10-22T17:54:45Z","published":"2024-10-22T17:54:45Z","title":"Large Language Models Empowered Personalized Web Agents","summary":"  Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB.\n","authors":["Hongru Cai","Yongqi Li","Wenjie Wang","Fengbin Zhu","Xiaoyu Shen","Wenjie Li","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.17236v1.pdf","comment":"The code and data are available on the project website\n  https://hongrucai.github.io/PersonalWAB/"},{"id":"http://arxiv.org/abs/2410.17233v1","updated":"2024-10-22T17:53:34Z","published":"2024-10-22T17:53:34Z","title":"Few-shot In-Context Preference Learning Using Large Language Models","summary":"  Designing reward functions is a core component of reinforcement learning but\ncan be challenging for truly complex behavior. Reinforcement Learning from\nHuman Feedback (RLHF) has been used to alleviate this challenge by replacing a\nhand-coded reward function with a reward function learned from preferences.\nHowever, it can be exceedingly inefficient to learn these rewards as they are\noften learned tabula rasa. We investigate whether Large Language Models (LLMs)\ncan reduce this query inefficiency by converting an iterative series of human\npreferences into code representing the rewards. We propose In-Context\nPreference Learning (ICPL), a method that uses the grounding of an LLM to\naccelerate learning reward functions from preferences. ICPL takes the\nenvironment context and task description, synthesizes a set of reward\nfunctions, and then repeatedly updates the reward functions using human\nrankings of videos of the resultant policies. Using synthetic preferences, we\ndemonstrate that ICPL is orders of magnitude more efficient than RLHF and is\neven competitive with methods that use ground-truth reward functions instead of\npreferences. Finally, we perform a series of human preference-learning trials\nand observe that ICPL extends beyond synthetic settings and can work\neffectively with humans-in-the-loop. Additional information and videos are\nprovided at https://sites.google.com/view/few-shot-icpl/home.\n","authors":["Chao Yu","Hong Lu","Jiaxuan Gao","Qixin Tan","Xinting Yang","Yu Wang","Yi Wu","Eugene Vinitsky"],"pdf_url":"https://arxiv.org/pdf/2410.17233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17229v1","updated":"2024-10-22T17:51:13Z","published":"2024-10-22T17:51:13Z","title":"Responsibility in a Multi-Value Strategic Setting","summary":"  Responsibility is a key notion in multi-agent systems and in creating safe,\nreliable and ethical AI. However, most previous work on responsibility has only\nconsidered responsibility for single outcomes. In this paper we present a model\nfor responsibility attribution in a multi-agent, multi-value setting. We also\nexpand our model to cover responsibility anticipation, demonstrating how\nconsiderations of responsibility can help an agent to select strategies that\nare in line with its values. In particular we show that non-dominated\nregret-minimising strategies reliably minimise an agent's expected degree of\nresponsibility.\n","authors":["Timothy Parker","Umberto Grandi","Emiliano Lorini"],"pdf_url":"https://arxiv.org/pdf/2410.17229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12883v2","updated":"2024-10-22T17:49:31Z","published":"2024-07-16T17:58:27Z","title":"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive\n  Retrieval","summary":"  Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. Our dataset consists of 1,384\nreal-world queries spanning diverse domains, such as economics, psychology,\nmathematics, and coding. These queries are drawn from naturally occurring and\ncarefully curated human data. Extensive evaluation reveals that even\nstate-of-the-art retrieval models perform poorly on BRIGHT. The leading model\non the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of\n59.0 nDCG@10, produces a score of nDCG@10 of 18.3 on BRIGHT. We show that\nincorporating explicit reasoning about the query improves retrieval performance\nby up to 12.2 points. Moreover, incorporating retrieved documents from the\ntop-performing retriever boosts question-answering performance by over 6.6\npoints. We believe that BRIGHT paves the way for future research on retrieval\nsystems in more realistic and challenging settings.\n","authors":["Hongjin Su","Howard Yen","Mengzhou Xia","Weijia Shi","Niklas Muennighoff","Han-yu Wang","Haisu Liu","Quan Shi","Zachary S. Siegel","Michael Tang","Ruoxi Sun","Jinsung Yoon","Sercan O. Arik","Danqi Chen","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2407.12883v2.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2410.12101v2","updated":"2024-10-22T17:48:56Z","published":"2024-10-15T22:52:45Z","title":"The Persian Rug: solving toy models of superposition using large-scale\n  symmetries","summary":"  We present a complete mechanistic description of the algorithm learned by a\nminimal non-linear sparse data autoencoder in the limit of large input\ndimension. The model, originally presented in arXiv:2209.10652, compresses\nsparse data vectors through a linear layer and decompresses using another\nlinear layer followed by a ReLU activation. We notice that when the data is\npermutation symmetric (no input feature is privileged) large models reliably\nlearn an algorithm that is sensitive to individual weights only through their\nlarge-scale statistics. For these models, the loss function becomes\nanalytically tractable. Using this understanding, we give the explicit scalings\nof the loss at high sparsity, and show that the model is near-optimal among\nrecently proposed architectures. In particular, changing or adding to the\nactivation function any elementwise or filtering operation can at best improve\nthe model's performance by a constant factor. Finally, we forward-engineer a\nmodel with the requisite symmetries and show that its loss precisely matches\nthat of the trained models. Unlike the trained model weights, the low\nrandomness in the artificial weights results in miraculous fractal structures\nresembling a Persian rug, to which the algorithm is oblivious. Our work\ncontributes to neural network interpretability by introducing techniques for\nunderstanding the structure of autoencoders. Code to reproduce our results can\nbe found at https://github.com/KfirD/PersianRug .\n","authors":["Aditya Cowsik","Kfir Dolev","Alex Infanger"],"pdf_url":"https://arxiv.org/pdf/2410.12101v2.pdf","comment":"Improved arguments, presentation. No changes to results"},{"id":"http://arxiv.org/abs/2410.17218v1","updated":"2024-10-22T17:43:39Z","published":"2024-10-22T17:43:39Z","title":"Creativity in AI: Progresses and Challenges","summary":"  Creativity is the ability to produce novel, useful, and surprising ideas, and\nhas been widely studied as a crucial aspect of human cognition. Machine\ncreativity on the other hand has been a long-standing challenge. With the rise\nof advanced generative AI, there has been renewed interest and debate regarding\nAI's creative capabilities. Therefore, it is imperative to revisit the state of\ncreativity in AI and identify key progresses and remaining challenges. In this\nwork, we survey leading works studying the creative capabilities of AI systems,\nfocusing on creative problem-solving, linguistic, artistic, and scientific\ncreativity. Our review suggests that while the latest AI models are largely\ncapable of producing linguistically and artistically creative outputs such as\npoems, images, and musical pieces, they struggle with tasks that require\ncreative problem-solving, abstract thinking and compositionality and their\ngenerations suffer from a lack of diversity, originality, long-range\nincoherence and hallucinations. We also discuss key questions concerning\ncopyright and authorship issues with generative models. Furthermore, we\nhighlight the need for a comprehensive evaluation of creativity that is\nprocess-driven and considers several dimensions of creativity. Finally, we\npropose future research directions to improve the creativity of AI outputs,\ndrawing inspiration from cognitive science and psychology.\n","authors":["Mete Ismayilzada","Debjit Paul","Antoine Bosselut","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2410.17218v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2410.17212v1","updated":"2024-10-22T17:37:18Z","published":"2024-10-22T17:37:18Z","title":"Neuroevolution Neural Architecture Search for Evolving RNNs in Stock\n  Return Prediction and Portfolio Trading","summary":"  Stock return forecasting is a major component of numerous finance\napplications. Predicted stock returns can be incorporated into portfolio\ntrading algorithms to make informed buy or sell decisions which can optimize\nreturns. In such portfolio trading applications, the predictive performance of\na time series forecasting model is crucial. In this work, we propose the use of\nthe Evolutionary eXploration of Augmenting Memory Models (EXAMM) algorithm to\nprogressively evolve recurrent neural networks (RNNs) for stock return\npredictions. RNNs are evolved independently for each stocks and portfolio\ntrading decisions are made based on the predicted stock returns. The portfolio\nused for testing consists of the 30 companies in the Dow-Jones Index (DJI) with\neach stock have the same weight. Results show that using these evolved RNNs and\na simple daily long-short strategy can generate higher returns than both the\nDJI index and the S&P 500 Index for both 2022 (bear market) and 2023 (bull\nmarket).\n","authors":["Zimeng Lyu","Amulya Saxena","Rohaan Nadeem","Hao Zhang","Travis Desell"],"pdf_url":"https://arxiv.org/pdf/2410.17212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17210v1","updated":"2024-10-22T17:34:59Z","published":"2024-10-22T17:34:59Z","title":"Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh\n  through Large Language Modeling","summary":"  Purpose: Bangladesh's legal system struggles with major challenges like\ndelays, complexity, high costs, and millions of unresolved cases, which deter\nmany from pursuing legal action due to lack of knowledge or financial\nconstraints. This research seeks to develop a specialized Large Language Model\n(LLM) to assist in the Bangladeshi legal system. Methods: We created\nUKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and\nscraping data on various legal acts. We fine-tuned the GPT-2 model on this\ndataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance\nin English. Results: The model was rigorously evaluated using semantic\nassessments, including case studies supported by expert opinions. The\nevaluation provided promising results, demonstrating the potential for the\nmodel to assist in legal matters within Bangladesh. Conclusion: Our work\nrepresents the first structured effort toward building an AI-based legal\nassistant for Bangladesh. While the results are encouraging, further\nrefinements are necessary to improve the model's accuracy, credibility, and\nsafety. This is a significant step toward creating a legal AI capable of\nserving the needs of a population of 180 million.\n","authors":["Azmine Toushik Wasi","Wahid Faisal","Mst Rafia Islam","Mahathir Mohammad Bappy"],"pdf_url":"https://arxiv.org/pdf/2410.17210v1.pdf","comment":"In Review"},{"id":"http://arxiv.org/abs/2308.02594v4","updated":"2024-10-22T17:29:53Z","published":"2023-08-03T21:08:51Z","title":"SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning\n  Agents","summary":"  Deep Reinforcement Learning (DRL) has made significant advancements in\nvarious fields, such as autonomous driving, healthcare, and robotics, by\nenabling agents to learn optimal policies through interactions with their\nenvironments. However, the application of DRL in safety-critical domains\npresents challenges, particularly concerning the safety of the learned\npolicies. DRL agents, which are focused on maximizing rewards, may select\nunsafe actions, leading to safety violations. Runtime safety monitoring is thus\nessential to ensure the safe operation of these agents, especially in\nunpredictable and dynamic environments. This paper introduces SMARLA, a\nblack-box safety monitoring approach specifically designed for DRL agents.\nSMARLA utilizes machine learning to predict safety violations by observing the\nagent's behavior during execution. The approach is based on Q-values, which\nreflect the expected reward for taking actions in specific states. SMARLA\nemploys state abstraction to reduce the complexity of the state space,\nenhancing the predictive capabilities of the monitoring model. Such abstraction\nenables the early detection of unsafe states, allowing for the implementation\nof corrective and preventive measures before incidents occur. We quantitatively\nand qualitatively validated SMARLA on three well-known case studies widely used\nin DRL research. Empirical results reveal that SMARLA is accurate at predicting\nsafety violations, with a low false positive rate, and can predict violations\nat an early stage, approximately halfway through the execution of the agent,\nbefore violations occur. We also discuss different decision criteria, based on\nconfidence intervals of the predicted violation probabilities, to trigger\nsafety mechanisms aiming at a trade-off between early detection and low false\npositive rates.\n","authors":["Amirhossein Zolfagharian","Manel Abdellatif","Lionel C. Briand","Ramesh S"],"pdf_url":"https://arxiv.org/pdf/2308.02594v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05669v2","updated":"2024-10-22T17:16:17Z","published":"2024-10-08T03:48:57Z","title":"ACPBench: Reasoning about Action, Change, and Planning","summary":"  There is an increasing body of work using Large Language Models (LLMs) as\nagents for orchestrating workflows and making decisions in domains that require\nplanning and multi-step reasoning. As a result, it is imperative to evaluate\nLLMs on core skills required for planning. In this work, we present ACPBench, a\nbenchmark for evaluating the reasoning tasks in the field of planning. The\nbenchmark consists of 7 reasoning tasks over 13 planning domains. The\ncollection is constructed from planning domains described in a formal language.\nThis allows us to synthesize problems with provably correct solutions across\nmany tasks and domains. Further, it allows us the luxury of scale without\nadditional human effort, i.e., many additional problems can be created\nautomatically. Our extensive evaluation of 22 LLMs and OpenAI o1 reasoning\nmodels highlights the significant gap in the reasoning capability of the LLMs.\nOur findings with OpenAI o1, a multi-turn reasoning model, reveal significant\ngains in performance on multiple-choice questions, yet surprisingly, no notable\nprogress is made on boolean questions.\n  The ACPBench collection is available at https://ibm.github.io/ACPBench.\n","authors":["Harsha Kokel","Michael Katz","Kavitha Srinivas","Shirin Sohrabi"],"pdf_url":"https://arxiv.org/pdf/2410.05669v2.pdf","comment":"Added OpenAI o1 results"},{"id":"http://arxiv.org/abs/2410.17196v1","updated":"2024-10-22T17:15:20Z","published":"2024-10-22T17:15:20Z","title":"VoiceBench: Benchmarking LLM-Based Voice Assistants","summary":"  Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.\n","authors":["Yiming Chen","Xianghu Yue","Chen Zhang","Xiaoxue Gao","Robby T. Tan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.17196v1.pdf","comment":"Work in progress. Data is available at\n  https://github.com/MatthewCYM/VoiceBench"},{"id":"http://arxiv.org/abs/2410.17195v1","updated":"2024-10-22T17:13:38Z","published":"2024-10-22T17:13:38Z","title":"Language Model Non-myopic Generation for Reasoning and Planning","summary":"  Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.\n","authors":["Chang Ma","Haiteng Zhao","Junlei Zhang","Junxian He","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.17195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17193v1","updated":"2024-10-22T17:13:19Z","published":"2024-10-22T17:13:19Z","title":"Emphasizing Discriminative Features for Dataset Distillation in Complex\n  Scenarios","summary":"  Dataset distillation has demonstrated strong performance on simple datasets\nlike CIFAR, MNIST, and TinyImageNet but struggles to achieve similar results in\nmore complex scenarios. In this paper, we propose EDF (emphasizes the\ndiscriminative features), a dataset distillation method that enhances key\ndiscriminative regions in synthetic images using Grad-CAM activation maps. Our\napproach is inspired by a key observation: in simple datasets, high-activation\nareas typically occupy most of the image, whereas in complex scenarios, the\nsize of these areas is much smaller. Unlike previous methods that treat all\npixels equally when synthesizing images, EDF uses Grad-CAM activation maps to\nenhance high-activation areas. From a supervision perspective, we downplay\nsupervision signals that have lower losses, as they contain common patterns.\nAdditionally, to help the DD community better explore complex scenarios, we\nbuild the Complex Dataset Distillation (Comp-DD) benchmark by meticulously\nselecting sixteen subsets, eight easy and eight hard, from ImageNet-1K. In\nparticular, EDF consistently outperforms SOTA results in complex scenarios,\nsuch as ImageNet-1K subsets. Hopefully, more researchers will be inspired and\nencouraged to improve the practicality and efficacy of DD. Our code and\nbenchmark will be made public at https://github.com/NUS-HPC-AI-Lab/EDF.\n","authors":["Kai Wang","Zekai Li","Zhi-Qi Cheng","Samir Khaki","Ahmad Sajedi","Ramakrishna Vedantam","Konstantinos N Plataniotis","Alexander Hauptmann","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.17193v1.pdf","comment":"24 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.17186v1","updated":"2024-10-22T17:07:26Z","published":"2024-10-22T17:07:26Z","title":"DyPNIPP: Predicting Environment Dynamics for RL-based Robust Informative\n  Path Planning","summary":"  Informative path planning (IPP) is an important planning paradigm for various\nreal-world robotic applications such as environment monitoring. IPP involves\nplanning a path that can learn an accurate belief of the quantity of interest,\nwhile adhering to planning constraints. Traditional IPP methods typically\nrequire high computation time during execution, giving rise to reinforcement\nlearning (RL) based IPP methods. However, the existing RL-based methods do not\nconsider spatio-temporal environments which involve their own challenges due to\nvariations in environment characteristics. In this paper, we propose DyPNIPP, a\nrobust RL-based IPP framework, designed to operate effectively across\nspatio-temporal environments with varying dynamics. To achieve this, DyPNIPP\nincorporates domain randomization to train the agent across diverse\nenvironments and introduces a dynamics prediction model to capture and adapt\nthe agent actions to specific environment dynamics. Our extensive experiments\nin a wildfire environment demonstrate that DyPNIPP outperforms existing\nRL-based IPP algorithms by significantly improving robustness and performing\nacross diverse environment conditions.\n","authors":["Srujan Deolasee","Siva Kailas","Wenhao Luo","Katia Sycara","Woojun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17186v1.pdf","comment":"8 pages, 4 figures, submitted to IEEE RA-L"},{"id":"http://arxiv.org/abs/2409.13686v2","updated":"2024-10-22T17:06:17Z","published":"2024-09-20T17:54:16Z","title":"The Impact of Large Language Models in Academia: from Writing to\n  Speaking","summary":"  Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.\n","authors":["Mingmeng Geng","Caixi Chen","Yanru Wu","Dongping Chen","Yao Wan","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.13686v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2407.14344v2","updated":"2024-10-22T16:59:12Z","published":"2024-07-19T14:28:07Z","title":"LLMs left, right, and center: Assessing GPT's capabilities to label\n  political bias from web domains","summary":"  This research investigates whether OpenAI's GPT-4, a state-of-the-art large\nlanguage model, can accurately classify the political bias of news sources\nbased solely on their URLs. Given the subjective nature of political labels,\nthird-party bias ratings like those from Ad Fontes Media, AllSides, and Media\nBias/Fact Check (MBFC) are often used in research to analyze news source\ndiversity. This study aims to determine if GPT-4 can replicate these human\nratings on a seven-degree scale (\"far-left\" to \"far-right\"). The analysis\ncompares GPT-4's classifications against MBFC's, and controls for website\npopularity using Open PageRank scores. Findings reveal a high correlation\n($\\text{Spearman's } \\rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and\nMBFC's ratings, indicating the model's potential reliability. However, GPT-4\nabstained from classifying approximately $\\frac{2}{3}$ of the dataset. It is\nmore likely to abstain from rating unpopular websites, which also suffer from\nless accurate assessments. The LLM tends to avoid classifying sources that MBFC\nconsiders to be centrist, resulting in more polarized outputs. Finally, this\nanalysis shows a slight leftward skew in GPT's classifications compared to\nMBFC's. Therefore, while this paper suggests that while GPT-4 can be a\nscalable, cost-effective tool for political bias classification of news\nwebsites, its use should be as a complement to human judgment to mitigate\nbiases.\n","authors":["Raphael Hernandes","Giulio Corsi"],"pdf_url":"https://arxiv.org/pdf/2407.14344v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.17379v2","updated":"2024-10-22T16:58:31Z","published":"2024-08-30T16:15:28Z","title":"EMPOWER: Embodied Multi-role Open-vocabulary Planning with Online\n  Grounding and Execution","summary":"  Task planning for robots in real-life settings presents significant\nchallenges. These challenges stem from three primary issues: the difficulty in\nidentifying grounded sequences of steps to achieve a goal; the lack of a\nstandardized mapping between high-level actions and low-level commands; and the\nchallenge of maintaining low computational overhead given the limited resources\nof robotic hardware. We introduce EMPOWER, a framework designed for\nopen-vocabulary online grounding and planning for embodied agents aimed at\naddressing these issues. By leveraging efficient pre-trained foundation models\nand a multi-role mechanism, EMPOWER demonstrates notable improvements in\ngrounded planning and execution. Quantitative results highlight the\neffectiveness of our approach, achieving an average success rate of 0.73 across\nsix different real-life scenarios using a TIAGo robot.\n","authors":["Francesco Argenziano","Michele Brienza","Vincenzo Suriani","Daniele Nardi","Domenico D. Bloisi"],"pdf_url":"https://arxiv.org/pdf/2408.17379v2.pdf","comment":"Accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2410.17172v1","updated":"2024-10-22T16:50:34Z","published":"2024-10-22T16:50:34Z","title":"KANICE: Kolmogorov-Arnold Networks with Interactive Convolutional\n  Elements","summary":"  We introduce KANICE (Kolmogorov-Arnold Networks with Interactive\nConvolutional Elements), a novel neural architecture that combines\nConvolutional Neural Networks (CNNs) with Kolmogorov-Arnold Network (KAN)\nprinciples. KANICE integrates Interactive Convolutional Blocks (ICBs) and KAN\nlinear layers into a CNN framework. This leverages KANs' universal\napproximation capabilities and ICBs' adaptive feature learning. KANICE captures\ncomplex, non-linear data relationships while enabling dynamic,\ncontext-dependent feature extraction based on the Kolmogorov-Arnold\nrepresentation theorem. We evaluated KANICE on four datasets: MNIST,\nFashion-MNIST, EMNIST, and SVHN, comparing it against standard CNNs, CNN-KAN\nhybrids, and ICB variants. KANICE consistently outperformed baseline models,\nachieving 99.35% accuracy on MNIST and 90.05% on the SVHN dataset.\n  Furthermore, we introduce KANICE-mini, a compact variant designed for\nefficiency. A comprehensive ablation study demonstrates that KANICE-mini\nachieves comparable performance to KANICE with significantly fewer parameters.\nKANICE-mini reached 90.00% accuracy on SVHN with 2,337,828 parameters, compared\nto KANICE's 25,432,000. This study highlights the potential of KAN-based\narchitectures in balancing performance and computational efficiency in image\nclassification tasks. Our work contributes to research in adaptive neural\nnetworks, integrates mathematical theorems into deep learning architectures,\nand explores the trade-offs between model complexity and performance, advancing\ncomputer vision and pattern recognition. The source code for this paper is\npublicly accessible through our GitHub repository\n(https://github.com/m-ferdaus/kanice).\n","authors":["Md Meftahul Ferdaus","Mahdi Abdelguerfi","Elias Ioup","David Dobson","Kendall N. Niles","Ken Pathak","Steven Sloan"],"pdf_url":"https://arxiv.org/pdf/2410.17172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17173v1","updated":"2024-10-22T16:50:34Z","published":"2024-10-22T16:50:34Z","title":"Reinforcement learning on structure-conditioned categorical diffusion\n  for protein inverse folding","summary":"  Protein inverse folding-that is, predicting an amino acid sequence that will\nfold into the desired 3D structure-is an important problem for structure-based\nprotein design. Machine learning based methods for inverse folding typically\nuse recovery of the original sequence as the optimization objective. However,\ninverse folding is a one-to-many problem where several sequences can fold to\nthe same structure. Moreover, for many practical applications, it is often\ndesirable to have multiple, diverse sequences that fold into the target\nstructure since it allows for more candidate sequences for downstream\noptimizations. Here, we demonstrate that although recent inverse folding\nmethods show increased sequence recovery, their \"foldable diversity\"-i.e. their\nability to generate multiple non-similar sequences that fold into the\nstructures consistent with the target-does not increase. To address this, we\npresent RL-DIF, a categorical diffusion model for inverse folding that is\npre-trained on sequence recovery and tuned via reinforcement learning on\nstructural consistency. We find that RL-DIF achieves comparable sequence\nrecovery and structural consistency to benchmark models but shows greater\nfoldable diversity: experiments show RL-DIF can achieve an foldable diversity\nof 29% on CATH 4.2, compared to 23% from models trained on the same dataset.\nThe PyTorch model weights and sampling code are available on GitHub.\n","authors":["Yasha Ektefaie","Olivia Viessmann","Siddharth Narayanan","Drew Dresser","J. Mark Kim","Armen Mkrtchyan"],"pdf_url":"https://arxiv.org/pdf/2410.17173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17160v1","updated":"2024-10-22T16:34:24Z","published":"2024-10-22T16:34:24Z","title":"Layered LA-MAPF: a decomposition of large agent MAPF instance to\n  accelerate solving without compromising solvability","summary":"  Multi-Agent Path Finding (MAPF) has been widely studied in recent years.\nHowever, most existing MAPF algorithms assume that an agent occupies only a\nsingle grid in a grid-based map. This assumption limits their applicability in\nmany real-world domains where agents have geometric shapes, rather than being\npoint-like. Such agents, which can occupy multiple cells simultaneously, are\nreferred to as ``large'' agents. When considering the shape and size of agents\nin MAPF, the computational complexity increases significantly as the number of\nagents grows, primarily due to the increased overhead in conflict detection\nbetween geometric agents. In this paper, we propose two types of subproblems\nfor the LA-MAPF (Large-Agent MAPF) problem: \\textbf{cluster} (which has no\nconstraints on the order of solution) and \\textbf{level} (which imposes\nconstraints on the solution order). We introduce \\textbf{Layered LA-MAPF}, a\nmethod that decomposes a MAPF instance involving geometric agents into\nclusters, and then further decomposes each cluster into levels. This approach\naims to reduce time complexity when solving LA-MAPF problems. Our results\ndemonstrate the performance of our method as the number of agents increases\nacross various maps, and how it accelerates LA-MAPF methods, such as LA-CBS and\nLA-LaCAM. Experiments show that our LA-MAPF method with instance decomposition\n\\textbf{halves the time cost (reducing from an average of 40s to 20s) and\ntriples the success rate (from an average of 0.27 to 0.80)} in finding a\nsolution within 60 seconds. To facilitate further research, we have made the\nsource code for Layered LA-MAPF publicly available at\n\\url{https://github.com/JoeYao-bit/LayeredMAPF/algorithm/LA-MAPF}.\n","authors":["Zhuo Yao"],"pdf_url":"https://arxiv.org/pdf/2410.17160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02581v3","updated":"2024-10-22T16:26:40Z","published":"2024-10-03T15:25:37Z","title":"Boosting Sample Efficiency and Generalization in Multi-agent\n  Reinforcement Learning via Equivariance","summary":"  Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency\nand poor generalization [1]. These challenges are partially due to a lack of\nstructure or inductive bias in the neural networks typically used in learning\nthe policy. One such form of structure that is commonly observed in multi-agent\nscenarios is symmetry. The field of Geometric Deep Learning has developed\nEquivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to\nrotations, translations, and reflections of nodes. Incorporating equivariance\nhas been shown to improve learning efficiency and decrease error [ 2 ]. In this\npaper, we demonstrate that EGNNs improve the sample efficiency and\ngeneralization in MARL. However, we also show that a naive application of EGNNs\nto MARL results in poor early exploration due to a bias in the EGNN structure.\nTo mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural\nNetworks or E2GN2. We compare E2GN2 to other common function approximators\nusing common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant\nimprovement in sample efficiency, greater final reward convergence, and a 2x-5x\ngain in over standard GNNs in our generalization tests. These results pave the\nway for more reliable and effective solutions in complex multi-agent systems.\n","authors":["Joshua McClellan","Naveed Haghani","John Winder","Furong Huang","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2410.02581v3.pdf","comment":"accepted as a poster at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17145v1","updated":"2024-10-22T16:26:03Z","published":"2024-10-22T16:26:03Z","title":"Can General-Purpose Large Language Models Generalize to English-Thai\n  Machine Translation ?","summary":"  Large language models (LLMs) perform well on common tasks but struggle with\ngeneralization in low-resource and low-computation settings. We examine this\nlimitation by testing various LLMs and specialized translation models on\nEnglish-Thai machine translation and code-switching datasets. Our findings\nreveal that under more strict computational constraints, such as 4-bit\nquantization, LLMs fail to translate effectively. In contrast, specialized\nmodels, with comparable or lower computational requirements, consistently\noutperform LLMs. This underscores the importance of specialized models for\nmaintaining performance under resource constraints.\n","authors":["Jirat Chiaranaipanich","Naiyarat Hanmatheekuna","Jitkapat Sawatphol","Krittamate Tiankanon","Jiramet Kinchagawat","Amrest Chinkamol","Parinthapat Pengpun","Piyalitt Ittichaiwong","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2410.17145v1.pdf","comment":"Accepted in GenBench EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17141v1","updated":"2024-10-22T16:18:41Z","published":"2024-10-22T16:18:41Z","title":"Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements","summary":"  Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models.\n","authors":["Isamu Isozaki","Manil Shrestha","Rick Console","Edward Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17141v1.pdf","comment":"Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures"},{"id":"http://arxiv.org/abs/2410.17139v1","updated":"2024-10-22T16:10:10Z","published":"2024-10-22T16:10:10Z","title":"Trustworthy XAI and Application","summary":"  One of today's most significant and transformative technologies is the\nrapidly developing field of artificial intelligence (AI). Deined as a computer\nsystem that simulates human cognitive processes, AI is present in many aspects\nof our daily lives, from the self-driving cars on the road to the intelligence\n(AI) because some AI systems are so complex and opaque. With millions of\nparameters and layers, these system-deep neural networks in particular-make it\ndifficult for humans to comprehend accountability, prejudice, and justice are\nraised by the opaqueness of its decision-making process. AI has a lot of\npotential, but it also comes with a lot of difficulties and moral dilemmas. In\nthe context of explainable artificial intelligence (XAI), trust is crucial as\nit ensures that AI systems behave consistently, fairly, and ethically. In the\npresent article, we explore XAI, reliable XAI, and several practical uses for\nreliable XAI. Once more, we go over the three main components-transparency,\nexplainability, and trustworthiness of XAI-that we determined are pertinent in\nthis situation. We present an overview of recent scientific studies that employ\ntrustworthy XAI in various application fields. In the end, trustworthiness is\ncrucial for establishing and maintaining trust between humans and AI systems,\nfacilitating the integration of AI systems into various applications and\ndomains for the benefit of society.\n","authors":["MD Abdullah Al Nasim","Parag Biswas","Abdur Rashid","Angona Biswas","Kishor Datta Gupta"],"pdf_url":"https://arxiv.org/pdf/2410.17139v1.pdf","comment":"28 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.17126v1","updated":"2024-10-22T15:59:58Z","published":"2024-10-22T15:59:58Z","title":"Exploring RL-based LLM Training for Formal Language Tasks with\n  Programmed Rewards","summary":"  Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning\nfrom Human Feedback to align large language models (LLMs) with downstream\ntasks. This paper investigates the feasibility of using PPO for direct\nreinforcement learning (RL) from explicitly programmed reward signals, as\nopposed to indirect learning from human feedback via an intermediary reward\nmodel. We focus on tasks expressed through formal languages, such as\nmathematics and programming, where explicit reward functions can be programmed\nto automatically assess the quality of generated outputs. We apply this\napproach to a sentiment alignment task, a simple arithmetic task, and a more\ncomplex game synthesis task. The sentiment alignment task replicates prior\nresearch and serves to validate our experimental setup. Our results show that\npure RL-based training for the two formal language tasks is challenging, with\nsuccess being limited even for the simple arithmetic task. We propose a novel\nbatch-entropy regularization term to aid exploration, although training is not\nyet entirely stable. Our findings suggest that direct RL training of LLMs may\nbe more suitable for relatively minor changes, such as alignment, than for\nlearning new tasks altogether, even if an informative reward signal can be\nexpressed programmatically.\n","authors":["Alexander G. Padula","Dennis J. N. J. Soemers"],"pdf_url":"https://arxiv.org/pdf/2410.17126v1.pdf","comment":"Accepted at BNAIC 2024"},{"id":"http://arxiv.org/abs/2410.17124v1","updated":"2024-10-22T15:59:07Z","published":"2024-10-22T15:59:07Z","title":"Automated neuroradiological support systems for multiple cerebrovascular\n  disease markers -- A systematic review and meta-analysis","summary":"  Cerebrovascular diseases (CVD) can lead to stroke and dementia. Stroke is the\nsecond leading cause of death world wide and dementia incidence is increasing\nby the year. There are several markers of CVD that are visible on brain\nimaging, including: white matter hyperintensities (WMH), acute and chronic\nischaemic stroke lesions (ISL), lacunes, enlarged perivascular spaces (PVS),\nacute and chronic haemorrhagic lesions, and cerebral microbleeds (CMB). Brain\natrophy also occurs in CVD. These markers are important for patient management\nand intervention, since they indicate elevated risk of future stroke and\ndementia. We systematically reviewed automated systems designed to support\nradiologists reporting on these CVD imaging findings. We considered\ncommercially available software and research publications which identify at\nleast two CVD markers. In total, we included 29 commercial products and 13\nresearch publications. Two distinct types of commercial support system were\navailable: those which identify acute stroke lesions (haemorrhagic and\nischaemic) from computed tomography (CT) scans, mainly for the purpose of\npatient triage; and those which measure WMH and atrophy regionally and\nlongitudinally. In research, WMH and ISL were the markers most frequently\nanalysed together, from magnetic resonance imaging (MRI) scans; lacunes and PVS\nwere each targeted only twice and CMB only once. For stroke, commercially\navailable systems largely support the emergency setting, whilst research\nsystems consider also follow-up and routine scans. The systems to quantify WMH\nand atrophy are focused on neurodegenerative disease support, where these CVD\nmarkers are also of significance. There are currently no openly validated\nsystems, commercially, or in research, performing a comprehensive joint\nanalysis of all CVD markers (WMH, ISL, lacunes, PVS, haemorrhagic lesions, CMB,\nand atrophy).\n","authors":["Jesse Phitidis","Alison Q. O'Neil","William N. Whiteley","Beatrice Alex","Joanna M. Wardlaw","Miguel O. Bernabeu","Maria Valdés Hernández"],"pdf_url":"https://arxiv.org/pdf/2410.17124v1.pdf","comment":"62 pages, 10 figures"},{"id":"http://arxiv.org/abs/2312.10219v2","updated":"2024-10-22T15:52:15Z","published":"2023-12-15T21:31:30Z","title":"The Complexity of Optimizing Atomic Congestion","summary":"  Atomic congestion games are a classic topic in network design, routing, and\nalgorithmic game theory, and are capable of modeling congestion and flow\noptimization tasks in various application areas. While both the price of\nanarchy for such games as well as the computational complexity of computing\ntheir Nash equilibria are by now well-understood, the computational complexity\nof computing a system-optimal set of strategies -- that is, a centrally planned\nrouting that minimizes the average cost of agents -- is severely understudied\nin the literature. We close this gap by identifying the exact boundaries of\ntractability for the problem through the lens of the parameterized complexity\nparadigm. After showing that the problem remains highly intractable even on\nextremely simple networks, we obtain a set of results which demonstrate that\nthe structural parameters which control the computational (in)tractability of\nthe problem are not vertex-separator based in nature (such as, e.g.,\ntreewidth), but rather based on edge separators. We conclude by extending our\nanalysis towards the (even more challenging) min-max variant of the problem.\n","authors":["Cornelius Brand","Robert Ganian","Subrahmanyam Kalyanasundaram","Fionn Mc Inerney"],"pdf_url":"https://arxiv.org/pdf/2312.10219v2.pdf","comment":"Short version appeared at AAAI 2024. Long version accepted in the\n  Journal of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2404.01596v3","updated":"2024-10-22T15:47:12Z","published":"2024-04-02T02:36:31Z","title":"PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction\n  in Off-road Driving","summary":"  Motion prediction is critical for autonomous off-road driving, however, it\npresents significantly more challenges than on-road driving because of the\ncomplex interaction between the vehicle and the terrain. Traditional\nphysics-based approaches encounter difficulties in accurately modeling dynamic\nsystems and external disturbance. In contrast, data-driven neural networks\nrequire extensive datasets and struggle with explicitly capturing the\nfundamental physical laws, which can easily lead to poor generalization. By\nmerging the advantages of both methods, neuro-symbolic approaches present a\npromising direction. These methods embed physical laws into neural models,\npotentially significantly improving generalization capabilities. However, no\nprior works were evaluated in real-world settings for off-road driving. To\nbridge this gap, we present PhysORD, a neural-symbolic approach integrating the\nconservation law, i.e., the Euler-Lagrange equation, into data-driven neural\nmodels for motion prediction in off-road driving. Our experiments showed that\nPhysORD can accurately predict vehicle motion and tolerate external disturbance\nby modeling uncertainties. The learned dynamics model achieves 46.7% higher\naccuracy using only 3.1% of the parameters compared to data-driven methods,\ndemonstrating the data efficiency and superior generalization ability of our\nneural-symbolic method.\n","authors":["Zhipeng Zhao","Bowen Li","Yi Du","Taimeng Fu","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2404.01596v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17111v1","updated":"2024-10-22T15:36:04Z","published":"2024-10-22T15:36:04Z","title":"Permutation Picture of Graph Combinatorial Optimization Problems","summary":"  This paper proposes a framework that formulates a wide range of graph\ncombinatorial optimization problems using permutation-based representations.\nThese problems include the travelling salesman problem, maximum independent\nset, maximum cut, and various other related problems. This work potentially\nopens up new avenues for algorithm design in neural combinatorial optimization,\nbridging the gap between discrete and continuous optimization techniques.\n","authors":["Yimeng Min"],"pdf_url":"https://arxiv.org/pdf/2410.17111v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.14516v2","updated":"2024-10-22T15:20:00Z","published":"2024-10-18T14:55:14Z","title":"Do LLMs \"know\" internally when they follow instructions?","summary":"  Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.\n","authors":["Juyeon Heo","Christina Heinze-Deml","Oussama Elachqar","Shirley Ren","Udhay Nallasamy","Andy Miller","Kwan Ho Ryan Chan","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14582v2","updated":"2024-10-22T15:16:14Z","published":"2024-10-18T16:32:10Z","title":"Do LLMs estimate uncertainty well in instruction-following?","summary":"  Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.\n","authors":["Juyeon Heo","Miao Xiong","Christina Heinze-Deml","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17088v1","updated":"2024-10-22T15:14:54Z","published":"2024-10-22T15:14:54Z","title":"Science Out of Its Ivory Tower: Improving Accessibility with\n  Reinforcement Learning","summary":"  A vast amount of scholarly work is published daily, yet much of it remains\ninaccessible to the general public due to dense jargon and complex language. To\naddress this challenge in science communication, we introduce a reinforcement\nlearning framework that fine-tunes a language model to rewrite scholarly\nabstracts into more comprehensible versions. Guided by a carefully balanced\ncombination of word- and sentence-level accessibility rewards, our language\nmodel effectively substitutes technical terms with more accessible\nalternatives, a task which models supervised fine-tuned or guided by\nconventional readability measures struggle to accomplish. Our best model\nadjusts the readability level of scholarly abstracts by approximately six U.S.\ngrade levels -- in other words, from a postgraduate to a high school level.\nThis translates to roughly a 90% relative boost over the supervised fine-tuning\nbaseline, all while maintaining factual accuracy and high-quality language. An\nin-depth analysis of our approach shows that balanced rewards lead to\nsystematic modifications in the base model, likely contributing to smoother\noptimization and superior performance. We envision this work as a step toward\nbridging the gap between scholarly research and the general public,\nparticularly younger readers and those without a college degree.\n","authors":["Haining Wang","Jason Clark","Hannah McKelvey","Leila Sterman","Zheng Gao","Zuoyu Tian","Sandra Kübler","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16264v3","updated":"2024-10-22T15:09:58Z","published":"2024-06-24T02:03:57Z","title":"One Thousand and One Pairs: A \"novel\" challenge for long-context\n  language models","summary":"  Synthetic long-context LLM benchmarks (e.g., \"needle-in-the-haystack\") test\nonly surface-level retrieval capabilities, but how well can long-context LLMs\nretrieve, synthesize, and reason over information across book-length inputs? We\naddress this question by creating NoCha, a dataset of 1,001 minimally different\npairs of true and false claims about 67 recently-published English fictional\nbooks, written by human readers of those books. In contrast to existing\nlong-context benchmarks, our annotators confirm that the largest share of pairs\nin NoCha require global reasoning over the entire book to verify. Our\nexperiments show that while human readers easily perform this task, it is\nenormously challenging for all ten long-context LLMs that we evaluate: no\nopen-weight model performs above random chance (despite their strong\nperformance on synthetic benchmarks), while GPT-4o achieves the highest\naccuracy at 55.8%. Further analysis reveals that (1) on average, models perform\nmuch better on pairs that require only sentence-level retrieval vs. global\nreasoning; (2) model-generated explanations for their decisions are often\ninaccurate even for correctly-labeled claims; and (3) models perform\nsubstantially worse on speculative fiction books that contain extensive\nworld-building. The methodology proposed in NoCha allows for the evolution of\nthe benchmark dataset and the easy analysis of future models.\n","authors":["Marzena Karpinska","Katherine Thai","Kyle Lo","Tanya Goyal","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2406.16264v3.pdf","comment":"EMNLP 2024, camera ready"},{"id":"http://arxiv.org/abs/2410.15028v2","updated":"2024-10-22T14:55:54Z","published":"2024-10-19T07:59:10Z","title":"A Novel Reinforcement Learning Model for Post-Incident Malware\n  Investigations","summary":"  This Research proposes a Novel Reinforcement Learning (RL) model to optimise\nmalware forensics investigation during cyber incident response. It aims to\nimprove forensic investigation efficiency by reducing false negatives and\nadapting current practices to evolving malware signatures. The proposed RL\nframework leverages techniques such as Q-learning and the Markov Decision\nProcess (MDP) to train the system to identify malware patterns in live memory\ndumps, thereby automating forensic tasks. The RL model is based on a detailed\nmalware workflow diagram that guides the analysis of malware artefacts using\nstatic and behavioural techniques as well as machine learning algorithms.\nFurthermore, it seeks to address challenges in the UK justice system by\nensuring the accuracy of forensic evidence. We conduct testing and evaluation\nin controlled environments, using datasets created with Windows operating\nsystems to simulate malware infections. The experimental results demonstrate\nthat RL improves malware detection rates compared to conventional methods, with\nthe RL model's performance varying depending on the complexity and learning\nrate of the environment. The study concludes that while RL offers promising\npotential for automating malware forensics, its efficacy across diverse malware\ntypes requires ongoing refinement of reward systems and feature extraction\nmethods.\n","authors":["Dipo Dunsin","Mohamed Chahine Ghanem","Karim Ouazzane","Vassil Vassilev"],"pdf_url":"https://arxiv.org/pdf/2410.15028v2.pdf","comment":"8 pages. arXiv admin note: substantial text overlap with\n  arXiv:2408.01999"},{"id":"http://arxiv.org/abs/2407.05180v2","updated":"2024-10-22T14:54:42Z","published":"2024-04-22T10:33:06Z","title":"ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in\n  Robotic Surgical Skill Assessment","summary":"  In surgical skill assessment, Objective Structured Assessments of Technical\nSkills (OSATS scores) and the Global Rating Scale (GRS) are established tools\nfor evaluating the performance of surgeons during training. These metrics,\ncoupled with feedback on their performance, enable surgeons to improve and\nachieve standards of practice. Recent studies on the open-source dataset\nJIGSAW, which contains both GRS and OSATS labels, have focused on regressing\nGRS scores from kinematic signals, video data, or a combination of both. In\nthis paper, we argue that regressing the GRS score, a unitless value, by itself\nis too restrictive, and variations throughout the surgical trial do not hold\nsignificant clinical meaning. To address this gap, we developed a recurrent\ntransformer model that outputs the surgeon's performance throughout their\ntraining session by relating the model's hidden states to five OSATS scores\nderived from kinematic signals. These scores are averaged and aggregated to\nproduce a GRS prediction, enabling assessment of the model's performance\nagainst the state-of-the-art (SOTA). We report Spearman's Correlation\nCoefficient (SCC), demonstrating that our model outperforms SOTA models for all\ntasks, except for Suturing under the leave-one-subject-out (LOSO) scheme (SCC\n0.68-0.89), while achieving comparable performance for suturing and across\ntasks under the leave-one-user-out (LOUO) scheme (SCC 0.45-0.68) and beating\nSOTA for Needle Passing (0.69). We argue that relating final OSATS scores to\nshort instances throughout a surgeon's procedure is more clinically meaningful\nthan a single GRS score. This approach also allows us to translate quantitative\npredictions into qualitative feedback, which is crucial for any automated\nsurgical skill assessment pipeline. A senior surgeon validated our model's\nbehaviour and agreed with the semi-supervised predictions 77 \\% (p = 0.006) of\nthe time.\n","authors":["Julien Quarez","Matthew Elliot","Oscar Maccormac","Marc Modat","Sebastien Ourselin","Jonathan Shapey","Alejandro Granados"],"pdf_url":"https://arxiv.org/pdf/2407.05180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17050v1","updated":"2024-10-22T14:30:03Z","published":"2024-10-22T14:30:03Z","title":"UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs","summary":"  The key components of machine learning are data samples for training, model\nfor learning patterns, and loss function for optimizing accuracy. Analogously,\nunlearning can potentially be achieved through anti-data samples (or\nanti-samples), unlearning method, and reversed loss function. While prior\nresearch has explored unlearning methods and reversed loss functions, the\npotential of anti-samples remains largely untapped. In this paper, we introduce\nUnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language\nmodels (LLMs). Our contributions are threefold; first, we propose a novel\nconcept of anti-sample-induced unlearning; second, we generate anti-samples by\nleveraging misleading rationales, which help reverse learned associations and\naccelerate the unlearning process; and third, we enable fine-grained targeted\nunlearning, allowing for the selective removal of specific associations without\nimpacting related knowledge - something not achievable by previous works.\nResults demonstrate that anti-samples offer an efficient, targeted unlearning\nstrategy for LLMs, opening new avenues for privacy-preserving machine learning\nand model modification.\n","authors":["Yash Sinha","Murari Mandal","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2410.17050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17049v1","updated":"2024-10-22T14:27:43Z","published":"2024-10-22T14:27:43Z","title":"A Comparison of Baseline Models and a Transformer Network for SOC\n  Prediction in Lithium-Ion Batteries","summary":"  Accurately predicting the state of charge of Lithium-ion batteries is\nessential to the performance of battery management systems of electric\nvehicles. One of the main reasons for the slow global adoption of electric cars\nis driving range anxiety. The ability of a battery management system to\naccurately estimate the state of charge can help alleviate this problem. In\nthis paper, a comparison between data-driven state-of-charge estimation methods\nis conducted. The paper compares different neural network-based models and\ncommon regression models for SOC estimation. These models include several\nablated transformer networks, a neural network, a lasso regression model, a\nlinear regression model and a decision tree. Results of various experiments\nconducted on data obtained from natural driving cycles of the BMW i3 battery\nshow that the decision tree outperformed all other models including the more\ncomplex transformer network with self-attention and positional encoding.\n","authors":["Hadeel Aboueidah","Abdulrahman Altahhan"],"pdf_url":"https://arxiv.org/pdf/2410.17049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17042v1","updated":"2024-10-22T14:16:49Z","published":"2024-10-22T14:16:49Z","title":"Deep Memory Search: A Metaheuristic Approach for Optimizing Heuristic\n  Search","summary":"  Metaheuristic search methods have proven to be essential tools for tackling\ncomplex optimization challenges, but their full potential is often constrained\nby conventional algorithmic frameworks. In this paper, we introduce a novel\napproach called Deep Heuristic Search (DHS), which models metaheuristic search\nas a memory-driven process. DHS employs multiple search layers and memory-based\nexploration-exploitation mechanisms to navigate large, dynamic search spaces.\nBy utilizing model-free memory representations, DHS enhances the ability to\ntraverse temporal trajectories without relying on probabilistic transition\nmodels. The proposed method demonstrates significant improvements in search\nefficiency and performance across a range of heuristic optimization problems.\n","authors":["Abdel-Rahman Hedar","Alaa E. Abdel-Hakim","Wael Deabes","Youseef Alotaibi","Kheir Eddine Bouazza"],"pdf_url":"https://arxiv.org/pdf/2410.17042v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.16320v2","updated":"2024-10-22T14:09:10Z","published":"2024-09-21T03:45:05Z","title":"Developing a Thailand solar irradiance map using Himawari-8 satellite\n  imageries and deep learning models","summary":"  This paper presents an online platform that shows Thailand's solar irradiance\nmap every 30 minutes. It is available at https://www.cusolarforecast.com. The\nmethodology for estimating global horizontal irradiance (GHI) across Thailand\nrelies on cloud index extracted from Himawari-8 satellite imagery, Ineichen\nclear-sky model with locally-tuned Linke turbidity, and machine learning\nmodels. The methods take clear-sky irradiance, cloud index, re-analyzed GHI and\ntemperature data from the MERRA-2 database, and date-time as inputs for GHI\nestimation models, including LightGBM, LSTM, Informer, and Transformer. These\nare benchmarked with the estimate from a commercial service X by evaluating\n15-minute ground GHI data from 53 ground stations over 1.5 years from\n2022-2023. The results show that the four models have competitive performances\nand outperform the service X. The best model is LightGBM, with an MAE of 78.58\nW/sqm and RMSE of 118.97 W/sqm. Obtaining re-analyzed MERRA-2 data for Thailand\nis not economically feasible for deployment. When removing these features, the\nInformer model has a winning performance of 78.67 W/sqm in MAE. The obtained\nperformance aligns with existing literature by taking the climate zone and time\ngranularity of data into consideration. As the map shows an estimate of GHI\nover 93,000 grids with a frequent update, the paper also describes a\ncomputational framework for displaying the entire map. It tests the runtime\nperformance of deep learning models in the GHI estimation process.\n","authors":["Suwichaya Suwanwimolkul","Natanon Tongamrak","Nuttamon Thungka","Naebboon Hoonchareon","Jitkomut Songsiri"],"pdf_url":"https://arxiv.org/pdf/2409.16320v2.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.07389v2","updated":"2024-10-22T14:07:54Z","published":"2024-03-12T07:57:33Z","title":"Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from\n  Duplex to Monoplex IHC Images","summary":"  Generative models enable the translation from a source image domain where\nreadily trained models are available to a target domain unseen during training.\nWhile Cycle Generative Adversarial Networks (GANs) are well established, the\nassociated cycle consistency constrain relies on that an invertible mapping\nexists between the two domains. This is, however, not the case for the\ntranslation between images stained with chromogenic monoplex and duplex\nimmunohistochemistry (IHC) assays. Focusing on the translation from the latter\nto the first, we propose - through the introduction of a novel training design,\nan alternative constrain leveraging a set of immunofluorescence (IF) images as\nan auxiliary unpaired image domain. Quantitative and qualitative results on a\ndownstream segmentation task show the benefit of the proposed method in\ncomparison to baseline approaches.\n","authors":["Nicolas Brieu","Nicolas Triltsch","Philipp Wortmann","Dominik Winter","Shashank Saran","Marlon Rebelatto","Günter Schmidt"],"pdf_url":"https://arxiv.org/pdf/2403.07389v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2410.17032v1","updated":"2024-10-22T13:59:21Z","published":"2024-10-22T13:59:21Z","title":"Insights on Disagreement Patterns in Multimodal Safety Perception across\n  Diverse Rater Groups","summary":"  AI systems crucially rely on human ratings, but these ratings are often\naggregated, obscuring the inherent diversity of perspectives in real-world\nphenomenon. This is particularly concerning when evaluating the safety of\ngenerative AI, where perceptions and associated harms can vary significantly\nacross socio-cultural contexts. While recent research has studied the impact of\ndemographic differences on annotating text, there is limited understanding of\nhow these subjective variations affect multimodal safety in generative AI. To\naddress this, we conduct a large-scale study employing highly-parallel safety\nratings of about 1000 text-to-image (T2I) generations from a demographically\ndiverse rater pool of 630 raters balanced across 30 intersectional groups\nacross age, gender, and ethnicity. Our study shows that (1) there are\nsignificant differences across demographic groups (including intersectional\ngroups) on how severe they assess the harm to be, and that these differences\nvary across different types of safety violations, (2) the diverse rater pool\ncaptures annotation patterns that are substantially different from expert\nraters trained on specific set of safety policies, and (3) the differences we\nobserve in T2I safety are distinct from previously documented group level\ndifferences in text-based safety tasks. To further understand these varying\nperspectives, we conduct a qualitative analysis of the open-ended explanations\nprovided by raters. This analysis reveals core differences into the reasons why\ndifferent groups perceive harms in T2I generations. Our findings underscore the\ncritical need for incorporating diverse perspectives into safety evaluation of\ngenerative AI ensuring these systems are truly inclusive and reflect the values\nof all users.\n","authors":["Charvi Rastogi","Tian Huey Teh","Pushkar Mishra","Roma Patel","Zoe Ashwood","Aida Mostafazadeh Davani","Mark Diaz","Michela Paganini","Alicia Parrish","Ding Wang","Vinodkumar Prabhakaran","Lora Aroyo","Verena Rieser"],"pdf_url":"https://arxiv.org/pdf/2410.17032v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17031v1","updated":"2024-10-22T13:57:55Z","published":"2024-10-22T13:57:55Z","title":"GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks","summary":"  The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation.\n","authors":["Shuyang Hou","Zhangxiao Shen","Anqi Zhao","Jianyuan Liang","Zhipeng Gui","Xuefeng Guan","Rui Li","Huayi Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17028v1","updated":"2024-10-22T13:52:51Z","published":"2024-10-22T13:52:51Z","title":"Can a Machine Distinguish High and Low Amount of Social Creak in Speech?","summary":"  Objectives: ncreased prevalence of social creak particularly among female\nspeakers has been reported in several studies. The study of social creak has\nbeen previously conducted by combining perceptual evaluation of speech with\nconventional acoustical parameters such as the harmonic-to-noise ratio and\ncepstral peak prominence. In the current study, machine learning (ML) was used\nto automatically distinguish speech of low amount of social creak from speech\nof high amount of social creak.\n  Methods: The amount of creak in continuous speech samples produced in Finnish\nby 90 female speakers was first perceptually assessed by two voice specialists.\nBased on their assessments, the speech samples were divided into two categories\n(low $vs$. high amount of creak). Using the speech signals and their creak\nlabels, seven different ML models were trained. Three spectral representations\nwere used as feature for each model.\n  Results: The results show that the best performance (accuracy of 71.1\\%) was\nobtained by the following two systems: an Adaboost classifier using the\nmel-spectrogram feature and a decision tree classifier using the mel-frequency\ncepstral coefficient feature.\n  Conclusions: The study of social creak is becoming increasingly popular in\nsociolinguistic and vocological research. The conventional human perceptual\nassessment of the amount of creak is laborious and therefore ML technology\ncould be used to assist researchers studying social creak. The classification\nsystems reported in this study could be considered as baselines in future\nML-based studies on social creak.\n","authors":["Anne-Maria Laukkanen","Sudarsana Reddy Kadiri","Shrikanth Narayanan","Paavo Alku"],"pdf_url":"https://arxiv.org/pdf/2410.17028v1.pdf","comment":"Accepted in Journal of Voice"},{"id":"http://arxiv.org/abs/2406.02362v3","updated":"2024-10-22T13:43:01Z","published":"2024-06-04T14:39:51Z","title":"Temporal Graph Rewiring with Expander Graphs","summary":"  Evolving relations in real-world networks are often modelled by temporal\ngraphs. Temporal Graph Neural Networks (TGNNs) emerged to model evolutionary\nbehaviour of such graphs by leveraging the message passing primitive at the\ncore of Graph Neural Networks (GNNs). It is well-known that GNNs are vulnerable\nto several issues directly related to the input graph topology, such as\nunder-reaching and over-squashing - we argue that these issues can often get\nexacerbated in temporal graphs, particularly as the result of stale nodes and\nedges. While graph rewiring techniques have seen frequent usage in GNNs to make\nthe graph topology more favourable for message passing, they have not seen any\nmainstream usage on TGNNs. In this work, we propose Temporal Graph Rewiring\n(TGR), the first approach for graph rewiring on temporal graphs, to the best of\nour knowledge. TGR constructs message passing highways between temporally\ndistant nodes in a continuous-time dynamic graph by utilizing expander graph\npropagation, a prominent framework used for graph rewiring on static graphs\nwhich makes minimal assumptions on the underlying graph structure. On the\nchallenging TGB benchmark, TGR achieves state-of-the-art results on\ntgbl-review, tgbl-coin, tgbl-comment and tgbl-flight datasets at the time of\nwriting. For tgbl-review, TGR has 50.5% improvement in MRR over the base TGN\nmodel and 22.2% improvement over the base TNCN model. The significant\nimprovement over base models demonstrates clear benefits of temporal graph\nrewiring.\n","authors":["Katarina Petrović","Shenyang Huang","Farimah Poursafaei","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2406.02362v3.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.18460v3","updated":"2024-10-22T13:37:04Z","published":"2023-11-30T11:11:26Z","title":"Causal Fairness under Unobserved Confounding: A Neural Sensitivity\n  Framework","summary":"  Fairness for machine learning predictions is widely required in practice for\nlegal, ethical, and societal reasons. Existing work typically focuses on\nsettings without unobserved confounding, even though unobserved confounding can\nlead to severe violations of causal fairness and, thus, unfair predictions. In\nthis work, we analyze the sensitivity of causal fairness to unobserved\nconfounding. Our contributions are three-fold. First, we derive bounds for\ncausal fairness metrics under different sources of unobserved confounding. This\nenables practitioners to examine the sensitivity of their machine learning\nmodels to unobserved confounding in fairness-critical applications. Second, we\npropose a novel neural framework for learning fair predictions, which allows us\nto offer worst-case guarantees of the extent to which causal fairness can be\nviolated due to unobserved confounding. Third, we demonstrate the effectiveness\nof our framework in a series of experiments, including a real-world case study\nabout predicting prison sentences. To the best of our knowledge, ours is the\nfirst work to study causal fairness under unobserved confounding. To this end,\nour work is of direct practical value as a refutation strategy to ensure the\nfairness of predictions in high-stakes applications.\n","authors":["Maresa Schröder","Dennis Frauen","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2311.18460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12142v2","updated":"2024-10-22T13:32:34Z","published":"2024-06-17T23:08:46Z","title":"Slicing Through Bias: Explaining Performance Gaps in Medical Image\n  Analysis using Slice Discovery Methods","summary":"  Machine learning models have achieved high overall accuracy in medical image\nanalysis. However, performance disparities on specific patient groups pose\nchallenges to their clinical utility, safety, and fairness. This can affect\nknown patient groups - such as those based on sex, age, or disease subtype - as\nwell as previously unknown and unlabeled groups. Furthermore, the root cause of\nsuch observed performance disparities is often challenging to uncover,\nhindering mitigation efforts. In this paper, to address these issues, we\nleverage Slice Discovery Methods (SDMs) to identify interpretable\nunderperforming subsets of data and formulate hypotheses regarding the cause of\nobserved performance disparities. We introduce a novel SDM and apply it in a\ncase study on the classification of pneumothorax and atelectasis from chest\nx-rays. Our study demonstrates the effectiveness of SDMs in hypothesis\nformulation and yields an explanation of previously observed but unexplained\nperformance disparities between male and female patients in widely used chest\nX-ray datasets and models. Our findings indicate shortcut learning in both\nclassification tasks, through the presence of chest drains and ECG wires,\nrespectively. Sex-based differences in the prevalence of these shortcut\nfeatures appear to cause the observed classification performance gap,\nrepresenting a previously underappreciated interaction between shortcut\nlearning and model fairness analyses.\n","authors":["Vincent Olesen","Nina Weng","Aasa Feragen","Eike Petersen"],"pdf_url":"https://arxiv.org/pdf/2406.12142v2.pdf","comment":"MICCAI 2024 Workshop on Fairness of AI in Medical Imaging"},{"id":"http://arxiv.org/abs/2410.17005v1","updated":"2024-10-22T13:25:28Z","published":"2024-10-22T13:25:28Z","title":"Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced\n  Tabletability","summary":"  Co-crystallization is an accessible way to control physicochemical\ncharacteristics of organic crystals, which finds many biomedical applications.\nIn this work, we present Generative Method for Co-crystal Design (GEMCODE), a\nnovel pipeline for automated co-crystal screening based on the hybridization of\ndeep generative models and evolutionary optimization for broader exploration of\nthe target chemical space. GEMCODE enables fast de novo co-crystal design with\ntarget tabletability profiles, which is crucial for the development of\npharmaceuticals. With a series of experimental studies highlighting validation\nand discovery cases, we show that GEMCODE is effective even under realistic\ncomputational constraints. Furthermore, we explore the potential of language\nmodels in generating co-crystals. Finally, we present numerous previously\nunknown co-crystals predicted by GEMCODE and discuss its potential in\naccelerating drug development.\n","authors":["Nina Gubina","Andrei Dmitrenko","Gleb Solovev","Lyubov Yamshchikova","Oleg Petrov","Ivan Lebedev","Nikita Serov","Grigorii Kirgizov","Nikolay Nikitin","Vladimir Vinogradov"],"pdf_url":"https://arxiv.org/pdf/2410.17005v1.pdf","comment":"Accepted at 38th Conference on Neural Information Processing Systems\n  (NeurIPS)"},{"id":"http://arxiv.org/abs/2410.16991v1","updated":"2024-10-22T13:12:47Z","published":"2024-10-22T13:12:47Z","title":"An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and\n  Geometric Reasoning Skills Using Computer Graphics Questions","summary":"  CG (Computer Graphics) is a popular field of CS (Computer Science), but many\nstudents find this topic difficult due to it requiring a large number of\nskills, such as mathematics, programming, geometric reasoning, and creativity.\nOver the past few years, researchers have investigated ways to harness the\npower of GenAI (Generative Artificial Intelligence) to improve teaching. In CS,\nmuch of the research has focused on introductory computing. A recent study\nevaluating the performance of an LLM (Large Language Model), GPT-4 (text-only),\non CG questions, indicated poor performance and reliance on detailed\ndescriptions of image content, which often required considerable insight from\nthe user to return reasonable results. So far, no studies have investigated the\nabilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG\nquestions and how these abilities can be used to improve teaching.\n  In this study, we construct two datasets of CG questions requiring varying\ndegrees of visual perception skills and geometric reasoning skills, and\nevaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find\nthat although GPT-4o exhibits great potential in solving questions with visual\ninformation independently, major limitations still exist to the accuracy and\nquality of the generated results. We propose several novel approaches for CG\neducators to incorporate GenAI into CG teaching despite these limitations. We\nhope that our guidelines further encourage learning and engagement in CG\nclassrooms.\n","authors":["Tony Haoran Feng","Paul Denny","Burkhard C. Wünsche","Andrew Luxton-Reilly","Jacqueline Whalley"],"pdf_url":"https://arxiv.org/pdf/2410.16991v1.pdf","comment":"8 pages, 8 figures, 1 table, to be published in SIGGRAPH Asia 2024\n  Educator's Forum"},{"id":"http://arxiv.org/abs/2410.10851v2","updated":"2024-10-22T13:08:02Z","published":"2024-10-06T12:53:07Z","title":"LLM Gesticulator: Leveraging Large Language Models for Scalable and\n  Controllable Co-Speech Gesture Synthesis","summary":"  In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.\n","authors":["Haozhou Pang","Tianwei Ding","Lanshan He","Ming Tao","Lu Zhang","Qi Gan"],"pdf_url":"https://arxiv.org/pdf/2410.10851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16983v1","updated":"2024-10-22T13:05:11Z","published":"2024-10-22T13:05:11Z","title":"Order Matters: Exploring Order Sensitivity in Multimodal Large Language\n  Models","summary":"  Multimodal Large Language Models (MLLMs) utilize multimodal contexts\nconsisting of text, images, or videos to solve various multimodal tasks.\nHowever, we find that changing the order of multimodal input can cause the\nmodel's performance to fluctuate between advanced performance and random\nguessing. This phenomenon exists in both single-modality (text-only or\nimage-only) and mixed-modality (image-text-pair) contexts. Furthermore, we\ndemonstrate that popular MLLMs pay special attention to certain multimodal\ncontext positions, particularly the beginning and end. Leveraging this special\nattention, we place key video frames and important image/text content in\nspecial positions within the context and submit them to the MLLM for inference.\nThis method results in average performance gains of 14.7% for video-caption\nmatching and 17.8% for visual question answering tasks. Additionally, we\npropose a new metric, Position-Invariant Accuracy (PIA), to address order bias\nin MLLM evaluation. Our research findings contribute to a better understanding\nof Multi-Modal In-Context Learning (MMICL) and provide practical strategies for\nenhancing MLLM performance without increasing computational costs.\n","authors":["Zhijie Tan","Xu Chu","Weiping Li","Tong Mo"],"pdf_url":"https://arxiv.org/pdf/2410.16983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09553v3","updated":"2024-10-22T13:04:29Z","published":"2024-06-28T08:21:49Z","title":"DPEC: Dual-Path Error Compensation Method for Enhanced Low-Light Image\n  Clarity","summary":"  For the task of low-light image enhancement, deep learning-based algorithms\nhave demonstrated superiority and effectiveness compared to traditional\nmethods. Existing deep learning algorithms are proposed mainly based on the\nRetinex theory but overlook the noise and color distortion present in the\ninput, which frequently results in significant noise amplification and local\ncolor distortion in the final results. To address this, we propose a Dual-Path\nError Compensation method (DPEC), which aims to improve image quality in\nlow-light conditions. DPEC performs precise pixel-level error estimation, which\naccurately captures subtle pixels differences, and independent denoising, which\neffectively removes unnecessary noise. This method restores image brightness\nwhile preserving local texture details and avoiding noise amplification.\nFurthermore, to compensate for the traditional CNN's limited ability to capture\nlong-range semantic information and considering both computational speed and\nresource efficiency, we integrated the VMamba architecture into the backbone of\nDPEC. In addition, we introduced the HIS-Retinex loss to constrain the training\nof DPEC, ensuring that the overall brightness distribution of the images more\nclosely aligns with real-world conditions. Comprehensive quantitative and\nqualitative experimental results demonstrate that our algorithm significantly\noutperforms state-of-the-art methods across six benchmark tests.\n","authors":["Shuang Wang","Qianwen Lu","Yihe Nie","Qingchuan Tao","Yanmei Yu"],"pdf_url":"https://arxiv.org/pdf/2407.09553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16973v1","updated":"2024-10-22T12:51:51Z","published":"2024-10-22T12:51:51Z","title":"Learning Mathematical Rules with Large Language Models","summary":"  In this paper, we study the ability of large language models to learn\nspecific mathematical rules such as distributivity or simplifying equations. We\npresent an empirical analysis of their ability to generalize these rules, as\nwell as to reuse them in the context of word problems. For this purpose, we\nprovide a rigorous methodology to build synthetic data incorporating such\nrules, and perform fine-tuning of large language models on such data. Our\nexperiments show that our model can learn and generalize these rules to some\nextent, as well as suitably reuse them in the context of word problems.\n","authors":["Antoine Gorceix","Bastien Le Chenadec","Ahmad Rammal","Nelson Vadori","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2410.16973v1.pdf","comment":"4th MATH-AI Workshop at NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.13226v2","updated":"2024-10-22T12:28:28Z","published":"2024-10-17T05:17:01Z","title":"Research on Travel Route Planing Problems Based on Greedy Algorithm","summary":"  The route planning problem based on the greedy algorithm represents a method\nof identifying the optimal or near-optimal route between a given start point\nand end point. In this paper, the PCA method is employed initially to downscale\nthe city evaluation indexes, extract the key principal components, and then\ndownscale the data using the KMO and TOPSIS algorithms, all of which are based\non the MindSpore framework. Secondly, for the dataset that does not pass the\nKMO test, the entropy weight method and TOPSIS method will be employed for\ncomprehensive evaluation. Finally, a route planning algorithm is proposed and\noptimised based on the greedy algorithm, which provides personalised route\ncustomisation according to the different needs of tourists. In addition, the\nlocal travelling efficiency, the time required to visit tourist attractions and\nthe necessary daily breaks are considered in order to reduce the cost and avoid\nfalling into the locally optimal solution.\n","authors":["Yiquan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13226v2.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2304.07063v4","updated":"2024-10-22T12:28:13Z","published":"2023-04-14T11:35:35Z","title":"Rethinking Complex Queries on Knowledge Graphs with Neural Link\n  Predictors","summary":"  Reasoning on knowledge graphs is a challenging task because it utilizes\nobserved information to predict the missing one. Particularly, answering\ncomplex queries based on first-order logic is one of the crucial tasks to\nverify learning to reason abilities for generalization and composition.\nRecently, the prevailing method is query embedding which learns the embedding\nof a set of entities and treats logic operations as set operations and has\nshown great empirical success. Though there has been much research following\nthe same formulation, many of its claims lack a formal and systematic\ninspection. In this paper, we rethink this formulation and justify many of the\nprevious claims by characterizing the scope of queries investigated previously\nand precisely identifying the gap between its formulation and its goal, as well\nas providing complexity analysis for the currently investigated queries.\nMoreover, we develop a new dataset containing ten new types of queries with\nfeatures that have never been considered and therefore can provide a thorough\ninvestigation of complex queries. Finally, we propose a new neural-symbolic\nmethod, Fuzzy Inference with Truth value (FIT), where we equip the neural link\npredictors with fuzzy logic theory to support end-to-end learning using complex\nqueries with provable reasoning capability. Empirical results show that our\nmethod outperforms previous methods significantly in the new dataset and also\nsurpasses previous methods in the existing dataset at the same time.\n","authors":["Hang Yin","Zihao Wang","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2304.07063v4.pdf","comment":"Received in ICLR 2024"},{"id":"http://arxiv.org/abs/2410.16950v1","updated":"2024-10-22T12:24:41Z","published":"2024-10-22T12:24:41Z","title":"Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In","summary":"  Following the advancement of large language models (LLMs), the development of\nLLM-based autonomous agents has become increasingly prevalent. As a result, the\nneed to understand the security vulnerabilities of these agents has become a\ncritical task. We examine how ReAct agents can be exploited using a\nstraightforward yet effective method we refer to as the foot-in-the-door\nattack. Our experiments show that indirect prompt injection attacks, prompted\nby harmless and unrelated requests (such as basic calculations) can\nsignificantly increase the likelihood of the agent performing subsequent\nmalicious actions. Our results show that once a ReAct agents thought includes a\nspecific tool or action, the likelihood of executing this tool in the\nsubsequent steps increases significantly, as the agent seldom re-evaluates its\nactions. Consequently, even random, harmless requests can establish a\nfoot-in-the-door, allowing an attacker to embed malicious instructions into the\nagents thought process, making it more susceptible to harmful directives. To\nmitigate this vulnerability, we propose implementing a simple reflection\nmechanism that prompts the agent to reassess the safety of its actions during\nexecution, which can help reduce the success of such attacks.\n","authors":["Itay Nakash","George Kour","Guy Uziel","Ateret Anaby-Tavor"],"pdf_url":"https://arxiv.org/pdf/2410.16950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16945v1","updated":"2024-10-22T12:20:15Z","published":"2024-10-22T12:20:15Z","title":"IdenBAT: Disentangled Representation Learning for Identity-Preserved\n  Brain Age Transformation","summary":"  Brain age transformation aims to convert reference brain images into\nsynthesized images that accurately reflect the age-specific features of a\ntarget age group. The primary objective of this task is to modify only the\nage-related attributes of the reference image while preserving all other\nage-irrelevant attributes. However, achieving this goal poses substantial\nchallenges due to the inherent entanglement of various image attributes within\nfeatures extracted from a backbone encoder, resulting in simultaneous\nalterations during the image generation. To address this challenge, we propose\na novel architecture that employs disentangled representation learning for\nidentity-preserved brain age transformation called IdenBAT. This approach\nfacilitates the decomposition of image features, ensuring the preservation of\nindividual traits while selectively transforming age-related characteristics to\nmatch those of the target age group. Through comprehensive experiments\nconducted on both 2D and full-size 3D brain datasets, our method adeptly\nconverts input images to target age while retaining individual characteristics\naccurately. Furthermore, our approach demonstrates superiority over existing\nstate-of-the-art regarding performance fidelity.\n","authors":["Junyeong Maeng","Kwanseok Oh","Wonsik Jung","Heung-Il Suk"],"pdf_url":"https://arxiv.org/pdf/2410.16945v1.pdf","comment":"16 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.08809v2","updated":"2024-10-22T12:18:27Z","published":"2024-06-13T05:00:27Z","title":"Are We There Yet? A Brief Survey of Music Emotion Prediction Datasets,\n  Models and Outstanding Challenges","summary":"  Deep learning models for music have advanced drastically in recent years, but\nhow good are machine learning models at capturing emotion, and what challenges\nare researchers facing? In this paper, we provide a comprehensive overview of\nthe available music-emotion datasets and discuss evaluation standards as well\nas competitions in the field. We also offer a brief overview of various types\nof music emotion prediction models that have been built over the years,\nproviding insights into the diverse approaches within the field. Through this\nexamination, we highlight the challenges that persist in accurately capturing\nemotion in music, including issues related to dataset quality, annotation\nconsistency, and model generalization. Additionally, we explore the impact of\ndifferent modalities, such as audio, MIDI, and physiological signals, on the\neffectiveness of emotion prediction models. Recognizing the dynamic nature of\nthis field, we have complemented our findings with an accompanying GitHub\nrepository. This repository contains a comprehensive list of music emotion\ndatasets and recent predictive models.\n","authors":["Jaeyong Kang","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2406.08809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16930v1","updated":"2024-10-22T12:00:58Z","published":"2024-10-22T12:00:58Z","title":"Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities\n  Using Only Forward Passes","summary":"  Math reasoning is a highly active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence. However, few works have\nexplored how math reasoning is encoded within LLM parameters and if it is a\nskill that can be isolated within a model. Doing so could allow targeted\nintervention to improve math performance without altering non-math behavior and\nfoster understanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a method for isolating math-specific parameters in\nLLMs using only forward passes. MathNeuro builds on existing work by using\nweights and activations to calculate parameter importance, but isolates\nmath-specific parameters by removing those important for general language\ntasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning\nability without destroying its general language ability. Scaling these\nparameters by a small constant improves a pretrained or instruction-tuned LLM's\nperformance by 4-17% on GSM8K while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.\n","authors":["Bryan R. Christ","Zack Gottesman","Jonathan Kropko","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2410.16930v1.pdf","comment":"21 pages, 29 figures"},{"id":"http://arxiv.org/abs/2410.16927v1","updated":"2024-10-22T11:58:54Z","published":"2024-10-22T11:58:54Z","title":"Revealing Hidden Bias in AI: Lessons from Large Language Models","summary":"  As large language models (LLMs) become integral to recruitment processes,\nconcerns about AI-induced bias have intensified. This study examines biases in\ncandidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5,\nand Llama 3.1 405B, focusing on characteristics such as gender, race, and age.\nWe evaluate the effectiveness of LLM-based anonymization in reducing these\nbiases. Findings indicate that while anonymization reduces certain biases,\nparticularly gender bias, the degree of effectiveness varies across models and\nbias types. Notably, Llama 3.1 405B exhibited the lowest overall bias.\nMoreover, our methodology of comparing anonymized and non-anonymized data\nreveals a novel approach to assessing inherent biases in LLMs beyond\nrecruitment applications. This study underscores the importance of careful LLM\nselection and suggests best practices for minimizing bias in AI applications,\npromoting fairness and inclusivity.\n","authors":["Django Beatty","Kritsada Masanthia","Teepakorn Kaphol","Niphan Sethi"],"pdf_url":"https://arxiv.org/pdf/2410.16927v1.pdf","comment":"13 pages, 18 figures. This paper presents a technical analysis of\n  bias in large language models, focusing on bias detection and mitigation"},{"id":"http://arxiv.org/abs/2410.16924v1","updated":"2024-10-22T11:56:34Z","published":"2024-10-22T11:56:34Z","title":"SleepCoT: A Lightweight Personalized Sleep Health Model via\n  Chain-of-Thought Distillation","summary":"  We present a novel approach to personalized sleep health management using\nfew-shot Chain-of-Thought (CoT) distillation, enabling small-scale language\nmodels (> 2B parameters) to rival the performance of large language models\n(LLMs) in specialized health domains. Our method simultaneously distills\nproblem-solving strategies, long-tail expert knowledge, and personalized\nrecommendation capabilities from larger models into more efficient, compact\nmodels. Unlike existing systems, our approach offers three key functionalities:\ngenerating personalized sleep health recommendations, supporting user-specific\nfollow-up inquiries, and providing responses to domain-specific knowledge\nquestions. We focus on sleep health due to its measurability via wearable\ndevices and its impact on overall well-being. Our experimental setup, involving\nGPT-4o for data synthesis, Qwen-max for instruction set creation, and Qwen2.5\n1.5B for model distillation, demonstrates significant improvements over\nbaseline small-scale models in penalization, reasoning, and knowledge\napplication. Experiments using 100 simulated sleep reports and 1,000\ndomain-specific questions shows our model achieves comparable performance to\nlarger models while maintaining efficiency for real-world deployment. This\nresearch not only advances AI-driven health management but also provides a\nnovel approach to leveraging LLM capabilities in resource-constrained\nenvironments, potentially enhancing the accessibility of personalized\nhealthcare solutions.\n","authors":["Huimin Zheng","Xiaofeng Xing","Xiangmin Xu"],"pdf_url":"https://arxiv.org/pdf/2410.16924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07114v3","updated":"2024-10-22T11:55:46Z","published":"2024-09-19T19:48:31Z","title":"System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam","summary":"  The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch average of 40.63 points. Neither model had access to the\nexam figures. Since there was a risk of model contamination (i.e., the\nknowledge cutoff of o1-preview and GPT-4o was after the exam was published\nonline), we repeated the procedure with a new Mathematics B exam that was\npublished after the cutoff date. The results again indicated that o1-preview\nperformed strongly (97.8th percentile), which suggests that contamination was\nnot a factor. We also show that there is some variability in the output of\no1-preview, which means that sometimes there is 'luck' (the answer is correct)\nor 'bad luck' (the output has diverged into something that is incorrect). We\ndemonstrate that a self-consistency approach, where repeated prompts are given\nand the most common answer is selected, is a useful strategy for identifying\nthe correct answer. It is concluded that while OpenAI's new model series holds\ngreat potential, certain risks must be considered.\n","authors":["Joost de Winter","Dimitra Dodou","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2410.07114v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16919v1","updated":"2024-10-22T11:52:22Z","published":"2024-10-22T11:52:22Z","title":"EnvBridge: Bridging Diverse Environments with Cross-Environment\n  Knowledge Transfer for Embodied AI","summary":"  In recent years, Large Language Models (LLMs) have demonstrated high\nreasoning capabilities, drawing attention for their applications as agents in\nvarious decision-making processes. One notably promising application of LLM\nagents is robotic manipulation. Recent research has shown that LLMs can\ngenerate text planning or control code for robots, providing substantial\nflexibility and interaction capabilities. However, these methods still face\nchallenges in terms of flexibility and applicability across different\nenvironments, limiting their ability to adapt autonomously. Current approaches\ntypically fall into two categories: those relying on environment-specific\npolicy training, which restricts their transferability, and those generating\ncode actions based on fixed prompts, which leads to diminished performance when\nconfronted with new environments. These limitations significantly constrain the\ngeneralizability of agents in robotic manipulation. To address these\nlimitations, we propose a novel method called EnvBridge. This approach involves\nthe retention and transfer of successful robot control codes from source\nenvironments to target environments. EnvBridge enhances the agent's\nadaptability and performance across diverse settings by leveraging insights\nfrom multiple environments. Notably, our approach alleviates environmental\nconstraints, offering a more flexible and generalizable solution for robotic\nmanipulation tasks. We validated the effectiveness of our method using robotic\nmanipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments\ndemonstrate that LLM agents can successfully leverage diverse knowledge sources\nto solve complex tasks. Consequently, our approach significantly enhances the\nadaptability and robustness of robotic manipulation agents in planning across\ndiverse environments.\n","authors":["Tomoyuki Kagaya","Yuxuan Lou","Thong Jing Yuan","Subramanian Lakshmi","Jayashree Karlekar","Sugiri Pranata","Natsuki Murakami","Akira Kinose","Koki Oguri","Felix Wick","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.16919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17066v2","updated":"2024-10-22T11:47:04Z","published":"2024-09-25T16:25:45Z","title":"VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large\n  Language Models","summary":"  Scaling model size significantly challenges the deployment and inference of\nLarge Language Models (LLMs). Due to the redundancy in LLM weights, recent\nresearch has focused on pushing weight-only quantization to extremely low-bit\n(even down to 2 bits). It reduces memory requirements, optimizes storage costs,\nand decreases memory bandwidth needs during inference. However, due to\nnumerical representation limitations, traditional scalar-based weight\nquantization struggles to achieve such extreme low-bit. Recent research on\nVector Quantization (VQ) for LLMs has demonstrated the potential for extremely\nlow-bit model quantization by compressing vectors into indices using lookup\ntables.\n  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for\nextremely low-bit quantization of LLMs. We use Second-Order Optimization to\nformulate the LLM VQ problem and guide our quantization algorithm design by\nsolving the optimization. We further refine the weights using\nChannel-Independent Second-Order Optimization for a granular VQ. In addition,\nby decomposing the optimization problem, we propose a brief and effective\ncodebook initialization algorithm. We also extend VPTQ to support residual and\noutlier quantization, which enhances model accuracy and further compresses the\nmodel. Our experimental results show that VPTQ reduces model quantization\nperplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,\n$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy\nimprovement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on\nLLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the\nquantization algorithm execution time, resulting in a $1.6$-$1.8\\times$\nincrease in inference throughput compared to SOTA.\n","authors":["Yifei Liu","Jicheng Wen","Yang Wang","Shengyu Ye","Li Lyna Zhang","Ting Cao","Cheng Li","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2409.17066v2.pdf","comment":"EMNLP 2024, Main, Poster"},{"id":"http://arxiv.org/abs/2410.16908v1","updated":"2024-10-22T11:28:39Z","published":"2024-10-22T11:28:39Z","title":"Mitigating Vanishing Activations in Deep CapsNets Using Channel Pruning","summary":"  Capsule Networks outperform Convolutional Neural Networks in learning the\npart-whole relationships with viewpoint invariance, and the credit goes to\ntheir multidimensional capsules. It was assumed that increasing the number of\ncapsule layers in the capsule networks would enhance the model performance.\nHowever, recent studies found that Capsule Networks lack scalability due to\nvanishing activations in the capsules of deeper layers. This paper thoroughly\ninvestigates the vanishing activation problem in deep Capsule Networks. To\nanalyze this issue and understand how increasing capsule dimensions can\nfacilitate deeper networks, various Capsule Network models are constructed and\nevaluated with different numbers of capsules, capsule dimensions, and\nintermediate layers for this paper. Unlike traditional model pruning, which\nreduces the number of model parameters and expedites model training, this study\nuses pruning to mitigate the vanishing activations in the deeper capsule\nlayers. In addition, the backbone network and capsule layers are pruned with\ndifferent pruning ratios to reduce the number of inactive capsules and achieve\nbetter model accuracy than the unpruned models.\n","authors":["Siddharth Sahu","Abdulrahman Altahhan"],"pdf_url":"https://arxiv.org/pdf/2410.16908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00467v2","updated":"2024-10-22T10:47:13Z","published":"2024-10-01T07:49:24Z","title":"Dynamic Planning for LLM-based Graphical User Interface Automation","summary":"  The advent of large language models (LLMs) has spurred considerable interest\nin advancing autonomous LLMs-based agents, particularly in intriguing\napplications within smartphone graphical user interfaces (GUIs). When presented\nwith a task goal, these agents typically emulate human actions within a GUI\nenvironment until the task is completed. However, a key challenge lies in\ndevising effective plans to guide action prediction in GUI tasks, though\nplanning have been widely recognized as effective for decomposing complex tasks\ninto a series of steps. Specifically, given the dynamic nature of environmental\nGUIs following action execution, it is crucial to dynamically adapt plans based\non environmental feedback and action history.We show that the widely-used ReAct\napproach fails due to the excessively long historical dialogues. To address\nthis challenge, we propose a novel approach called Dynamic Planning of Thoughts\n(D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of\nplanning based on the environmental feedback and execution history.\nExperimental results reveal that the proposed D-PoT significantly surpassed the\nstrong GPT-4V baseline by +12.7% (34.66% $\\rightarrow$ 47.36%) in accuracy. The\nanalysis highlights the generality of dynamic planning in different backbone\nLLMs, as well as the benefits in mitigating hallucinations and adapting to\nunseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.\n","authors":["Shaoqing Zhang","Zhuosheng Zhang","Kehai Chen","Xinbei Ma","Muyun Yang","Tiejun Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.00467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15955v4","updated":"2024-10-22T10:41:58Z","published":"2024-09-24T10:36:40Z","title":"A Historical Trajectory Assisted Optimization Method for Zeroth-Order\n  Federated Learning","summary":"  Federated learning heavily relies on distributed gradient descent techniques.\nIn the situation where gradient information is not available, the gradients\nneed to be estimated from zeroth-order information, which typically involves\ncomputing finite-differences along isotropic random directions. This method\nsuffers from high estimation errors, as the geometric features of the objective\nlandscape may be overlooked during the isotropic sampling. In this work, we\npropose a non-isotropic sampling method to improve the gradient estimation\nprocedure. Gradients in our method are estimated in a subspace spanned by\nhistorical trajectories of solutions, aiming to encourage the exploration of\npromising regions and hence improve the convergence. The proposed method uses a\ncovariance matrix for sampling which is a convex combination of two parts. The\nfirst part is a thin projection matrix containing the basis of the subspace\nwhich is designed to improve the exploitation ability. The second part is the\nhistorical trajectories. We implement this method in zeroth-order federated\nsettings, and show that the convergence rate aligns with existing ones while\nintroducing no significant overheads in communication or local computation. The\neffectiveness of our proposal is verified on several numerical experiments in\ncomparison to several commonly-used zeroth-order federated optimization\nalgorithms.\n","authors":["Chenlin Wu","Xiaoyu He","Zike Li","Jing Gong","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.15955v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16882v1","updated":"2024-10-22T10:36:15Z","published":"2024-10-22T10:36:15Z","title":"Large Language Model-based Augmentation for Imbalanced Node\n  Classification on Text-Attributed Graphs","summary":"  Node classification on graphs frequently encounters the challenge of class\nimbalance, leading to biased performance and posing significant risks in\nreal-world applications. Although several data-centric solutions have been\nproposed, none of them focus on Text-Attributed Graphs (TAGs), and therefore\noverlook the potential of leveraging the rich semantics encoded in textual\nfeatures for boosting the classification of minority nodes. Given this crucial\ngap, we investigate the possibility of augmenting graph data in the text space,\nleveraging the textual generation power of Large Language Models (LLMs) to\nhandle imbalanced node classification on TAGs. Specifically, we propose a novel\napproach called LA-TAG (LLM-based Augmentation on Text-Attributed Graphs),\nwhich prompts LLMs to generate synthetic texts based on existing node texts in\nthe graph. Furthermore, to integrate these synthetic text-attributed nodes into\nthe graph, we introduce a text-based link predictor to connect the synthesized\nnodes with the existing nodes. Our experiments across multiple datasets and\nevaluation metrics show that our framework significantly outperforms\ntraditional non-textual-based data augmentation strategies and specific node\nimbalance solutions. This highlights the promise of using LLMs to resolve\nimbalance issues on TAGs.\n","authors":["Leyao Wang","Yu Wang","Bo Ni","Yuying Zhao","Tyler Derr"],"pdf_url":"https://arxiv.org/pdf/2410.16882v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.15319v2","updated":"2024-10-22T10:31:59Z","published":"2024-05-24T08:00:00Z","title":"Stacking Your Transformers: A Closer Look at Model Growth for Efficient\n  LLM Pre-Training","summary":"  LLMs are computationally expensive to pre-train due to their large scale.\nModel growth emerges as a promising approach by leveraging smaller models to\naccelerate the training of larger ones. However, the viability of these model\ngrowth methods in efficient LLM pre-training remains underexplored. This work\nidentifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1)\nlack of comprehensive evaluation, ($\\textit{O}$2) untested viability for\nscaling, and ($\\textit{O}$3) lack of empirical guidelines. To tackle\n$\\textit{O}$1, we summarize existing approaches into four atomic growth\noperators and systematically evaluate them in a standardized LLM pre-training\nsetting. Our findings reveal that a depthwise stacking operator, called\n$G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to\ndecreased loss and improved overall performance on eight standard NLP\nbenchmarks compared to strong baselines. Motivated by these promising results,\nwe conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to\naddress $\\textit{O}$2 and $\\textit{O}$3. For $\\textit{O}$2 (untested\nscalability), our study shows that $G_{\\text{stack}}$ is scalable and\nconsistently performs well, with experiments up to 7B LLMs after growth and\npre-training LLMs with 750B tokens. For example, compared to a conventionally\ntrained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to\nthe same loss with 194B tokens, resulting in a 54.6\\% speedup. We further\naddress $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines\nto determine growth timing and growth factor for $G_{\\text{stack}}$, making it\npractical in general LLM pre-training. We also provide in-depth discussions and\ncomprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained\nmodel are available at https://llm-stacking.github.io.\n","authors":["Wenyu Du","Tongxu Luo","Zihan Qiu","Zeyu Huang","Yikang Shen","Reynold Cheng","Yike Guo","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2405.15319v2.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.16879v1","updated":"2024-10-22T10:31:23Z","published":"2024-10-22T10:31:23Z","title":"Contrasting Attitudes Towards Current and Future AI Applications for\n  Computerised Interpretation of ECG: A Clinical Stakeholder Interview Study","summary":"  Objectives: To investigate clinicians' attitudes towards current automated\ninterpretation of ECG and novel AI technologies and their perception of\ncomputer-assisted interpretation. Materials and Methods: We conducted a series\nof interviews with clinicians in the UK. Our study: (i) explores the potential\nfor AI, specifically future 'human-like' computing approaches, to facilitate\nECG interpretation and support clinical decision making, and (ii) elicits their\nopinions about the importance of explainability and trustworthiness of AI\nalgorithms. Results: We performed inductive thematic analysis on interview\ntranscriptions from 23 clinicians and identified the following themes: (i) a\nlack of trust in current systems, (ii) positive attitudes towards future AI\napplications and requirements for these, (iii) the relationship between the\naccuracy and explainability of algorithms, and (iv) opinions on education,\npossible deskilling, and the impact of AI on clinical competencies. Discussion:\nClinicians do not trust current computerised methods, but welcome future 'AI'\ntechnologies. Where clinicians trust future AI interpretation to be accurate,\nthey are less concerned that it is explainable. They also preferred ECG\ninterpretation that demonstrated the results of the algorithm visually. Whilst\nclinicians do not fear job losses, they are concerned about deskilling and the\nneed to educate the workforce to use AI responsibly. Conclusion: Clinicians are\npositive about the future application of AI in clinical decision-making.\nAccuracy is a key factor of uptake and visualisations are preferred over\ncurrent computerised methods. This is viewed as a potential means of training\nand upskilling, in contrast to the deskilling that automation might be\nperceived to bring.\n","authors":["Lukas Hughes-Noehrer","Leda Channer","Gabriel Strain","Gregory Yates","Richard Body","Caroline Jay"],"pdf_url":"https://arxiv.org/pdf/2410.16879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17442v4","updated":"2024-10-22T10:30:19Z","published":"2024-02-27T11:57:28Z","title":"Insights from the Usage of the Ansible Lightspeed Code Completion\n  Service","summary":"  The availability of Large Language Models (LLMs) which can generate code, has\nmade it possible to create tools that improve developer productivity.\nIntegrated development environments or IDEs which developers use to write\nsoftware are often used as an interface to interact with LLMs. Although many\nsuch tools have been released, almost all of them focus on general-purpose\nprogramming languages. Domain-specific languages, such as those crucial for\nInformation Technology (IT) automation, have not received much attention.\nAnsible is one such YAML-based IT automation-specific language. Ansible\nLightspeed is an LLM-based service designed explicitly to generate Ansible\nYAML, given natural language prompt.\n  In this paper, we present the design and implementation of the Ansible\nLightspeed service. We then evaluate its utility to developers using diverse\nindicators, including extended utilization, analysis of user edited\nsuggestions, as well as user sentiments analysis. The evaluation is based on\ndata collected for 10,696 real users including 3,910 returning users. The code\nfor Ansible Lightspeed service and the analysis framework is made available for\nothers to use.\n  To our knowledge, our study is the first to involve thousands of users of\ncode assistants for domain-specific languages. We are also the first code\ncompletion tool to present N-Day user retention figures, which is 13.66% on Day\n30. We propose an improved version of user acceptance rate, called Strong\nAcceptance rate, where a suggestion is considered accepted only if less than\n50% of it is edited and these edits do not change critical parts of the\nsuggestion. By focusing on Ansible, Lightspeed is able to achieve a strong\nacceptance rate of 49.08% for multi-line Ansible task suggestions. With our\nfindings we provide insights into the effectiveness of small, dedicated models\nin a domain-specific context.\n","authors":["Priyam Sahoo","Saurabh Pujar","Ganesh Nalawade","Richard Gebhardt","Louis Mandel","Luca Buratti"],"pdf_url":"https://arxiv.org/pdf/2402.17442v4.pdf","comment":"This paper has been published at the 39th IEEE/ACM International\n  Conference on Automated Software Engineering (ASE 2024), Industry Showcase\n  under the title \"Ansible Lightspeed: A Code Generation Service for IT\n  Automation\""},{"id":"http://arxiv.org/abs/2410.16864v1","updated":"2024-10-22T10:06:50Z","published":"2024-10-22T10:06:50Z","title":"Pedestrian motion prediction evaluation for urban autonomous driving","summary":"  Pedestrian motion prediction is a key part of the modular-based autonomous\ndriving pipeline, ensuring safe, accurate, and timely awareness of human\nagents' possible future trajectories. The autonomous vehicle can use this\ninformation to prevent any possible accidents and create a comfortable and\npleasant driving experience for the passengers and pedestrians. A wealth of\nresearch was done on the topic from the authors of robotics, computer vision,\nintelligent transportation systems, and other fields. However, a relatively\nunexplored angle is the integration of the state-of-art solutions into existing\nautonomous driving stacks and evaluating them in real-life conditions rather\nthan sanitized datasets. We analyze selected publications with provided\nopen-source solutions and provide a perspective obtained by integrating them\ninto existing Autonomous Driving framework - Autoware Mini and performing\nexperiments in natural urban conditions in Tartu, Estonia to determine\nvaluability of traditional motion prediction metrics. This perspective should\nbe valuable to any potential autonomous driving or robotics engineer looking\nfor the real-world performance of the existing state-of-art pedestrian motion\nprediction problem. The code with instructions on accessing the dataset is\navailable at https://github.com/dmytrozabolotnii/autoware_mini.\n","authors":["Dmytro Zabolotnii","Yar Muhammad","Naveed Muhammad"],"pdf_url":"https://arxiv.org/pdf/2410.16864v1.pdf","comment":"7 pages, 2 figures, 4 tables This work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2402.02500v3","updated":"2024-10-22T09:42:39Z","published":"2024-02-04T14:18:45Z","title":"Point Cloud Matters: Rethinking the Impact of Different Observation\n  Spaces on Robot Learning","summary":"  In robot learning, the observation space is crucial due to the distinct\ncharacteristics of different modalities, which can potentially become a\nbottleneck alongside policy design. In this study, we explore the influence of\nvarious observation spaces on robot learning, focusing on three predominant\nmodalities: RGB, RGB-D, and point cloud. We introduce OBSBench, a benchmark\ncomprising two simulators and 125 tasks, along with standardized pipelines for\nvarious encoders and policy baselines. Extensive experiments on diverse\ncontact-rich manipulation tasks reveal a notable trend: point cloud-based\nmethods, even those with the simplest designs, frequently outperform their RGB\nand RGB-D counterparts. This trend persists in both scenarios: training from\nscratch and utilizing pre-training. Furthermore, our findings demonstrate that\npoint cloud observations often yield better policy performance and\nsignificantly stronger generalization capabilities across various geometric and\nvisual conditions. These outcomes suggest that the 3D point cloud is a valuable\nobservation modality for intricate robotic tasks. We also suggest that\nincorporating both appearance and coordinate information can enhance the\nperformance of point cloud methods. We hope our work provides valuable insights\nand guidance for designing more generalizable and robust robotic models. Codes\nare available at https://github.com/HaoyiZhu/PointCloudMatters.\n","authors":["Haoyi Zhu","Yating Wang","Di Huang","Weicai Ye","Wanli Ouyang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2402.02500v3.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.16845v1","updated":"2024-10-22T09:33:29Z","published":"2024-10-22T09:33:29Z","title":"Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating\n  Few-Shot Node Classification","summary":"  Graph Neural Networks (GNNs) have shown superior performance in node\nclassification. However, GNNs perform poorly in the Few-Shot Node\nClassification (FSNC) task that requires robust generalization to make accurate\npredictions for unseen classes with limited labels. To tackle the challenge, we\npropose the integration of Sharpness-Aware Minimization (SAM)--a technique\ndesigned to enhance model generalization by finding a flat minimum of the loss\nlandscape--into GNN training. The standard SAM approach, however, consists of\ntwo forward-backward steps in each training iteration, doubling the\ncomputational cost compared to the base optimizer (e.g., Adam). To mitigate\nthis drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware\nMinimization (FGSAM), that integrates the rapid training of Multi-Layer\nPerceptrons (MLPs) with the superior performance of GNNs. Specifically, we\nutilize GNNs for parameter perturbation while employing MLPs to minimize the\nperturbed loss so that we can find a flat minimum with good generalization more\nefficiently. Moreover, our method reutilizes the gradient from the perturbation\nphase to incorporate graph topology into the minimization process at almost\nzero additional cost. To further enhance training efficiency, we develop FGSAM+\nthat executes exact perturbations periodically. Extensive experiments\ndemonstrate that our proposed algorithm outperforms the standard SAM with lower\ncomputational costs in FSNC tasks. In particular, our FGSAM+ as a SAM variant\noffers a faster optimization than the base optimizer in most cases. In addition\nto FSNC, our proposed methods also demonstrate competitive performance in the\nstandard node classification task for heterophilic graphs, highlighting the\nbroad applicability. The code is available at\nhttps://github.com/draym28/FGSAM_NeurIPS24.\n","authors":["Yihong Luo","Yuhan Chen","Siya Qiu","Yiwei Wang","Chen Zhang","Yan Zhou","Xiaochun Cao","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2410.16845v1.pdf","comment":"NeurIPS24; The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2404.03348v2","updated":"2024-10-22T09:31:49Z","published":"2024-04-04T10:28:55Z","title":"Knowledge Distillation-Based Model Extraction Attack using GAN-based\n  Private Counterfactual Explanations","summary":"  In recent years, there has been a notable increase in the deployment of\nmachine learning (ML) models as services (MLaaS) across diverse production\nsoftware applications. In parallel, explainable AI (XAI) continues to evolve,\naddressing the necessity for transparency and trustworthiness in ML models. XAI\ntechniques aim to enhance the transparency of ML models by providing insights,\nin terms of model's explanations, into their decision-making process.\nSimultaneously, some MLaaS platforms now offer explanations alongside the ML\nprediction outputs. This setup has elevated concerns regarding vulnerabilities\nin MLaaS, particularly in relation to privacy leakage attacks such as model\nextraction attacks (MEA). This is due to the fact that explanations can unveil\ninsights about the inner workings of the model which could be exploited by\nmalicious users. In this work, we focus on investigating how model\nexplanations, particularly counterfactual explanations (CFs), can be exploited\nfor performing MEA within the MLaaS platform. We also delve into assessing the\neffectiveness of incorporating differential privacy (DP) as a mitigation\nstrategy. To this end, we first propose a novel approach for MEA based on\nKnowledge Distillation (KD) to enhance the efficiency of extracting a\nsubstitute model of a target model exploiting CFs, without any knowledge about\nthe training data distribution by the attacker. Then, we advise an approach for\ntraining CF generators incorporating DP to generate private CFs. We conduct\nthorough experimental evaluations on real-world datasets and demonstrate that\nour proposed KD-based MEA can yield a high-fidelity substitute model with a\nreduced number of queries with respect to baseline approaches. Furthermore, our\nfindings reveal that including a privacy layer can allow mitigating the MEA.\nHowever, on the account of the quality of CFs, impacts the performance of the\nexplanations.\n","authors":["Fatima Ezzeddine","Omran Ayoub","Silvia Giordano"],"pdf_url":"https://arxiv.org/pdf/2404.03348v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2404.16656v2","updated":"2024-10-22T09:30:36Z","published":"2024-04-25T14:48:29Z","title":"A Self-Organizing Clustering System for Unsupervised Distribution Shift\n  Detection","summary":"  Modeling non-stationary data is a challenging problem in the field of\ncontinual learning, and data distribution shifts may result in negative\nconsequences on the performance of a machine learning model. Classic learning\ntools are often vulnerable to perturbations of the input covariates, and are\nsensitive to outliers and noise, and some tools are based on rigid algebraic\nassumptions. Distribution shifts are frequently occurring due to changes in raw\nmaterials for production, seasonality, a different user base, or even\nadversarial attacks. Therefore, there is a need for more effective distribution\nshift detection techniques. In this work, we propose a continual learning\nframework for monitoring and detecting distribution changes. We explore the\nproblem in a latent space generated by a bio-inspired self-organizing\nclustering and statistical aspects of the latent space. In particular, we\ninvestigate the projections made by two topology-preserving maps: the\nSelf-Organizing Map and the Scale Invariant Map. Our method can be applied in\nboth a supervised and an unsupervised context. We construct the assessment of\nchanges in the data distribution as a comparison of Gaussian signals, making\nthe proposed method fast and robust. We compare it to other unsupervised\ntechniques, specifically Principal Component Analysis (PCA) and Kernel-PCA. Our\ncomparison involves conducting experiments using sequences of images (based on\nMNIST and injected shifts with adversarial samples), chemical sensor\nmeasurements, and the environmental variable related to ozone levels. The\nempirical study reveals the potential of the proposed approach.\n","authors":["Sebastián Basterrech","Line Clemmensen","Gerardo Rubino"],"pdf_url":"https://arxiv.org/pdf/2404.16656v2.pdf","comment":"Revised version of the accepted manuscript to IJCNN'2024. Main\n  corrections were in Section 2.2 and Section 3.3. In Section 2.2 was corrected\n  expression (3), and in Section 3.3 in the definition of the elements of the\n  matrix $D$ it was a typo where $\\phi(x)$ was written instead of $x$"},{"id":"http://arxiv.org/abs/2410.16842v1","updated":"2024-10-22T09:25:04Z","published":"2024-10-22T09:25:04Z","title":"Assessment of Transformer-Based Encoder-Decoder Model for Human-Like\n  Summarization","summary":"  In recent times, extracting valuable information from large text is making\nsignificant progress. Especially in the current era of social media, people\nexpect quick bites of information. Automatic text summarization seeks to tackle\nthis by slimming large texts down into more manageable summaries. This\nimportant research area can aid in decision-making by digging out salient\ncontent from large text. With the progress in deep learning models, significant\nwork in language models has emerged. The encoder-decoder framework in deep\nlearning has become the central approach for automatic text summarization. This\nwork leverages transformer-based BART model for human-like summarization which\nis an open-ended problem with many challenges. On training and fine-tuning the\nencoder-decoder model, it is tested with diverse sample articles and the\nquality of summaries of diverse samples is assessed based on human evaluation\nparameters. Further, the finetuned model performance is compared with the\nbaseline pretrained model based on evaluation metrics like ROUGE score and\nBERTScore. Additionally, domain adaptation of the model is required for\nimproved performance of abstractive summarization of dialogues between\ninterlocutors. On investigating, the above popular evaluation metrics are found\nto be insensitive to factual errors. Further investigation of the summaries\ngenerated by finetuned model is done using the contemporary evaluation metrics\nof factual consistency like WeCheck and SummaC. Empirical results on BBC News\narticles highlight that the gold standard summaries written by humans are more\nfactually consistent by 17% than the abstractive summaries generated by\nfinetuned model.\n","authors":["Sindhu Nair","Y. S. Rao","Radha Shankarmani"],"pdf_url":"https://arxiv.org/pdf/2410.16842v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2409.15503v2","updated":"2024-10-22T09:23:03Z","published":"2024-09-23T19:46:19Z","title":"From Text to Treatment Effects: A Meta-Learning Approach to Handling\n  Text-Based Confounding","summary":"  One of the central goals of causal machine learning is the accurate\nestimation of heterogeneous treatment effects from observational data. In\nrecent years, meta-learning has emerged as a flexible, model-agnostic paradigm\nfor estimating conditional average treatment effects (CATE) using any\nsupervised model. This paper examines the performance of meta-learners when the\nconfounding variables are expressed in text. Through synthetic data\nexperiments, we show that learners using pre-trained text representations of\nconfounders, in addition to tabular background variables, achieve improved CATE\nestimates compared to those relying solely on the tabular variables,\nparticularly when sufficient data is available. However, due to the entangled\nnature of the text embeddings, these models do not fully match the performance\nof meta-learners with perfect confounder knowledge. These findings highlight\nboth the potential and the limitations of pre-trained text representations for\ncausal inference and open up interesting avenues for future research.\n","authors":["Henri Arno","Paloma Rabaey","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2409.15503v2.pdf","comment":"Presented at the Causal Representation Learning workshop at NeurIPS\n  2024"},{"id":"http://arxiv.org/abs/2312.01344v2","updated":"2024-10-22T09:22:33Z","published":"2023-12-03T10:40:07Z","title":"Enhancing Algorithm Performance Understanding through tsMorph:\n  Generating Semi-Synthetic Time Series for Robust Forecasting Evaluation","summary":"  Time series forecasting is a subject of significant scientific and industrial\nimportance. Despite the widespread utilization of forecasting methods, there is\na dearth of research aimed at comprehending the conditions under which these\nmethods yield favorable or unfavorable performances. Empirical studies,\nalthough common, are challenged by the limited availability of time series\ndatasets, restricting the extraction of reliable insights. To address this\nlimitation, we present tsMorph, a tool for generating semi-synthetic time\nseries through dataset morphing. tsMorph works by creating a sequence of\ndatasets from two original datasets. The characteristics of the generated\ndatasets progressively depart from those of one of the datasets and converge\ntoward the attributes of the other dataset. This method provides a valuable\nalternative for obtaining substantial datasets. In this paper, we show the\nbenefits of tsMorph by assessing the predictive performance of the Long\nShort-Term Memory Network and DeepAR forecasting algorithms. The time series\nused for the experiments comes from the NN5 Competition. The experimental\nresults provide important insights. Notably, the performances of the two\nalgorithms improve proportionally with the frequency of the time series. These\nexperiments confirm that tsMorph can be an effective tool for better\nunderstanding the behavior of forecasting algorithms, delivering a pathway to\novercoming the limitations posed by empirical studies and enabling more\nextensive and reliable experiments.\n","authors":["Moisés Santos","André de Carvalho","Carlos Soares"],"pdf_url":"https://arxiv.org/pdf/2312.01344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07563v2","updated":"2024-10-22T09:06:38Z","published":"2024-10-10T02:59:36Z","title":"PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency","summary":"  We introduce PLaMo-100B, a large-scale language model designed for Japanese\nproficiency. The model was trained from scratch using 2 trillion tokens, with\narchitecture such as QK Normalization and Z-Loss to ensure training stability\nduring the training process. Post-training techniques, including Supervised\nFine-Tuning and Direct Preference Optimization, were applied to refine the\nmodel's performance. Benchmark evaluations suggest that PLaMo-100B performs\nwell, particularly in Japanese-specific tasks, achieving results that are\ncompetitive with frontier models like GPT-4. The base model is available at\nhttps://huggingface.co/pfnet/plamo-100b.\n","authors":["Preferred Elements"," :","Kenshin Abe","Kaizaburo Chubachi","Yasuhiro Fujita","Yuta Hirokawa","Kentaro Imajo","Toshiki Kataoka","Hiroyoshi Komatsu","Hiroaki Mikami","Tsuguo Mogami","Shogo Murai","Kosuke Nakago","Daisuke Nishino","Toru Ogawa","Daisuke Okanohara","Yoshihiko Ozaki","Shotaro Sano","Shuji Suzuki","Tianqi Xu","Toshihiko Yanase"],"pdf_url":"https://arxiv.org/pdf/2410.07563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14710v2","updated":"2024-10-22T09:00:19Z","published":"2024-09-23T05:12:13Z","title":"ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning","summary":"  Role-playing is an emerging application in the field of Human-Computer\nInteraction (HCI), primarily implemented through the alignment training of a\nlarge language model (LLM) with assigned characters. Despite significant\nprogress, role-playing agents (RPLAs) still struggle with maintaining\nrole-consistency across conversations, particularly when confronted with\nboundary queries subtly related to character attributes. In this paper, we\npresent ERABAL, a framework aimed at enhancing RPLAs' role-playing capabilities\nthrough boundary-aware learning. ERABAL encompasses a generation pipeline for\nrole-specific dialogues and a concomitant methodology for alignment training.\nThrough comprehensive evaluations, we demonstrate that ERABAL is both efficient\nand effective. By training with significantly fewer dialogues than those used\nin leading approaches, ERABAL achieves notable improvements across\nWikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared\nto the generalist baseline models. Our code and datasets will be made publicly\navailable to support further research.\n","authors":["Yihong Tang","Jiao Ou","Che Liu","Fuzheng Zhang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2409.14710v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.10618"},{"id":"http://arxiv.org/abs/2410.16824v1","updated":"2024-10-22T08:57:17Z","published":"2024-10-22T08:57:17Z","title":"PerspectiveNet: Multi-View Perception for Dynamic Scene Understanding","summary":"  Generating detailed descriptions from multiple cameras and viewpoints is\nchallenging due to the complex and inconsistent nature of visual data. In this\npaper, we introduce PerspectiveNet, a lightweight yet efficient model for\ngenerating long descriptions across multiple camera views. Our approach\nutilizes a vision encoder, a compact connector module to convert visual\nfeatures into a fixed-size tensor, and large language models (LLMs) to harness\nthe strong natural language generation capabilities of LLMs. The connector\nmodule is designed with three main goals: mapping visual features onto LLM\nembeddings, emphasizing key information needed for description generation, and\nproducing a fixed-size feature matrix. Additionally, we augment our solution\nwith a secondary task, the correct frame sequence detection, enabling the model\nto search for the correct sequence of frames to generate descriptions. Finally,\nwe integrate the connector module, the secondary task, the LLM, and a visual\nfeature extraction model into a single architecture, which is trained for the\nTraffic Safety Description and Analysis task. This task requires generating\ndetailed, fine-grained descriptions of events from multiple cameras and\nviewpoints. The resulting model is lightweight, ensuring efficient training and\ninference, while remaining highly effective.\n","authors":["Vinh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.16824v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.16822v1","updated":"2024-10-22T08:48:52Z","published":"2024-10-22T08:48:52Z","title":"Can Large Language Models Act as Ensembler for Multi-GNNs?","summary":"  Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual nodesattributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto integrate multiple GNNs and leverage LLM's strengths, resulting in better\nperformance. Experimental results show that LensGNN outperforms existing\nmodels. This research advances text-attributed graph ensemble learning by\nproviding a robust, superior solution for integrating semantic and structural\ninformation. We provide our code and data here:\nhttps://anonymous.4open.science/r/EnsemGNN-E267/.\n","authors":["Hanqi Duan","Yao Cheng","Jianxiang Yu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2410.16822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16803v1","updated":"2024-10-22T08:28:05Z","published":"2024-10-22T08:28:05Z","title":"Context-aware Inductive Knowledge Graph Completion with Latent Type\n  Constraints and Subgraph Reasoning","summary":"  Inductive knowledge graph completion (KGC) aims to predict missing triples\nwith unseen entities. Recent works focus on modeling reasoning paths between\nthe head and tail entity as direct supporting evidence. However, these methods\ndepend heavily on the existence and quality of reasoning paths, which limits\ntheir general applicability in different scenarios. In addition, we observe\nthat latent type constraints and neighboring facts inherent in KGs are also\nvital in inferring missing triples. To effectively utilize all useful\ninformation in KGs, we introduce CATS, a novel context-aware inductive KGC\nsolution. With sufficient guidance from proper prompts and supervised\nfine-tuning, CATS activates the strong semantic understanding and reasoning\ncapabilities of large language models to assess the existence of query triples,\nwhich consist of two modules. First, the type-aware reasoning module evaluates\nwhether the candidate entity matches the latent entity type as required by the\nquery relation. Then, the subgraph reasoning module selects relevant reasoning\npaths and neighboring facts, and evaluates their correlation to the query\ntriple. Experiment results on three widely used datasets demonstrate that CATS\nsignificantly outperforms state-of-the-art methods in 16 out of 18\ntransductive, inductive, and few-shot settings with an average absolute MRR\nimprovement of 7.2%.\n","authors":["Muzhi Li","Cehao Yang","Chengjin Xu","Zixing Song","Xuhui Jiang","Jian Guo","Ho-fung Leung","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2410.16803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16801v1","updated":"2024-10-22T08:27:23Z","published":"2024-10-22T08:27:23Z","title":"Controlled Low-Rank Adaptation with Subspace Regularization for\n  Continued Training on Large Language Models","summary":"  Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsubspace regularization method on LoRA structure. Aiming to reduce the scale of\noutput change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix null space. Experimental\nresults on commonly used LLM finetuning tasks reveal that CLoRA significantly\noutperforms existing LoRA subsequent methods on both in-domain and outdomain\nevaluations, highlighting the superority of CLoRA as a effective\nparameter-efficient finetuning method with catastrophic forgetting mitigating.\nFurther investigation for model parameters indicates that CLoRA effectively\nbalances the trade-off between model capacity and degree of forgetting.\n","authors":["Yuheng Lu","Bingshuo Qian","Caixia Yuan","Huixing Jiang","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15052v2","updated":"2024-10-22T08:22:46Z","published":"2024-10-19T09:49:12Z","title":"Mining Glitch Tokens in Large Language Models via Gradient-based\n  Discrete Optimization","summary":"  Glitch tokens in Large Language Models (LLMs) can trigger unpredictable\nbehaviors, compromising model reliability and safety. Existing detection\nmethods often rely on manual observation to infer the prior distribution of\nglitch tokens, which is inefficient and lacks adaptability across diverse model\narchitectures. To address these limitations, we introduce GlitchMiner, a\ngradient-based discrete optimization framework designed for efficient glitch\ntoken detection in LLMs. GlitchMiner leverages an entropy-based loss function\nto quantify the uncertainty in model predictions and integrates first-order\nTaylor approximation with a local search strategy to effectively explore the\ntoken space. Our evaluation across various mainstream LLM architectures\ndemonstrates that GlitchMiner surpasses existing methods in both detection\nprecision and adaptability. In comparison to the previous state-of-the-art,\nGlitchMiner achieves an average improvement of 19.07% in precision@1000 for\nglitch token detection. By enabling efficient detection of glitch tokens,\nGlitchMiner provides a valuable tool for assessing and mitigating potential\nvulnerabilities in LLMs, contributing to their overall security.\n","authors":["Zihui Wu","Haichang Gao","Ping Wang","Shudong Zhang","Zhaoxiang Liu","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2410.15052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16795v1","updated":"2024-10-22T08:17:33Z","published":"2024-10-22T08:17:33Z","title":"Traj-Explainer: An Explainable and Robust Multi-modal Trajectory\n  Prediction Approach","summary":"  Navigating complex traffic environments has been significantly enhanced by\nadvancements in intelligent technologies, enabling accurate environment\nperception and trajectory prediction for automated vehicles. However, existing\nresearch often neglects the consideration of the joint reasoning of scenario\nagents and lacks interpretability in trajectory prediction models, thereby\nlimiting their practical application in real-world scenarios. To this purpose,\nan explainability-oriented trajectory prediction model is designed in this\nwork, named Explainable Conditional Diffusion based Multimodal Trajectory\nPrediction Traj-Explainer, to retrieve the influencing factors of prediction\nand help understand the intrinsic mechanism of prediction. In Traj-Explainer, a\nmodified conditional diffusion is well designed to capture the scenario\nmultimodal trajectory pattern, and meanwhile, a modified Shapley Value model is\nassembled to rationally learn the importance of the global and scenario\nfeatures. Numerical experiments are carried out by several trajectory\nprediction datasets, including Waymo, NGSIM, HighD, and MoCAD datasets.\nFurthermore, we evaluate the identified input factors which indicates that they\nare in agreement with the human driving experience, indicating the capability\nof the proposed model in appropriately learning the prediction. Code available\nin our open-source repository:\n\\url{https://anonymous.4open.science/r/Interpretable-Prediction}.\n","authors":["Pei Liu","Haipeng Liu","Yiqun Li","Tianyu Shi","Meixin Zhu","Ziyuan Pu"],"pdf_url":"https://arxiv.org/pdf/2410.16795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09185v3","updated":"2024-10-22T08:17:21Z","published":"2022-07-19T10:46:02Z","title":"Multimodal hierarchical Variational AutoEncoders with Factor Analysis\n  latent space","summary":"  Purpose: Handling heterogeneous and mixed data types has become increasingly\ncritical with the exponential growth in real-world databases. While deep\ngenerative models attempt to merge diverse data views into a common latent\nspace, they often sacrifice interpretability, flexibility, and modularity. This\nstudy proposes a novel method to address these limitations by combining\nVariational AutoEncoders (VAEs) with a Factor Analysis latent space (FA-VAE).\n  Methods: The proposed FA-VAE method employs multiple VAEs to learn a private\nrepresentation for each heterogeneous data view in a continuous latent space.\nInformation is shared between views using a low-dimensional latent space,\ngenerated via a linear projection matrix. This modular design creates a\nhierarchical dependency between private and shared latent spaces, allowing for\nthe flexible addition of new views and conditioning of pre-trained models.\n  Results: The FA-VAE approach facilitates cross-generation of data from\ndifferent domains and enables transfer learning between generative models. This\nallows for effective integration of information across diverse data views while\npreserving their distinct characteristics.\n  Conclusions: By overcoming the limitations of existing methods, the FA-VAE\nprovides a more interpretable, flexible, and modular solution for managing\nheterogeneous data types. It offers a pathway to more efficient and scalable\ndata-handling strategies, enhancing the potential for cross-domain data\nsynthesis and model transferability.\n","authors":["Alejandro Guerrero-López","Carlos Sevilla-Salcedo","Vanessa Gómez-Verdejo","Pablo M. Olmos"],"pdf_url":"https://arxiv.org/pdf/2207.09185v3.pdf","comment":"21 pages main work, 2 pages supplementary, 14 figures"},{"id":"http://arxiv.org/abs/2410.16794v1","updated":"2024-10-22T08:17:20Z","published":"2024-10-22T08:17:20Z","title":"One-Step Diffusion Distillation through Score Implicit Matching","summary":"  Despite their strong performances on many generative tasks, diffusion models\nrequire a large number of sampling steps in order to generate realistic\nsamples. This has motivated the community to develop effective methods to\ndistill pre-trained diffusion models into more efficient models, but these\nmethods still typically require few-step inference or perform substantially\nworse than the underlying model. In this paper, we present Score Implicit\nMatching (SIM) a new approach to distilling pre-trained diffusion models into\nsingle-step generator models, while maintaining almost the same sample\ngeneration ability as the original model as well as being data-free with no\nneed of training samples for distillation. The method rests upon the fact that,\nalthough the traditional score-based loss is intractable to minimize for\ngenerator models, under certain conditions we can efficiently compute the\ngradients for a wide class of score-based divergences between a diffusion model\nand a generator. SIM shows strong empirical performances for one-step\ngenerators: on the CIFAR10 dataset, it achieves an FID of 2.06 for\nunconditional generation and 1.96 for class-conditional generation. Moreover,\nby applying SIM to a leading transformer-based diffusion model, we distill a\nsingle-step generator for text-to-image (T2I) generation that attains an\naesthetic score of 6.42 with no performance decline over the original\nmulti-step counterpart, clearly outperforming the other one-step generators\nincluding SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85. We\nwill release this industry-ready one-step transformer-based T2I generator along\nwith this paper.\n","authors":["Weijian Luo","Zemin Huang","Zhengyang Geng","J. Zico Kolter","Guo-jun Qi"],"pdf_url":"https://arxiv.org/pdf/2410.16794v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16788v1","updated":"2024-10-22T08:04:32Z","published":"2024-10-22T08:04:32Z","title":"Correct after Answer: Enhancing Multi-Span Question Answering with\n  Post-Processing Method","summary":"  Multi-Span Question Answering (MSQA) requires models to extract one or\nmultiple answer spans from a given context to answer a question. Prior work\nmainly focuses on designing specific methods or applying heuristic strategies\nto encourage models to predict more correct predictions. However, these models\nare trained on gold answers and fail to consider the incorrect predictions.\nThrough a statistical analysis, we observe that models with stronger abilities\ndo not predict less incorrect predictions compared with other models. In this\nwork, we propose Answering-Classifying-Correcting (ACC) framework, which\nemploys a post-processing strategy to handle incorrect predictions.\nSpecifically, the ACC framework first introduces a classifier to classify the\npredictions into three types and exclude \"wrong predictions\", then introduces a\ncorrector to modify \"partially correct predictions\". Experiments on several\nMSQA datasets show that ACC framework significantly improves the Exact Match\n(EM) scores, and further analysis demostrates that ACC framework efficiently\nreduces the number of incorrect predictions, improving the quality of\npredictions.\n","authors":["Jiayi Lin","Chenyang Zhang","Haibo Tong","Dongyu Zhang","Qingqing Hong","Bingxuan Hou","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16788v1.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2211.12702v2","updated":"2024-10-22T07:57:55Z","published":"2022-11-23T04:48:49Z","title":"Evaluating Feature Attribution Methods for Electrocardiogram","summary":"  The performance of cardiac arrhythmia detection with electrocardiograms(ECGs)\nhas been considerably improved since the introduction of deep learning models.\nIn practice, the high performance alone is not sufficient and a proper\nexplanation is also required. Recently, researchers have started adopting\nfeature attribution methods to address this requirement, but it has been\nunclear which of the methods are appropriate for ECG. In this work, we identify\nand customize three evaluation metrics for feature attribution methods based on\nthe characteristics of ECG: localization score, pointing game, and degradation\nscore. Using the three evaluation metrics, we evaluate and analyze eleven\nwidely-used feature attribution methods. We find that some of the feature\nattribution methods are much more adequate for explaining ECG, where Grad-CAM\noutperforms the second-best method by a large margin.\n","authors":["Jangwon Suh","Jimyeong Kim","Euna Jung","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2211.12702v2.pdf","comment":"This is preliminary research related to\n  https://www.sciencedirect.com/science/article/pii/S0010482524011739 . Code is\n  available at https://github.com/SNU-DRL/Attribution-ECG"},{"id":"http://arxiv.org/abs/2410.16780v1","updated":"2024-10-22T07:53:41Z","published":"2024-10-22T07:53:41Z","title":"Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems","summary":"  The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task.\n","authors":["Krishna Sayana","Raghavendra Vasudeva","Yuri Vasilevski","Kun Su","Liam Hebert","Hubert Pham","Ambarish Jash","Sukhdeep Sodhi"],"pdf_url":"https://arxiv.org/pdf/2410.16780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16770v1","updated":"2024-10-22T07:40:20Z","published":"2024-10-22T07:40:20Z","title":"The Scene Language: Representing Scenes with Programs, Words, and\n  Embeddings","summary":"  We introduce the Scene Language, a visual scene representation that concisely\nand precisely describes the structure, semantics, and identity of visual\nscenes. It represents a scene with three key components: a program that\nspecifies the hierarchical and relational structure of entities in the scene,\nwords in natural language that summarize the semantic class of each entity, and\nembeddings that capture the visual identity of each entity. This representation\ncan be inferred from pre-trained language models via a training-free inference\ntechnique, given text or image inputs. The resulting scene can be rendered into\nimages using traditional, neural, or hybrid graphics renderers. Together, this\nforms a robust, automated system for high-quality 3D and 4D scene generation.\nCompared with existing representations like scene graphs, our proposed Scene\nLanguage generates complex scenes with higher fidelity, while explicitly\nmodeling the scene structures to enable precise control and editing.\n","authors":["Yunzhi Zhang","Zizhang Li","Matt Zhou","Shangzhe Wu","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2410.16770v1.pdf","comment":"Project page:\n  https://ai.stanford.edu/~yzzhang/projects/scene-language/"},{"id":"http://arxiv.org/abs/2410.16765v1","updated":"2024-10-22T07:33:34Z","published":"2024-10-22T07:33:34Z","title":"Survival Models: Proper Scoring Rule and Stochastic Optimization with\n  Competing Risks","summary":"  When dealing with right-censored data, where some outcomes are missing due to\na limited observation period, survival analysis -- known as time-to-event\nanalysis -- focuses on predicting the time until an event of interest occurs.\nMultiple classes of outcomes lead to a classification variant: predicting the\nmost likely event, a less explored area known as competing risks. Classic\ncompeting risks models couple architecture and loss, limiting scalability.To\naddress these issues, we design a strictly proper censoring-adjusted separable\nscoring rule, allowing optimization on a subset of the data as each observation\nis evaluated independently. The loss estimates outcome probabilities and\nenables stochastic optimization for competing risks, which we use for efficient\ngradient boosting trees. SurvivalBoost not only outperforms 12 state-of-the-art\nmodels across several metrics on 4 real-life datasets, both in competing risks\nand survival settings, but also provides great calibration, the ability to\npredict across any time horizon, and computation times faster than existing\nmethods.\n","authors":["Julie Alberge","Vincent Maladière","Olivier Grisel","Judith Abécassis","Gaël Varoquaux"],"pdf_url":"https://arxiv.org/pdf/2410.16765v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.14085"},{"id":"http://arxiv.org/abs/2410.16762v1","updated":"2024-10-22T07:29:05Z","published":"2024-10-22T07:29:05Z","title":"Deep-Sea A*+: An Advanced Path Planning Method Integrating Enhanced A*\n  and Dynamic Window Approach for Autonomous Underwater Vehicles","summary":"  As terrestrial resources become increasingly depleted, the demand for\ndeep-sea resource exploration has intensified. However, the extreme conditions\nin the deep-sea environment pose significant challenges for underwater\noperations, necessitating the development of robust detection robots. In this\npaper, we propose an advanced path planning methodology that integrates an\nimproved A* algorithm with the Dynamic Window Approach (DWA). By optimizing the\nsearch direction of the traditional A* algorithm and introducing an enhanced\nevaluation function, our improved A* algorithm accelerates path searching and\nreduces computational load. Additionally, the path-smoothing process has been\nrefined to improve continuity and smoothness, minimizing sharp turns. This\nmethod also integrates global path planning with local dynamic obstacle\navoidance via DWA, improving the real-time response of underwater robots in\ndynamic environments. Simulation results demonstrate that our proposed method\nsurpasses the traditional A* algorithm in terms of path smoothness, obstacle\navoidance, and real-time performance. The robustness of this approach in\ncomplex environments with both static and dynamic obstacles highlights its\npotential in autonomous underwater vehicle (AUV) navigation and obstacle\navoidance.\n","authors":["Yinyi Lai","Jiaqi Shang","Zenghui Liu","Zheyu Jiang","Yuyang Li","Longchao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16762v1.pdf","comment":"Accepted by 2024 International Conference on Big Data, Artificial\n  Intelligence and Internet of Things Engineering (ICBAIE 2024)"},{"id":"http://arxiv.org/abs/2410.16759v1","updated":"2024-10-22T07:25:17Z","published":"2024-10-22T07:25:17Z","title":"Towards Efficient IMC Accelerator Design Through Joint Hardware-Workload\n  Co-optimization","summary":"  Designing generalized in-memory computing (IMC) hardware that efficiently\nsupports a variety of workloads requires extensive design space exploration,\nwhich is infeasible to perform manually. Optimizing hardware individually for\neach workload or solely for the largest workload often fails to yield the most\nefficient generalized solutions. To address this, we propose a joint\nhardware-workload optimization framework that identifies optimised IMC chip\narchitecture parameters, enabling more efficient, workload-flexible hardware.\nWe show that joint optimization achieves 36%, 36%, 20%, and 69% better\nenergy-latency-area scores for VGG16, ResNet18, AlexNet, and MobileNetV3,\nrespectively, compared to the separate architecture parameters search\noptimizing for a single largest workload. Additionally, we quantify the\nperformance trade-offs and losses of the resulting generalized IMC hardware\ncompared to workload-specific IMC designs.\n","authors":["Olga Krestinskaya","Mohammed E. Fouda","Ahmed Eltawil","Khaled N. Salama"],"pdf_url":"https://arxiv.org/pdf/2410.16759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14748v2","updated":"2024-10-22T07:19:40Z","published":"2024-10-17T19:38:55Z","title":"ETF: An Entity Tracing Framework for Hallucination Detection in Code\n  Summaries","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced their ability to understand both natural language and code, driving\ntheir use in tasks like natural language-to-code (NL2Code) and code\nsummarization. However, LLMs are prone to hallucination-outputs that stray from\nintended meanings. Detecting hallucinations in code summarization is especially\ndifficult due to the complex interplay between programming and natural\nlanguages. We introduce a first-of-its-kind dataset with $\\sim$10K samples,\ncurated specifically for hallucination detection in code summarization. We\nfurther propose a novel Entity Tracing Framework (ETF) that a) utilizes static\nprogram analysis to identify code entities from the program and b) uses LLMs to\nmap and verify these entities and their intents within generated code\nsummaries. Our experimental analysis demonstrates the effectiveness of the\nframework, leading to a 0.73 F1 score. This approach provides an interpretable\nmethod for detecting hallucinations by grounding entities, allowing us to\nevaluate summary accuracy.\n","authors":["Kishan Maharaj","Vitobha Munigala","Srikanth G. Tamilselvam","Prince Kumar","Sayandeep Sen","Palani Kodeswaran","Abhijit Mishra","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2410.14748v2.pdf","comment":"11 pages, 6 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2401.06960v2","updated":"2024-10-22T07:17:47Z","published":"2024-01-13T03:17:57Z","title":"Transformer for Object Re-Identification: A Survey","summary":"  Object Re-identification (Re-ID) aims to identify specific objects across\ndifferent times and scenes, which is a widely researched task in computer\nvision. For a prolonged period, this field has been predominantly driven by\ndeep learning technology based on convolutional neural networks. In recent\nyears, the emergence of Vision Transformers has spurred a growing number of\nstudies delving deeper into Transformer-based Re-ID, continuously breaking\nperformance records and witnessing significant progress in the Re-ID field.\nOffering a powerful, flexible, and unified solution, Transformers cater to a\nwide array of Re-ID tasks with unparalleled efficacy. This paper provides a\ncomprehensive review and in-depth analysis of the Transformer-based Re-ID. In\ncategorizing existing works into Image/Video-Based Re-ID, Re-ID with limited\ndata/annotations, Cross-Modal Re-ID, and Special Re-ID Scenarios, we thoroughly\nelucidate the advantages demonstrated by the Transformer in addressing a\nmultitude of challenges across these domains. Considering the trending\nunsupervised Re-ID, we propose a new Transformer baseline, UntransReID,\nachieving state-of-the-art performance on both single/cross modal tasks. For\nthe under-explored animal Re-ID, we devise a standardized experimental\nbenchmark and conduct extensive experiments to explore the applicability of\nTransformer for this task and facilitate future research. Finally, we discuss\nsome important yet under-investigated open issues in the large foundation model\nera, we believe it will serve as a new handbook for researchers in this field.\nA periodically updated website will be available at\nhttps://github.com/mangye16/ReID-Survey.\n","authors":["Mang Ye","Shuoyi Chen","Chenyue Li","Wei-Shi Zheng","David Crandall","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2401.06960v2.pdf","comment":"Accepted by International Journal of Computer Vision (IJCV) in\n  October 2024"},{"id":"http://arxiv.org/abs/2410.16748v1","updated":"2024-10-22T07:06:00Z","published":"2024-10-22T07:06:00Z","title":"Uncovering Key Trends in Industry 5.0 through Advanced AI Techniques","summary":"  This article analyzes around 200 online articles to identify trends within\nIndustry 5.0 using artificial intelligence techniques. Specifically, it applies\nalgorithms such as LDA, BERTopic, LSA, and K-means, in various configurations,\nto extract and compare the central themes present in the literature. The\nresults reveal a convergence around a core set of themes while also\nhighlighting that Industry 5.0 spans a wide range of topics. The study\nconcludes that Industry 5.0, as an evolution of Industry 4.0, is a broad\nconcept that lacks a clear definition, making it difficult to focus on and\napply effectively. Therefore, for Industry 5.0 to be useful, it needs to be\nrefined and more clearly defined. Furthermore, the findings demonstrate that\nwell-known AI techniques can be effectively utilized for trend identification,\nparticularly when the available literature is extensive and the subject matter\nlacks precise boundaries. This study showcases the potential of AI in\nextracting meaningful insights from large and diverse datasets, even in cases\nwhere the thematic structure of the domain is not clearly delineated.\n","authors":["Panos Fitsilis","Paraskevi Tsoutsa","Vyron Damasiotis","Vasileios Kyriatzis"],"pdf_url":"https://arxiv.org/pdf/2410.16748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16746v1","updated":"2024-10-22T07:00:43Z","published":"2024-10-22T07:00:43Z","title":"SpikMamba: When SNN meets Mamba in Event-based Human Action Recognition","summary":"  Human action recognition (HAR) plays a key role in various applications such\nas video analysis, surveillance, autonomous driving, robotics, and healthcare.\nMost HAR algorithms are developed from RGB images, which capture detailed\nvisual information. However, these algorithms raise concerns in\nprivacy-sensitive environments due to the recording of identifiable features.\nEvent cameras offer a promising solution by capturing scene brightness changes\nsparsely at the pixel level, without capturing full images. Moreover, event\ncameras have high dynamic ranges that can effectively handle scenarios with\ncomplex lighting conditions, such as low light or high contrast environments.\nHowever, using event cameras introduces challenges in modeling the spatially\nsparse and high temporal resolution event data for HAR. To address these\nissues, we propose the SpikMamba framework, which combines the energy\nefficiency of spiking neural networks and the long sequence modeling capability\nof Mamba to efficiently capture global features from spatially sparse and high\na temporal resolution event data. Additionally, to improve the locality of\nmodeling, a spiking window-based linear attention mechanism is used. Extensive\nexperiments show that SpikMamba achieves remarkable recognition performance,\nsurpassing the previous state-of-the-art by 1.45%, 7.22%, 0.15%, and 3.92% on\nthe PAF, HARDVS, DVS128, and E-FAction datasets, respectively. The code is\navailable at https://github.com/Typistchen/SpikMamba.\n","authors":["Jiaqi Chen","Yan Yang","Shizhuo Deng","Da Teng","Liyuan Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16746v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.16739v1","updated":"2024-10-22T06:46:28Z","published":"2024-10-22T06:46:28Z","title":"Corrected Soft Actor Critic for Continuous Control","summary":"  The Soft Actor-Critic (SAC) algorithm is known for its stability and high\nsample efficiency in deep reinforcement learning. However, the tanh\ntransformation applied to sampled actions in SAC distorts the action\ndistribution, hindering the selection of the most probable actions. This paper\npresents a novel action sampling method that directly identifies and selects\nthe most probable actions within the transformed distribution, thereby\naddressing this issue. Extensive experiments on standard continuous control\nbenchmarks demonstrate that the proposed method significantly enhances SAC's\nperformance, resulting in faster convergence and higher cumulative rewards\ncompared to the original algorithm.\n","authors":["Yanjun Chen","Xinming Zhang","Xianghui Wang","Zhiqiang Xu","Xiaoyu Shen","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03876v2","updated":"2024-10-22T06:36:46Z","published":"2023-12-06T19:46:06Z","title":"Scaling transformer neural networks for skillful and reliable\n  medium-range weather forecasting","summary":"  Weather forecasting is a fundamental problem for anticipating and mitigating\nthe impacts of climate change. Recently, data-driven approaches for weather\nforecasting based on deep learning have shown great promise, achieving\naccuracies that are competitive with operational systems. However, those\nmethods often employ complex, customized architectures without sufficient\nablation analysis, making it difficult to understand what truly contributes to\ntheir success. Here we introduce Stormer, a simple transformer model that\nachieves state-of-the-art performance on weather forecasting with minimal\nchanges to the standard transformer backbone. We identify the key components of\nStormer through careful empirical analyses, including weather-specific\nembedding, randomized dynamics forecast, and pressure-weighted loss. At the\ncore of Stormer is a randomized forecasting objective that trains the model to\nforecast the weather dynamics over varying time intervals. During inference,\nthis allows us to produce multiple forecasts for a target lead time and combine\nthem to obtain better forecast accuracy. On WeatherBench 2, Stormer performs\ncompetitively at short to medium-range forecasts and outperforms current\nmethods beyond 7 days, while requiring orders-of-magnitude less training data\nand compute. Additionally, we demonstrate Stormer's favorable scaling\nproperties, showing consistent improvements in forecast accuracy with increases\nin model size and training tokens. Code and checkpoints are available at\nhttps://github.com/tung-nd/stormer.\n","authors":["Tung Nguyen","Rohan Shah","Hritik Bansal","Troy Arcomano","Romit Maulik","Veerabhadra Kotamarthi","Ian Foster","Sandeep Madireddy","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2312.03876v2.pdf","comment":"Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.16733v1","updated":"2024-10-22T06:33:48Z","published":"2024-10-22T06:33:48Z","title":"50 questions on Active Assisted Living technologies. Global edition","summary":"  This booklet on Active Assisted Living (AAL) technologies has been created as\npart of the GoodBrother COST Action, which has run from 2020 to 2024. COST\nActions are European research programs that promote collaboration across\nborders, uniting researchers, professionals, and institutions to address key\nsocietal challenges. GoodBrother focused on ethical and privacy concerns\nsurrounding video and audio monitoring in care settings. The aim was to ensure\nthat while AAL technologies help older adults and vulnerable individuals, their\nprivacy and data protection rights remain a top priority.\n  This booklet is designed to guide you through the role that AAL technologies\nplay in improving the quality of life for older adults, caregivers, and people\nwith disabilities. AAL technologies offer tools for those facing cognitive or\nphysical challenges. They can enhance independence, assist with daily routines,\nand promote a safer living environment. However, the rise of these technologies\nalso brings important questions about data protection and user autonomy.\n  This resource is intended for a wide audience, including end users,\ncaregivers, healthcare professionals, and policymakers. It provides practical\nguidance on integrating AAL technologies into care settings while safeguarding\nprivacy and ensuring ethical use. The insights offered here aim to empower\nusers and caregivers to make informed choices that enhance both the quality of\ncare and respect for personal autonomy.\n","authors":["Francisco Florez-Revuelta","Alin Ake-Kob","Pau Climent-Perez","Paulo Coelho","Liane Colonna","Laila Dahabiyeh","Carina Dantas","Esra Dogru-Huzmeli","Hazim Kemal Ekenel","Aleksandar Jevremovic","Nina Hosseini-Kivanani","Aysegul Ilgaz","Mladjan Jovanovic","Andrzej Klimczuk","Maksymilian M. Kuźmicz","Petre Lameski","Ferlanda Luna","Natália Machado","Tamara Mujirishvili","Zada Pajalic","Galidiya Petrova","Nathalie G. S. Puaschitz","Maria Jose Santofimia","Agusti Solanas","Wilhelmina van Staalduinen","Ziya Ata Yazici"],"pdf_url":"https://arxiv.org/pdf/2410.16733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16726v1","updated":"2024-10-22T06:25:16Z","published":"2024-10-22T06:25:16Z","title":"Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap","summary":"  While automatic speech recognition (ASR) systems have achieved remarkable\nperformance with large-scale datasets, their efficacy remains inadequate in\nlow-resource settings, encompassing dialects, accents, minority languages, and\nlong-tail hotwords, domains with significant practical relevance. With the\nadvent of versatile and powerful text-to-speech (TTS) models, capable of\ngenerating speech with human-level naturalness, expressiveness, and diverse\nspeaker profiles, leveraging TTS for ASR data augmentation provides a\ncost-effective and practical approach to enhancing ASR performance.\nComprehensive experiments on an unprecedentedly rich variety of low-resource\ndatasets demonstrate consistent and substantial performance improvements,\nproving that the proposed method of enhancing low-resource ASR through a\nversatile TTS model is highly effective and has broad application prospects.\nFurthermore, we delve deeper into key characteristics of synthesized speech\ndata that contribute to ASR improvement, examining factors such as text\ndiversity, speaker diversity, and the volume of synthesized data, with text\ndiversity being studied for the first time in this work. We hope our findings\nprovide helpful guidance and reference for the practical application of\nTTS-based data augmentation and push the advancement of low-resource ASR one\nstep further.\n","authors":["Guanrou Yang","Fan Yu","Ziyang Ma","Zhihao Du","Zhifu Gao","Shiliang Zhang","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16635v3","updated":"2024-10-22T06:19:20Z","published":"2024-01-30T00:17:37Z","title":"Improving Reinforcement Learning from Human Feedback with Efficient\n  Reward Model Ensemble","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a widely adopted\napproach for aligning large language models with human values. However, RLHF\nrelies on a reward model that is trained with a limited amount of human\npreference data, which could lead to inaccurate predictions. As a result, RLHF\nmay produce outputs that are misaligned with human values. To mitigate this\nissue, we contribute a reward ensemble method that allows the reward model to\nmake more accurate predictions. As using an ensemble of large language\nmodel-based reward models can be computationally and resource-expensive, we\nexplore efficient ensemble methods including linear-layer ensemble and\nLoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy\nOptimization with our ensembled reward models, and verify that our ensemble\nmethods help improve the alignment performance of RLHF outputs.\n","authors":["Shun Zhang","Zhenfang Chen","Sunli Chen","Yikang Shen","Zhiqing Sun","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2401.16635v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16723v1","updated":"2024-10-22T06:12:04Z","published":"2024-10-22T06:12:04Z","title":"Resource-Efficient Sensor Fusion via System-Wide Dynamic Gated Neural\n  Networks","summary":"  Mobile systems will have to support multiple AI-based applications, each\nleveraging heterogeneous data sources through DNN architectures collaboratively\nexecuted within the network. To minimize the cost of the AI inference task\nsubject to requirements on latency, quality, and - crucially - reliability of\nthe inference process, it is vital to optimize (i) the set of sensors/data\nsources and (ii) the DNN architecture, (iii) the network nodes executing\nsections of the DNN, and (iv) the resources to use. To this end, we leverage\ndynamic gated neural networks with branches, and propose a novel algorithmic\nstrategy called Quantile-constrained Inference (QIC), based upon\nquantile-Constrained policy optimization. QIC makes joint, high-quality, swift\ndecisions on all the above aspects of the system, with the aim to minimize\ninference energy cost. We remark that this is the first contribution connecting\ngated dynamic DNNs with infrastructure-level decision making. We evaluate QIC\nusing a dynamic gated DNN with stems and branches for optimal sensor fusion and\ninference, trained on the RADIATE dataset offering Radar, LiDAR, and Camera\ndata, and real-world wireless measurements. Our results confirm that QIC\nmatches the optimum and outperforms its alternatives by over 80%.\n","authors":["Chetna Singhal","Yashuo Wu","Francesco Malandrino","Sharon Ladron de Guevara Contreras","Marco Levorato","Carla Fabiana Chiasserini"],"pdf_url":"https://arxiv.org/pdf/2410.16723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16713v1","updated":"2024-10-22T05:49:24Z","published":"2024-10-22T05:49:24Z","title":"Collapse or Thrive? Perils and Promises of Synthetic Data in a\n  Self-Generating World","summary":"  The increasing presence of AI-generated content on the internet raises a\ncritical question: What happens when generative machine learning models are\npretrained on web-scale datasets containing data created by earlier models?\nSome authors prophesy $\\textit{model collapse}$ under a \"$\\textit{replace}$\"\nscenario: a sequence of models, the first trained with real data and each later\none trained only on synthetic data from its preceding model. In this scenario,\nmodels successively degrade. Others see collapse as easily avoidable; in an\n\"$\\textit{accumulate}$' scenario, a sequence of models is trained, but each\ntraining uses all real and synthetic data generated so far. In this work, we\ndeepen and extend the study of these contrasting scenarios. First, collapse\nversus avoidance of collapse is studied by comparing the replace and accumulate\nscenarios on each of three prominent generative modeling settings; we find the\nsame contrast emerges in all three settings. Second, we study a compromise\nscenario; the available data remains the same as in the accumulate scenario --\nbut unlike $\\textit{accumulate}$ and like $\\textit{replace}$, each model is\ntrained using a fixed compute budget; we demonstrate that model test loss on\nreal data is larger than in the $\\textit{accumulate}$ scenario, but apparently\nplateaus, unlike the divergence seen with $\\textit{replace}$. Third, we study\nthe relative importance of cardinality and proportion of real data for avoiding\nmodel collapse. Surprisingly, we find a non-trivial interaction between real\nand synthetic data, where the value of synthetic data for reducing test loss\ndepends on the absolute quantity of real data. Our insights are particularly\nimportant when forecasting whether future frontier generative models will\ncollapse or thrive, and our results open avenues for empirically and\nmathematically studying the context-dependent value of synthetic data.\n","authors":["Joshua Kazdan","Rylan Schaeffer","Apratim Dey","Matthias Gerstgrasser","Rafael Rafailov","David L. Donoho","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2410.16713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11216v2","updated":"2024-10-22T05:45:46Z","published":"2024-04-17T10:00:56Z","title":"Position Engineering: Boosting Large Language Models through Positional\n  Information Manipulation","summary":"  The performance of large language models (LLMs) is significantly influenced\nby the quality of the prompts provided. In response, researchers have developed\nenormous prompt engineering strategies aimed at modifying the prompt text to\nenhance task performance. In this paper, we introduce a novel technique termed\nposition engineering, which offers a more efficient way to guide large language\nmodels. Unlike prompt engineering, which requires substantial effort to modify\nthe text provided to LLMs, position engineering merely involves altering the\npositional information in the prompt without modifying the text itself. We have\nevaluated position engineering in two widely-used LLM scenarios:\nretrieval-augmented generation (RAG) and in-context learning (ICL). Our\nfindings show that position engineering substantially improves upon the\nbaseline in both cases. Position engineering thus represents a promising new\nstrategy for exploiting the capabilities of large language models.\n","authors":["Zhiyuan He","Huiqiang Jiang","Zilong Wang","Yuqing Yang","Luna Qiu","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2404.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16711v1","updated":"2024-10-22T05:37:51Z","published":"2024-10-22T05:37:51Z","title":"Development of CNN Architectures using Transfer Learning Methods for\n  Medical Image Classification","summary":"  The application of deep learning-based architecture has seen a tremendous\nrise in recent years. For example, medical image classification using deep\nlearning achieved breakthrough results. Convolutional Neural Networks (CNNs)\nare implemented predominantly in medical image classification and segmentation.\nOn the other hand, transfer learning has emerged as a prominent supporting tool\nfor enhancing the efficiency and accuracy of deep learning models. This paper\ninvestigates the development of CNN architectures using transfer learning\ntechniques in the field of medical image classification using a timeline\nmapping model for key image classification challenges. Our findings help make\nan informed decision while selecting the optimum and state-of-the-art CNN\narchitectures.\n","authors":["Ganga Prasad Basyal","David Zeng","Bhaskar Pm Rimal"],"pdf_url":"https://arxiv.org/pdf/2410.16711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16710v1","updated":"2024-10-22T05:32:40Z","published":"2024-10-22T05:32:40Z","title":"Influential Language Data Selection via Gradient Trajectory Pursuit","summary":"  Curating a desirable dataset for training has been the core of building\nhighly capable large language models (Touvron et al., 2023; Achiam et al.,\n2023; Team et al.,2024). Gradient influence scores (Pruthi et al., 2020; Xia et\nal., 2024) are shown to be correlated with model performance and are commonly\nused as the criterion for data selection. However, existing methods are built\nupon either individual sample rankings or inefficient matching process, leading\nto suboptimal performance or scaling up issues.In this paper, we propose\nGradient Trajectory Pursuit (GTP), an algorithm that performs pursuit of\ngradient trajectories via jointly selecting data points under an L0-norm\nregularized objective. The proposed algorithm highlights: (1) joint selection\ninstead of independent top-k selection, which automatically de-duplicates\nsamples; (2) higher efficiency with compressive sampling processes, which can\nbe further sped up using a distributed framework. In the experiments, we\ndemonstrate the algorithm in both in-domain and target-domain selection\nbenchmarks and show that it outperforms top-k selection and competitive\nalgorithms consistently, for example, our algorithm chooses as low as 0.5% data\nto achieve full performance on the targeted instruction tuning tasks\n","authors":["Zhiwei Deng","Tao Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2410.16710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16709v1","updated":"2024-10-22T05:27:01Z","published":"2024-10-22T05:27:01Z","title":"Universal approximation property of ODENet and ResNet with a single\n  activation function","summary":"  We study a universal approximation property of ODENet and ResNet. The ODENet\nis a map from an initial value to the final value of an ODE system in a finite\ninterval. It is considered a mathematical model of a ResNet-type deep learning\nsystem. We consider dynamical systems with vector fields given by a single\ncomposition of the activation function and an affine mapping, which is the most\ncommon choice of the ODENet or ResNet vector field in actual machine learning\nsystems. We show that such an ODENet and ResNet with a restricted vector field\ncan uniformly approximate ODENet with a general vector field.\n","authors":["Masato Kimura","Kazunori Matsui","Yosuke Mizuno"],"pdf_url":"https://arxiv.org/pdf/2410.16709v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.16705v1","updated":"2024-10-22T05:20:21Z","published":"2024-10-22T05:20:21Z","title":"Privacy-hardened and hallucination-resistant synthetic data generation\n  with logic-solvers","summary":"  Machine-generated data is a valuable resource for training Artificial\nIntelligence algorithms, evaluating rare workflows, and sharing data under\nstricter data legislations. The challenge is to generate data that is accurate\nand private. Current statistical and deep learning methods struggle with large\ndata volumes, are prone to hallucinating scenarios incompatible with reality,\nand seldom quantify privacy meaningfully. Here we introduce Genomator, a logic\nsolving approach (SAT solving), which efficiently produces private and\nrealistic representations of the original data. We demonstrate the method on\ngenomic data, which arguably is the most complex and private information.\nSynthetic genomes hold great potential for balancing underrepresented\npopulations in medical research and advancing global data exchange. We\nbenchmark Genomator against state-of-the-art methodologies (Markov generation,\nRestricted Boltzmann Machine, Generative Adversarial Network and Conditional\nRestricted Boltzmann Machines), demonstrating an 84-93% accuracy improvement\nand 95-98% higher privacy. Genomator is also 1000-1600 times more efficient,\nmaking it the only tested method that scales to whole genomes. We show the\nuniversal trade-off between privacy and accuracy, and use Genomator's tuning\ncapability to cater to all applications along the spectrum, from provable\nprivate representations of sensitive cohorts, to datasets with\nindistinguishable pharmacogenomic profiles. Demonstrating the production-scale\ngeneration of tuneable synthetic data can increase trust and pave the way into\nthe clinic.\n","authors":["Mark A. Burgess","Brendan Hosking","Roc Reguant","Anubhav Kaphle","Mitchell J. O'Brien","Letitia M. F. Sng","Yatish Jain","Denis C. Bauer"],"pdf_url":"https://arxiv.org/pdf/2410.16705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16703v1","updated":"2024-10-22T05:16:19Z","published":"2024-10-22T05:16:19Z","title":"PLDR-LLM: Large Language Model from Power Law Decoder Representations","summary":"  We present the Large Language Model from Power Law Decoder Representations\n(PLDR-LLM), a language model that leverages non-linear and linear\ntransformations through Power Law Graph Attention mechanism to generate\nwell-defined deductive and inductive outputs. We pretrain the PLDR-LLMs of\nvarying layer sizes with a small batch size of 32 and $\\sim$8B tokens from the\nRefinedWeb dataset, and show that they achieve competitive performance in\nzero-shot and few-shot settings compared to scaled dot-product LLMs of similar\nmodel size reported in the literature. We show that deductive outputs of\nPLDR-LLMs can be used to compare model characteristics or improve the\nperformance by introducing the Directed Acyclic Graph (DAG) loss as a metric\nand regularizer. Our results indicate that the initial maximum learning rate\nand warm-up steps have a lasting impact on deductive outputs throughout the\npretraining. We provide a detailed description of PLDR-LLM architecture, its\nimplementation and the pretraining procedure.\n","authors":["Burc Gokden"],"pdf_url":"https://arxiv.org/pdf/2410.16703v1.pdf","comment":"22 pages, 4 figures, 10 tables"},{"id":"http://arxiv.org/abs/2407.00219v2","updated":"2024-10-22T05:13:15Z","published":"2024-06-28T20:06:30Z","title":"Evaluating Human Alignment and Model Faithfulness of LLM Rationale","summary":"  We study how well large language models (LLMs) explain their generations\nthrough rationales -- a set of tokens extracted from the input text that\nreflect the decision-making process of LLMs. Specifically, we systematically\nstudy rationales derived using two approaches: (1) popular prompting-based\nmethods, where prompts are used to guide LLMs in generating rationales, and (2)\ntechnical attribution-based methods, which leverage attention or gradients to\nidentify important tokens. Our analysis spans three classification datasets\nwith annotated rationales, encompassing tasks with varying performance levels.\nWhile prompting-based self-explanations are widely used, our study reveals that\nthese explanations are not always as \"aligned\" with the human rationale as\nattribution-based explanations. Even more so, fine-tuning LLMs to enhance\nclassification task accuracy does not enhance the alignment of prompting-based\nrationales. Still, it does considerably improve the alignment of\nattribution-based methods (e.g., InputXGradient). More importantly, we show\nthat prompting-based self-explanation is also less \"faithful\" than\nattribution-based explanations, failing to provide a reliable account of the\nmodel's decision-making process. To evaluate faithfulness, unlike prior studies\nthat excluded misclassified examples, we evaluate all instances and also\nexamine the impact of fine-tuning and accuracy on alignment and faithfulness.\nOur findings suggest that inconclusive faithfulness results reported in earlier\nstudies may stem from low classification accuracy. These findings underscore\nthe importance of more rigorous and comprehensive evaluations of LLM\nrationales.\n","authors":["Mohsen Fayyaz","Fan Yin","Jiao Sun","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2407.00219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06682v2","updated":"2024-10-22T05:12:24Z","published":"2023-12-09T07:08:00Z","title":"Learning to Denoise Biomedical Knowledge Graph for Robust Molecular\n  Interaction Prediction","summary":"  Molecular interaction prediction plays a crucial role in forecasting unknown\ninteractions between molecules, such as drug-target interaction (DTI) and\ndrug-drug interaction (DDI), which are essential in the field of drug discovery\nand therapeutics. Although previous prediction methods have yielded promising\nresults by leveraging the rich semantics and topological structure of\nbiomedical knowledge graphs (KGs), they have primarily focused on enhancing\npredictive performance without addressing the presence of inevitable noise and\ninconsistent semantics. This limitation has hindered the advancement of\nKG-based prediction methods. To address this limitation, we propose BioKDN\n(Biomedical Knowledge Graph Denoising Network) for robust molecular interaction\nprediction. BioKDN refines the reliable structure of local subgraphs by\ndenoising noisy links in a learnable manner, providing a general module for\nextracting task-relevant interactions. To enhance the reliability of the\nrefined structure, BioKDN maintains consistent and robust semantics by\nsmoothing relations around the target interaction. By maximizing the mutual\ninformation between reliable structure and smoothed relations, BioKDN\nemphasizes informative semantics to enable precise predictions. Experimental\nresults on real-world datasets show that BioKDN surpasses state-of-the-art\nmodels in DTI and DDI prediction tasks, confirming the effectiveness and\nrobustness of BioKDN in denoising unreliable interactions within contaminated\nKGs\n","authors":["Tengfei Ma","Yujie Chen","Wen Tao","Dashun Zheng","Xuan Lin","Patrick Cheong-lao Pang","Yiping Liu","Yijun Wang","Longyue Wang","Bosheng Song","Xiangxiang Zeng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.06682v2.pdf","comment":"13 pages, Accepted at TKDE"},{"id":"http://arxiv.org/abs/2410.16700v1","updated":"2024-10-22T05:11:54Z","published":"2024-10-22T05:11:54Z","title":"AskBeacon -- Performing genomic data exchange and analytics with natural\n  language","summary":"  Enabling clinicians and researchers to directly interact with global genomic\ndata resources by removing technological barriers is vital for medical\ngenomics. AskBeacon enables Large Language Models to be applied to securely\nshared cohorts via the GA4GH Beacon protocol. By simply \"asking\" Beacon,\nactionable insights can be gained, analyzed and made publication-ready.\n","authors":["Anuradha Wickramarachchi","Shakila Tonni","Sonali Majumdar","Sarvnaz Karimi","Sulev Kõks","Brendan Hosking","Jordi Rambla","Natalie A. Twine","Yatish Jain","Denis C. Bauer"],"pdf_url":"https://arxiv.org/pdf/2410.16700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16699v1","updated":"2024-10-22T05:11:45Z","published":"2024-10-22T05:11:45Z","title":"Graph Transformers Dream of Electric Flow","summary":"  We show theoretically and empirically that the linear Transformer, when\napplied to graph data, can implement algorithms that solve canonical problems\nsuch as electric flow and eigenvector decomposition. The input to the\nTransformer is simply the graph incidence matrix; no other explicit positional\nencoding information is provided. We present explicit weight configurations for\nimplementing each such graph algorithm, and we bound the errors of the\nconstructed Transformers by the errors of the underlying algorithms. Our\ntheoretical findings are corroborated by experiments on synthetic data.\nAdditionally, on a real-world molecular regression task, we observe that the\nlinear Transformer is capable of learning a more effective positional encoding\nthan the default one based on Laplacian eigenvectors. Our work is an initial\nstep towards elucidating the inner-workings of the Transformer for graph data.\n","authors":["Xiang Cheng","Lawrence Carin","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2410.16699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16695v1","updated":"2024-10-22T04:57:28Z","published":"2024-10-22T04:57:28Z","title":"MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark","summary":"  Phytoplankton are a crucial component of aquatic ecosystems, and effective\nmonitoring of them can provide valuable insights into ocean environments and\necosystem changes. Traditional phytoplankton monitoring methods are often\ncomplex and lack timely analysis. Therefore, deep learning algorithms offer a\npromising approach for automated phytoplankton monitoring. However, the lack of\nlarge-scale, high-quality training samples has become a major bottleneck in\nadvancing phytoplankton tracking. In this paper, we propose a challenging\nbenchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse\nbackground information and variations in motion during observation. The dataset\nincludes 27 species of phytoplankton and zooplankton, 14 different backgrounds\nto simulate diverse and complex underwater environments, and a total of 140\nvideos. To enable accurate real-time observation of phytoplankton, we introduce\na multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion\nTracker(DSFT), which addresses issues such as focus shifts during tracking and\nthe loss of small target information when computing frame-to-frame similarity.\nSpecifically, we introduce an additional feature extractor to predict the\nresiduals of the standard feature extractor's output, and compute multi-scale\nframe-to-frame similarity based on features from different layers of the\nextractor. Extensive experiments on the MPT have demonstrated the validity of\nthe dataset and the superiority of DSFT in tracking phytoplankton, providing an\neffective solution for phytoplankton monitoring.\n","authors":["Yang Yu","Yuezun Li","Xin Sun","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2410.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14568v4","updated":"2024-10-22T04:40:48Z","published":"2022-11-26T13:48:05Z","title":"BeGin: Extensive Benchmark Scenarios and An Easy-to-use Framework for\n  Graph Continual Learning","summary":"  Continual Learning (CL) is the process of learning ceaselessly a sequence of\ntasks. Most existing CL methods deal with independent data (e.g., images and\ntext) for which many benchmark frameworks and results under standard\nexperimental settings are available. Compared to them, however, CL methods for\ngraph data (graph CL) are relatively underexplored because of (a) the lack of\nstandard experimental settings, especially regarding how to deal with the\ndependency between instances, (b) the lack of benchmark datasets and scenarios,\nand (c) high complexity in implementation and evaluation due to the dependency.\nIn this paper, regarding (a) we define four standard incremental settings\n(task-, class-, domain-, and time-incremental) for node-, link-, and\ngraph-level problems, extending the previously explored scope. Regarding (b),\nwe provide 35 benchmark scenarios based on 24 real-world graphs. Regarding (c),\nwe develop BeGin, an easy and fool-proof framework for graph CL. BeGin is\neasily extended since it is modularized with reusable modules for data\nprocessing, algorithm design, and evaluation. Especially, the evaluation module\nis completely separated from user code to eliminate potential mistakes.\nRegarding benchmark results, we cover 3x more combinations of incremental\nsettings and levels of problems than the latest benchmark. All assets for the\nbenchmark framework are publicly available at\nhttps://github.com/ShinhwanKang/BeGin.\n","authors":["Jihoon Ko","Shinhwan Kang","Taehyung Kwon","Heechan Moon","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2211.14568v4.pdf","comment":"Full version of the ACM TIST paper with the same title"},{"id":"http://arxiv.org/abs/2410.08660v2","updated":"2024-10-22T04:22:45Z","published":"2024-10-11T09:39:11Z","title":"RePD: Defending Jailbreak Attack through a Retrieval-based Prompt\n  Decomposition Process","summary":"  In this study, we introduce RePD, an innovative attack Retrieval-based Prompt\nDecomposition framework designed to mitigate the risk of jailbreak attacks on\nlarge language models (LLMs). Despite rigorous pretraining and finetuning\nfocused on ethical alignment, LLMs are still susceptible to jailbreak exploits.\nRePD operates on a one-shot learning model, wherein it accesses a database of\npre-collected jailbreak prompt templates to identify and decompose harmful\ninquiries embedded within user prompts. This process involves integrating the\ndecomposition of the jailbreak prompt into the user's original query into a\none-shot learning example to effectively teach the LLM to discern and separate\nmalicious components. Consequently, the LLM is equipped to first neutralize any\npotentially harmful elements before addressing the user's prompt in a manner\nthat aligns with its ethical guidelines. RePD is versatile and compatible with\na variety of open-source LLMs acting as agents. Through comprehensive\nexperimentation with both harmful and benign prompts, we have demonstrated the\nefficacy of our proposed RePD in enhancing the resilience of LLMs against\njailbreak attacks, without compromising their performance in responding to\ntypical user requests.\n","authors":["Peiran Wang","Xiaogeng Liu","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.08660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16676v1","updated":"2024-10-22T04:18:19Z","published":"2024-10-22T04:18:19Z","title":"Improving Causal Reasoning in Large Language Models: A Survey","summary":"  Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning.\n","authors":["Siheng Xiong","Delin Chen","Qingyang Wu","Longxuan Yu","Qingzhen Liu","Dawei Li","Zhikai Chen","Xiaoze Liu","Liangming Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18390v2","updated":"2024-10-22T04:16:23Z","published":"2024-09-27T02:12:56Z","title":"Speech to Reality: On-Demand Production using Natural Language, 3D\n  Generative AI, and Discrete Robotic Assembly","summary":"  We present a system that transforms speech into physical objects by combining\n3D generative Artificial Intelligence with robotic assembly. The system\nleverages natural language input to make design and manufacturing more\naccessible, enabling individuals without expertise in 3D modeling or robotic\nprogramming to create physical objects. We propose utilizing discrete robotic\nassembly of lattice-based voxel components to address the challenges of using\ngenerative AI outputs in physical production, such as design variability,\nfabrication speed, structural integrity, and material waste. The system\ninterprets speech to generate 3D objects, discretizes them into voxel\ncomponents, computes an optimized assembly sequence, and generates a robotic\ntoolpath. The results are demonstrated through the assembly of various objects,\nranging from chairs to shelves, which are prompted via speech and realized\nwithin 5 minutes using a 6-axis robotic arm.\n","authors":["Alexander Htet Kyaw","Se Hwan Jeon","Miana Smith","Neil Gershenfeld"],"pdf_url":"https://arxiv.org/pdf/2409.18390v2.pdf","comment":"This work has been submitted to the IEEE for possible publication. An\n  updated version will replace this version"},{"id":"http://arxiv.org/abs/2402.11793v4","updated":"2024-10-22T04:15:49Z","published":"2024-02-19T02:48:40Z","title":"Generative Kaleidoscopic Networks","summary":"  We discovered that the neural networks, especially the deep ReLU networks,\ndemonstrate an `over-generalization' phenomenon. That is, the output values for\nthe inputs that were not seen during training are mapped close to the output\nrange that were observed during the learning process. In other words, the\nneural networks learn a many-to-one mapping and this effect is more prominent\nas we increase the number of layers or the depth of the neural network. We\nutilize this property of neural networks to design a dataset kaleidoscope,\ntermed as `Generative Kaleidoscopic Networks'. Succinctly, if we learn a model\nto map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$,\nthe proposed `Kaleidoscopic sampling' procedure starts with a random input\nnoise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots\nf_\\mathcal{N}(z)\\cdots )$. After a burn-in period duration, we start observing\nsamples from the input distribution and the quality of samples recovered\nimproves as we increase the depth of the model. Scope: We observed this\nphenomenon to various degrees for the other deep learning architectures like\nCNNs, Transformers & U-Nets and we are currently investigating them further.\n","authors":["Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2402.11793v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08777v3","updated":"2024-10-22T04:14:08Z","published":"2024-02-13T20:21:29Z","title":"DNABERT-S: Pioneering Species Differentiation with Species-Aware DNA\n  Embeddings","summary":"  We introduce DNABERT-S, a tailored genome model that develops species-aware\nembeddings to naturally cluster and segregate DNA sequences of different\nspecies in the embedding space. Differentiating species from genomic sequences\n(i.e., DNA and RNA) is vital yet challenging, since many real-world species\nremain uncharacterized, lacking known genomes for reference. Embedding-based\nmethods are therefore used to differentiate species in an unsupervised manner.\nDNABERT-S builds upon a pre-trained genome foundation model named DNABERT-2. To\nencourage effective embeddings to error-prone long-read DNA sequences, we\nintroduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes\nthe hidden representations of DNA sequences at randomly selected layers and\ntrains the model to recognize and differentiate these mixed proportions at the\noutput layer. We further enhance it with the proposed Curriculum Contrastive\nLearning (C$^2$LR) strategy. Empirical results on 23 diverse datasets show\nDNABERT-S's effectiveness, especially in realistic label-scarce scenarios. For\nexample, it identifies twice more species from a mixture of unlabeled genomic\nsequences, doubles the Adjusted Rand Index (ARI) in species clustering, and\noutperforms the top baseline's performance in 10-shot species classification\nwith just a 2-shot training. Model, codes, and data are publicly available at\n\\url{https://github.com/MAGICS-LAB/DNABERT_S}.\n","authors":["Zhihan Zhou","Weimin Wu","Harrison Ho","Jiayi Wang","Lizhen Shi","Ramana V Davuluri","Zhong Wang","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2402.08777v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16672v1","updated":"2024-10-22T04:08:27Z","published":"2024-10-22T04:08:27Z","title":"DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy\n  Conflicts in Large Language Models","summary":"  Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is\ncritical. Interestingly, we discover a counter-intuitive trade-off phenomenon\nthat enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT)\nmethods significantly decreases its fairness awareness with thousands of\nsamples. To address this issue, inspired by the information theory, we\nintroduce a training-free method to \\textbf{DEA}ctivate the fairness and\nprivacy coupled \\textbf{N}eurons (\\textbf{DEAN}), which theoretically and\nempirically decrease the mutual information between fairness and privacy\nawareness. Extensive experimental results demonstrate that DEAN eliminates the\ntrade-off phenomenon and significantly improves LLMs' fairness and privacy\nawareness simultaneously, \\eg improving Qwen-2-7B-Instruct's fairness awareness\nby 12.2\\% and privacy awareness by 14.0\\%. More crucially, DEAN remains robust\nand effective with limited annotated data or even when only malicious\nfine-tuning data is available, whereas SFT methods may fail to perform properly\nin such scenarios. We hope this study provides valuable insights into\nconcurrently addressing fairness and privacy concerns in LLMs and can be\nintegrated into comprehensive frameworks to develop more ethical and\nresponsible AI systems. Our code is available at\n\\url{https://github.com/ChnQ/DEAN}.\n","authors":["Chen Qian","Dongrui Liu","Jie Zhang","Yong Liu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2410.16672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16670v1","updated":"2024-10-22T03:59:53Z","published":"2024-10-22T03:59:53Z","title":"CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing","summary":"  Sequential reasoning in agent systems has been significantly advanced by\nlarge language models (LLMs), yet existing approaches face limitations.\nReflection-driven reasoning relies solely on knowledge in pretrained models,\nlimiting performance in novel scenarios, while experience-assisted reasoning\noften depends on external experiences and lacks clear principles for selecting\nrepresentative experiences. We address these limitations by proposing CoPS\n(Cross-Task Experience Sharing), a generalizable algorithm that enhances\nsequential reasoning by cross-task experience sharing and selection. In detail,\nCoPS leverages agents' experiences on previous tasks, selecting\ndistribution-matched experiences via a provable pessimism-based strategy to\nmaximize utility while minimizing risks from distribution shifts. Extensive\nexperimental results on benchmarks like Alfworld, Webshop, and HotPotQA\ndemonstrate that CoPS consistently outperforms state-of-the-art baselines, with\nsuperior sample efficiency suitable for resource-constrained scenarios.\nTheoretically, we show that the performance of our algorithm depends on both\nthe quality of the pretrained LLM and the matching between the agent's\ntask-dependent trial distribution and that generated by the LLM. Our work\nbridges the gap between existing sequential reasoning paradigms and validates\nthe effectiveness of leveraging cross-task experiences, shedding light on the\npotential to improve agents' generalization and adaptability across diverse\ntasks. Our codes are available at\n$\\href{https://github.com/uclaml/COPS}{\\text{https://github.com/uclaml/COPS}}$.\n","authors":["Chen Yang","Chenyang Zhao","Quanquan Gu","Dongruo Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.16670v1.pdf","comment":"25 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2410.16668v1","updated":"2024-10-22T03:53:46Z","published":"2024-10-22T03:53:46Z","title":"Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling","summary":"  Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance.\n","authors":["Chenyi Li","Guande Wu","Gromit Yeuk-Yin Chan","Dishita G Turakhia","Sonia Castelo Quispe","Dong Li","Leslie Welch","Claudio Silva","Jing Qian"],"pdf_url":"https://arxiv.org/pdf/2410.16668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11680v3","updated":"2024-10-22T03:46:41Z","published":"2023-09-20T23:24:22Z","title":"Federated Learning with Neural Graphical Models","summary":"  Federated Learning (FL) addresses the need to create models based on\nproprietary data in such a way that multiple clients retain exclusive control\nover their data, while all benefit from improved model accuracy due to pooled\nresources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic\nGraphical models that utilize the expressive power of neural networks to learn\ncomplex non-linear dependencies between the input features. They learn to\ncapture the underlying data distribution and have efficient algorithms for\ninference and sampling. We develop a FL framework which maintains a global NGM\nmodel that learns the averaged information from the local NGM models while\nkeeping the training data within the client's environment. Our design, FedNGMs,\navoids the pitfalls and shortcomings of neuron matching frameworks like\nFederated Matched Averaging that suffers from model parameter explosion. Our\nglobal model size remains constant throughout the process. In the cases where\nclients have local variables that are not part of the combined global\ndistribution, we propose a `Stitching' algorithm, which personalizes the global\nNGM models by merging the additional variables using the client's data. FedNGM\nis robust to data heterogeneity, large number of participants, and limited\ncommunication bandwidth. We experimentally demonstrated the use of FedNGMs for\nextracting insights from CDC's Infant Mortality dataset and discuss interesting\nfuture applications.\n","authors":["Urszula Chajewska","Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2309.11680v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04833v4","updated":"2024-10-22T03:41:08Z","published":"2024-07-05T19:38:10Z","title":"3D Adaptive Structural Convolution Network for Domain-Invariant Point\n  Cloud Recognition","summary":"  Adapting deep learning networks for point cloud data recognition in\nself-driving vehicles faces challenges due to the variability in datasets and\nsensor technologies, emphasizing the need for adaptive techniques to maintain\naccuracy across different conditions. In this paper, we introduce the 3D\nAdaptive Structural Convolution Network (3D-ASCN), a cutting-edge framework for\n3D point cloud recognition. It combines 3D convolution kernels, a structural\ntree structure, and adaptive neighborhood sampling for effective geometric\nfeature extraction. This method obtains domain-invariant features and\ndemonstrates robust, adaptable performance on a variety of point cloud\ndatasets, ensuring compatibility across diverse sensor configurations without\nthe need for parameter adjustments. This highlights its potential to\nsignificantly enhance the reliability and efficiency of self-driving vehicle\ntechnology.\n","authors":["Younggun Kim","Beomsik Cho","Seonghoon Ryoo","Soomok Lee"],"pdf_url":"https://arxiv.org/pdf/2407.04833v4.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.00664v2","updated":"2024-10-22T03:34:43Z","published":"2024-06-30T11:22:36Z","title":"SCMIL: Sparse Context-aware Multiple Instance Learning for Predicting\n  Cancer Survival Probability Distribution in Whole Slide Images","summary":"  Cancer survival prediction is a challenging task that involves analyzing of\nthe tumor microenvironment within Whole Slide Image (WSI). Previous methods\ncannot effectively capture the intricate interaction features among instances\nwithin the local area of WSI. Moreover, existing methods for cancer survival\nprediction based on WSI often fail to provide better clinically meaningful\npredictions. To overcome these challenges, we propose a Sparse Context-aware\nMultiple Instance Learning (SCMIL) framework for predicting cancer survival\nprobability distributions. SCMIL innovatively segments patches into various\nclusters based on their morphological features and spatial location\ninformation, subsequently leveraging sparse self-attention to discern the\nrelationships between these patches with a context-aware perspective.\nConsidering many patches are irrelevant to the task, we introduce a learnable\npatch filtering module called SoftFilter, which ensures that only interactions\nbetween task-relevant patches are considered. To enhance the clinical relevance\nof our prediction, we propose a register-based mixture density network to\nforecast the survival probability distribution for individual patients. We\nevaluate SCMIL on two public WSI datasets from the The Cancer Genome Atlas\n(TCGA) specifically focusing on lung adenocarcinom (LUAD) and kidney renal\nclear cell carcinoma (KIRC). Our experimental results indicate that SCMIL\noutperforms current state-of-the-art methods for survival prediction, offering\nmore clinically meaningful and interpretable outcomes. Our code is accessible\nat https://github.com/yang-ze-kang/SCMIL.\n","authors":["Zekang Yang","Hong Liu","Xiangdong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.00664v2.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2410.01812v3","updated":"2024-10-22T03:30:01Z","published":"2024-09-14T02:35:29Z","title":"From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice","summary":"  Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.\n","authors":["Qian Niu","Keyu Chen","Ming Li","Pohsun Feng","Ziqian Bi","Lawrence KQ Yan","Yichao Zhang","Caitlyn Heqi Yin","Cheng Fei","Junyu Liu","Benji Peng"],"pdf_url":"https://arxiv.org/pdf/2410.01812v3.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.16662v1","updated":"2024-10-22T03:28:41Z","published":"2024-10-22T03:28:41Z","title":"Visual Question Answering in Ophthalmology: A Progressive and Practical\n  Perspective","summary":"  Accurate diagnosis of ophthalmic diseases relies heavily on the\ninterpretation of multimodal ophthalmic images, a process often time-consuming\nand expertise-dependent. Visual Question Answering (VQA) presents a potential\ninterdisciplinary solution by merging computer vision and natural language\nprocessing to comprehend and respond to queries about medical images. This\nreview article explores the recent advancements and future prospects of VQA in\nophthalmology from both theoretical and practical perspectives, aiming to\nprovide eye care professionals with a deeper understanding and tools for\nleveraging the underlying models. Additionally, we discuss the promising trend\nof large language models (LLM) in enhancing various components of the VQA\nframework to adapt to multimodal ophthalmic tasks. Despite the promising\noutlook, ophthalmic VQA still faces several challenges, including the scarcity\nof annotated multimodal image datasets, the necessity of comprehensive and\nunified evaluation methods, and the obstacles to achieving effective real-world\napplications. This article highlights these challenges and clarifies future\ndirections for advancing ophthalmic VQA with LLMs. The development of LLM-based\nophthalmic VQA systems calls for collaborative efforts between medical\nprofessionals and AI experts to overcome existing obstacles and advance the\ndiagnosis and care of eye diseases.\n","authors":["Xiaolan Chen","Ruoyu Chen","Pusheng Xu","Weiyi Zhang","Xianwen Shang","Mingguang He","Danli Shi"],"pdf_url":"https://arxiv.org/pdf/2410.16662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16659v1","updated":"2024-10-22T03:21:59Z","published":"2024-10-22T03:21:59Z","title":"RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary\n  Detection in Partially Machine Generated Texts","summary":"  With increasing usage of generative models for text generation and widespread\nuse of machine generated texts in various domains, being able to distinguish\nbetween human written and machine generated texts is a significant challenge.\nWhile existing models and proprietary systems focus on identifying whether\ngiven text is entirely human written or entirely machine generated, only a few\nsystems provide insights at sentence or paragraph level at likelihood of being\nmachine generated at a non reliable accuracy level, working well only for a set\nof domains and generators. This paper introduces few reliable approaches for\nthe novel task of identifying which part of a given text is machine generated\nat a word level while comparing results from different approaches and methods.\nWe present a comparison with proprietary systems , performance of our model on\nunseen domains' and generators' texts. The findings reveal significant\nimprovements in detection accuracy along with comparison on other aspects of\ndetection capabilities. Finally we discuss potential avenues for improvement\nand implications of our work. The proposed model is also well suited for\ndetecting which parts of a text are machine generated in outputs of Instruct\nvariants of many LLMs.\n","authors":["Ram Mohan Rao Kadiyala"],"pdf_url":"https://arxiv.org/pdf/2410.16659v1.pdf","comment":"published at naacl 2024"},{"id":"http://arxiv.org/abs/2401.17045v5","updated":"2024-10-22T03:06:48Z","published":"2024-01-30T14:27:37Z","title":"Explaining Explanations in Probabilistic Logic Programming","summary":"  The emergence of tools based on artificial intelligence has also led to the\nneed of producing explanations which are understandable by a human being. In\nmost approaches, the system is considered a black box, making it difficult to\ngenerate appropriate explanations. In this work, though, we consider a setting\nwhere models are transparent: probabilistic logic programming (PLP), a paradigm\nthat combines logic programming for knowledge representation and probability to\nmodel uncertainty. However, given a query, the usual notion of explanation is\nassociated with a set of choices, one for each random variable of the model.\nUnfortunately, such a set does not explain why the query is true and, in fact,\nit may contain choices that are actually irrelevant for the considered query.\nTo improve this situation, we present in this paper an approach to explaining\nexplanations which is based on defining a new query-driven inference mechanism\nfor PLP where proofs are labeled with \"choice expressions\", a compact and easy\nto manipulate representation for sets of choices. The combination of proof\ntrees and choice expressions allows us to produce comprehensible query\njustifications with a causal structure.\n","authors":["Germán Vidal"],"pdf_url":"https://arxiv.org/pdf/2401.17045v5.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Programming Languages and Systems (Proceedings of APLAS 2024),\n  Springer LNCS, 2024, and is available online at\n  https://doi.org/10.1007/978-981-97-8943-6_7"},{"id":"http://arxiv.org/abs/2410.16655v1","updated":"2024-10-22T02:59:47Z","published":"2024-10-22T02:59:47Z","title":"Semantic-guided Search for Efficient Program Repair with Large Language\n  Models","summary":"  In this paper, we first show that increases in beam size of even just\nsmall-sized LLM (1B-7B parameters) require an extensive GPU resource\nconsumption, leading to up to 80% of recurring crashes due to memory overloads\nin LLM-based APR. Seemingly simple solutions to reduce memory consumption are\n(1) to quantize LLM models, i.e., converting the weights of a LLM from\nhigh-precision values to lower-precision ones. and (2) to make beam search\nsequential, i.e., forwarding each beam through the model sequentially and then\nconcatenate them back into a single model output. However, we show that these\napproaches still do not work via both theoretical analysis and experiments. To\naddress this, we introduce FLAMES, a novel LLM-based APR technique that employs\nsemantic-guided patch generation to enhance repair effectiveness and memory\nefficiency. Unlike conventional methods that rely on beam search, FLAMES\nutilizes greedy decoding to enhance memory efficiency while steering the search\nto more potentially good repair candidates via a semantic-guided best-first\nsearch algorithm. At each decoding step, FLAMES uses semantic feedback from\ntest validation such as the number of passing and failing test cases to select\nthe most promising token to explore further. Our empirical evaluation on the\nDefects4J and HumanEval-Java datasets shows that FLAMES not only substantially\nreduces memory consumption by up to 83% compared to conventional LLM-based APR,\nbut also accelerates the repair process. Remarkably, FLAMES successfully\ngenerated 133 and 103 correct fixes for 333 and 163 bugs in the Defects4J and\nHumanEval-Java datasets, respectively. This suggests that FLAMES is not only\nmore efficient but also outperforms state-of-the-art techniques, fixing at\nleast 10 and 11 more bugs than SOTA baselines in the Defects4J and\nHumanEval-Java datasets, respectively.\n","authors":["Thanh Le-Cong","Bach Le","Toby Murray"],"pdf_url":"https://arxiv.org/pdf/2410.16655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16653v1","updated":"2024-10-22T02:57:44Z","published":"2024-10-22T02:57:44Z","title":"Enhancing Two-Player Performance Through Single-Player Knowledge\n  Transfer: An Empirical Study on Atari 2600 Games","summary":"  Playing two-player games using reinforcement learning and self-play can be\nchallenging due to the complexity of two-player environments and the possible\ninstability in the training process. We propose that a reinforcement learning\nalgorithm can train more efficiently and achieve improved performance in a\ntwo-player game if it leverages the knowledge from the single-player version of\nthe same game. This study examines the proposed idea in ten different Atari\n2600 environments using the Atari 2600 RAM as the input state. We discuss the\nadvantages of using transfer learning from a single-player training process\nover training in a two-player setting from scratch, and demonstrate our results\nin a few measures such as training time and average total reward. We also\ndiscuss a method of calculating RAM complexity and its relationship to\nperformance.\n","authors":["Kimiya Saadat","Richard Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13228v2","updated":"2024-10-22T02:53:15Z","published":"2024-10-17T05:30:59Z","title":"From PINNs to PIKANs: Recent Advances in Physics-Informed Machine\n  Learning","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a key tool in\nScientific Machine Learning since their introduction in 2017, enabling the\nefficient solution of ordinary and partial differential equations using sparse\nmeasurements. Over the past few years, significant advancements have been made\nin the training and optimization of PINNs, covering aspects such as network\narchitectures, adaptive refinement, domain decomposition, and the use of\nadaptive weights and activation functions. A notable recent development is the\nPhysics-Informed Kolmogorov-Arnold Networks (PIKANS), which leverage a\nrepresentation model originally proposed by Kolmogorov in 1957, offering a\npromising alternative to traditional PINNs. In this review, we provide a\ncomprehensive overview of the latest advancements in PINNs, focusing on\nimprovements in network design, feature expansion, optimization techniques,\nuncertainty quantification, and theoretical insights. We also survey key\napplications across a range of fields, including biomedicine, fluid and solid\nmechanics, geophysics, dynamical systems, heat transfer, chemical engineering,\nand beyond. Finally, we review computational frameworks and software tools\ndeveloped by both academia and industry to support PINN research and\napplications.\n","authors":["Juan Diego Toscano","Vivek Oommen","Alan John Varghese","Zongren Zou","Nazanin Ahmadi Daryakenari","Chenxi Wu","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2410.13228v2.pdf","comment":"physics-informed neural networks, Kolmogorov-Arnold networks,\n  optimization algorithms, separable PINNs, self-adaptive weights, uncertainty\n  quantification"},{"id":"http://arxiv.org/abs/2410.16647v1","updated":"2024-10-22T02:45:59Z","published":"2024-10-22T02:45:59Z","title":"GE2E-KWS: Generalized End-to-End Training and Evaluation for Zero-shot\n  Keyword Spotting","summary":"  We propose GE2E-KWS -- a generalized end-to-end training and evaluation\nframework for customized keyword spotting. Specifically, enrollment utterances\nare separated and grouped by keywords from the training batch and their\nembedding centroids are compared to all other test utterance embeddings to\ncompute the loss. This simulates runtime enrollment and verification stages,\nand improves convergence stability and training speed by optimizing matrix\noperations compared to SOTA triplet loss approaches. To benchmark different\nmodels reliably, we propose an evaluation process that mimics the production\nenvironment and compute metrics that directly measure keyword matching\naccuracy. Trained with GE2E loss, our 419KB quantized conformer model beats a\n7.5GB ASR encoder by 23.6% relative AUC, and beats a same size triplet loss\nmodel by 60.7% AUC. Our KWS models are natively streamable with low memory\nfootprints, and designed to continuously run on-device with no retraining\nneeded for new keywords (zero-shot).\n","authors":["Pai Zhu","Jacob W. Bartel","Dhruuv Agarwal","Kurt Partridge","Hyun Jin Park","Quan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16647v1.pdf","comment":"8 pages, 6 figures, 2 tables The paper is accepted in IEEE Spoken\n  Language Technology (SLT) 2024"},{"id":"http://arxiv.org/abs/2410.16645v1","updated":"2024-10-22T02:45:09Z","published":"2024-10-22T02:45:09Z","title":"Chatting with Bots: AI, Speech Acts, and the Edge of Assertion","summary":"  This paper addresses the question of whether large language model-powered\nchatbots are capable of assertion. According to what we call the Thesis of\nChatbot Assertion (TCA), chatbots are the kinds of things that can assert, and\nat least some of the output produced by current-generation chatbots qualifies\nas assertion. We provide some motivation for TCA, arguing that it ought to be\ntaken seriously and not simply dismissed. We also review recent objections to\nTCA, arguing that these objections are weighty. We thus confront the following\ndilemma: how can we do justice to both the considerations for and against TCA?\nWe consider two influential responses to this dilemma - the first appeals to\nthe notion of proxy-assertion; the second appeals to fictionalism - and argue\nthat neither is satisfactory. Instead, reflecting on the ontogenesis of\nassertion, we argue that we need to make space for a category of\nproto-assertion. We then apply the category of proto-assertion to chatbots,\narguing that treating chatbots as proto-assertors provides a satisfactory\nresolution to the dilemma of chatbot assertion.\n","authors":["Iwan Williams","Tim Bayne"],"pdf_url":"https://arxiv.org/pdf/2410.16645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16644v1","updated":"2024-10-22T02:44:10Z","published":"2024-10-22T02:44:10Z","title":"CKSP: Cross-species Knowledge Sharing and Preserving for Universal\n  Animal Activity Recognition","summary":"  Deep learning techniques are dominating automated animal activity recognition\n(AAR) tasks with wearable sensors due to their high performance on large-scale\nlabelled data. However, current deep learning-based AAR models are trained\nsolely on datasets of individual animal species, constraining their\napplicability in practice and performing poorly when training data are limited.\nIn this study, we propose a one-for-many framework, dubbed Cross-species\nKnowledge Sharing and Preserving (CKSP), based on sensor data of diverse animal\nspecies. Given the coexistence of generic and species-specific behavioural\npatterns among different species, we design a Shared-Preserved Convolution\n(SPConv) module. This module assigns an individual low-rank convolutional layer\nto each species for extracting species-specific features and employs a shared\nfull-rank convolutional layer to learn generic features, enabling the CKSP\nframework to learn inter-species complementarity and alleviating data\nlimitations via increasing data diversity. Considering the training conflict\narising from discrepancies in data distributions among species, we devise a\nSpecies-specific Batch Normalization (SBN) module, that involves multiple BN\nlayers to separately fit the distributions of different species. To validate\nCKSP's effectiveness, experiments are performed on three public datasets from\nhorses, sheep, and cattle, respectively. The results show that our approach\nremarkably boosts the classification performance compared to the baseline\nmethod (one-for-one framework) solely trained on individual-species data, with\nincrements of 6.04%, 2.06%, and 3.66% in accuracy, and 10.33%, 3.67%, and 7.90%\nin F1-score for the horse, sheep, and cattle datasets, respectively. This\nproves the promising capabilities of our method in leveraging multi-species\ndata to augment classification performance.\n","authors":["Axiu Mao","Meilu Zhu","Zhaojin Guo","Zheng He","Tomas Norton","Kai Liu"],"pdf_url":"https://arxiv.org/pdf/2410.16644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05857v2","updated":"2024-10-22T02:41:48Z","published":"2023-08-10T21:06:18Z","title":"Knowledge Propagation over Conditional Independence Graphs","summary":"  Conditional Independence (CI) graph is a special type of a Probabilistic\nGraphical Model (PGM) where the feature connections are modeled using an\nundirected graph and the edge weights show the partial correlation strength\nbetween the features. Since the CI graphs capture direct dependence between\nfeatures, they have been garnering increasing interest within the research\ncommunity for gaining insights into the systems from various domains, in\nparticular discovering the domain topology. In this work, we propose algorithms\nfor performing knowledge propagation over the CI graphs. Our experiments\ndemonstrate that our techniques improve upon the state-of-the-art on the\npublicly available Cora and PubMed datasets.\n","authors":["Urszula Chajewska","Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2308.05857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11493v2","updated":"2024-10-22T02:31:19Z","published":"2024-10-15T10:57:02Z","title":"Towards Fair Graph Representation Learning in Social Networks","summary":"  With the widespread use of Graph Neural Networks (GNNs) for representation\nlearning from network data, the fairness of GNN models has raised great\nattention lately. Fair GNNs aim to ensure that node representations can be\naccurately classified, but not easily associated with a specific group.\nExisting advanced approaches essentially enhance the generalisation of node\nrepresentation in combination with data augmentation strategy, and do not\ndirectly impose constraints on the fairness of GNNs. In this work, we identify\nthat a fundamental reason for the unfairness of GNNs in social network learning\nis the phenomenon of social homophily, i.e., users in the same group are more\ninclined to congregate. The message-passing mechanism of GNNs can cause users\nin the same group to have similar representations due to social homophily,\nleading model predictions to establish spurious correlations with sensitive\nattributes. Inspired by this reason, we propose a method called Equity-Aware\nGNN (EAGNN) towards fair graph representation learning. Specifically, to ensure\nthat model predictions are independent of sensitive attributes while\nmaintaining prediction performance, we introduce constraints for fair\nrepresentation learning based on three principles: sufficiency, independence,\nand separation. We theoretically demonstrate that our EAGNN method can\neffectively achieve group fairness. Extensive experiments on three datasets\nwith varying levels of social homophily illustrate that our EAGNN method\nachieves the state-of-the-art performance across two fairness metrics and\noffers competitive effectiveness.\n","authors":["Guixian Zhang","Guan Yuan","Debo Cheng","Lin Liu","Jiuyong Li","Shichao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.11493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16167v3","updated":"2024-10-22T02:29:22Z","published":"2024-09-24T15:08:41Z","title":"Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering","summary":"  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.\n","authors":["Ziyu Zhao","Tao Shen","Didi Zhu","Zexi Li","Jing Su","Xuwu Wang","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16167v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16638v1","updated":"2024-10-22T02:27:57Z","published":"2024-10-22T02:27:57Z","title":"LLMScan: Causal Scan for LLM Misbehavior Detection","summary":"  Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.\n","authors":["Mengdi Zhang","Kai Kiat Goh","Peixin Zhang","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.16638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16633v1","updated":"2024-10-22T02:21:42Z","published":"2024-10-22T02:21:42Z","title":"Graph-Structured Trajectory Extraction from Travelogues","summary":"  Previous studies on sequence-based extraction of human movement trajectories\nhave an issue of inadequate trajectory representation. Specifically, a pair of\nlocations may not be lined up in a sequence especially when one location\nincludes the other geographically. In this study, we propose a graph\nrepresentation that retains information on the geographic hierarchy as well as\nthe temporal order of visited locations, and have constructed a benchmark\ndataset for graph-structured trajectory extraction. The experiments with our\nbaselines have demonstrated that it is possible to accurately predict visited\nlocations and the order among them, but it remains a challenge to predict the\nhierarchical relations.\n","authors":["Aitaro Yamamoto","Hiroyuki Otomo","Hiroki Ouchi","Shohei Higashiyama","Hiroki Teranishi","Hiroyuki Shindo","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.16633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16624v1","updated":"2024-10-22T02:16:02Z","published":"2024-10-22T02:16:02Z","title":"EVC-MF: End-to-end Video Captioning Network with Multi-scale Features","summary":"  Conventional approaches for video captioning leverage a variety of\noffline-extracted features to generate captions. Despite the availability of\nvarious offline-feature-extractors that offer diverse information from\ndifferent perspectives, they have several limitations due to fixed parameters.\nConcretely, these extractors are solely pre-trained on image/video\ncomprehension tasks, making them less adaptable to video caption datasets.\nAdditionally, most of these extractors only capture features prior to the\nclassifier of the pre-training task, ignoring a significant amount of valuable\nshallow information. Furthermore, employing multiple offline-features may\nintroduce redundant information. To address these issues, we propose an\nend-to-end encoder-decoder-based network (EVC-MF) for video captioning, which\nefficiently utilizes multi-scale visual and textual features to generate video\ndescriptions. Specifically, EVC-MF consists of three modules. Firstly, instead\nof relying on multiple feature extractors, we directly feed video frames into a\ntransformer-based network to obtain multi-scale visual features and update\nfeature extractor parameters. Secondly, we fuse the multi-scale features and\ninput them into a masked encoder to reduce redundancy and encourage learning\nuseful features. Finally, we utilize an enhanced transformer-based decoder,\nwhich can efficiently leverage shallow textual information, to generate video\ndescriptions. To evaluate our proposed model, we conduct extensive experiments\non benchmark datasets. The results demonstrate that EVC-MF yields competitive\nperformance compared with the state-of-theart methods.\n","authors":["Tian-Zi Niu","Zhen-Duo Chen","Xin Luo","Xin-Shun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.16624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09564v2","updated":"2024-10-22T02:14:24Z","published":"2024-06-13T20:12:46Z","title":"Towards Domain Adaptive Neural Contextual Bandits","summary":"  Contextual bandit algorithms are essential for solving real-world decision\nmaking problems. In practice, collecting a contextual bandit's feedback from\ndifferent domains may involve different costs. For example, measuring drug\nreaction from mice (as a source domain) and humans (as a target domain).\nUnfortunately, adapting a contextual bandit algorithm from a source domain to a\ntarget domain with distribution shift still remains a major challenge and\nlargely unexplored. In this paper, we introduce the first general domain\nadaptation method for contextual bandits. Our approach learns a bandit model\nfor the target domain by collecting feedback from the source domain. Our\ntheoretical analysis shows that our algorithm maintains a sub-linear regret\nbound even adapting across domains. Empirical results show that our approach\noutperforms the state-of-the-art contextual bandit algorithms on real-world\ndatasets.\n","authors":["Ziyan Wang","Xiaoming Huo","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16194v2","updated":"2024-10-22T02:06:35Z","published":"2024-05-25T11:53:23Z","title":"Diffusion-Reward Adversarial Imitation Learning","summary":"  Imitation learning aims to learn a policy from observing expert\ndemonstrations without access to reward signals from environments. Generative\nadversarial imitation learning (GAIL) formulates imitation learning as\nadversarial learning, employing a generator policy learning to imitate expert\nbehaviors and discriminator learning to distinguish the expert demonstrations\nfrom agent trajectories. Despite its encouraging results, GAIL training is\noften brittle and unstable. Inspired by the recent dominance of diffusion\nmodels in generative modeling, we propose Diffusion-Reward Adversarial\nImitation Learning (DRAIL), which integrates a diffusion model into GAIL,\naiming to yield more robust and smoother rewards for policy learning.\nSpecifically, we propose a diffusion discriminative classifier to construct an\nenhanced discriminator, and design diffusion rewards based on the classifier's\noutput for policy learning. Extensive experiments are conducted in navigation,\nmanipulation, and locomotion, verifying DRAIL's effectiveness compared to prior\nimitation learning methods. Moreover, additional experimental results\ndemonstrate the generalizability and data efficiency of DRAIL. Visualized\nlearned reward functions of GAIL and DRAIL suggest that DRAIL can produce more\nrobust and smoother rewards. Project page:\nhttps://nturobotlearninglab.github.io/DRAIL/\n","authors":["Chun-Mao Lai","Hsiang-Chun Wang","Ping-Chun Hsieh","Yu-Chiang Frank Wang","Min-Hung Chen","Shao-Hua Sun"],"pdf_url":"https://arxiv.org/pdf/2405.16194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16327v5","updated":"2024-10-22T01:35:27Z","published":"2022-11-29T16:10:11Z","title":"On the Power of Foundation Models","summary":"  With infinitely many high-quality data points, infinite computational power,\nan infinitely large foundation model with a perfect training algorithm and\nguaranteed zero generalization error on the pretext task, can the model be used\nfor everything? This question cannot be answered by the existing theory of\nrepresentation, optimization or generalization, because the issues they mainly\ninvestigate are assumed to be nonexistent here. In this paper, we show that\ncategory theory provides powerful machinery to answer this question. We have\nproved three results. The first one limits the power of prompt-based learning,\nsaying that the model can solve a downstream task with prompts if and only if\nthe task is representable. The second one says fine tuning does not have this\nlimit, as a foundation model with the minimum required power (up to symmetry)\ncan theoretically solve downstream tasks for the category defined by pretext\ntask, with fine tuning and enough resources. Our final result can be seen as a\nnew type of generalization theorem, showing that the foundation model can\ngenerate unseen objects from the target category (e.g., images) using the\nstructural information from the source category (e.g., texts). Along the way,\nwe provide a categorical framework for supervised and self-supervised learning,\nwhich might be of independent interest.\n","authors":["Yang Yuan"],"pdf_url":"https://arxiv.org/pdf/2211.16327v5.pdf","comment":"ICML'23. This version fixed a bug when applying prompt tuning theorem\n  to LLM"},{"id":"http://arxiv.org/abs/2410.16606v1","updated":"2024-10-22T01:32:46Z","published":"2024-10-22T01:32:46Z","title":"GALA: Graph Diffusion-based Alignment with Jigsaw for Source-free Domain\n  Adaptation","summary":"  Source-free domain adaptation is a crucial machine learning topic, as it\ncontains numerous applications in the real world, particularly with respect to\ndata privacy. Existing approaches predominantly focus on Euclidean data, such\nas images and videos, while the exploration of non-Euclidean graph data remains\nscarce. Recent graph neural network (GNN) approaches can suffer from serious\nperformance decline due to domain shift and label scarcity in source-free\nadaptation scenarios. In this study, we propose a novel method named Graph\nDiffusion-based Alignment with Jigsaw (GALA), tailored for source-free graph\ndomain adaptation. To achieve domain alignment, GALA employs a graph diffusion\nmodel to reconstruct source-style graphs from target data. Specifically, a\nscore-based graph diffusion model is trained using source graphs to learn the\ngenerative source styles. Then, we introduce perturbations to target graphs via\na stochastic differential equation instead of sampling from a prior, followed\nby the reverse process to reconstruct source-style graphs. We feed the\nsource-style graphs into an off-the-shelf GNN and introduce class-specific\nthresholds with curriculum learning, which can generate accurate and unbiased\npseudo-labels for target graphs. Moreover, we develop a simple yet effective\ngraph-mixing strategy named graph jigsaw to combine confident graphs and\nunconfident graphs, which can enhance generalization capabilities and\nrobustness via consistency learning. Extensive experiments on benchmark\ndatasets validate the effectiveness of GALA.\n","authors":["Junyu Luo","Yiyang Gu","Xiao Luo","Wei Ju","Zhiping Xiao","Yusheng Zhao","Jingyang Yuan","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16606v1.pdf","comment":"IEEE TPAMI"},{"id":"http://arxiv.org/abs/2406.16030v2","updated":"2024-10-22T01:31:31Z","published":"2024-06-23T06:38:56Z","title":"Zero-Shot Cross-Lingual NER Using Phonemic Representations for\n  Low-Resource Languages","summary":"  Existing zero-shot cross-lingual NER approaches require substantial prior\nknowledge of the target language, which is impractical for low-resource\nlanguages. In this paper, we propose a novel approach to NER using phonemic\nrepresentation based on the International Phonetic Alphabet (IPA) to bridge the\ngap between representations of different languages. Our experiments show that\nour method significantly outperforms baseline models in extremely low-resource\nlanguages, with the highest average F1 score (46.38%) and lowest standard\ndeviation (12.67), particularly demonstrating its robustness with non-Latin\nscripts. Our codes are available at\nhttps://github.com/Gabriel819/zeroshot_ner.git\n","authors":["Jimin Sohn","Haeji Jung","Alex Cheng","Jooeon Kang","Yilin Du","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2406.16030v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2409.13705v2","updated":"2024-10-22T01:01:56Z","published":"2024-09-05T14:35:35Z","title":"Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble","summary":"  Increasing use of large language models (LLMs) demand performant guardrails\nto ensure the safety of inputs and outputs of LLMs. When these safeguards are\ntrained on imbalanced data, they can learn the societal biases. We present a\nlight-weight, post-processing method for mitigating counterfactual fairness in\nclosed-source text safety classifiers. Our approach involves building an\nensemble that not only outperforms the input classifiers and policy-aligns\nthem, but also acts as a debiasing regularizer. We introduce two\nthreshold-agnostic metrics to assess the counterfactual fairness of a model,\nand demonstrate how combining these metrics with Fair Data Reweighting (FDW)\nhelps mitigate biases. We create an expanded Open AI dataset, and a new\ntemplated LLM-generated dataset based on user-prompts, both of which are\ncounterfactually balanced across identity groups and cover four key areas of\nsafety; we will work towards publicly releasing these datasets. Our results\nshow that our approach improves counterfactual fairness with minimal impact on\nmodel performance.\n","authors":["Olivia Sturman","Aparna Joshi","Bhaktipriya Radharapu","Piyush Kumar","Renee Shelby"],"pdf_url":"https://arxiv.org/pdf/2409.13705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16600v1","updated":"2024-10-22T00:55:04Z","published":"2024-10-22T00:55:04Z","title":"Convex Markov Games: A Framework for Fairness, Imitation, and Creativity\n  in Multi-Agent Learning","summary":"  Expert imitation, behavioral diversity, and fairness preferences give rise to\npreferences in sequential decision making domains that do not decompose\nadditively across time. We introduce the class of convex Markov games that\nallow general convex preferences over occupancy measures. Despite infinite time\nhorizon and strictly higher generality than Markov games, pure strategy Nash\nequilibria exist under strict convexity. Furthermore, equilibria can be\napproximated efficiently by performing gradient descent on an upper bound of\nexploitability. Our experiments imitate human choices in ultimatum games,\nreveal novel solutions to the repeated prisoner's dilemma, and find fair\nsolutions in a repeated asymmetric coordination game. In the prisoner's\ndilemma, our algorithm finds a policy profile that deviates from observed human\nplay only slightly, yet achieves higher per-player utility while also being\nthree orders of magnitude less exploitable.\n","authors":["Ian Gemp","Andreas Haupt","Luke Marris","Siqi Liu","Georgios Piliouras"],"pdf_url":"https://arxiv.org/pdf/2410.16600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11020v3","updated":"2024-10-22T00:42:39Z","published":"2024-10-14T19:16:56Z","title":"Improving the Language Understanding Capabilities of Large Language\n  Models Using Reinforcement Learning","summary":"  Large language models (LLMs), built on decoder-only transformers, excel in\nnatural language generation and adapt to diverse tasks using zero-shot and\nfew-shot prompting. However, these prompting methods often struggle on natural\nlanguage understanding (NLU) tasks, where encoder-only models like BERT-base\noutperform LLMs on benchmarks like GLUE and SuperGLUE. This paper explores two\napproaches-supervised fine-tuning (SFT) and proximal policy optimization\n(PPO)-to enhance LLMs' NLU abilities. To reduce the cost of full-model\nfine-tuning, we integrate low-rank adaptation (LoRA) layers, limiting updates\nto these layers during both SFT and PPO. In SFT, task-specific prompts are\nconcatenated with input queries and ground-truth labels, optimizing with\nnext-token prediction. Despite this, LLMs still underperform compared to models\nlike BERT-base on several NLU tasks. To close this gap, we apply PPO, a\nreinforcement learning technique that treats each token generation as an action\nand uses a reward function based on alignment with ground-truth answers. PPO\nthen updates the model to maximize these rewards, aligning outputs with correct\nlabels. Our experiments with LLAMA2-7B show that PPO improves performance, with\na 6.3-point gain over SFT on GLUE. PPO exceeds zero-shot by 38.7 points and\nfew-shot by 26.1 points on GLUE, while surpassing these by 28.8 and 28.5 points\non SuperGLUE. Additionally, PPO outperforms BERT-large by 2.7 points on GLUE\nand 9.3 points on SuperGLUE. The improvements are consistent across models like\nQwen2.5-7B and MPT-7B, highlighting PPO's robustness in enhancing LLMs' NLU\ncapabilities.\n","authors":["Bokai Hu","Sai Ashish Somayajula","Xin Pan","Zihan Huang","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2410.11020v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16593v1","updated":"2024-10-22T00:30:31Z","published":"2024-10-22T00:30:31Z","title":"Graph Sampling for Scalable and Expressive Graph Neural Networks on\n  Homophilic Graphs","summary":"  Graph Neural Networks (GNNs) excel in many graph machine learning tasks but\nface challenges when scaling to large networks. GNN transferability allows\ntraining on smaller graphs and applying the model to larger ones, but existing\nmethods often rely on random subsampling, leading to disconnected subgraphs and\nreduced model expressivity. We propose a novel graph sampling algorithm that\nleverages feature homophily to preserve graph structure. By minimizing the\ntrace of the data correlation matrix, our method better preserves the graph\nLaplacian's rank than random sampling while achieving lower complexity than\nspectral methods. Experiments on citation networks show improved performance in\npreserving graph rank and GNN transferability compared to random sampling.\n","authors":["Haolin Li","Luana Ruiz"],"pdf_url":"https://arxiv.org/pdf/2410.16593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16589v1","updated":"2024-10-22T00:14:36Z","published":"2024-10-22T00:14:36Z","title":"Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis\n  with Large Language Models","summary":"  Sentiment analysis has become increasingly important for assessing public\nopinion and informing decision-making. Large language models (LLMs) have\nrevolutionized this field by capturing nuanced language patterns. However,\nadapting LLMs to domain-specific sentiment analysis tasks remains challenging\ndue to computational constraints and the need for optimal fine-tuning. To\naddress these challenges, we propose a novel Dynamic Adaptive Rank Space\nExploration (DARSE) framework for efficient and effective sentiment analysis\nusing LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the\noptimal rank range, a fine-grained exploration algorithm to refine rank\nselection, and a dynamic rank allocation method to determine the optimal rank\ncombination for each LLM layer. Extensive experiments demonstrate that DARSE\nsignificantly improves sentiment analysis accuracy, achieving a 15.1%\nimprovement in MSE and a 4.3% improvement in accuracy compared to previous\nwork. Our framework strikes a balance between computational efficiency and\nmodel performance, making it a promising approach for sentiment analysis with\nLLMs.\n","authors":["Hongcheng Ding","Fuzhen Hu","Xuanze Zhao","Zixiao Jiang","Shamsul Nahar Abdullah","Deshinta Arrova Dewi"],"pdf_url":"https://arxiv.org/pdf/2410.16589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16586v1","updated":"2024-10-22T00:11:41Z","published":"2024-10-22T00:11:41Z","title":"Optimizing LLMs with Direct Preferences: A Data Efficiency Perspective","summary":"  Aligning the output of Large Language Models (LLMs) with human preferences\n(e.g., by means of reinforcement learning with human feedback, or RLHF) is\nessential for ensuring their effectiveness in real-world scenarios. Despite\nsignificant advancements in LLM alignment techniques, the impact of different\ntype of preference data on model performance has yet to be systematically\nexplored. In this study, we investigate the scalability, data efficiency, and\neffectiveness of Direct Preference Optimization (DPO) in fine-tuning\npre-trained LLMs, aiming to reduce their dependency on extensive amounts of\npreference data, which is expensive to collect. We (1) systematically compare\nthe performance of models fine-tuned with varying percentages of a combined\npreference judgement dataset to define the improvement curve of DPO and assess\nits effectiveness in data-constrained environments; and (2) provide insights\nfor the development of an optimal approach for selective preference data usage.\nOur study reveals that increasing the amount of data used for training\ngenerally enhances and stabilizes model performance. Moreover, the use of a\ncombination of diverse datasets significantly improves model effectiveness.\nFurthermore, when models are trained separately using different types of\nprompts, models trained with conversational prompts outperformed those trained\nwith question answering prompts.\n","authors":["Pietro Bernardelle","Gianluca Demartini"],"pdf_url":"https://arxiv.org/pdf/2410.16586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.11515v4","updated":"2024-10-22T00:06:40Z","published":"2022-06-23T07:51:10Z","title":"plingo: A system for probabilistic reasoning in clingo based on lpmln","summary":"  We present plingo, an extension of the ASP system clingo with various\nprobabilistic reasoning modes. Plingo is centered upon LP^MLN, a probabilistic\nextension of ASP based on a weight scheme from Markov Logic. This choice is\nmotivated by the fact that the core probabilistic reasoning modes can be mapped\nonto optimization problems and that LP^MLN may serve as a middle-ground\nformalism connecting to other probabilistic approaches. As a result, plingo\noffers three alternative frontends, for LP^MLN, P-log, and ProbLog. The\ncorresponding input languages and reasoning modes are implemented by means of\nclingo's multi-shot and theory solving capabilities. The core of plingo amounts\nto a re-implementation of LP^MLN in terms of modern ASP technology, extended by\nan approximation technique based on a new method for answer set enumeration in\nthe order of optimality. We evaluate plingo's performance empirically by\ncomparing it to other probabilistic systems.\n","authors":["Susana Hahn","Tomi Janhunen","Roland Kaminski","Javier Romero","Nicolas Rühling","Torsten Schaub"],"pdf_url":"https://arxiv.org/pdf/2206.11515v4.pdf","comment":"Under consideration in Theory and Practice of Logic Programming\n  (TPLP)"},{"id":"http://arxiv.org/abs/2405.11100v2","updated":"2024-10-22T00:02:28Z","published":"2024-05-17T21:27:32Z","title":"Are Large Language Models Moral Hypocrites? A Study Based on Moral\n  Foundations","summary":"  Large language models (LLMs) have taken centre stage in debates on Artificial\nIntelligence. Yet there remains a gap in how to assess LLMs' conformity to\nimportant human values. In this paper, we investigate whether state-of-the-art\nLLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid\nresults) are moral hypocrites. We employ two research instruments based on the\nMoral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which\ninvestigates which values are considered morally relevant in abstract moral\njudgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate\nmoral cognition in concrete scenarios related to each moral foundation. We\ncharacterise conflicts in values between these different abstractions of moral\nevaluation as hypocrisy. We found that both models displayed reasonable\nconsistency within each instrument compared to humans, but they displayed\ncontradictory and hypocritical behaviour when we compared the abstract values\npresent in the MFQ to the evaluation of concrete moral violations of the MFV.\n","authors":["José Luiz Nunes","Guilherme F. C. F. Almeida","Marcelo de Araujo","Simone D. J. Barbosa"],"pdf_url":"https://arxiv.org/pdf/2405.11100v2.pdf","comment":"Final version available at:\n  https://ojs.aaai.org/index.php/AIES/article/view/31704 13 pages, 4 figures, 2\n  tables"},{"id":"http://arxiv.org/abs/2410.17479v1","updated":"2024-10-22T23:57:37Z","published":"2024-10-22T23:57:37Z","title":"Composing Diffusion Policies for Few-shot Learning of Movement\n  Trajectories","summary":"  Humans can perform various combinations of physical skills without having to\nrelearn skills from scratch every single time. For example, we can swing a bat\nwhen walking without having to re-learn such a policy from scratch by composing\nthe individual skills of walking and bat swinging. Enabling robots to combine\nor compose skills is essential so they can learn novel skills and tasks faster\nwith fewer real world samples. To this end, we propose a novel compositional\napproach called DSE- Diffusion Score Equilibrium that enables few-shot learning\nfor novel skills by utilizing a combination of base policy priors. Our method\nis based on probabilistically composing diffusion policies to better model the\nfew-shot demonstration data-distribution than any individual policy. Our goal\nhere is to learn robot motions few-shot and not necessarily goal oriented\ntrajectories. Unfortunately we lack a general purpose metric to evaluate the\nerror between a skill or motion and the provided demonstrations. Hence, we\npropose a probabilistic measure - Maximum Mean Discrepancy on the Forward\nKinematics Kernel (MMD-FK), that is task and action space agnostic. By using\nour few-shot learning approach DSE, we show that we are able to achieve a\nreduction of over 30% in MMD-FK across skills and number of demonstrations.\nMoreover, we show the utility of our approach through real world experiments by\nteaching novel trajectories to a robot in 5 demonstrations.\n","authors":["Omkar Patil","Anant Sah","Nakul Gopalan"],"pdf_url":"https://arxiv.org/pdf/2410.17479v1.pdf","comment":"6(+1) pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17477v1","updated":"2024-10-22T23:24:15Z","published":"2024-10-22T23:24:15Z","title":"Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination","summary":"  The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to \\textit{hallucinate} false or misleading information, limiting\ntheir reliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.\n","authors":["Jerry Huang","Prasanna Parthasarathi","Mehdi Rezagholizadeh","Boxing Chen","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2410.17477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11402v2","updated":"2024-10-22T23:13:34Z","published":"2024-09-17T17:59:06Z","title":"NVLM: Open Frontier-Class Multimodal LLMs","summary":"  We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B\nand will open-source the training code for the community soon.\n","authors":["Wenliang Dai","Nayeon Lee","Boxin Wang","Zhuolin Yang","Zihan Liu","Jon Barker","Tuomas Rintamaki","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2409.11402v2.pdf","comment":"Fixed the typos. For more information, please visit our project page\n  at: https://research.nvidia.com/labs/adlr/NVLM-1"},{"id":"http://arxiv.org/abs/2410.17469v1","updated":"2024-10-22T22:52:14Z","published":"2024-10-22T22:52:14Z","title":"AdaptoML-UX: An Adaptive User-centered GUI-based AutoML Toolkit for\n  Non-AI Experts and HCI Researchers","summary":"  The increasing integration of machine learning across various domains has\nunderscored the necessity for accessible systems that non-experts can utilize\neffectively. To address this need, the field of automated machine learning\n(AutoML) has developed tools to simplify the construction and optimization of\nML pipelines. However, existing AutoML solutions often lack efficiency in\ncreating online pipelines and ease of use for Human-Computer Interaction (HCI)\napplications. Therefore, in this paper, we introduce AdaptoML-UX, an adaptive\nframework that incorporates automated feature engineering, machine learning,\nand incremental learning to assist non-AI experts in developing robust,\nuser-centered ML models. Our toolkit demonstrates the capability to adapt\nefficiently to diverse problem domains and datasets, particularly in HCI,\nthereby reducing the necessity for manual experimentation and conserving time\nand resources. Furthermore, it supports model personalization through\nincremental learning, customizing models to individual user behaviors. HCI\nresearchers can employ AdaptoML-UX\n(\\url{https://github.com/MichaelSargious/AdaptoML_UX}) without requiring\nspecialized expertise, as it automates the selection of algorithms, feature\nengineering, and hyperparameter tuning based on the unique characteristics of\nthe data.\n","authors":["Amr Gomaa","Michael Sargious","Antonio Krüger"],"pdf_url":"https://arxiv.org/pdf/2410.17469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17462v1","updated":"2024-10-22T22:43:14Z","published":"2024-10-22T22:43:14Z","title":"Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain\n  Annotation","summary":"  Time series data is ubiquitous across various domains, including\nmanufacturing, finance, and healthcare. High-quality annotations are essential\nfor effectively understanding time series and facilitating downstream tasks;\nhowever, obtaining such annotations is challenging, particularly in\nmission-critical domains. In this paper, we propose TESSA, a multi-agent system\ndesigned to automatically generate both general and domain-specific annotations\nfor time series data. TESSA introduces two agents: a general annotation agent\nand a domain-specific annotation agent. The general agent captures common\npatterns and knowledge across multiple source domains, leveraging both\ntime-series-wise and text-wise features to generate general annotations.\nMeanwhile, the domain-specific agent utilizes limited annotations from the\ntarget domain to learn domain-specific terminology and generate targeted\nannotations. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate that TESSA effectively generates high-quality annotations,\noutperforming existing methods.\n","authors":["Minhua Lin","Zhengzhang Chen","Yanchi Liu","Xujiang Zhao","Zongyu Wu","Junxiang Wang","Xiang Zhang","Suhang Wang","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.17462v1.pdf","comment":"23 pages, 9 figures, 24 tables"},{"id":"http://arxiv.org/abs/2410.17459v1","updated":"2024-10-22T22:31:03Z","published":"2024-10-22T22:31:03Z","title":"Data Obfuscation through Latent Space Projection (LSP) for\n  Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and\n  Finance Fraud Detection","summary":"  As AI systems increasingly integrate into critical societal sectors, the\ndemand for robust privacy-preserving methods has escalated. This paper\nintroduces Data Obfuscation through Latent Space Projection (LSP), a novel\ntechnique aimed at enhancing AI governance and ensuring Responsible AI\ncompliance. LSP uses machine learning to project sensitive data into a latent\nspace, effectively obfuscating it while preserving essential features for model\ntraining and inference. Unlike traditional privacy methods like differential\nprivacy or homomorphic encryption, LSP transforms data into an abstract,\nlower-dimensional form, achieving a delicate balance between data utility and\nprivacy. Leveraging autoencoders and adversarial training, LSP separates\nsensitive from non-sensitive information, allowing for precise control over\nprivacy-utility trade-offs. We validate LSP's effectiveness through experiments\non benchmark datasets and two real-world case studies: healthcare cancer\ndiagnosis and financial fraud analysis. Our results show LSP achieves high\nperformance (98.7% accuracy in image classification) while providing strong\nprivacy (97.3% protection against sensitive attribute inference), outperforming\ntraditional anonymization and privacy-preserving methods. The paper also\nexamines LSP's alignment with global AI governance frameworks, such as GDPR,\nCCPA, and HIPAA, highlighting its contribution to fairness, transparency, and\naccountability. By embedding privacy within the machine learning pipeline, LSP\noffers a promising approach to developing AI systems that respect privacy while\ndelivering valuable insights. We conclude by discussing future research\ndirections, including theoretical privacy guarantees, integration with\nfederated learning, and enhancing latent space interpretability, positioning\nLSP as a critical tool for ethical AI advancement.\n","authors":["Mahesh Vaijainthymala Krishnamoorthy"],"pdf_url":"https://arxiv.org/pdf/2410.17459v1.pdf","comment":"19 pages, 6 figures, submitted to Conference ICADCML2025"},{"id":"http://arxiv.org/abs/2410.16239v2","updated":"2024-10-22T22:22:14Z","published":"2024-10-21T17:42:41Z","title":"MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays,\n  ECGs, and Diagnostic Report","summary":"  In this paper, we introduce a novel Multi-Modal Contrastive Pre-training\nFramework that synergistically combines X-rays, electrocardiograms (ECGs), and\nradiology/cardiology reports. Our approach leverages transformers to encode\nthese diverse modalities into a unified representation space, aiming to enhance\ndiagnostic accuracy and facilitate comprehensive patient assessments. We\nutilize LoRA-Peft to significantly reduce trainable parameters in the LLM and\nincorporate recent linear attention dropping strategy in the Vision\nTransformer(ViT) for smoother attention. Furthermore, we provide novel\nmultimodal attention explanations and retrieval for our model. To the best of\nour knowledge, we are the first to propose an integrated model that combines\nX-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing\ncontrastive loss, MoRE effectively aligns modality-specific features into a\ncoherent embedding, which supports various downstream tasks such as zero-shot\nclassification and multimodal retrieval. Employing our proposed methodology, we\nachieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and\nPtbXl downstream datasets, surpassing existing multimodal approaches. Our\nproposed framework shows significant improvements in capturing intricate\ninter-modal relationships and its robustness in medical diagnosis that\nestablishes a framework for future research in multimodal learning in the\nhealthcare sector.\n","authors":["Samrajya Thapa","Koushik Howlader","Subhankar Bhattacharjee","Wei le"],"pdf_url":"https://arxiv.org/pdf/2410.16239v2.pdf","comment":"10 pages, 5 figures, 9 tables. Supplementary detail in Appendix. Code\n  made available in Github for reproducibility"},{"id":"http://arxiv.org/abs/2410.14872v2","updated":"2024-10-22T22:18:14Z","published":"2024-10-18T21:38:21Z","title":"How to Evaluate Reward Models for RLHF","summary":"  We introduce a new benchmark for reward models that quantifies their ability\nto produce strong language models through RLHF (Reinforcement Learning from\nHuman Feedback). The gold-standard approach is to run a full RLHF training\npipeline and directly probe downstream LLM performance. However, this process\nis prohibitively expensive. To address this, we build a predictive model of\ndownstream LLM performance by evaluating the reward model on proxy tasks. These\nproxy tasks consist of a large-scale human preference and a verifiable\ncorrectness preference dataset, in which we measure 12 metrics across 12\ndomains. To investigate which reward model metrics are most correlated to\ngold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a\nlarge-scale crowdsourced human preference platform to view real reward model\ndownstream performance as ground truth. Ultimately, we compile our data and\nfindings into Preference Proxy Evaluations (PPE), the first reward model\nbenchmark explicitly linked to post-RLHF real-world human preference\nperformance, which we open-source for public use and further development. Our\ncode and evaluations can be found at https://github.com/lmarena/PPE .\n","authors":["Evan Frick","Tianle Li","Connor Chen","Wei-Lin Chiang","Anastasios N. Angelopoulos","Jiantao Jiao","Banghua Zhu","Joseph E. Gonzalez","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.14872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20423v3","updated":"2024-10-22T22:10:18Z","published":"2024-09-30T15:47:22Z","title":"Stream-level flow matching from a Bayesian decision theoretic\n  perspective","summary":"  Flow matching (FM) is a family of training algorithms for fitting continuous\nnormalizing flows (CNFs). A standard approach to FM, called conditional flow\nmatching (CFM), exploits the fact that the marginal vector field of a CNF can\nbe learned by fitting least-square regression to the so-called conditional\nvector field specified given one or both ends of the flow path. We show that\nviewing CFM training from a Bayesian decision theoretic perspective on\nparameter estimation opens the door to generalizations of CFM algorithms. We\npropose one such extension by introducing a CFM algorithm based on defining\nconditional probability paths given what we refer to as ``streams'', instances\nof latent stochastic paths that connect pairs of noise and observed data.\nFurther, we advocate the modeling of these latent streams using Gaussian\nprocesses (GPs). The unique distributional properties of GPs, and in particular\nthe fact that the velocity of a GP is still a GP, allows drawing samples from\nthe resulting stream-augmented conditional probability path without simulating\nthe actual streams, and hence the ``simulation-free\" nature of CFM training is\npreserved. We show that this generalization of the CFM can substantially reduce\nthe variance in the estimated marginal vector field at a moderate computational\ncost, thereby improving the quality of the generated samples under common\nmetrics. Additionally, we show that adopting the GP on the streams allows for\nflexibly linking multiple related training data points (e.g., time series) and\nincorporating additional prior information. We empirically validate our claim\nthrough both simulations and applications to two hand-written image datasets.\n","authors":["Ganchao Wei","Li Ma"],"pdf_url":"https://arxiv.org/pdf/2409.20423v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17448v1","updated":"2024-10-22T21:50:52Z","published":"2024-10-22T21:50:52Z","title":"In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models","summary":"  Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We also demonstrate how strategic prompting improves the\nmodel's performance and how the natural language interface simplifies\nintegrating theory with data. Although this approach does not outperform\nestablished SR programs where target equations are more complex, LLMs can\nnonetheless iterate toward improved solutions while following instructions and\nincorporating scientific context in natural language.\n","authors":["Samiha Sharlin","Tyler R. Josephson"],"pdf_url":"https://arxiv.org/pdf/2410.17448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07215v2","updated":"2024-10-22T21:44:03Z","published":"2024-08-13T21:54:10Z","title":"Can Large Language Models Reason? A Characterization via 3-SAT","summary":"  Large Language Models (LLMs) have been touted as AI models possessing\nadvanced reasoning abilities. However, recent works have shown that LLMs often\nbypass true reasoning using shortcuts, sparking skepticism. To study the\nreasoning capabilities in a principled fashion, we adopt a computational theory\nperspective and propose an experimental protocol centered on 3-SAT -- the\nprototypical NP-complete problem lying at the core of logical reasoning and\nconstraint satisfaction tasks. Specifically, we examine the phase transitions\nin random 3-SAT and characterize the reasoning abilities of LLMs by varying the\ninherent hardness of the problem instances. Our experimental evidence shows\nthat LLMs are incapable of performing true reasoning, as required for solving\n3-SAT problems. Moreover, we observe significant performance variation based on\nthe inherent hardness of the problems -- performing poorly on harder instances\nand vice versa. Importantly, we show that integrating external reasoners can\nconsiderably enhance LLM performance. By following a principled experimental\nprotocol, our study draws concrete conclusions and moves beyond the anecdotal\nevidence often found in LLM reasoning research.\n","authors":["Rishi Hazra","Gabriele Venturato","Pedro Zuidberg Dos Martires","Luc De Raedt"],"pdf_url":"https://arxiv.org/pdf/2408.07215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17439v1","updated":"2024-10-22T21:30:58Z","published":"2024-10-22T21:30:58Z","title":"Evaluating AI-Generated Essays with GRE Analytical Writing Assessment","summary":"  The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing GPT-4o received an average score of 4.67, falling between\n\"generally thoughtful, well-developed analysis of the issue and conveys meaning\nclearly\" and \"presents a competent analysis of the issue and conveys meaning\nwith acceptable clarity\" according to the GRE scoring guideline. We also\nevaluated the detection accuracy of these essays, with detectors trained on\nessays generated by the same and different LLMs.\n","authors":["Yang Zhong","Jiangang Hao","Michael Fauss","Chen Li","Yuan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17439v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17438v1","updated":"2024-10-22T21:30:01Z","published":"2024-10-22T21:30:01Z","title":"Interpreting Affine Recurrence Learning in GPT-style Transformers","summary":"  Understanding the internal mechanisms of GPT-style transformers, particularly\ntheir capacity to perform in-context learning (ICL), is critical for advancing\nAI alignment and interpretability. In-context learning allows transformers to\ngeneralize during inference without modifying their weights, yet the precise\noperations driving this capability remain largely opaque. This paper presents\nan investigation into the mechanistic interpretability of these transformers,\nfocusing specifically on their ability to learn and predict affine recurrences\nas an ICL task. To address this, we trained a custom three-layer transformer to\npredict affine recurrences and analyzed the model's internal operations using\nboth empirical and theoretical approaches. Our findings reveal that the model\nforms an initial estimate of the target sequence using a copying mechanism in\nthe zeroth layer, which is subsequently refined through negative similarity\nheads in the second layer. These insights contribute to a deeper understanding\nof transformer behaviors in recursive tasks and offer potential avenues for\nimproving AI alignment through mechanistic interpretability. Finally, we\ndiscuss the implications of our results for future work, including extensions\nto higher-dimensional recurrences and the exploration of polynomial sequences.\n","authors":["Samarth Bhargav","Alexander Gu"],"pdf_url":"https://arxiv.org/pdf/2410.17438v1.pdf","comment":"21 pages, 18 figures"},{"id":"http://arxiv.org/abs/2410.17433v1","updated":"2024-10-22T21:17:19Z","published":"2024-10-22T21:17:19Z","title":"Revisiting Technical Bias Mitigation Strategies","summary":"  Efforts to mitigate bias and enhance fairness in the artificial intelligence\n(AI) community have predominantly focused on technical solutions. While\nnumerous reviews have addressed bias in AI, this review uniquely focuses on the\npractical limitations of technical solutions in healthcare settings, providing\na structured analysis across five key dimensions affecting their real-world\nimplementation: who defines bias and fairness; which mitigation strategy to use\nand prioritize among dozens that are inconsistent and incompatible; when in the\nAI development stages the solutions are most effective; for which populations;\nand the context in which the solutions are designed. We illustrate each\nlimitation with empirical studies focusing on healthcare and biomedical\napplications. Moreover, we discuss how value-sensitive AI, a framework derived\nfrom technology design, can engage stakeholders and ensure that their values\nare embodied in bias and fairness mitigation solutions. Finally, we discuss\nareas that require further investigation and provide practical recommendations\nto address the limitations covered in the study.\n","authors":["Abdoul Jalil Djiberou Mahamadou","Artem A. Trotsyuk"],"pdf_url":"https://arxiv.org/pdf/2410.17433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17423v1","updated":"2024-10-22T20:52:51Z","published":"2024-10-22T20:52:51Z","title":"Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis","summary":"  The current surge in Artificial Intelligence (AI) interest, reflected in\nheightened media coverage since 2009, has sparked significant debate on AI's\nimplications for privacy, social justice, workers' rights, and democracy. The\nmedia plays a crucial role in shaping public perception and acceptance of AI\ntechnologies. However, research into how AI appears in media has primarily\nfocused on anglophone contexts, leaving a gap in understanding how AI is\nrepresented globally. This study addresses this gap by analyzing 3,560 news\narticles from Brazilian media published between July 1, 2023, and February 29,\n2024, from 13 popular online news outlets. Using Computational Grounded Theory\n(CGT), the study applies Latent Dirichlet Allocation (LDA), BERTopic, and\nNamed-Entity Recognition to investigate the main topics in AI coverage and the\nentities represented. The findings reveal that Brazilian news coverage of AI is\ndominated by topics related to applications in the workplace and product\nlaunches, with limited space for societal concerns, which mostly focus on\ndeepfakes and electoral integrity. The analysis also highlights a significant\npresence of industry-related entities, indicating a strong influence of\ncorporate agendas in the country's news. This study underscores the need for a\nmore critical and nuanced discussion of AI's societal impacts in Brazilian\nmedia.\n","authors":["Raphael Hernandes","Giulio Corsi"],"pdf_url":"https://arxiv.org/pdf/2410.17423v1.pdf","comment":"18 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2312.02521v3","updated":"2024-10-22T20:52:38Z","published":"2023-12-05T06:04:16Z","title":"RetriBooru: Leakage-Free Retrieval of Conditions from Reference Images\n  for Subject-Driven Generation","summary":"  Diffusion-based methods have demonstrated remarkable capabilities in\ngenerating a diverse array of high-quality images, sparking interests for\nstyled avatars, virtual try-on, and more. Previous methods use the same\nreference image as the target. An overlooked aspect is the leakage of the\ntarget's spatial information, style, etc. from the reference, harming the\ngenerated diversity and causing shortcuts. However, this approach continues as\nwidely available datasets usually consist of single images not grouped by\nidentities, and it is expensive to recollect large-scale same-identity data.\nMoreover, existing metrics adopt decoupled evaluation on text alignment and\nidentity preservation, which fail at distinguishing between balanced outputs\nand those that over-fit to one aspect. In this paper, we propose a multi-level,\nsame-identity dataset RetriBooru, which groups anime characters by both face\nand cloth identities. RetriBooru enables adopting reference images of the same\ncharacter and outfits as the target, while keeping flexible gestures and\nactions. We benchmark previous methods on our dataset, and demonstrate the\neffectiveness of training with a reference image different from target (but\nsame identity). We introduce a new concept composition task, where the\nconditioning encoder learns to retrieve different concepts from several\nreference images, and modify a baseline network RetriNet for the new task.\nFinally, we introduce a novel class of metrics named Similarity Weighted\nDiversity (SWD), to measure the overlooked diversity and better evaluate the\nalignment between similarity and diversity.\n","authors":["Haoran Tang","Jieren Deng","Zhihong Pan","Hao Tian","Pratik Chaudhari","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.02521v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12843v2","updated":"2024-10-22T20:50:56Z","published":"2024-07-04T15:10:51Z","title":"NutriBench: A Dataset for Evaluating Large Language Models in\n  Carbohydrate Estimation from Meal Descriptions","summary":"  Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide more accurate and faster estimates. Finally, we perform a\nreal-world risk assessment by simulating the effect of carbohydrate predictions\non the blood glucose levels of individuals with diabetes. Our work highlights\nthe opportunities and challenges of using LLMs for nutrition estimation,\ndemonstrating their potential to aid professionals and laypersons and improve\nhealth outcomes. Our benchmark is publicly available at:\nhttps://mehak126.github.io/nutribench.html\n","authors":["Andong Hua","Mehak Preet Dhaliwal","Ryan Burke","Yao Qin"],"pdf_url":"https://arxiv.org/pdf/2407.12843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14573v3","updated":"2024-10-22T20:49:38Z","published":"2024-05-23T13:48:54Z","title":"AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents","summary":"  Autonomous agents that execute human tasks by controlling computers can\nenhance human productivity and application accessibility. However, progress in\nthis field will be driven by realistic and reproducible benchmarks. We present\nAndroidWorld, a fully functional Android environment that provides reward\nsignals for 116 programmatic tasks across 20 real-world Android apps. Unlike\nexisting interactive environments, which provide a static test set,\nAndroidWorld dynamically constructs tasks that are parameterized and expressed\nin natural language in unlimited ways, thus enabling testing on a much larger\nand more realistic suite of tasks. To ensure reproducibility, each task\nincludes dedicated initialization, success-checking, and tear-down logic, which\nmodifies and inspects the device's system state. We experiment with baseline\nagents to test AndroidWorld and provide initial results on the benchmark. Our\nbest agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for\nfuture work. Furthermore, we adapt a popular desktop web agent to work on\nAndroid, which we find to be less effective on mobile, suggesting future\nresearch is needed to achieve universal, cross-platform agents. Finally, we\nalso conduct a robustness analysis, showing that task variations can\nsignificantly affect agent performance, demonstrating that without such\ntesting, agent performance metrics may not fully reflect practical challenges.\nAndroidWorld and the experiments in this paper are available at\ngithub.com/google-research/android_world.\n","authors":["Christopher Rawles","Sarah Clinckemaillie","Yifan Chang","Jonathan Waltz","Gabrielle Lau","Marybeth Fair","Alice Li","William Bishop","Wei Li","Folawiyo Campbell-Ajala","Daniel Toyama","Robert Berry","Divya Tyamagundlu","Timothy Lillicrap","Oriana Riva"],"pdf_url":"https://arxiv.org/pdf/2405.14573v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17415v1","updated":"2024-10-22T20:40:53Z","published":"2024-10-22T20:40:53Z","title":"End-to-End Optimization and Learning of Fair Court Schedules","summary":"  Criminal courts across the United States handle millions of cases every year,\nand the scheduling of those cases must accommodate a diverse set of\nconstraints, including the preferences and availability of courts, prosecutors,\nand defense teams. When criminal court schedules are formed, defendants'\nscheduling preferences often take the least priority, although defendants may\nface significant consequences (including arrest or detention) for missed court\ndates. Additionally, studies indicate that defendants' nonappearances impose\ncosts on the courts and other system stakeholders. To address these issues,\ncourts and commentators have begun to recognize that pretrial outcomes for\ndefendants and for the system would be improved with greater attention to court\nprocesses, including \\emph{court scheduling practices}. There is thus a need\nfor fair criminal court pretrial scheduling systems that account for\ndefendants' preferences and availability, but the collection of such data poses\nlogistical challenges. Furthermore, optimizing schedules fairly across various\nparties' preferences is a complex optimization problem, even when such data is\navailable. In an effort to construct such a fair scheduling system under data\nuncertainty, this paper proposes a joint optimization and learning framework\nthat combines machine learning models trained end-to-end with efficient\nmatching algorithms. This framework aims to produce court scheduling schedules\nthat optimize a principled measure of fairness, balancing the availability and\npreferences of all parties.\n","authors":["My H Dinh","James Kotary","Lauryn P. Gouldin","William Yeoh","Ferdinando Fioretto"],"pdf_url":"https://arxiv.org/pdf/2410.17415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17409v1","updated":"2024-10-22T20:33:10Z","published":"2024-10-22T20:33:10Z","title":"Geometric Graph Neural Network Modeling of Human Interactions in Crowded\n  Environments","summary":"  Modeling human trajectories in crowded environments is challenging due to the\ncomplex nature of pedestrian behavior and interactions. This paper proposes a\ngeometric graph neural network (GNN) architecture that integrates domain\nknowledge from psychological studies to model pedestrian interactions and\npredict future trajectories. Unlike prior studies using complete graphs, we\ndefine interaction neighborhoods using pedestrians' field of view, motion\ndirection, and distance-based kernel functions to construct graph\nrepresentations of crowds. Evaluations across multiple datasets demonstrate\nimproved prediction accuracy through reduced average and final displacement\nerror metrics. Our findings underscore the importance of integrating domain\nknowledge with data-driven approaches for effective modeling of human\ninteractions in crowds.\n","authors":["Sara Honarvar","Yancy Diaz-Mercado"],"pdf_url":"https://arxiv.org/pdf/2410.17409v1.pdf","comment":"\\c{opyright} 2024 the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND"},{"id":"http://arxiv.org/abs/2410.17397v1","updated":"2024-10-22T20:12:04Z","published":"2024-10-22T20:12:04Z","title":"Quantum Large Language Models via Tensor Network Disentanglers","summary":"  We propose a method to enhance the performance of Large Language Models\n(LLMs) by integrating quantum computing and quantum-inspired techniques.\nSpecifically, our approach involves replacing the weight matrices in the\nSelf-Attention and Multi-layer Perceptron layers with a combination of two\nvariational quantum circuits and a quantum-inspired tensor network, such as a\nMatrix Product Operator (MPO). This substitution enables the reproduction of\nclassical LLM functionality by decomposing weight matrices through the\napplication of tensor network disentanglers and MPOs, leveraging\nwell-established tensor network techniques. By incorporating more complex and\ndeeper quantum circuits, along with increasing the bond dimensions of the MPOs,\nour method captures additional correlations within the quantum-enhanced LLM,\nleading to improved accuracy beyond classical models while maintaining low\nmemory overhead.\n","authors":["Borja Aizpurua","Saeed S. Jahromi","Sukhbinder Singh","Roman Orus"],"pdf_url":"https://arxiv.org/pdf/2410.17397v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.17395v1","updated":"2024-10-22T20:02:25Z","published":"2024-10-22T20:02:25Z","title":"A 10.60 $μ$W 150 GOPS Mixed-Bit-Width Sparse CNN Accelerator for\n  Life-Threatening Ventricular Arrhythmia Detection","summary":"  This paper proposes an ultra-low power, mixed-bit-width sparse convolutional\nneural network (CNN) accelerator to accelerate ventricular arrhythmia (VA)\ndetection. The chip achieves 50% sparsity in a quantized 1D CNN using a sparse\nprocessing element (SPE) architecture. Measurement on the prototype chip TSMC\n40nm CMOS low-power (LP) process for the VA classification task demonstrates\nthat it consumes 10.60 $\\mu$W of power while achieving a performance of 150\nGOPS and a diagnostic accuracy of 99.95%. The computation power density is only\n0.57 $\\mu$W/mm$^2$, which is 14.23X smaller than state-of-the-art works, making\nit highly suitable for implantable and wearable medical devices.\n","authors":["Yifan Qin","Zhenge Jia","Zheyu Yan","Jay Mok","Manto Yung","Yu Liu","Xuejiao Liu","Wujie Wen","Luhong Liang","Kwang-Ting Tim Cheng","X. Sharon Hu","Yiyu Shi"],"pdf_url":"https://arxiv.org/pdf/2410.17395v1.pdf","comment":"2 pages, accepted to The 30th Asia and South Pacific Design\n  Automation Conference (ASP-DAC 2025)"},{"id":"http://arxiv.org/abs/2410.17394v1","updated":"2024-10-22T20:01:39Z","published":"2024-10-22T20:01:39Z","title":"packetLSTM: Dynamic LSTM Framework for Streaming Data with Varying\n  Feature Space","summary":"  We study the online learning problem characterized by the varying input\nfeature space of streaming data. Although LSTMs have been employed to\neffectively capture the temporal nature of streaming data, they cannot handle\nthe dimension-varying streams in an online learning setting. Therefore, we\npropose a dynamic LSTM-based novel method, called packetLSTM, to model the\ndimension-varying streams. The packetLSTM's dynamic framework consists of an\nevolving packet of LSTMs, each dedicated to processing one input feature. Each\nLSTM retains the local information of its corresponding feature, while a shared\ncommon memory consolidates global information. This configuration facilitates\ncontinuous learning and mitigates the issue of forgetting, even when certain\nfeatures are absent for extended time periods. The idea of utilizing one LSTM\nper feature coupled with a dimension-invariant operator for information\naggregation enhances the dynamic nature of packetLSTM. This dynamic nature is\nevidenced by the model's ability to activate, deactivate, and add new LSTMs as\nrequired, thus seamlessly accommodating varying input dimensions. The\npacketLSTM achieves state-of-the-art results on five datasets, and its\nunderlying principle is extended to other RNN types, like GRU and vanilla RNN.\n","authors":["Rohit Agarwal","Karaka Prasanth Naidu","Alexander Horsch","Krishna Agarwal","Dilip K. Prasad"],"pdf_url":"https://arxiv.org/pdf/2410.17394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17389v1","updated":"2024-10-22T19:52:08Z","published":"2024-10-22T19:52:08Z","title":"Navigating Noisy Feedback: Enhancing Reinforcement Learning with\n  Error-Prone Language Models","summary":"  The correct specification of reward models is a well-known challenge in\nreinforcement learning. Hand-crafted reward functions often lead to inefficient\nor suboptimal policies and may not be aligned with user values. Reinforcement\nlearning from human feedback is a successful technique that can mitigate such\nissues, however, the collection of human feedback can be laborious. Recent\nworks have solicited feedback from pre-trained large language models rather\nthan humans to reduce or eliminate human effort, however, these approaches\nyield poor performance in the presence of hallucination and other errors. This\npaper studies the advantages and limitations of reinforcement learning from\nlarge language model feedback and proposes a simple yet effective method for\nsoliciting and applying feedback as a potential-based shaping function. We\ntheoretically show that inconsistent rankings, which approximate ranking\nerrors, lead to uninformative rewards with our approach. Our method empirically\nimproves convergence speed and policy returns over commonly used baselines even\nwith significant ranking errors, and eliminates the need for complex\npost-processing of reward functions.\n","authors":["Muhan Lin","Shuyang Shi","Yue Guo","Behdad Chalaki","Vaishnav Tadiparthi","Ehsan Moradi Pari","Simon Stepputtis","Joseph Campbell","Katia Sycara"],"pdf_url":"https://arxiv.org/pdf/2410.17389v1.pdf","comment":"13 pages, 8 figures, The 2024 Conference on Empirical Methods in\n  Natural Language Processing"},{"id":"http://arxiv.org/abs/2410.17373v1","updated":"2024-10-22T19:12:42Z","published":"2024-10-22T19:12:42Z","title":"Episodic Future Thinking Mechanism for Multi-agent Reinforcement\n  Learning","summary":"  Understanding cognitive processes in multi-agent interactions is a primary\ngoal in cognitive science. It can guide the direction of artificial\nintelligence (AI) research toward social decision-making in multi-agent\nsystems, which includes uncertainty from character heterogeneity. In this\npaper, we introduce an episodic future thinking (EFT) mechanism for a\nreinforcement learning (RL) agent, inspired by cognitive processes observed in\nanimals. To enable future thinking functionality, we first develop a\nmulti-character policy that captures diverse characters with an ensemble of\nheterogeneous policies. Here, the character of an agent is defined as a\ndifferent weight combination on reward components, representing distinct\nbehavioral preferences. The future thinking agent collects observation-action\ntrajectories of the target agents and uses the pre-trained multi-character\npolicy to infer their characters. Once the character is inferred, the agent\npredicts the upcoming actions of target agents and simulates the potential\nfuture scenario. This capability allows the agent to adaptively select the\noptimal action, considering the predicted future scenario in multi-agent\ninteractions. To evaluate the proposed mechanism, we consider the multi-agent\nautonomous driving scenario with diverse driving traits and multiple particle\nenvironments. Simulation results demonstrate that the EFT mechanism with\naccurate character inference leads to a higher reward than existing multi-agent\nsolutions. We also confirm that the effect of reward improvement remains valid\nacross societies with different levels of character diversity.\n","authors":["Dongsu Lee","Minhae Kwon"],"pdf_url":"https://arxiv.org/pdf/2410.17373v1.pdf","comment":"NeurIPS 2024 (Web: https://sites.google.com/view/eftm-neurips2024)"},{"id":"http://arxiv.org/abs/2405.11724v2","updated":"2024-10-22T19:07:08Z","published":"2024-05-20T01:57:34Z","title":"Token-wise Influential Training Data Retrieval for Large Language Models","summary":"  Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.\n","authors":["Huawei Lin","Jikai Long","Zhaozhuo Xu","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.11724v2.pdf","comment":"Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution"},{"id":"http://arxiv.org/abs/2410.17363v1","updated":"2024-10-22T18:56:31Z","published":"2024-10-22T18:56:31Z","title":"DeLLiriuM: A large language model for delirium prediction in the ICU\n  using structured EHR","summary":"  Delirium is an acute confusional state that has been shown to affect up to\n31% of patients in the intensive care unit (ICU). Early detection of this\ncondition could lead to more timely interventions and improved health outcomes.\nWhile artificial intelligence (AI) models have shown great potential for ICU\ndelirium prediction using structured electronic health records (EHR), most of\nthem have not explored the use of state-of-the-art AI models, have been limited\nto single hospitals, or have been developed and validated on small cohorts. The\nuse of large language models (LLM), models with hundreds of millions to\nbillions of parameters, with structured EHR data could potentially lead to\nimproved predictive performance. In this study, we propose DeLLiriuM, a novel\nLLM-based delirium prediction model using EHR data available in the first 24\nhours of ICU admission to predict the probability of a patient developing\ndelirium during the rest of their ICU admission. We develop and validate\nDeLLiriuM on ICU admissions from 104,303 patients pertaining to 195 hospitals\nacross three large databases: the eICU Collaborative Research Database, the\nMedical Information Mart for Intensive Care (MIMIC)-IV, and the University of\nFlorida Health's Integrated Data Repository. The performance measured by the\narea under the receiver operating characteristic curve (AUROC) showed that\nDeLLiriuM outperformed all baselines in two external validation sets, with 0.77\n(95% confidence interval 0.76-0.78) and 0.84 (95% confidence interval\n0.83-0.85) across 77,543 patients spanning 194 hospitals. To the best of our\nknowledge, DeLLiriuM is the first LLM-based delirium prediction tool for the\nICU based on structured EHR data, outperforming deep learning baselines which\nemploy structured features and can provide helpful information to clinicians\nfor timely interventions.\n","authors":["Miguel Contreras","Sumit Kapoor","Jiaqing Zhang","Andrea Davidson","Yuanfang Ren","Ziyuan Guan","Tezcan Ozrazgat-Baslanti","Subhash Nerella","Azra Bihorac","Parisa Rashidi"],"pdf_url":"https://arxiv.org/pdf/2410.17363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17542v3","updated":"2024-10-22T18:51:01Z","published":"2024-06-25T13:29:14Z","title":"CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization","summary":"  Large language models (LLMs) have recently demonstrated remarkable\nperformance across diverse language tasks. But their deployment is often\nconstrained by their substantial computational and storage requirements.\nQuantization has emerged as a key technique for addressing this challenge,\nenabling the compression of large models with minimal impact on performance.\nThe recent GPTQ algorithm, a post-training quantization (PTQ) method, has\nproven highly effective for compressing LLMs, sparking a wave of research that\nleverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the\nPTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ\nwith improved performance. CDQuant uses greedy coordinate descent to minimize\nthe layer-wise reconstruction loss to achieve high-quality quantized weights.\nOur algorithm is easy to implement and scales efficiently to models with\nhundreds of billions of parameters. We perform extensive evaluation on Gemma,\nand PaLM2 model families, and demonstrate that CDQuant consistently outperforms\nGPTQ in 2-4 bit weight quantization. Moreover, CDQuant improves the performance\nof state-of-the-art PTQ techniques such as QuIP and FrameQuant when used as a\nreplacement for their GPTQ component, resulting in further gains in quality.\n","authors":["Pranav Ajit Nair","Arun Sai Suggala"],"pdf_url":"https://arxiv.org/pdf/2406.17542v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17358v1","updated":"2024-10-22T18:50:36Z","published":"2024-10-22T18:50:36Z","title":"FairLoRA: Unpacking Bias Mitigation in Vision Models with\n  Fairness-Driven Low-Rank Adaptation","summary":"  Recent advances in parameter-efficient fine-tuning methods, such as Low Rank\nAdaptation (LoRA), have gained significant attention for their ability to\nefficiently adapt large foundational models to various downstream tasks. These\nmethods are appreciated for achieving performance comparable to full\nfine-tuning on aggregate-level metrics, while significantly reducing\ncomputational costs. To systematically address fairness in LLMs previous\nstudies fine-tune on fairness specific data using a larger LoRA rank than\ntypically used. In this paper, we introduce FairLoRA, a novel fairness-specific\nregularizer for LoRA aimed at reducing performance disparities across data\nsubgroups by minimizing per-class variance in loss. To the best of our\nknowledge, we are the first to introduce a fairness based finetuning through\nLoRA. Our results demonstrate that the need for higher ranks to mitigate bias\nis not universal; it depends on factors such as the pre-trained model, dataset,\nand task. More importantly, we systematically evaluate FairLoRA across various\nvision models, including ViT, DiNO, and CLIP, in scenarios involving\ndistribution shifts. We further emphasize the necessity of using multiple\nfairness metrics to obtain a holistic assessment of fairness, rather than\nrelying solely on the metric optimized during training.\n","authors":["Rohan Sukumaran","Aarash Feizi","Adriana Romero-Sorian","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2410.17358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.05935v3","updated":"2024-10-22T18:45:43Z","published":"2022-09-13T12:31:33Z","title":"Variational Causal Inference","summary":"  Estimating an individual's potential outcomes under counterfactual treatments\nis a challenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, impulse\nresponses, human faces) and covariates are relatively limited. In this case, to\nconstruct one's outcome under a counterfactual treatment, it is crucial to\nleverage individual information contained in its observed factual outcome on\ntop of the covariates. We propose a deep variational Bayesian framework that\nrigorously integrates two main sources of information for outcome construction\nunder a counterfactual treatment: one source is the individual features\nembedded in the high-dimensional factual outcome; the other source is the\nresponse distribution of similar subjects (subjects with the same covariates)\nthat factually received this treatment of interest.\n","authors":["Yulun Wu","Layne C. Price","Zichen Wang","Vassilis N. Ioannidis","Robert A. Barton","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2209.05935v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01419v3","updated":"2024-10-22T18:31:22Z","published":"2024-05-02T16:08:08Z","title":"Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT","summary":"  This paper investigates the use of Large Language Models (LLMs) and natural\nlanguage prompts to generate hardware description code, namely Verilog.\nBuilding on our prior work, we employ OpenAI's ChatGPT4 and natural language\nprompts to synthesize an RTL Verilog module of a programmable recurrent spiking\nneural network, while also generating test benches to assess the system's\ncorrectness. The resultant design was validated in three simple machine\nlearning tasks, the exclusive OR, the IRIS flower classification and the MNIST\nhand-written digit classification. Furthermore, the design was validated on a\nField-Programmable Gate Array (FPGA) and subsequently synthesized in the\nSkyWater 130 nm technology by using an open-source electronic design automation\nflow. The design was submitted to Efabless Tiny Tapeout 6.\n","authors":["Paola Vitolo","George Psaltakis","Michael Tomlinson","Gian Domenico Licciardo","Andreas G. Andreou"],"pdf_url":"https://arxiv.org/pdf/2405.01419v3.pdf","comment":"This paper was presented at the IEEE/ACM International Conference on\n  Neuromorphic Systems (ICONS), July 30-Aug 2, 2024, Arlington, VA, USA"},{"id":"http://arxiv.org/abs/2410.02024v3","updated":"2024-10-22T18:22:11Z","published":"2024-10-02T20:45:51Z","title":"FLAG: Financial Long Document Classification via AMR-based GNN","summary":"  The advent of large language models (LLMs) has initiated much research into\ntheir various financial applications. However, in applying LLMs on long\ndocuments, semantic relations are not explicitly incorporated, and a full or\narbitrarily sparse attention operation is employed. In recent years, progress\nhas been made in Abstract Meaning Representation (AMR), which is a graph-based\nrepresentation of text to preserve its semantic relations. Since AMR can\nrepresent semantic relationships at a deeper level, it can be beneficially\nutilized by graph neural networks (GNNs) for constructing effective\ndocument-level graph representations built upon LLM embeddings to predict\ntarget metrics in the financial domain. We propose FLAG: Financial Long\ndocument classification via AMR-based GNN, an AMR graph based framework to\ngenerate document-level embeddings for long financial document classification.\nWe construct document-level graphs from sentence-level AMR graphs, endow them\nwith specialized LLM word embeddings in the financial domain, apply a deep\nlearning mechanism that utilizes a GNN, and examine the efficacy of our\nAMR-based approach in predicting labeled target data from long financial\ndocuments. Extensive experiments are conducted on a dataset of quarterly\nearnings calls transcripts of companies in various sectors of the economy, as\nwell as on a corpus of more recent earnings calls of companies in the S&P 1500\nComposite Index. We find that our AMR-based approach outperforms fine-tuning\nLLMs directly on text in predicting stock price movement trends at different\ntime horizons in both datasets. Our work also outperforms previous work\nutilizing document graphs and GNNs for text classification.\n","authors":["Bolun \"Namir\" Xia","Aparna Gupta","Mohammed J. Zaki"],"pdf_url":"https://arxiv.org/pdf/2410.02024v3.pdf","comment":"8 pages, 3 figures, to be published in CIFEr Conference 2024 as\n  \"Semantic Graph Learning for Trend Prediction from Long Financial Documents\""},{"id":"http://arxiv.org/abs/2410.17343v1","updated":"2024-10-22T18:18:48Z","published":"2024-10-22T18:18:48Z","title":"EEG-DIF: Early Warning of Epileptic Seizures through Generative\n  Diffusion Model-based Multi-channel EEG Signals Forecasting","summary":"  Multi-channel EEG signals are commonly used for the diagnosis and assessment\nof diseases such as epilepsy. Currently, various EEG diagnostic algorithms\nbased on deep learning have been developed. However, most research efforts\nfocus solely on diagnosing and classifying current signal data but do not\nconsider the prediction of future trends for early warning. Additionally, since\nmulti-channel EEG can be essentially regarded as the spatio-temporal signal\ndata received by detectors at different locations in the brain, how to\nconstruct spatio-temporal information representations of EEG signals to\nfacilitate future trend prediction for multi-channel EEG becomes an important\nproblem. This study proposes a multi-signal prediction algorithm based on\ngenerative diffusion models (EEG-DIF), which transforms the multi-signal\nforecasting task into an image completion task, allowing for comprehensive\nrepresentation and learning of the spatio-temporal correlations and future\ndevelopmental patterns of multi-channel EEG signals. Here, we employ a publicly\navailable epilepsy EEG dataset to construct and validate the EEG-DIF. The\nresults demonstrate that our method can accurately predict future trends for\nmulti-channel EEG signals simultaneously. Furthermore, the early warning\naccuracy for epilepsy seizures based on the generated EEG data reaches 0.89. In\ngeneral, EEG-DIF provides a novel approach for characterizing multi-channel EEG\nsignals and an innovative early warning algorithm for epilepsy seizures, aiding\nin optimizing and enhancing the clinical diagnosis process. The code is\navailable at https://github.com/JZK00/EEG-DIF.\n","authors":["Zekun Jiang","Wei Dai","Qu Wei","Ziyuan Qin","Kang Li","Le Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17343v1.pdf","comment":"9 pages, 4 figures, 3 tables, accepted by ACM BCB 2024"},{"id":"http://arxiv.org/abs/2410.17337v1","updated":"2024-10-22T18:11:43Z","published":"2024-10-22T18:11:43Z","title":"Captions Speak Louder than Images (CASLIE): Generalizing Foundation\n  Models for E-commerce from High-quality Multimodal Instruction Data","summary":"  Leveraging multimodal data to drive breakthroughs in e-commerce applications\nthrough Multimodal Foundation Models (MFMs) is gaining increasing attention\nfrom the research community. However, there are significant challenges that\nhinder the optimal use of multimodal e-commerce data by foundation models: (1)\nthe scarcity of large-scale, high-quality multimodal benchmark datasets; and\n(2) the lack of effective multimodal information integration methods. To\naddress these challenges, in this paper, we introduce MMECInstruct, the\nfirst-ever, large-scale, and high-quality multimodal instruction dataset for\ne-commerce. We also develop CASLIE, a simple, lightweight, yet effective\nframework for integrating multimodal information for e-commerce. Leveraging\nMMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted\nas CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models\nsubstantially outperform 5 categories of advanced baseline models in the\nin-domain evaluation. Moreover, CASLIE models show strong generalizability to\nout-of-domain settings. MMECInstruct and CASLIE models are publicly accessible\nthrough https://ninglab.github.io/CASLIE/.\n","authors":["Xinyi Ling","Bo Peng","Hanwen Du","Zhihui Zhu","Xia Ning"],"pdf_url":"https://arxiv.org/pdf/2410.17337v1.pdf","comment":"Xinyi Ling and Bo Peng contributed equally to this paper"},{"id":"http://arxiv.org/abs/2402.12416v3","updated":"2024-10-22T18:10:01Z","published":"2024-02-19T08:18:53Z","title":"Aligning Individual and Collective Objectives in Multi-Agent Cooperation","summary":"  Among the research topics in multi-agent learning, mixed-motive cooperation\nis one of the most prominent challenges, primarily due to the mismatch between\nindividual and collective goals. The cutting-edge research is focused on\nincorporating domain knowledge into rewards and introducing additional\nmechanisms to incentivize cooperation. However, these approaches often face\nshortcomings such as the effort on manual design and the absence of theoretical\ngroundings. To close this gap, we model the mixed-motive game as a\ndifferentiable game for the ease of illuminating the learning dynamics towards\ncooperation. More detailed, we introduce a novel optimization method named\n\\textbf{\\textit{A}}ltruistic \\textbf{\\textit{G}}radient\n\\textbf{\\textit{A}}djustment (\\textbf{\\textit{AgA}}) that employs gradient\nadjustments to progressively align individual and collective objectives.\nFurthermore, we theoretically prove that AgA effectively attracts gradients to\nstable fixed points of the collective objective while considering individual\ninterests, and we validate these claims with empirical evidence. We evaluate\nthe effectiveness of our algorithm AgA through benchmark environments for\ntesting mixed-motive collaboration with small-scale agents such as the\ntwo-player public good game and the sequential social dilemma games, Cleanup\nand Harvest, as well as our self-developed large-scale environment in the game\nStarCraft II.\n","authors":["Yang Li","Wenhao Zhang","Jianhong Wang","Shao Zhang","Yali Du","Ying Wen","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2402.12416v3.pdf","comment":"20 pages; Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17333v1","updated":"2024-10-22T18:08:25Z","published":"2024-10-22T18:08:25Z","title":"Are Large Language Models Ready for Travel Planning?","summary":"  While large language models (LLMs) show promise in hospitality and tourism,\ntheir ability to provide unbiased service across demographic groups remains\nunclear. This paper explores gender and ethnic biases when LLMs are utilized as\ntravel planning assistants. To investigate this issue, we apply machine\nlearning techniques to analyze travel suggestions generated from three\nopen-source LLMs. Our findings reveal that the performance of race and gender\nclassifiers substantially exceeds random chance, indicating differences in how\nLLMs engage with varied subgroups. Specifically, outputs align with cultural\nexpectations tied to certain races and genders. To minimize the effect of these\nstereotypes, we used a stop-word classification strategy, which decreased\nidentifiable differences, with no disrespectful terms found. However,\nhallucinations related to African American and gender minority groups were\nnoted. In conclusion, while LLMs can generate travel plans seemingly free from\nbias, it remains essential to verify the accuracy and appropriateness of their\nrecommendations.\n","authors":["Ruiping Ren","Xing Yao","Shu Cole","Haining Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17309v1","updated":"2024-10-22T18:00:00Z","published":"2024-10-22T18:00:00Z","title":"Literature Meets Data: A Synergistic Approach to Hypothesis Generation","summary":"  AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.\n","authors":["Haokun Liu","Yangqiaoyu Zhou","Mingxuan Li","Chenfei Yuan","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17309v1.pdf","comment":"30 pages, 7 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation"},{"id":"http://arxiv.org/abs/2410.15947v2","updated":"2024-10-22T17:58:06Z","published":"2024-10-21T12:26:53Z","title":"AI-Driven Approaches for Glaucoma Detection -- A Comprehensive Review","summary":"  The diagnosis of glaucoma plays a critical role in the management and\ntreatment of this vision-threatening disease. Glaucoma is a group of eye\ndiseases that cause blindness by damaging the optic nerve at the back of the\neye. Often called \"silent thief of sight\", it exhibits no symptoms during the\nearly stages. Therefore, early detection is crucial to prevent vision loss.\nWith the rise of Artificial Intelligence (AI), particularly Deep Learning (DL)\ntechniques, Computer-Aided Diagnosis (CADx) systems have emerged as promising\ntools to assist clinicians in accurately diagnosing glaucoma early. This paper\naims to provide a comprehensive overview of AI techniques utilized in CADx\nsystems for glaucoma diagnosis. Through a detailed analysis of current\nliterature, we identify key gaps and challenges in these systems, emphasizing\nthe need for improved safety, reliability, interpretability, and\nexplainability. By identifying research gaps, we aim to advance the field of\nCADx systems especially for the early diagnosis of glaucoma, in order to\nprevent any potential loss of vision.\n","authors":["Yuki Hagiwara","Octavia-Andreea Ciora","Maureen Monnet","Gino Lancho","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2410.15947v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.16152v2","updated":"2024-10-22T03:37:37Z","published":"2024-10-21T16:19:34Z","title":"Warped Diffusion: Solving Video Inverse Problems with Image Diffusion\n  Models","summary":"  Using image models naively for solving inverse video problems often suffers\nfrom flickering, texture-sticking, and temporal inconsistency in generated\nvideos. To tackle these problems, in this paper, we view frames as continuous\nfunctions in the 2D space, and videos as a sequence of continuous warping\ntransformations between different frames. This perspective allows us to train\nfunction space diffusion models only on images and utilize them to solve\ntemporally correlated inverse problems. The function space diffusion models\nneed to be equivariant with respect to the underlying spatial transformations.\nTo ensure temporal consistency, we introduce a simple post-hoc test-time\nguidance towards (self)-equivariant solutions. Our method allows us to deploy\nstate-of-the-art latent diffusion models such as Stable Diffusion XL to solve\nvideo inverse problems. We demonstrate the effectiveness of our method for\nvideo inpainting and $8\\times$ video super-resolution, outperforming existing\ntechniques based on noise transformations. We provide generated video results:\nhttps://giannisdaras.github.io/warped_diffusion.github.io/.\n","authors":["Giannis Daras","Weili Nie","Karsten Kreis","Alex Dimakis","Morteza Mardani","Nikola Borislavov Kovachki","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2410.16152v2.pdf","comment":"Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16100v2","updated":"2024-10-22T12:16:03Z","published":"2024-10-21T15:27:18Z","title":"ExDBN: Exact learning of Dynamic Bayesian Networks","summary":"  Causal learning from data has received much attention in recent years. One\nway of capturing causal relationships is by utilizing Bayesian networks. There,\none recovers a weighted directed acyclic graph, in which random variables are\nrepresented by vertices, and the weights associated with each edge represent\nthe strengths of the causal relationships between them. This concept is\nextended to capture dynamic effects by introducing a dependency on past data,\nwhich may be captured by the structural equation model, which is utilized in\nthe present contribution to formulate a score-based learning approach. A\nmixed-integer quadratic program is formulated and an algorithmic solution\nproposed, in which the pre-generation of exponentially many acyclicity\nconstraints is avoided by utilizing the so-called branch-and-cut (\"lazy\nconstraint\") method. Comparing the novel approach to the state of the art, we\nshow that the proposed approach turns out to produce excellent results when\napplied to small and medium-sized synthetic instances of up to 25 time-series.\nLastly, two interesting applications in bio-science and finance, to which the\nmethod is directly applied, further stress the opportunities in developing\nhighly accurate, globally convergent solvers that can handle modest instances.\n","authors":["Pavel Rytir","Ales Wodecki","Georgios Korpas","Jakub Marecek"],"pdf_url":"https://arxiv.org/pdf/2410.16100v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.16077v2","updated":"2024-10-22T09:37:45Z","published":"2024-10-21T14:55:59Z","title":"CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian\n  Product Routing in Mixture-of-Experts","summary":"  Large language models (LLM) have been attracting much attention from the\ncommunity recently, due to their remarkable performance in all kinds of\ndownstream tasks. According to the well-known scaling law, scaling up a dense\nLLM enhances its capabilities, but also significantly increases the\ncomputational complexity. Mixture-of-Experts (MoE) models address that by\nallowing the model size to grow without substantially raising training or\ninference costs. Yet MoE models face challenges regarding knowledge sharing\namong experts, making their performance somehow sensitive to routing accuracy.\nTo tackle that, previous works introduced shared experts and combined their\noutputs with those of the top $K$ routed experts in an ``addition'' manner. In\nthis paper, inspired by collective matrix factorization to learn shared\nknowledge among data, we propose CartesianMoE, which implements more effective\nknowledge sharing among experts in more like a ``multiplication'' manner.\nExtensive experimental results indicate that CartesianMoE outperforms previous\nMoE models for building LLMs, in terms of both perplexity and downstream task\nperformance. And we also find that CartesianMoE achieves better expert routing\nrobustness.\n","authors":["Zhenpeng Su","Xing Wu","Zijia Lin","Yizhe Xiong","Minxuan Lv","Guangyuan Ma","Hui Chen","Songlin Hu","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2410.16077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06003v4","updated":"2024-10-22T03:30:35Z","published":"2024-10-08T13:04:02Z","title":"Is the MMI Criterion Necessary for Interpretability? Degenerating\n  Non-causal Features to Plain Noise for Self-Rationalization","summary":"  An important line of research in the field of explainability is to extract a\nsmall subset of crucial rationales from the full input. The most widely used\ncriterion for rationale extraction is the maximum mutual information (MMI)\ncriterion. However, in certain datasets, there are spurious features\nnon-causally correlated with the label and also get high mutual information,\ncomplicating the loss landscape of MMI. Although some penalty-based methods\nhave been developed to penalize the spurious features (e.g., invariance\npenalty, intervention penalty, etc) to help MMI work better, these are merely\nremedial measures. In the optimization objectives of these methods, spurious\nfeatures are still distinguished from plain noise, which hinders the discovery\nof causal rationales. This paper aims to develop a new criterion that treats\nspurious features as plain noise, allowing the model to work on datasets rich\nin spurious features as if it were working on clean datasets, thereby making\nrationale extraction easier. We theoretically observe that removing either\nplain noise or spurious features from the input does not alter the conditional\ndistribution of the remaining components relative to the task label. However,\nsignificant changes in the conditional distribution occur only when causal\nfeatures are eliminated. Based on this discovery, the paper proposes a\ncriterion for \\textbf{M}aximizing the \\textbf{R}emaining \\textbf{D}iscrepancy\n(MRD). Experiments on six widely used datasets show that our MRD criterion\nimproves rationale quality (measured by the overlap with human-annotated\nrationales) by up to $10.4\\%$ as compared to several recent competitive MMI\nvariants. Code: \\url{https://github.com/jugechengzi/Rationalization-MRD}.\n","authors":["Wei Liu","Zhiying Deng","Zhongyu Niu","Jun Wang","Haozhao Wang","YuanKai Zhang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.06003v4.pdf","comment":"Accepted at NeurIPS 2024. arXiv admin note: text overlap with\n  arXiv:2309.13391"},{"id":"http://arxiv.org/abs/2410.15910v2","updated":"2024-10-22T05:06:36Z","published":"2024-10-21T11:33:14Z","title":"Diverse Policies Recovering via Pointwise Mutual Information Weighted\n  Imitation Learning","summary":"  Recovering a spectrum of diverse policies from a set of expert trajectories\nis an important research topic in imitation learning. After determining a\nlatent style for a trajectory, previous diverse policies recovering methods\nusually employ a vanilla behavioral cloning learning objective conditioned on\nthe latent style, treating each state-action pair in the trajectory with equal\nimportance. Based on an observation that in many scenarios, behavioral styles\nare often highly relevant with only a subset of state-action pairs, this paper\npresents a new principled method in diverse polices recovery. In particular,\nafter inferring or assigning a latent style for a trajectory, we enhance the\nvanilla behavioral cloning by incorporating a weighting mechanism based on\npointwise mutual information. This additional weighting reflects the\nsignificance of each state-action pair's contribution to learning the style,\nthus allowing our method to focus on state-action pairs most representative of\nthat style. We provide theoretical justifications for our new objective, and\nextensive empirical evaluations confirm the effectiveness of our method in\nrecovering diverse policies from expert data.\n","authors":["Hanlin Yang","Jian Yao","Weiming Liu","Qing Wang","Hanmin Qin","Hansheng Kong","Kirk Tang","Jiechao Xiong","Chao Yu","Kai Li","Junliang Xing","Hongwu Chen","Juchao Zhuo","Qiang Fu","Yang Wei","Haobo Fu"],"pdf_url":"https://arxiv.org/pdf/2410.15910v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.15859v2","updated":"2024-10-22T08:00:00Z","published":"2024-10-21T10:39:05Z","title":"Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs","summary":"  Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach.\n","authors":["Xin Ma","Yang Liu","Jingjing Liu","Xiaoxu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.15859v2.pdf","comment":"Accepted by NeurIPS 2024; 13 pages and 30 pages appendix"},{"id":"http://arxiv.org/abs/2410.15778v2","updated":"2024-10-22T05:01:28Z","published":"2024-10-21T08:42:30Z","title":"Reducing Hallucinations in Vision-Language Models via Latent Space\n  Steering","summary":"  Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.\n","authors":["Sheng Liu","Haotian Ye","Lei Xing","James Zou"],"pdf_url":"https://arxiv.org/pdf/2410.15778v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.15608v2","updated":"2024-10-22T13:55:26Z","published":"2024-10-21T03:13:20Z","title":"Moonshine: Speech Recognition for Live Transcription and Voice Commands","summary":"  This paper introduces Moonshine, a family of speech recognition models\noptimized for live transcription and voice command processing. Moonshine is\nbased on an encoder-decoder transformer architecture and employs Rotary\nPosition Embedding (RoPE) instead of traditional absolute position embeddings.\nThe model is trained on speech segments of various lengths, but without using\nzero-padding, leading to greater efficiency for the encoder during inference\ntime. When benchmarked against OpenAI's Whisper tiny-en, Moonshine Tiny\ndemonstrates a 5x reduction in compute requirements for transcribing a\n10-second speech segment while incurring no increase in word error rates across\nstandard evaluation datasets. These results highlight Moonshine's potential for\nreal-time and resource-constrained applications.\n","authors":["Nat Jeffries","Evan King","Manjunath Kudlur","Guy Nicholson","James Wang","Pete Warden"],"pdf_url":"https://arxiv.org/pdf/2410.15608v2.pdf","comment":"7 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.17242v1","updated":"2024-10-22T17:58:28Z","published":"2024-10-22T17:58:28Z","title":"LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias","summary":"  We propose the Large View Synthesis Model (LVSM), a novel transformer-based\napproach for scalable and generalizable novel view synthesis from sparse-view\ninputs. We introduce two architectures: (1) an encoder-decoder LVSM, which\nencodes input image tokens into a fixed number of 1D latent tokens, functioning\nas a fully learned scene representation, and decodes novel-view images from\nthem; and (2) a decoder-only LVSM, which directly maps input images to\nnovel-view outputs, completely eliminating intermediate scene representations.\nBoth models bypass the 3D inductive biases used in previous methods -- from 3D\nrepresentations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar\nprojections, plane sweeps) -- addressing novel view synthesis with a fully\ndata-driven approach. While the encoder-decoder model offers faster inference\ndue to its independent latent representation, the decoder-only LVSM achieves\nsuperior quality, scalability, and zero-shot generalization, outperforming\nprevious state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive\nevaluations across multiple datasets demonstrate that both LVSM variants\nachieve state-of-the-art novel view synthesis quality. Notably, our models\nsurpass all previous methods even with reduced computational resources (1-2\nGPUs). Please see our website for more details:\nhttps://haian-jin.github.io/projects/LVSM/ .\n","authors":["Haian Jin","Hanwen Jiang","Hao Tan","Kai Zhang","Sai Bi","Tianyuan Zhang","Fujun Luan","Noah Snavely","Zexiang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.17242v1.pdf","comment":"project page: https://haian-jin.github.io/projects/LVSM/"},{"id":"http://arxiv.org/abs/2410.17238v1","updated":"2024-10-22T17:56:08Z","published":"2024-10-22T17:56:08Z","title":"SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning","summary":"  Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.\n","authors":["Yizhou Chi","Yizhang Lin","Sirui Hong","Duyi Pan","Yaying Fei","Guanghao Mei","Bangbang Liu","Tianqi Pang","Jacky Kwok","Ceyao Zhang","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17238v1.pdf","comment":"The code is available at https://github.com/geekan/MetaGPT"},{"id":"http://arxiv.org/abs/2410.17234v1","updated":"2024-10-22T17:54:03Z","published":"2024-10-22T17:54:03Z","title":"Fine-Tuning Large Language Models to Appropriately Abstain with Semantic\n  Entropy","summary":"  Large Language Models (LLMs) are known to hallucinate, whereby they generate\nplausible but inaccurate text. This phenomenon poses significant risks in\ncritical applications, such as medicine or law, necessitating robust\nhallucination mitigation strategies. While recent works have proposed\nfine-tuning methods to teach LLMs to abstain from answering questions beyond\ntheir knowledge or capabilities, these methods rely on the existence of\nground-truth labels or are limited to short-form responses. To address these\nlimitations, we propose fine-tuning using semantic entropy, an uncertainty\nmeasure derived from introspection into the model which does not require\nexternal labels. We demonstrate that our approach matches or outperforms models\nfine-tuned using prior work and achieves strong performance for both short and\nlong-form generations on a range of datasets.\n","authors":["Benedict Aaron Tjandra","Muhammed Razzak","Jannik Kossen","Kunal Handa","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2410.17234v1.pdf","comment":"Accepted to NeurIPS Safe Generative AI Workshop 2024"},{"id":"http://arxiv.org/abs/2410.17233v1","updated":"2024-10-22T17:53:34Z","published":"2024-10-22T17:53:34Z","title":"Few-shot In-Context Preference Learning Using Large Language Models","summary":"  Designing reward functions is a core component of reinforcement learning but\ncan be challenging for truly complex behavior. Reinforcement Learning from\nHuman Feedback (RLHF) has been used to alleviate this challenge by replacing a\nhand-coded reward function with a reward function learned from preferences.\nHowever, it can be exceedingly inefficient to learn these rewards as they are\noften learned tabula rasa. We investigate whether Large Language Models (LLMs)\ncan reduce this query inefficiency by converting an iterative series of human\npreferences into code representing the rewards. We propose In-Context\nPreference Learning (ICPL), a method that uses the grounding of an LLM to\naccelerate learning reward functions from preferences. ICPL takes the\nenvironment context and task description, synthesizes a set of reward\nfunctions, and then repeatedly updates the reward functions using human\nrankings of videos of the resultant policies. Using synthetic preferences, we\ndemonstrate that ICPL is orders of magnitude more efficient than RLHF and is\neven competitive with methods that use ground-truth reward functions instead of\npreferences. Finally, we perform a series of human preference-learning trials\nand observe that ICPL extends beyond synthetic settings and can work\neffectively with humans-in-the-loop. Additional information and videos are\nprovided at https://sites.google.com/view/few-shot-icpl/home.\n","authors":["Chao Yu","Hong Lu","Jiaxuan Gao","Qixin Tan","Xinting Yang","Yu Wang","Yi Wu","Eugene Vinitsky"],"pdf_url":"https://arxiv.org/pdf/2410.17233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17230v1","updated":"2024-10-22T17:51:23Z","published":"2024-10-22T17:51:23Z","title":"Optimal Robust Estimation under Local and Global Corruptions: Stronger\n  Adversary and Smaller Error","summary":"  Algorithmic robust statistics has traditionally focused on the contamination\nmodel where a small fraction of the samples are arbitrarily corrupted. We\nconsider a recent contamination model that combines two kinds of corruptions:\n(i) small fraction of arbitrary outliers, as in classical robust statistics,\nand (ii) local perturbations, where samples may undergo bounded shifts on\naverage. While each noise model is well understood individually, the combined\ncontamination model poses new algorithmic challenges, with only partial results\nknown. Existing efficient algorithms are limited in two ways: (i) they work\nonly for a weak notion of local perturbations, and (ii) they obtain suboptimal\nerror for isotropic subgaussian distributions (among others). The latter\nlimitation led [NGS24, COLT'24] to hypothesize that improving the error might,\nin fact, be computationally hard. Perhaps surprisingly, we show that\ninformation theoretically optimal error can indeed be achieved in polynomial\ntime, under an even \\emph{stronger} local perturbation model (the\nsliced-Wasserstein metric as opposed to the Wasserstein metric). Notably, our\nanalysis reveals that the entire family of stability-based robust mean\nestimators continues to work optimally in a black-box manner for the combined\ncontamination model. This generalization is particularly useful in real-world\nscenarios where the specific form of data corruption is not known in advance.\nWe also present efficient algorithms for distribution learning and principal\ncomponent analysis in the combined contamination model.\n","authors":["Thanasis Pittas","Ankit Pensia"],"pdf_url":"https://arxiv.org/pdf/2410.17230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12101v2","updated":"2024-10-22T17:48:56Z","published":"2024-10-15T22:52:45Z","title":"The Persian Rug: solving toy models of superposition using large-scale\n  symmetries","summary":"  We present a complete mechanistic description of the algorithm learned by a\nminimal non-linear sparse data autoencoder in the limit of large input\ndimension. The model, originally presented in arXiv:2209.10652, compresses\nsparse data vectors through a linear layer and decompresses using another\nlinear layer followed by a ReLU activation. We notice that when the data is\npermutation symmetric (no input feature is privileged) large models reliably\nlearn an algorithm that is sensitive to individual weights only through their\nlarge-scale statistics. For these models, the loss function becomes\nanalytically tractable. Using this understanding, we give the explicit scalings\nof the loss at high sparsity, and show that the model is near-optimal among\nrecently proposed architectures. In particular, changing or adding to the\nactivation function any elementwise or filtering operation can at best improve\nthe model's performance by a constant factor. Finally, we forward-engineer a\nmodel with the requisite symmetries and show that its loss precisely matches\nthat of the trained models. Unlike the trained model weights, the low\nrandomness in the artificial weights results in miraculous fractal structures\nresembling a Persian rug, to which the algorithm is oblivious. Our work\ncontributes to neural network interpretability by introducing techniques for\nunderstanding the structure of autoencoders. Code to reproduce our results can\nbe found at https://github.com/KfirD/PersianRug .\n","authors":["Aditya Cowsik","Kfir Dolev","Alex Infanger"],"pdf_url":"https://arxiv.org/pdf/2410.12101v2.pdf","comment":"Improved arguments, presentation. No changes to results"},{"id":"http://arxiv.org/abs/2410.17225v1","updated":"2024-10-22T17:47:05Z","published":"2024-10-22T17:47:05Z","title":"Dhoroni: Exploring Bengali Climate Change and Environmental Views with a\n  Multi-Perspective News Dataset and Natural Language Processing","summary":"  Climate change poses critical challenges globally, disproportionately\naffecting low-income countries that often lack resources and linguistic\nrepresentation on the international stage. Despite Bangladesh's status as one\nof the most vulnerable nations to climate impacts, research gaps persist in\nBengali-language studies related to climate change and NLP. To address this\ndisparity, we introduce Dhoroni, a novel Bengali (Bangla) climate change and\nenvironmental news dataset, comprising a 2300 annotated Bangla news articles,\noffering multiple perspectives such as political influence,\nscientific/statistical data, authenticity, stance detection, and stakeholder\ninvolvement. Furthermore, we present an in-depth exploratory analysis of\nDhoroni and introduce BanglaBERT-Dhoroni family, a novel baseline model family\nfor climate and environmental opinion detection in Bangla, fine-tuned on our\ndataset. This research contributes significantly to enhancing accessibility and\nanalysis of climate discourse in Bengali (Bangla), addressing crucial\ncommunication and research gaps in climate-impacted regions like Bangladesh\nwith 180 million people.\n","authors":["Azmine Toushik Wasi","Wahid Faisal","Taj Ahmad","Abdur Rahman","Mst Rafia Islam"],"pdf_url":"https://arxiv.org/pdf/2410.17225v1.pdf","comment":"In Review"},{"id":"http://arxiv.org/abs/2410.17221v1","updated":"2024-10-22T17:45:45Z","published":"2024-10-22T17:45:45Z","title":"Scalable spectral representations for network multiagent control","summary":"  Network Markov Decision Processes (MDPs), a popular model for multi-agent\ncontrol, pose a significant challenge to efficient learning due to the\nexponential growth of the global state-action space with the number of agents.\nIn this work, utilizing the exponential decay property of network dynamics, we\nfirst derive scalable spectral local representations for network MDPs, which\ninduces a network linear subspace for the local $Q$-function of each agent.\nBuilding on these local spectral representations, we design a scalable\nalgorithmic framework for continuous state-action network MDPs, and provide\nend-to-end guarantees for the convergence of our algorithm. Empirically, we\nvalidate the effectiveness of our scalable representation-based approach on two\nbenchmark problems, and demonstrate the advantages of our approach over generic\nfunction approximation approaches to representing the local $Q$-functions.\n","authors":["Zhaolin Ren"," Runyu"," Zhang","Bo Dai","Na Li"],"pdf_url":"https://arxiv.org/pdf/2410.17221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17216v1","updated":"2024-10-22T17:41:14Z","published":"2024-10-22T17:41:14Z","title":"Hierarchical Upper Confidence Bounds for Constrained Online Learning","summary":"  The multi-armed bandit (MAB) problem is a foundational framework in\nsequential decision-making under uncertainty, extensively studied for its\napplications in areas such as clinical trials, online advertising, and resource\nallocation. Traditional MAB formulations, however, do not adequately capture\nscenarios where decisions are structured hierarchically, involve multi-level\nconstraints, or feature context-dependent action spaces. In this paper, we\nintroduce the hierarchical constrained bandits (HCB) framework, which extends\nthe contextual bandit problem to incorporate hierarchical decision structures\nand multi-level constraints. We propose the hierarchical constrained upper\nconfidence bound (HC-UCB) algorithm, designed to address the complexities of\nthe HCB problem by leveraging confidence bounds within a hierarchical setting.\nOur theoretical analysis establishes sublinear regret bounds for HC-UCB and\nprovides high-probability guarantees for constraint satisfaction at all\nhierarchical levels. Furthermore, we derive a minimax lower bound on the regret\nfor the HCB problem, demonstrating the near-optimality of our algorithm. The\nresults are significant for real-world applications where decision-making\nprocesses are inherently hierarchical and constrained, offering a robust and\nefficient solution that balances exploration and exploitation across multiple\nlevels of decision-making.\n","authors":["Ali Baheri"],"pdf_url":"https://arxiv.org/pdf/2410.17216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05145v2","updated":"2024-10-22T17:39:13Z","published":"2024-07-06T17:53:53Z","title":"On high-dimensional modifications of the nearest neighbor classifier","summary":"  Nearest neighbor classifier is arguably the most simple and popular\nnonparametric classifier available in the literature. However, due to the\nconcentration of pairwise distances and the violation of the neighborhood\nstructure, this classifier often suffers in high-dimension, low-sample size\n(HDLSS) situations, especially when the scale difference between the competing\nclasses dominates their location difference. Several attempts have been made in\nthe literature to take care of this problem. In this article, we discuss some\nof these existing methods and propose some new ones. We carry out some\ntheoretical investigations in this regard and analyze several simulated and\nbenchmark datasets to compare the empirical performances of proposed methods\nwith some of the existing ones.\n","authors":["Annesha Ghosh","Bilol Banerjee","Anil K. Ghosh"],"pdf_url":"https://arxiv.org/pdf/2407.05145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17212v1","updated":"2024-10-22T17:37:18Z","published":"2024-10-22T17:37:18Z","title":"Neuroevolution Neural Architecture Search for Evolving RNNs in Stock\n  Return Prediction and Portfolio Trading","summary":"  Stock return forecasting is a major component of numerous finance\napplications. Predicted stock returns can be incorporated into portfolio\ntrading algorithms to make informed buy or sell decisions which can optimize\nreturns. In such portfolio trading applications, the predictive performance of\na time series forecasting model is crucial. In this work, we propose the use of\nthe Evolutionary eXploration of Augmenting Memory Models (EXAMM) algorithm to\nprogressively evolve recurrent neural networks (RNNs) for stock return\npredictions. RNNs are evolved independently for each stocks and portfolio\ntrading decisions are made based on the predicted stock returns. The portfolio\nused for testing consists of the 30 companies in the Dow-Jones Index (DJI) with\neach stock have the same weight. Results show that using these evolved RNNs and\na simple daily long-short strategy can generate higher returns than both the\nDJI index and the S&P 500 Index for both 2022 (bear market) and 2023 (bull\nmarket).\n","authors":["Zimeng Lyu","Amulya Saxena","Rohaan Nadeem","Hao Zhang","Travis Desell"],"pdf_url":"https://arxiv.org/pdf/2410.17212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10796v2","updated":"2024-10-22T17:35:03Z","published":"2024-10-14T17:57:09Z","title":"Context-Parametric Inversion: Why Instruction Finetuning May Not\n  Actually Improve Context Reliance","summary":"  A standard practice when using large language models is for users to\nsupplement their instruction with an input context containing new information\nfor the model to process. However, models struggle to reliably follow the input\ncontext, especially when it conflicts with their parametric knowledge from\npretraining. In-principle, one would expect models to adapt to the user context\nbetter after instruction finetuning, particularly when handling knowledge\nconflicts. However, we observe a surprising failure mode: during instruction\ntuning, the context reliance under knowledge conflicts initially increases as\nexpected, but then gradually decreases as instruction finetuning progresses.\nThis happens while the performance on standard benchmarks keeps on increasing\nfar after this drop. We call this phenomenon context-parametric inversion and\nobserve it across multiple general purpose instruction tuning datasets such as\nTULU, Alpaca and Ultrachat, across different model families like Llama,\nMistral, and Pythia. We perform various controlled studies and theoretical\nanalysis to show that context-parametric inversion occurs due to examples in\nthe instruction finetuning data where the input context provides information\nthat aligns with model's parametric knowledge. Our analysis suggests some\nnatural mitigation strategies with limited but insightful gains, and serves as\na useful starting point in addressing this deficiency in instruction\nfinetuning.\n","authors":["Sachin Goyal","Christina Baek","J. Zico Kolter","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2410.10796v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.15291v2","updated":"2024-10-22T17:31:39Z","published":"2024-06-21T16:35:27Z","title":"Pessimistic asynchronous sampling in high-cost Bayesian optimization","summary":"  Asynchronous Bayesian optimization is a recently implemented technique that\nallows for parallel operation of experimental systems and disjointed workflows.\nContrasting with serial Bayesian optimization which individually selects\nexperiments one at a time after conducting a measurement for each experiment,\nasynchronous policies sequentially assign multiple experiments before\nmeasurements can be taken and evaluate new measurements continuously as they\nare made available. This technique allows for faster data generation and\ntherefore faster optimization of an experimental space. This work extends the\ncapabilities of asynchronous optimization methods beyond prior studies by\nevaluating four additional policies that incorporate pessimistic predictions in\nthe training data set. Combined with a conventional policy that uses model\npredictions, the five total policies were evaluated in a simulated environment\nand benchmarked with serial sampling. Under some conditions and parameter space\ndimensionalities, the pessimistic prediction asynchronous policy reached\noptimum experimental conditions in significantly fewer experiments than\nequivalent serial policies and proved to be less susceptible to convergence\nonto local optima at higher dimensions. Without accounting for the faster\nsampling rate, the pessimistic asynchronous algorithm presented in this work\ncould result in more efficient algorithm driven optimization of high-cost\nexperimental spaces. Accounting for sampling rate, the presented asynchronous\nalgorithm could allow for faster optimization in experimental spaces where\nmultiple experiments can be run before results are collected.\n","authors":["Amanda A. Volk","Kristofer G. Reyes","Jeffrey G. Ethier","Luke A. Baldwin"],"pdf_url":"https://arxiv.org/pdf/2406.15291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17209v1","updated":"2024-10-22T17:31:37Z","published":"2024-10-22T17:31:37Z","title":"Audio-to-Score Conversion Model Based on Whisper methodology","summary":"  This thesis develops a Transformer model based on Whisper, which extracts\nmelodies and chords from music audio and records them into ABC notation. A\ncomprehensive data processing workflow is customized for ABC notation,\nincluding data cleansing, formatting, and conversion, and a mutation mechanism\nis implemented to increase the diversity and quality of training data. This\nthesis innovatively introduces the \"Orpheus' Score\", a custom notation system\nthat converts music information into tokens, designs a custom vocabulary\nlibrary, and trains a corresponding custom tokenizer. Experiments show that\ncompared to traditional algorithms, the model has significantly improved\naccuracy and performance. While providing a convenient audio-to-score tool for\nmusic enthusiasts, this work also provides new ideas and tools for research in\nmusic information processing.\n","authors":["Hongyao Zhang","Bohang Sun"],"pdf_url":"https://arxiv.org/pdf/2410.17209v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2308.02594v4","updated":"2024-10-22T17:29:53Z","published":"2023-08-03T21:08:51Z","title":"SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning\n  Agents","summary":"  Deep Reinforcement Learning (DRL) has made significant advancements in\nvarious fields, such as autonomous driving, healthcare, and robotics, by\nenabling agents to learn optimal policies through interactions with their\nenvironments. However, the application of DRL in safety-critical domains\npresents challenges, particularly concerning the safety of the learned\npolicies. DRL agents, which are focused on maximizing rewards, may select\nunsafe actions, leading to safety violations. Runtime safety monitoring is thus\nessential to ensure the safe operation of these agents, especially in\nunpredictable and dynamic environments. This paper introduces SMARLA, a\nblack-box safety monitoring approach specifically designed for DRL agents.\nSMARLA utilizes machine learning to predict safety violations by observing the\nagent's behavior during execution. The approach is based on Q-values, which\nreflect the expected reward for taking actions in specific states. SMARLA\nemploys state abstraction to reduce the complexity of the state space,\nenhancing the predictive capabilities of the monitoring model. Such abstraction\nenables the early detection of unsafe states, allowing for the implementation\nof corrective and preventive measures before incidents occur. We quantitatively\nand qualitatively validated SMARLA on three well-known case studies widely used\nin DRL research. Empirical results reveal that SMARLA is accurate at predicting\nsafety violations, with a low false positive rate, and can predict violations\nat an early stage, approximately halfway through the execution of the agent,\nbefore violations occur. We also discuss different decision criteria, based on\nconfidence intervals of the predicted violation probabilities, to trigger\nsafety mechanisms aiming at a trade-off between early detection and low false\npositive rates.\n","authors":["Amirhossein Zolfagharian","Manel Abdellatif","Lionel C. Briand","Ramesh S"],"pdf_url":"https://arxiv.org/pdf/2308.02594v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08410v3","updated":"2024-10-22T17:29:47Z","published":"2023-12-13T11:27:15Z","title":"Universal approximation property of Banach space-valued random feature\n  models including random neural networks","summary":"  We introduce a Banach space-valued extension of random feature learning, a\ndata-driven supervised machine learning technique for large-scale kernel\napproximation. By randomly initializing the feature maps, only the linear\nreadout needs to be trained, which reduces the computational complexity\nsubstantially. Viewing random feature models as Banach space-valued random\nvariables, we prove a universal approximation result in the corresponding\nBochner space. Moreover, we derive approximation rates and an explicit\nalgorithm to learn an element of the given Banach space by such models. The\nframework of this paper includes random trigonometric/Fourier regression and in\nparticular random neural networks which are single-hidden-layer feedforward\nneural networks whose weights and biases are randomly initialized, whence only\nthe linear readout needs to be trained. For the latter, we can then lift the\nuniversal approximation property of deterministic neural networks to random\nneural networks, even within function spaces over non-compact domains, e.g.,\nweighted spaces, $L^p$-spaces, and (weighted) Sobolev spaces, where the latter\nincludes the approximation of the (weak) derivatives. In addition, we analyze\nwhen the training costs for approximating a given function grow polynomially in\nboth the input/output dimension and the reciprocal of a pre-specified tolerated\napproximation error. Furthermore, we demonstrate in a numerical example the\nempirical advantages of random feature models over their deterministic\ncounterparts.\n","authors":["Ariel Neufeld","Philipp Schmocker"],"pdf_url":"https://arxiv.org/pdf/2312.08410v3.pdf","comment":"64 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.14062v2","updated":"2024-10-22T17:23:30Z","published":"2024-10-17T22:07:53Z","title":"Data-driven rainfall prediction at a regional scale: a case study with\n  Ghana","summary":"  With a warming planet, tropical regions are expected to experience the brunt\nof climate change, with more intense and more volatile rainfall events.\nCurrently, state-of-the-art numerical weather prediction (NWP) models are known\nto struggle to produce skillful rainfall forecasts in tropical regions of\nAfrica. There is thus a pressing need for improved rainfall forecasting in\nthese regions. Over the last decade or so, the increased availability of\nlarge-scale meteorological datasets and the development of powerful machine\nlearning models have opened up new opportunities for data-driven weather\nforecasting. Focusing on Ghana in this study, we use these tools to develop two\nU-Net convolutional neural network (CNN) models, to predict 24h rainfall at 12h\nand 30h lead-time. The models were trained using data from the ERA5 reanalysis\ndataset, and the GPM-IMERG dataset. A special attention was paid to\ninterpretability. We developed a novel statistical methodology that allowed us\nto probe the relative importance of the meteorological variables input in our\nmodel, offering useful insights into the factors that drive precipitation in\nthe Ghana region. Empirically, we found that our 12h lead-time model has\nperformances that match, and in some accounts are better than the 18h lead-time\nforecasts produced by the ECMWF (as available in the TIGGE dataset). We also\nfound that combining our data-driven model with classical NWP further improves\nforecast accuracy.\n","authors":["Indrajit Kalita","Lucia Vilallonga","Yves Atchade"],"pdf_url":"https://arxiv.org/pdf/2410.14062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17932v2","updated":"2024-10-22T17:16:43Z","published":"2024-09-26T15:08:52Z","title":"Sample Compression Unleashed: New Generalization Bounds for Real Valued\n  Losses","summary":"  The sample compression theory provides generalization guarantees for\npredictors that can be fully defined using a subset of the training dataset and\na (short) message string, generally defined as a binary sequence. Previous\nworks provided generalization bounds for the zero-one loss, which is\nrestrictive notably when applied to deep learning approaches. In this paper, we\npresent a general framework for deriving new sample compression bounds that\nhold for real-valued unbounded losses. Using the Pick-To-Learn (P2L)\nmeta-algorithm, which transforms the training method of any machine-learning\npredictor to yield sample-compressed predictors, we empirically demonstrate the\ntightness of the bounds and their versatility by evaluating them on random\nforests and multiple types of neural networks.\n","authors":["Mathieu Bazinet","Valentina Zantedeschi","Pascal Germain"],"pdf_url":"https://arxiv.org/pdf/2409.17932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17194v1","updated":"2024-10-22T17:13:34Z","published":"2024-10-22T17:13:34Z","title":"Representation Shattering in Transformers: A Synthetic Study with\n  Knowledge Editing","summary":"  Knowledge Editing (KE) algorithms alter models' internal weights to perform\ntargeted updates to incorrect, outdated, or otherwise unwanted factual\nassociations. In order to better define the possibilities and limitations of\nthese approaches, recent work has shown that applying KE can adversely affect\nmodels' factual recall accuracy and diminish their general reasoning abilities.\nWhile these studies give broad insights into the potential harms of KE\nalgorithms, e.g., via performance evaluations on benchmarks, we argue little is\nunderstood as to why such destructive failures occur. Is it possible KE methods\ndistort representations of concepts beyond the targeted fact, hence hampering\nabilities at broad? If so, what is the extent of this distortion? To take a\nstep towards addressing such questions, we define a novel synthetic task\nwherein a Transformer is trained from scratch to internalize a ``structured''\nknowledge graph. The structure enforces relationships between entities of the\ngraph, such that editing a factual association has \"trickling effects\" on other\nentities in the graph (e.g., altering X's parent is Y to Z affects who X's\nsiblings' parent is). Through evaluations of edited models and analysis of\nextracted representations, we show that KE inadvertently affects\nrepresentations of entities beyond the targeted one, distorting relevant\nstructures that allow a model to infer unseen knowledge about an entity. We\ncall this phenomenon representation shattering and demonstrate that it results\nin degradation of factual recall and reasoning performance more broadly. To\ncorroborate our findings in a more naturalistic setup, we perform preliminary\nexperiments with a pretrained GPT-2-XL model and reproduce the representation\nshattering effect therein as well. Overall, our work yields a precise\nmechanistic hypothesis to explain why KE has adverse effects on model\ncapabilities.\n","authors":["Kento Nishi","Maya Okawa","Rahul Ramesh","Mikail Khona","Ekdeep Singh Lubana","Hidenori Tanaka"],"pdf_url":"https://arxiv.org/pdf/2410.17194v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.17191v1","updated":"2024-10-22T17:12:21Z","published":"2024-10-22T17:12:21Z","title":"On Functional Dimension and Persistent Pseudodimension","summary":"  For any fixed feedforward ReLU neural network architecture, it is well-known\nthat many different parameter settings can determine the same function. It is\nless well-known that the degree of this redundancy is inhomogeneous across\nparameter space. In this work, we discuss two locally applicable complexity\nmeasures for ReLU network classes and what we know about the relationship\nbetween them: (1) the local functional dimension [14, 18], and (2) a local\nversion of VC dimension that we call persistent pseudodimension. The former is\neasy to compute on finite batches of points; the latter should give local\nbounds on the generalization gap, which would inform an understanding of the\nmechanics of the double descent phenomenon [7].\n","authors":["J. Elisenda Grigsby","Kathryn Lindsey"],"pdf_url":"https://arxiv.org/pdf/2410.17191v1.pdf","comment":"41 pages"},{"id":"http://arxiv.org/abs/2409.13686v2","updated":"2024-10-22T17:06:17Z","published":"2024-09-20T17:54:16Z","title":"The Impact of Large Language Models in Academia: from Writing to\n  Speaking","summary":"  Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.\n","authors":["Mingmeng Geng","Caixi Chen","Yanru Wu","Dongping Chen","Yao Wan","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.13686v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.15473v2","updated":"2024-10-22T16:57:37Z","published":"2024-10-20T19:11:24Z","title":"A Bayesian Framework for Clustered Federated Learning","summary":"  One of the main challenges of federated learning (FL) is handling\nnon-independent and identically distributed (non-IID) client data, which may\noccur in practice due to unbalanced datasets and use of different data sources\nacross clients. Knowledge sharing and model personalization are key strategies\nfor addressing this issue. Clustered federated learning is a class of FL\nmethods that groups clients that observe similarly distributed data into\nclusters, such that every client is typically associated with one data\ndistribution and participates in training a model for that distribution along\ntheir cluster peers. In this paper, we present a unified Bayesian framework for\nclustered FL which associates clients to clusters. Then we propose several\npractical algorithms to handle the, otherwise growing, data associations in a\nway that trades off performance and computational complexity. This work\nprovides insights on client-cluster associations and enables client knowledge\nsharing in new ways. The proposed framework circumvents the need for unique\nclient-cluster associations, which is seen to increase the performance of the\nresulting models in a variety of experiments.\n","authors":["Peng Wu","Tales Imbiriba","Pau Closas"],"pdf_url":"https://arxiv.org/pdf/2410.15473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17175v1","updated":"2024-10-22T16:51:36Z","published":"2024-10-22T16:51:36Z","title":"Remote Timing Attacks on Efficient Language Model Inference","summary":"  Scaling up language models has significantly increased their capabilities.\nBut larger models are slower models, and so there is now an extensive body of\nwork (e.g., speculative sampling or parallel decoding) that improves the\n(average case) efficiency of language model generation. But these techniques\nintroduce data-dependent timing characteristics. We show it is possible to\nexploit these timing differences to mount a timing attack. By monitoring the\n(encrypted) network traffic between a victim user and a remote language model,\nwe can learn information about the content of messages by noting when responses\nare faster or slower. With complete black-box access, on open source systems we\nshow how it is possible to learn the topic of a user's conversation (e.g.,\nmedical advice vs. coding assistance) with 90%+ precision, and on production\nsystems like OpenAI's ChatGPT and Anthropic's Claude we can distinguish between\nspecific messages or infer the user's language. We further show that an active\nadversary can leverage a boosting attack to recover PII placed in messages\n(e.g., phone numbers or credit card numbers) for open source systems. We\nconclude with potential defenses and directions for future work.\n","authors":["Nicholas Carlini","Milad Nasr"],"pdf_url":"https://arxiv.org/pdf/2410.17175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11338v3","updated":"2024-10-22T16:39:19Z","published":"2023-06-20T07:14:37Z","title":"FDINet: Protecting against DNN Model Extraction via Feature Distortion\n  Index","summary":"  Machine Learning as a Service (MLaaS) platforms have gained popularity due to\ntheir accessibility, cost-efficiency, scalability, and rapid development\ncapabilities. However, recent research has highlighted the vulnerability of\ncloud-based models in MLaaS to model extraction attacks. In this paper, we\nintroduce FDINET, a novel defense mechanism that leverages the feature\ndistribution of deep neural network (DNN) models. Concretely, by analyzing the\nfeature distribution from the adversary's queries, we reveal that the feature\ndistribution of these queries deviates from that of the model's training set.\nBased on this key observation, we propose Feature Distortion Index (FDI), a\nmetric designed to quantitatively measure the feature distribution deviation of\nreceived queries. The proposed FDINET utilizes FDI to train a binary detector\nand exploits FDI similarity to identify colluding adversaries from distributed\nextraction attacks. We conduct extensive experiments to evaluate FDINET against\nsix state-of-the-art extraction attacks on four benchmark datasets and four\npopular model architectures. Empirical results demonstrate the following\nfindings FDINET proves to be highly effective in detecting model extraction,\nachieving a 100% detection accuracy on DFME and DaST. FDINET is highly\nefficient, using just 50 queries to raise an extraction alarm with an average\nconfidence of 96.08% for GTSRB. FDINET exhibits the capability to identify\ncolluding adversaries with an accuracy exceeding 91%. Additionally, it\ndemonstrates the ability to detect two types of adaptive attacks.\n","authors":["Hongwei Yao","Zheng Li","Haiqin Weng","Feng Xue","Zhan Qin","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2306.11338v3.pdf","comment":"Accepted to IEEE Transactions on Dependable and Secure Computing"},{"id":"http://arxiv.org/abs/2410.17161v1","updated":"2024-10-22T16:34:36Z","published":"2024-10-22T16:34:36Z","title":"Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence","summary":"  We propose a novel approach for learning interchangeable tokens in language\nmodels to obtain an extendable vocabulary that can generalize to new tokens.\nOur method is designed to address alpha-equivalence, the principle that\nrenaming bound variables in a syntactic expression preserves semantics. This\nproperty arises in many formal languages such as temporal logics, in which all\nproposition symbols represent the same concept but are distinguishable from\neach other. To handle such tokens, we develop a dual-part embedding approach.\nThe first part is shared across all interchangeable tokens, thereby enforcing\nthat they represent the same core concept. The second part is randomly\ngenerated for each token, which enables distinguishability. We evaluate our\nmethod in a Transformer encoder-decoder model on two tasks: solving linear\ntemporal logic formulae and copying with extendable vocabulary. Our method\ndemonstrates promising generalization capabilities in addition to introducing a\nfavorable inductive bias for alpha-equivalence.\n","authors":["İlker Işık","Ramazan Gokberk Cinbis","Ebru Aydin Gol"],"pdf_url":"https://arxiv.org/pdf/2410.17161v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.17159v1","updated":"2024-10-22T16:33:54Z","published":"2024-10-22T16:33:54Z","title":"LiNo: Advancing Recursive Residual Decomposition of Linear and Nonlinear\n  Patterns for Robust Time Series Forecasting","summary":"  Forecasting models are pivotal in a data-driven world with vast volumes of\ntime series data that appear as a compound of vast Linear and Nonlinear\npatterns. Recent deep time series forecasting models struggle to utilize\nseasonal and trend decomposition to separate the entangled components. Such a\nstrategy only explicitly extracts simple linear patterns like trends, leaving\nthe other linear modes and vast unexplored nonlinear patterns to the residual.\nTheir flawed linear and nonlinear feature extraction models and shallow-level\ndecomposition limit their adaptation to the diverse patterns present in\nreal-world scenarios. Given this, we innovate Recursive Residual Decomposition\nby introducing explicit extraction of both linear and nonlinear patterns. This\ndeeper-level decomposition framework, which is named LiNo, captures linear\npatterns using a Li block which can be a moving average kernel, and models\nnonlinear patterns using a No block which can be a Transformer encoder. The\nextraction of these two patterns is performed alternatively and recursively. To\nachieve the full potential of LiNo, we develop the current simple linear\npattern extractor to a general learnable autoregressive model, and design a\nnovel No block that can handle all essential nonlinear patterns. Remarkably,\nthe proposed LiNo achieves state-of-the-art on thirteen real-world benchmarks\nunder univariate and multivariate forecasting scenarios. Experiments show that\ncurrent forecasting models can deliver more robust and precise results through\nthis advanced Recursive Residual Decomposition. We hope this work could offer\ninsight into designing more effective forecasting models. Code is available at\nthis Repository: https://github.com/Levi-Ackman/LiNo.\n","authors":["Guoqi Yu","Yaoming Li","Xiaoyu Guo","Dayu Wang","Zirui Liu","Shujun Wang","Tong Yang"],"pdf_url":"https://arxiv.org/pdf/2410.17159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17147v1","updated":"2024-10-22T16:27:29Z","published":"2024-10-22T16:27:29Z","title":"Covariance estimation using Markov chain Monte Carlo","summary":"  We investigate the complexity of covariance matrix estimation for Gibbs\ndistributions based on dependent samples from a Markov chain. We show that when\n$\\pi$ satisfies a Poincar\\'e inequality and the chain possesses a spectral gap,\nwe can achieve similar sample complexity using MCMC as compared to an estimator\nconstructed using i.i.d. samples, with potentially much better query\ncomplexity. As an application of our methods, we show improvements for the\nquery complexity in both constrained and unconstrained settings for concrete\ninstances of MCMC. In particular, we provide guarantees regarding isotropic\nrounding procedures for sampling uniformly on convex bodies.\n","authors":["Yunbum Kook","Matthew S. Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17147v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2406.01205v2","updated":"2024-10-22T16:26:55Z","published":"2024-06-03T11:15:16Z","title":"ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and\n  Zero-shot Language Style Control With Decoupled Codec","summary":"  In this paper, we present ControlSpeech, a text-to-speech (TTS) system\ncapable of fully cloning the speaker's voice and enabling arbitrary control and\nadjustment of speaking style, merely based on a few seconds of audio prompt and\na simple textual style description prompt. Prior zero-shot TTS models and\ncontrollable TTS models either could only mimic the speaker's voice without\nfurther control and adjustment capabilities or were unrelated to\nspeaker-specific voice generation. Therefore, ControlSpeech focuses on a more\nchallenging new task-a TTS system with controllable timbre, content, and style\nat the same time. ControlSpeech takes speech prompts, content prompts, and\nstyle prompts as inputs and utilizes bidirectional attention and mask-based\nparallel decoding to capture corresponding codec representations in a discrete\ndecoupling codec space. Moreover, we discovered the issue of text style\ncontrollability in a many-to-many mapping fashion and proposed the Style\nMixture Semantic Density (SMSD) model to resolve this problem. SMSD module\nwhich is based on Gaussian mixture density networks, is designed to enhance the\nfine-grained partitioning and sampling capabilities of style semantic\ninformation and generate speech with more diverse styles. In terms of\nexperiments, we make available a controllable model toolkit called\nControlToolkit with a new style controllable dataset, some replicated baseline\nmodels and propose new metrics to evaluate both the control capability and the\nquality of generated audio in ControlSpeech. The relevant ablation studies\nvalidate the necessity of each component in ControlSpeech is necessary. We hope\nthat ControlSpeech can establish the next foundation paradigm of controllable\nspeech synthesis. The relevant code and demo are available at\nhttps://github.com/jishengpeng/ControlSpeech .\n","authors":["Shengpeng Ji","Jialong Zuo","Wen Wang","Minghui Fang","Siqi Zheng","Qian Chen","Ziyue Jiang","Hai Huang","Zehan Wang","Xize Cheng","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.01205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02581v3","updated":"2024-10-22T16:26:40Z","published":"2024-10-03T15:25:37Z","title":"Boosting Sample Efficiency and Generalization in Multi-agent\n  Reinforcement Learning via Equivariance","summary":"  Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency\nand poor generalization [1]. These challenges are partially due to a lack of\nstructure or inductive bias in the neural networks typically used in learning\nthe policy. One such form of structure that is commonly observed in multi-agent\nscenarios is symmetry. The field of Geometric Deep Learning has developed\nEquivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to\nrotations, translations, and reflections of nodes. Incorporating equivariance\nhas been shown to improve learning efficiency and decrease error [ 2 ]. In this\npaper, we demonstrate that EGNNs improve the sample efficiency and\ngeneralization in MARL. However, we also show that a naive application of EGNNs\nto MARL results in poor early exploration due to a bias in the EGNN structure.\nTo mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural\nNetworks or E2GN2. We compare E2GN2 to other common function approximators\nusing common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant\nimprovement in sample efficiency, greater final reward convergence, and a 2x-5x\ngain in over standard GNNs in our generalization tests. These results pave the\nway for more reliable and effective solutions in complex multi-agent systems.\n","authors":["Joshua McClellan","Naveed Haghani","John Winder","Furong Huang","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2410.02581v3.pdf","comment":"accepted as a poster at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17146v1","updated":"2024-10-22T16:26:05Z","published":"2024-10-22T16:26:05Z","title":"LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances\n  Model Merging","summary":"  Large pre-trained models exhibit impressive zero-shot performance across\ndiverse tasks, but fine-tuning often leads to catastrophic forgetting, where\nimprovements on a target domain degrade generalization on other tasks. To\naddress this challenge, we introduce LiNeS, Layer-increasing Network Scaling, a\npost-training editing technique designed to preserve pre-trained generalization\nwhile enhancing fine-tuned task performance. LiNeS scales parameter updates\nlinearly based on their layer depth within the network, maintaining shallow\nlayers close to their pre-trained values to preserve general features while\nallowing deeper layers to retain task-specific representations. We further\nextend this approach to multi-task model merging scenarios, where layer-wise\nscaling of merged parameters reduces negative task interference. LiNeS\ndemonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nIt mitigates forgetting, enhances out-of-distribution generalization,\nintegrates seamlessly with existing multi-task model merging baselines\nimproving their performance across benchmarks and model sizes, and can boost\ngeneralization when merging LLM policies aligned with different rewards via\nRLHF. Importantly, our method is simple to implement and complementary to many\nexisting techniques.\n","authors":["Ke Wang","Nikolaos Dimitriadis","Alessandro Favero","Guillermo Ortiz-Jimenez","Francois Fleuret","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2410.17146v1.pdf","comment":"The first two authors contributed equally to this work; Project\n  website: \\url{https://lines-merging.github.io/}"},{"id":"http://arxiv.org/abs/2410.17145v1","updated":"2024-10-22T16:26:03Z","published":"2024-10-22T16:26:03Z","title":"Can General-Purpose Large Language Models Generalize to English-Thai\n  Machine Translation ?","summary":"  Large language models (LLMs) perform well on common tasks but struggle with\ngeneralization in low-resource and low-computation settings. We examine this\nlimitation by testing various LLMs and specialized translation models on\nEnglish-Thai machine translation and code-switching datasets. Our findings\nreveal that under more strict computational constraints, such as 4-bit\nquantization, LLMs fail to translate effectively. In contrast, specialized\nmodels, with comparable or lower computational requirements, consistently\noutperform LLMs. This underscores the importance of specialized models for\nmaintaining performance under resource constraints.\n","authors":["Jirat Chiaranaipanich","Naiyarat Hanmatheekuna","Jitkapat Sawatphol","Krittamate Tiankanon","Jiramet Kinchagawat","Amrest Chinkamol","Parinthapat Pengpun","Piyalitt Ittichaiwong","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2410.17145v1.pdf","comment":"Accepted in GenBench EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17142v1","updated":"2024-10-22T16:19:13Z","published":"2024-10-22T16:19:13Z","title":"Coniferest: a complete active anomaly detection framework","summary":"  We present coniferest, an open source generic purpose active anomaly\ndetection framework written in Python. The package design and implemented\nalgorithms are described. Currently, static outlier detection analysis is\nsupported via the Isolation forest algorithm. Moreover, Active Anomaly\nDiscovery (AAD) and Pineforest algorithms are available to tackle active\nanomaly detection problems. The algorithms and package performance are\nevaluated on a series of synthetic datasets. We also describe a few success\ncases which resulted from applying the package to real astronomical data in\nactive anomaly detection tasks within the SNAD project.\n","authors":["M. V. Kornilov","V. S. Korolev","K. L. Malanchev","A. D. Lavrukhina","E. Russeil","T. A. Semenikhin","E. Gangler","E. E. O. Ishida","M. V. Pruzhinskaya","A. A. Volnova","S. Sreejith"],"pdf_url":"https://arxiv.org/pdf/2410.17142v1.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.17135v1","updated":"2024-10-22T16:07:55Z","published":"2024-10-22T16:07:55Z","title":"Reinforcement Learning for Data-Driven Workflows in Radio\n  Interferometry. I. Principal Demonstration in Calibration","summary":"  Radio interferometry is an observational technique used to study\nastrophysical phenomena. Data gathered by an interferometer requires\nsubstantial processing before astronomers can extract the scientific\ninformation from it. Data processing consists of a sequence of calibration and\nanalysis procedures where choices must be made about the sequence of procedures\nas well as the specific configuration of the procedure itself. These choices\nare typically based on a combination of measurable data characteristics, an\nunderstanding of the instrument itself, an appreciation of the trade-offs\nbetween compute cost and accuracy, and a learned understanding of what is\nconsidered \"best practice\". A metric of absolute correctness is not always\navailable and validity is often subject to human judgment. The underlying\nprinciples and software configurations to discern a reasonable workflow for a\ngiven dataset is the subject of training workshops for students and scientists.\nOur goal is to use objective metrics that quantify best practice, and\nnumerically map out the decision space with respect to our metrics. With these\nobjective metrics we demonstrate an automated, data-driven, decision system\nthat is capable of sequencing the optimal action(s) for processing\ninterferometric data. This paper introduces a simplified description of the\nprinciples behind interferometry and the procedures required for data\nprocessing. We highlight the issues with current automation approaches and\npropose our ideas for solving these bottlenecks. A prototype is demonstrated\nand the results are discussed.\n","authors":["Brian M. Kirk","Urvashi Rau","Ramyaa Ramyaa"],"pdf_url":"https://arxiv.org/pdf/2410.17135v1.pdf","comment":"22 pages, 13 figures; accepted for publication in The Astronomical\n  Journal October 18, 2024"},{"id":"http://arxiv.org/abs/2410.17128v1","updated":"2024-10-22T16:00:44Z","published":"2024-10-22T16:00:44Z","title":"Understanding Transfer Learning via Mean-field Analysis","summary":"  We propose a novel framework for exploring generalization errors of transfer\nlearning through the lens of differential calculus on the space of probability\nmeasures. In particular, we consider two main transfer learning scenarios,\n$\\alpha$-ERM and fine-tuning with the KL-regularized empirical risk\nminimization and establish generic conditions under which the generalization\nerror and the population risk convergence rates for these scenarios are\nstudied. Based on our theoretical results, we show the benefits of transfer\nlearning with a one-hidden-layer neural network in the mean-field regime under\nsome suitable integrability and regularity assumptions on the loss and\nactivation functions.\n","authors":["Gholamali Aminian","Samuel N. Cohen","Łukasz Szpruch"],"pdf_url":"https://arxiv.org/pdf/2410.17128v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.17126v1","updated":"2024-10-22T15:59:58Z","published":"2024-10-22T15:59:58Z","title":"Exploring RL-based LLM Training for Formal Language Tasks with\n  Programmed Rewards","summary":"  Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning\nfrom Human Feedback to align large language models (LLMs) with downstream\ntasks. This paper investigates the feasibility of using PPO for direct\nreinforcement learning (RL) from explicitly programmed reward signals, as\nopposed to indirect learning from human feedback via an intermediary reward\nmodel. We focus on tasks expressed through formal languages, such as\nmathematics and programming, where explicit reward functions can be programmed\nto automatically assess the quality of generated outputs. We apply this\napproach to a sentiment alignment task, a simple arithmetic task, and a more\ncomplex game synthesis task. The sentiment alignment task replicates prior\nresearch and serves to validate our experimental setup. Our results show that\npure RL-based training for the two formal language tasks is challenging, with\nsuccess being limited even for the simple arithmetic task. We propose a novel\nbatch-entropy regularization term to aid exploration, although training is not\nyet entirely stable. Our findings suggest that direct RL training of LLMs may\nbe more suitable for relatively minor changes, such as alignment, than for\nlearning new tasks altogether, even if an informative reward signal can be\nexpressed programmatically.\n","authors":["Alexander G. Padula","Dennis J. N. J. Soemers"],"pdf_url":"https://arxiv.org/pdf/2410.17126v1.pdf","comment":"Accepted at BNAIC 2024"},{"id":"http://arxiv.org/abs/2410.17118v1","updated":"2024-10-22T15:49:53Z","published":"2024-10-22T15:49:53Z","title":"Learning Load Balancing with GNN in MPTCP-Enabled Heterogeneous Networks","summary":"  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks are a\npromising paradigm of heterogeneous network (HetNet), attributed to the\ncomplementary physical properties of optical spectra and radio frequency.\nHowever, the current development of such HetNets is mostly bottlenecked by the\nexisting transmission control protocol (TCP), which restricts the user\nequipment (UE) to connecting one access point (AP) at a time. While the ongoing\ninvestigation on multipath TCP (MPTCP) can bring significant benefits, it\ncomplicates the network topology of HetNets, making the existing load balancing\n(LB) learning models less effective. Driven by this, we propose a graph neural\nnetwork (GNN)-based model to tackle the LB problem for MPTCP-enabled HetNets,\nwhich results in a partial mesh topology. Such a topology can be modeled as a\ngraph, with the channel state information and data rate requirement embedded as\nnode features, while the LB solutions are deemed as edge labels. Compared to\nthe conventional deep neural network (DNN), the proposed GNN-based model\nexhibits two key strengths: i) it can better interpret a complex network\ntopology; and ii) it can handle various numbers of APs and UEs with a single\ntrained model. Simulation results show that against the traditional\noptimisation method, the proposed learning model can achieve near-optimal\nthroughput within a gap of 11.5%, while reducing the inference time by 4 orders\nof magnitude. In contrast to the DNN model, the new method can improve the\nnetwork throughput by up to 21.7%, at a similar inference time level.\n","authors":["Han Ji","Xiping Wu","Zhihong Zeng","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2410.17118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09656v5","updated":"2024-10-22T15:36:34Z","published":"2023-02-19T19:03:26Z","title":"Credal Bayesian Deep Learning","summary":"  Uncertainty quantification and robustness to distribution shifts are\nimportant goals in machine learning and artificial intelligence. Although\nBayesian Neural Networks (BNNs) allow for uncertainty in the predictions to be\nassessed, different sources of predictive uncertainty cannot be distinguished\nproperly. We present Credal Bayesian Deep Learning (CBDL). Heuristically, CBDL\nallows to train an (uncountably) infinite ensemble of BNNs, using only finitely\nmany elements. This is possible thanks to prior and likelihood finitely\ngenerated credal sets (FGCSs), a concept from the imprecise probability\nliterature. Intuitively, convex combinations of a finite collection of\nprior-likelihood pairs are able to represent infinitely many such pairs. After\ntraining, CBDL outputs a set of posteriors on the parameters of the neural\nnetwork. At inference time, such posterior set is used to derive a set of\npredictive distributions that is in turn utilized to distinguish between\n(predictive) aleatoric and epistemic uncertainties, and to quantify them. The\npredictive set also produces either (i) a collection of outputs enjoying\ndesirable probabilistic guarantees, or (ii) the single output that is deemed\nthe best, that is, the one having the highest predictive lower probability --\nanother imprecise-probabilistic concept. CBDL is more robust than single BNNs\nto prior and likelihood misspecification, and to distribution shift. We show\nthat CBDL is better at quantifying and disentangling different types of\n(predictive) uncertainties than single BNNs and ensemble of BNNs. In addition,\nwe apply CBDL to two case studies to demonstrate its downstream tasks\ncapabilities: one, for motion prediction in autonomous driving scenarios, and\ntwo, to model blood glucose and insulin dynamics for artificial pancreas\ncontrol. We show that CBDL performs better when compared to an ensemble of BNNs\nbaseline.\n","authors":["Michele Caprio","Souradeep Dutta","Kuk Jin Jang","Vivian Lin","Radoslav Ivanov","Oleg Sokolsky","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2302.09656v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17111v1","updated":"2024-10-22T15:36:04Z","published":"2024-10-22T15:36:04Z","title":"Permutation Picture of Graph Combinatorial Optimization Problems","summary":"  This paper proposes a framework that formulates a wide range of graph\ncombinatorial optimization problems using permutation-based representations.\nThese problems include the travelling salesman problem, maximum independent\nset, maximum cut, and various other related problems. This work potentially\nopens up new avenues for algorithm design in neural combinatorial optimization,\nbridging the gap between discrete and continuous optimization techniques.\n","authors":["Yimeng Min"],"pdf_url":"https://arxiv.org/pdf/2410.17111v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.17099v1","updated":"2024-10-22T15:22:58Z","published":"2024-10-22T15:22:58Z","title":"Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations","summary":"  The quality is a crucial issue for crowd annotations. Answer aggregation is\nan important type of solution. The aggregated answers estimated from multiple\ncrowd answers to the same instance are the eventually collected annotations,\nrather than the individual crowd answers themselves. Recently, the capability\nof Large Language Models (LLMs) on data annotation tasks has attracted interest\nfrom researchers. Most of the existing studies mainly focus on the average\nperformance of individual crowd workers; several recent works studied the\nscenarios of aggregation on categorical labels and LLMs used as label creators.\nHowever, the scenario of aggregation on text answers and the role of LLMs as\naggregators are not yet well-studied. In this paper, we investigate the\ncapability of LLMs as aggregators in the scenario of close-ended crowd text\nanswer aggregation. We propose a human-LLM hybrid text answer aggregation\nmethod with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We\nmake the experiments based on public crowdsourcing datasets. The results show\nthe effectiveness of our approach based on the collaboration of crowd workers\nand LLMs.\n","authors":["Jiyi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17099v1.pdf","comment":"Accepted in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17086v1","updated":"2024-10-22T15:13:13Z","published":"2024-10-22T15:13:13Z","title":"Exploration and Persuasion","summary":"  How to incentivize self-interested agents to explore when they prefer to\nexploit? Consider a population of self-interested agents that make decisions\nunder uncertainty. They \"explore\" to acquire new information and \"exploit\" this\ninformation to make good decisions. Collectively they need to balance these two\nobjectives, but their incentives are skewed toward exploitation. This is\nbecause exploration is costly, but its benefits are spread over many agents in\nthe future.\n  \"Incentivized Exploration\" addresses this issue via strategic communication.\nConsider a benign ``principal\" which can communicate with the agents and make\nrecommendations, but cannot force the agents to comply. Moreover, suppose the\nprincipal can observe the agents' decisions and the outcomes of these\ndecisions. The goal is to design a communication and recommendation policy\nwhich (i) achieves a desirable balance between exploration and exploitation,\nand (ii) incentivizes the agents to follow recommendations. What makes it\nfeasible is \"information asymmetry\": the principal knows more than any one\nagent, as it collects information from many. It is essential that the principal\ndoes not fully reveal all its knowledge to the agents.\n  Incentivized exploration combines two important problems in, resp., machine\nlearning and theoretical economics. First, if agents always follow\nrecommendations, the principal faces a multi-armed bandit problem: essentially,\ndesign an algorithm that balances exploration and exploitation. Second,\ninteraction with a single agent corresponds to \"Bayesian persuasion\", where a\nprincipal leverages information asymmetry to convince an agent to take a\nparticular action. We provide a brief but self-contained introduction to each\nproblem through the lens of incentivized exploration, solving a key special\ncase of the former as a sub-problem of the latter.\n","authors":["Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2410.17086v1.pdf","comment":"This is a chapter published in \"Online and Matching-Based Markets\",\n  Cambridge University Press, 2023. It has been available from the author's\n  website since 2021"},{"id":"http://arxiv.org/abs/2401.00691v3","updated":"2024-10-22T15:06:05Z","published":"2024-01-01T08:03:52Z","title":"Stochastic Gradient Descent for Nonparametric Regression","summary":"  This paper introduces an iterative algorithm for training nonparametric\nadditive models that enjoys favorable memory storage and computational\nrequirements. The algorithm can be viewed as the functional counterpart of\nstochastic gradient descent, applied to the coefficients of a truncated basis\nexpansion of the component functions. We show that the resulting estimator\nsatisfies an oracle inequality that allows for model mis-specification. In the\nwell-specified setting, by choosing the learning rate carefully across three\ndistinct stages of training, we demonstrate that its risk is minimax optimal in\nterms of the dependence on the dimensionality of the data and the size of the\ntraining sample. We also provide polynomial convergence rates even when the\ncovariates do not have full support on their domain.\n","authors":["Xin Chen","Jason M. Klusowski"],"pdf_url":"https://arxiv.org/pdf/2401.00691v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05180v2","updated":"2024-10-22T14:54:42Z","published":"2024-04-22T10:33:06Z","title":"ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in\n  Robotic Surgical Skill Assessment","summary":"  In surgical skill assessment, Objective Structured Assessments of Technical\nSkills (OSATS scores) and the Global Rating Scale (GRS) are established tools\nfor evaluating the performance of surgeons during training. These metrics,\ncoupled with feedback on their performance, enable surgeons to improve and\nachieve standards of practice. Recent studies on the open-source dataset\nJIGSAW, which contains both GRS and OSATS labels, have focused on regressing\nGRS scores from kinematic signals, video data, or a combination of both. In\nthis paper, we argue that regressing the GRS score, a unitless value, by itself\nis too restrictive, and variations throughout the surgical trial do not hold\nsignificant clinical meaning. To address this gap, we developed a recurrent\ntransformer model that outputs the surgeon's performance throughout their\ntraining session by relating the model's hidden states to five OSATS scores\nderived from kinematic signals. These scores are averaged and aggregated to\nproduce a GRS prediction, enabling assessment of the model's performance\nagainst the state-of-the-art (SOTA). We report Spearman's Correlation\nCoefficient (SCC), demonstrating that our model outperforms SOTA models for all\ntasks, except for Suturing under the leave-one-subject-out (LOSO) scheme (SCC\n0.68-0.89), while achieving comparable performance for suturing and across\ntasks under the leave-one-user-out (LOUO) scheme (SCC 0.45-0.68) and beating\nSOTA for Needle Passing (0.69). We argue that relating final OSATS scores to\nshort instances throughout a surgeon's procedure is more clinically meaningful\nthan a single GRS score. This approach also allows us to translate quantitative\npredictions into qualitative feedback, which is crucial for any automated\nsurgical skill assessment pipeline. A senior surgeon validated our model's\nbehaviour and agreed with the semi-supervised predictions 77 \\% (p = 0.006) of\nthe time.\n","authors":["Julien Quarez","Matthew Elliot","Oscar Maccormac","Marc Modat","Sebastien Ourselin","Jonathan Shapey","Alejandro Granados"],"pdf_url":"https://arxiv.org/pdf/2407.05180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17075v1","updated":"2024-10-22T14:52:46Z","published":"2024-10-22T14:52:46Z","title":"Combinatorial Logistic Bandits","summary":"  We introduce a novel framework called combinatorial logistic bandits (CLogB),\nwhere in each round, a subset of base arms (called the super arm) is selected,\nwith the outcome of each base arm being binary and its expectation following a\nlogistic parametric model. The feedback is governed by a general arm triggering\nprocess. Our study covers CLogB with reward functions satisfying two smoothness\nconditions, capturing application scenarios such as online content delivery,\nonline learning to rank, and dynamic channel allocation. We first propose a\nsimple yet efficient algorithm, CLogUCB, utilizing a variance-agnostic\nexploration bonus. Under the 1-norm triggering probability modulated (TPM)\nsmoothness condition, CLogUCB achieves a regret bound of\n$\\tilde{O}(d\\sqrt{\\kappa KT})$, where $\\tilde{O}$ ignores logarithmic factors,\n$d$ is the dimension of the feature vector, $\\kappa$ represents the\nnonlinearity of the logistic model, and $K$ is the maximum number of base arms\na super arm can trigger. This result improves on prior work by a factor of\n$\\tilde{O}(\\sqrt{\\kappa})$. We then enhance CLogUCB with a variance-adaptive\nversion, VA-CLogUCB, which attains a regret bound of $\\tilde{O}(d\\sqrt{KT})$\nunder the same 1-norm TPM condition, improving another\n$\\tilde{O}(\\sqrt{\\kappa})$ factor. VA-CLogUCB shows even greater promise under\nthe stronger triggering probability and variance modulated (TPVM) condition,\nachieving a leading $\\tilde{O}(d\\sqrt{T})$ regret, thus removing the additional\ndependency on the action-size $K$. Furthermore, we enhance the computational\nefficiency of VA-CLogUCB by eliminating the nonconvex optimization process when\nthe context feature map is time-invariant while maintaining the tight\n$\\tilde{O}(d\\sqrt{T})$ regret. Finally, experiments on synthetic and real-world\ndatasets demonstrate the superior performance of our algorithms compared to\nbenchmark algorithms.\n","authors":["Xutong Liu","Xiangxiang Dai","Xuchuang Wang","Mohammad Hajiesmaili","John C. S. Lui"],"pdf_url":"https://arxiv.org/pdf/2410.17075v1.pdf","comment":"Accepted to ACM SIGMETRICS 2025"},{"id":"http://arxiv.org/abs/2401.03069v4","updated":"2024-10-22T14:50:29Z","published":"2024-01-05T21:30:13Z","title":"Towards Enhancing the Reproducibility of Deep Learning Bugs: An\n  Empirical Study","summary":"  Context: Deep learning has achieved remarkable progress in various domains.\nHowever, like any software system, deep learning systems contain bugs, some of\nwhich can have severe impacts, as evidenced by crashes involving autonomous\nvehicles. Despite substantial advancements in deep learning techniques, little\nresearch has focused on reproducing deep learning bugs, which is an essential\nstep for their resolution. Existing literature suggests that only 3% of deep\nlearning bugs are reproducible, underscoring the need for further research.\n  Objective: This paper examines the reproducibility of deep learning bugs. We\nidentify edit actions and useful information that could improve the\nreproducibility of deep learning bugs.\n  Method: First, we construct a dataset of 668 deep-learning bugs from Stack\nOverflow and GitHub across three frameworks and 22 architectures. Second, out\nof the 668 bugs, we select 165 bugs using stratified sampling and attempt to\ndetermine their reproducibility. While reproducing these bugs, we identify edit\nactions and useful information for their reproduction. Third, we used the\nApriori algorithm to identify useful information and edit actions required to\nreproduce specific types of bugs. Finally, we conducted a user study involving\n22 developers to assess the effectiveness of our findings in real-life\nsettings.\n  Results: We successfully reproduced 148 out of 165 bugs attempted. We\nidentified ten edit actions and five useful types of component information that\ncan help us reproduce the deep learning bugs. With the help of our findings,\nthe developers were able to reproduce 22.92% more bugs and reduce their\nreproduction time by 24.35%.\n  Conclusions: Our research addresses the critical issue of deep learning bug\nreproducibility. Practitioners and researchers can leverage our findings to\nimprove deep learning bug reproducibility.\n","authors":["Mehil B. Shah","Mohammad Masudur Rahman","Foutse Khomh"],"pdf_url":"https://arxiv.org/pdf/2401.03069v4.pdf","comment":"Accepted at the Journal of Empirical Software Engineering (EMSE)"},{"id":"http://arxiv.org/abs/2410.17066v1","updated":"2024-10-22T14:46:20Z","published":"2024-10-22T14:46:20Z","title":"Neuronal Competition Groups with Supervised STDP for Spike-Based\n  Classification","summary":"  Spike Timing-Dependent Plasticity (STDP) is a promising substitute to\nbackpropagation for local training of Spiking Neural Networks (SNNs) on\nneuromorphic hardware. STDP allows SNNs to address classification tasks by\ncombining unsupervised STDP for feature extraction and supervised STDP for\nclassification. Unsupervised STDP is usually employed with Winner-Takes-All\n(WTA) competition to learn distinct patterns. However, WTA for supervised STDP\nclassification faces unbalanced competition challenges. In this paper, we\npropose a method to effectively implement WTA competition in a spiking\nclassification layer employing first-spike coding and supervised STDP training.\nWe introduce the Neuronal Competition Group (NCG), an architecture that\nimproves classification capabilities by promoting the learning of various\npatterns per class. An NCG is a group of neurons mapped to a specific class,\nimplementing intra-class WTA and a novel competition regulation mechanism based\non two-compartment thresholds. We incorporate our proposed architecture into\nspiking classification layers trained with state-of-the-art supervised STDP\nrules. On top of two different unsupervised feature extractors, we obtain\nsignificant accuracy improvements on image recognition datasets such as\nCIFAR-10 and CIFAR-100. We show that our competition regulation mechanism is\ncrucial for ensuring balanced competition and improved class separation.\n","authors":["Gaspard Goupy","Pierre Tirilly","Ioan Marius Bilasco"],"pdf_url":"https://arxiv.org/pdf/2410.17066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16532v2","updated":"2024-10-22T14:40:15Z","published":"2024-08-29T13:43:36Z","title":"WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling","summary":"  Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.\n","authors":["Shengpeng Ji","Ziyue Jiang","Wen Wang","Yifu Chen","Minghui Fang","Jialong Zuo","Qian Yang","Xize Cheng","Zehan Wang","Ruiqi Li","Ziang Zhang","Xiaoda Yang","Rongjie Huang","Yidi Jiang","Qian Chen","Siqi Zheng","Wen Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.16532v2.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2410.17055v1","updated":"2024-10-22T14:36:44Z","published":"2024-10-22T14:36:44Z","title":"Optimal Design for Reward Modeling in RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) has become a popular\napproach to align language models (LMs) with human preferences. This method\ninvolves collecting a large dataset of human pairwise preferences across\nvarious text generations and using it to infer (implicitly or explicitly) a\nreward model. Numerous methods have been proposed to learn the reward model and\nalign a LM with it. However, the costly process of collecting human preferences\nhas received little attention and could benefit from theoretical insights. This\npaper addresses this issue and aims to formalize the reward training model in\nRLHF. We frame the selection of an effective dataset as a simple regret\nminimization task, using a linear contextual dueling bandit method. Given the\npotentially large number of arms, this approach is more coherent than the\nbest-arm identification setting. We then propose an offline framework for\nsolving this problem. Under appropriate assumptions - linearity of the reward\nmodel in the embedding space, and boundedness of the reward parameter - we\nderive bounds on the simple regret. Finally, we provide a lower bound that\nmatches our upper bound up to constant and logarithmic terms. To our knowledge,\nthis is the first theoretical contribution in this area to provide an offline\napproach as well as worst-case guarantees.\n","authors":["Antoine Scheid","Etienne Boursier","Alain Durmus","Michael I. Jordan","Pierre Ménard","Eric Moulines","Michal Valko"],"pdf_url":"https://arxiv.org/pdf/2410.17055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17050v1","updated":"2024-10-22T14:30:03Z","published":"2024-10-22T14:30:03Z","title":"UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs","summary":"  The key components of machine learning are data samples for training, model\nfor learning patterns, and loss function for optimizing accuracy. Analogously,\nunlearning can potentially be achieved through anti-data samples (or\nanti-samples), unlearning method, and reversed loss function. While prior\nresearch has explored unlearning methods and reversed loss functions, the\npotential of anti-samples remains largely untapped. In this paper, we introduce\nUnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language\nmodels (LLMs). Our contributions are threefold; first, we propose a novel\nconcept of anti-sample-induced unlearning; second, we generate anti-samples by\nleveraging misleading rationales, which help reverse learned associations and\naccelerate the unlearning process; and third, we enable fine-grained targeted\nunlearning, allowing for the selective removal of specific associations without\nimpacting related knowledge - something not achievable by previous works.\nResults demonstrate that anti-samples offer an efficient, targeted unlearning\nstrategy for LLMs, opening new avenues for privacy-preserving machine learning\nand model modification.\n","authors":["Yash Sinha","Murari Mandal","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2410.17050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17049v1","updated":"2024-10-22T14:27:43Z","published":"2024-10-22T14:27:43Z","title":"A Comparison of Baseline Models and a Transformer Network for SOC\n  Prediction in Lithium-Ion Batteries","summary":"  Accurately predicting the state of charge of Lithium-ion batteries is\nessential to the performance of battery management systems of electric\nvehicles. One of the main reasons for the slow global adoption of electric cars\nis driving range anxiety. The ability of a battery management system to\naccurately estimate the state of charge can help alleviate this problem. In\nthis paper, a comparison between data-driven state-of-charge estimation methods\nis conducted. The paper compares different neural network-based models and\ncommon regression models for SOC estimation. These models include several\nablated transformer networks, a neural network, a lasso regression model, a\nlinear regression model and a decision tree. Results of various experiments\nconducted on data obtained from natural driving cycles of the BMW i3 battery\nshow that the decision tree outperformed all other models including the more\ncomplex transformer network with self-attention and positional encoding.\n","authors":["Hadeel Aboueidah","Abdulrahman Altahhan"],"pdf_url":"https://arxiv.org/pdf/2410.17049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03372v3","updated":"2024-10-22T14:24:55Z","published":"2023-06-06T03:21:28Z","title":"Online Tensor Learning: Computational and Statistical Trade-offs,\n  Adaptivity and Optimal Regret","summary":"  Large tensor learning algorithms are typically computationally expensive and\nrequire storing a vast amount of data. In this paper, we propose a unified\nonline Riemannian gradient descent (oRGrad) algorithm for tensor learning,\nwhich is computationally efficient, consumes much less memory, and can handle\nsequentially arriving data while making timely predictions. The algorithm is\napplicable to both linear and generalized linear models. If the time horizon T\nis known, oRGrad achieves statistical optimality by choosing an appropriate\nfixed step size. We find that noisy tensor completion particularly benefits\nfrom online algorithms by avoiding the trimming procedure and ensuring sharp\nentry-wise statistical error, which is often technically challenging for\noffline methods. The regret of oRGrad is analyzed, revealing a fascinating\ntrilemma concerning the computational convergence rate, statistical error, and\nregret bound. By selecting an appropriate constant step size, oRGrad achieves\nan $O(T^{1/2})$ regret. We then introduce the adaptive-oRGrad algorithm, which\ncan achieve the optimal $O(\\log T)$ regret by adaptively selecting step sizes,\nregardless of whether the time horizon is known. The adaptive-oRGrad algorithm\ncan attain a statistically optimal error rate without knowing the horizon.\nComprehensive numerical simulations corroborate our theoretical findings. We\nshow that oRGrad significantly outperforms its offline counterpart in\npredicting the solar F10.7 index with tensor predictors that monitor space\nweather impacts.\n","authors":["Jingyang Li","Jian-Feng Cai","Yang Chen","Dong Xia"],"pdf_url":"https://arxiv.org/pdf/2306.03372v3.pdf","comment":"Add initialization algorithms and new application"},{"id":"http://arxiv.org/abs/2405.14677v2","updated":"2024-10-22T14:21:19Z","published":"2024-05-23T15:12:15Z","title":"RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance","summary":"  Customizing diffusion models to generate identity-preserving images from\nuser-provided reference images is an intriguing new problem. The prevalent\napproaches typically require training on extensive domain-specific images to\nachieve identity preservation, which lacks flexibility across different use\ncases. To address this issue, we exploit classifier guidance, a training-free\ntechnique that steers diffusion models using an existing classifier, for\npersonalized image generation. Our study shows that based on a recent rectified\nflow framework, the major limitation of vanilla classifier guidance in\nrequiring a special classifier can be resolved with a simple fixed-point\nsolution, allowing flexible personalization with off-the-shelf image\ndiscriminators. Moreover, its solving procedure proves to be stable when\nanchored to a reference flow trajectory, with a convergence guarantee. The\nderived method is implemented on rectified flow with different off-the-shelf\nimage discriminators, delivering advantageous personalization results for human\nfaces, live subjects, and certain objects. Code is available at\nhttps://github.com/feifeiobama/RectifID.\n","authors":["Zhicheng Sun","Zhenhao Yang","Yang Jin","Haozhe Chi","Kun Xu","Kun Xu","Liwei Chen","Hao Jiang","Yang Song","Kun Gai","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2405.14677v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17043v1","updated":"2024-10-22T14:19:29Z","published":"2024-10-22T14:19:29Z","title":"Optimizing Mixture-of-Experts Inference Time Combining Model Deployment\n  and Communication Scheduling","summary":"  As machine learning models scale in size and complexity, their computational\nrequirements become a significant barrier. Mixture-of-Experts (MoE) models\nalleviate this issue by selectively activating relevant experts. Despite this,\nMoE models are hindered by high communication overhead from all-to-all\noperations, low GPU utilization due to the synchronous communication\nconstraint, and complications from heterogeneous GPU environments.\n  This paper presents Aurora, which optimizes both model deployment and\nall-to-all communication scheduling to address these challenges in MoE\ninference. Aurora achieves minimal communication times by strategically\nordering token transmissions in all-to-all communications. It improves GPU\nutilization by colocating experts from different models on the same device,\navoiding the limitations of synchronous all-to-all communication. We analyze\nAurora's optimization strategies theoretically across four common GPU cluster\nsettings: exclusive vs. colocated models on GPUs, and homogeneous vs.\nheterogeneous GPUs. Aurora provides optimal solutions for three cases, and for\nthe remaining NP-hard scenario, it offers a polynomial-time sub-optimal\nsolution with only a 1.07x degradation from the optimal.\n  Aurora is the first approach to minimize MoE inference time via optimal model\ndeployment and communication scheduling across various scenarios. Evaluations\ndemonstrate that Aurora significantly accelerates inference, achieving speedups\nof up to 2.38x in homogeneous clusters and 3.54x in heterogeneous environments.\nMoreover, Aurora enhances GPU utilization by up to 1.5x compared to existing\nmethods.\n","authors":["Jialong Li","Shreyansh Tripathi","Lakshay Rastogi","Yiming Lei","Rui Pan","Yiting Xia"],"pdf_url":"https://arxiv.org/pdf/2410.17043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17042v1","updated":"2024-10-22T14:16:49Z","published":"2024-10-22T14:16:49Z","title":"Deep Memory Search: A Metaheuristic Approach for Optimizing Heuristic\n  Search","summary":"  Metaheuristic search methods have proven to be essential tools for tackling\ncomplex optimization challenges, but their full potential is often constrained\nby conventional algorithmic frameworks. In this paper, we introduce a novel\napproach called Deep Heuristic Search (DHS), which models metaheuristic search\nas a memory-driven process. DHS employs multiple search layers and memory-based\nexploration-exploitation mechanisms to navigate large, dynamic search spaces.\nBy utilizing model-free memory representations, DHS enhances the ability to\ntraverse temporal trajectories without relying on probabilistic transition\nmodels. The proposed method demonstrates significant improvements in search\nefficiency and performance across a range of heuristic optimization problems.\n","authors":["Abdel-Rahman Hedar","Alaa E. Abdel-Hakim","Wael Deabes","Youseef Alotaibi","Kheir Eddine Bouazza"],"pdf_url":"https://arxiv.org/pdf/2410.17042v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.06997v3","updated":"2024-10-22T14:09:57Z","published":"2024-04-10T13:24:27Z","title":"Agent-driven Generative Semantic Communication with Cross-Modality and\n  Prediction","summary":"  In the era of 6G, with compelling visions of intelligent transportation\nsystems and digital twins, remote surveillance is poised to become a ubiquitous\npractice. Substantial data volume and frequent updates present challenges in\nwireless networks. To address these challenges, we propose a novel agent-driven\ngenerative semantic communication (A-GSC) framework based on reinforcement\nlearning. In contrast to the existing research on semantic communication\n(SemCom), which mainly focuses on either semantic extraction or semantic\nsampling, we seamlessly integrate both by jointly considering the intrinsic\nattributes of source information and the contextual information regarding the\ntask. Notably, the introduction of generative artificial intelligence (GAI)\nenables the independent design of semantic encoders and decoders. In this work,\nwe develop an agent-assisted semantic encoder with cross-modality capability,\nwhich can track the semantic changes, channel condition, to perform adaptive\nsemantic extraction and sampling. Accordingly, we design a semantic decoder\nwith both predictive and generative capabilities, consisting of two tailored\nmodules. Moreover, the effectiveness of the designed models has been verified\nusing the UA-DETRAC dataset, demonstrating the performance gains of the overall\nA-GSC framework in both energy saving and reconstruction accuracy.\n","authors":["Wanting Yang","Zehui Xiong","Yanli Yuan","Wenchao Jiang","Tony Q. S. Quek","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2404.06997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16320v2","updated":"2024-10-22T14:09:10Z","published":"2024-09-21T03:45:05Z","title":"Developing a Thailand solar irradiance map using Himawari-8 satellite\n  imageries and deep learning models","summary":"  This paper presents an online platform that shows Thailand's solar irradiance\nmap every 30 minutes. It is available at https://www.cusolarforecast.com. The\nmethodology for estimating global horizontal irradiance (GHI) across Thailand\nrelies on cloud index extracted from Himawari-8 satellite imagery, Ineichen\nclear-sky model with locally-tuned Linke turbidity, and machine learning\nmodels. The methods take clear-sky irradiance, cloud index, re-analyzed GHI and\ntemperature data from the MERRA-2 database, and date-time as inputs for GHI\nestimation models, including LightGBM, LSTM, Informer, and Transformer. These\nare benchmarked with the estimate from a commercial service X by evaluating\n15-minute ground GHI data from 53 ground stations over 1.5 years from\n2022-2023. The results show that the four models have competitive performances\nand outperform the service X. The best model is LightGBM, with an MAE of 78.58\nW/sqm and RMSE of 118.97 W/sqm. Obtaining re-analyzed MERRA-2 data for Thailand\nis not economically feasible for deployment. When removing these features, the\nInformer model has a winning performance of 78.67 W/sqm in MAE. The obtained\nperformance aligns with existing literature by taking the climate zone and time\ngranularity of data into consideration. As the map shows an estimate of GHI\nover 93,000 grids with a frequent update, the paper also describes a\ncomputational framework for displaying the entire map. It tests the runtime\nperformance of deep learning models in the GHI estimation process.\n","authors":["Suwichaya Suwanwimolkul","Natanon Tongamrak","Nuttamon Thungka","Naebboon Hoonchareon","Jitkomut Songsiri"],"pdf_url":"https://arxiv.org/pdf/2409.16320v2.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.17028v1","updated":"2024-10-22T13:52:51Z","published":"2024-10-22T13:52:51Z","title":"Can a Machine Distinguish High and Low Amount of Social Creak in Speech?","summary":"  Objectives: ncreased prevalence of social creak particularly among female\nspeakers has been reported in several studies. The study of social creak has\nbeen previously conducted by combining perceptual evaluation of speech with\nconventional acoustical parameters such as the harmonic-to-noise ratio and\ncepstral peak prominence. In the current study, machine learning (ML) was used\nto automatically distinguish speech of low amount of social creak from speech\nof high amount of social creak.\n  Methods: The amount of creak in continuous speech samples produced in Finnish\nby 90 female speakers was first perceptually assessed by two voice specialists.\nBased on their assessments, the speech samples were divided into two categories\n(low $vs$. high amount of creak). Using the speech signals and their creak\nlabels, seven different ML models were trained. Three spectral representations\nwere used as feature for each model.\n  Results: The results show that the best performance (accuracy of 71.1\\%) was\nobtained by the following two systems: an Adaboost classifier using the\nmel-spectrogram feature and a decision tree classifier using the mel-frequency\ncepstral coefficient feature.\n  Conclusions: The study of social creak is becoming increasingly popular in\nsociolinguistic and vocological research. The conventional human perceptual\nassessment of the amount of creak is laborious and therefore ML technology\ncould be used to assist researchers studying social creak. The classification\nsystems reported in this study could be considered as baselines in future\nML-based studies on social creak.\n","authors":["Anne-Maria Laukkanen","Sudarsana Reddy Kadiri","Shrikanth Narayanan","Paavo Alku"],"pdf_url":"https://arxiv.org/pdf/2410.17028v1.pdf","comment":"Accepted in Journal of Voice"},{"id":"http://arxiv.org/abs/2410.17020v1","updated":"2024-10-22T13:44:10Z","published":"2024-10-22T13:44:10Z","title":"LFME: A Simple Framework for Learning from Multiple Experts in Domain\n  Generalization","summary":"  Domain generalization (DG) methods aim to maintain good performance in an\nunseen target domain by using training data from multiple source domains. While\nsuccess on certain occasions are observed, enhancing the baseline across most\nscenarios remains challenging. This work introduces a simple yet effective\nframework, dubbed learning from multiple experts (LFME), that aims to make the\ntarget model an expert in all source domains to improve DG. Specifically,\nbesides learning the target model used in inference, LFME will also train\nmultiple experts specialized in different domains, whose output probabilities\nprovide professional guidance by simply regularizing the logit of the target\nmodel. Delving deep into the framework, we reveal that the introduced logit\nregularization term implicitly provides effects of enabling the target model to\nharness more information, and mining hard samples from the experts during\ntraining. Extensive experiments on benchmarks from different DG tasks\ndemonstrate that LFME is consistently beneficial to the baseline and can\nachieve comparable performance to existing arts. Code is available\nat~\\url{https://github.com/liangchen527/LFME}.\n","authors":["Liang Chen","Yong Zhang","Yibing Song","Zhiqiang Shen","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17020v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.02362v3","updated":"2024-10-22T13:43:01Z","published":"2024-06-04T14:39:51Z","title":"Temporal Graph Rewiring with Expander Graphs","summary":"  Evolving relations in real-world networks are often modelled by temporal\ngraphs. Temporal Graph Neural Networks (TGNNs) emerged to model evolutionary\nbehaviour of such graphs by leveraging the message passing primitive at the\ncore of Graph Neural Networks (GNNs). It is well-known that GNNs are vulnerable\nto several issues directly related to the input graph topology, such as\nunder-reaching and over-squashing - we argue that these issues can often get\nexacerbated in temporal graphs, particularly as the result of stale nodes and\nedges. While graph rewiring techniques have seen frequent usage in GNNs to make\nthe graph topology more favourable for message passing, they have not seen any\nmainstream usage on TGNNs. In this work, we propose Temporal Graph Rewiring\n(TGR), the first approach for graph rewiring on temporal graphs, to the best of\nour knowledge. TGR constructs message passing highways between temporally\ndistant nodes in a continuous-time dynamic graph by utilizing expander graph\npropagation, a prominent framework used for graph rewiring on static graphs\nwhich makes minimal assumptions on the underlying graph structure. On the\nchallenging TGB benchmark, TGR achieves state-of-the-art results on\ntgbl-review, tgbl-coin, tgbl-comment and tgbl-flight datasets at the time of\nwriting. For tgbl-review, TGR has 50.5% improvement in MRR over the base TGN\nmodel and 22.2% improvement over the base TNCN model. The significant\nimprovement over base models demonstrates clear benefits of temporal graph\nrewiring.\n","authors":["Katarina Petrović","Shenyang Huang","Farimah Poursafaei","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2406.02362v3.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.18460v3","updated":"2024-10-22T13:37:04Z","published":"2023-11-30T11:11:26Z","title":"Causal Fairness under Unobserved Confounding: A Neural Sensitivity\n  Framework","summary":"  Fairness for machine learning predictions is widely required in practice for\nlegal, ethical, and societal reasons. Existing work typically focuses on\nsettings without unobserved confounding, even though unobserved confounding can\nlead to severe violations of causal fairness and, thus, unfair predictions. In\nthis work, we analyze the sensitivity of causal fairness to unobserved\nconfounding. Our contributions are three-fold. First, we derive bounds for\ncausal fairness metrics under different sources of unobserved confounding. This\nenables practitioners to examine the sensitivity of their machine learning\nmodels to unobserved confounding in fairness-critical applications. Second, we\npropose a novel neural framework for learning fair predictions, which allows us\nto offer worst-case guarantees of the extent to which causal fairness can be\nviolated due to unobserved confounding. Third, we demonstrate the effectiveness\nof our framework in a series of experiments, including a real-world case study\nabout predicting prison sentences. To the best of our knowledge, ours is the\nfirst work to study causal fairness under unobserved confounding. To this end,\nour work is of direct practical value as a refutation strategy to ensure the\nfairness of predictions in high-stakes applications.\n","authors":["Maresa Schröder","Dennis Frauen","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2311.18460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10060v2","updated":"2024-10-22T13:35:07Z","published":"2024-06-14T14:16:39Z","title":"PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner","summary":"  In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.\n","authors":["Kota Kondo","Claudius T. Tewari","Andrea Tagliabue","Jesus Tordesillas","Parker C. Lusk","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2406.10060v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.15551v2","updated":"2024-10-22T13:32:59Z","published":"2024-05-24T13:37:48Z","title":"Thinking Forward: Memory-Efficient Federated Finetuning of Language\n  Models","summary":"  Finetuning large language models (LLMs) in federated learning (FL) settings\nhas become increasingly important as it allows resource-constrained devices to\nfinetune a model using private data. However, finetuning LLMs using\nbackpropagation requires excessive memory (especially from intermediate\nactivations) for resource-constrained devices. While Forward-mode\nAuto-Differentiation (AD) can significantly reduce memory footprint from\nactivations, we observe that directly applying it to LLM finetuning results in\nslow convergence and poor accuracy. In this paper, we introduce Spry, an FL\nalgorithm that splits trainable weights of an LLM among participating clients,\nsuch that each client computes gradients using forward-mode AD that are closer\nestimations of the true gradients. Spry achieves a low memory footprint, high\naccuracy, and fast convergence. We formally prove that the global gradients in\nSpry are unbiased estimators of true global gradients for homogeneous data\ndistributions across clients, while heterogeneity increases bias of the\nestimates. We also derive Spry's convergence rate, showing that the gradients\ndecrease inversely proportional to the number of FL rounds, indicating the\nconvergence up to the limits of heterogeneity. Empirically, Spry reduces the\nmemory footprint during training by 1.4-7.1x in contrast to backpropagation,\nwhile reaching comparable accuracy, across a wide range of language tasks,\nmodels, and FL settings. Spry reduces the convergence time by 1.2-20.3x and\nachieves 5.2-13.5% higher accuracy against zero-order methods. When finetuning\nLlama2-7B with LoRA, compared to the peak memory consumption of 33.9GB of\nbackpropagation, Spry only consumes 6.2GB of peak memory. For OPT13B, the\nreduction is from 76.5GB to 10.8GB. Spry makes feasible previously impossible\nFL deployments on commodity edge devices. Our source code is available at\nhttps://github.com/Astuary/Spry.\n","authors":["Kunjal Panchal","Nisarg Parikh","Sunav Choudhary","Lijun Zhang","Yuriy Brun","Hui Guan"],"pdf_url":"https://arxiv.org/pdf/2405.15551v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.12142v2","updated":"2024-10-22T13:32:34Z","published":"2024-06-17T23:08:46Z","title":"Slicing Through Bias: Explaining Performance Gaps in Medical Image\n  Analysis using Slice Discovery Methods","summary":"  Machine learning models have achieved high overall accuracy in medical image\nanalysis. However, performance disparities on specific patient groups pose\nchallenges to their clinical utility, safety, and fairness. This can affect\nknown patient groups - such as those based on sex, age, or disease subtype - as\nwell as previously unknown and unlabeled groups. Furthermore, the root cause of\nsuch observed performance disparities is often challenging to uncover,\nhindering mitigation efforts. In this paper, to address these issues, we\nleverage Slice Discovery Methods (SDMs) to identify interpretable\nunderperforming subsets of data and formulate hypotheses regarding the cause of\nobserved performance disparities. We introduce a novel SDM and apply it in a\ncase study on the classification of pneumothorax and atelectasis from chest\nx-rays. Our study demonstrates the effectiveness of SDMs in hypothesis\nformulation and yields an explanation of previously observed but unexplained\nperformance disparities between male and female patients in widely used chest\nX-ray datasets and models. Our findings indicate shortcut learning in both\nclassification tasks, through the presence of chest drains and ECG wires,\nrespectively. Sex-based differences in the prevalence of these shortcut\nfeatures appear to cause the observed classification performance gap,\nrepresenting a previously underappreciated interaction between shortcut\nlearning and model fairness analyses.\n","authors":["Vincent Olesen","Nina Weng","Aasa Feragen","Eike Petersen"],"pdf_url":"https://arxiv.org/pdf/2406.12142v2.pdf","comment":"MICCAI 2024 Workshop on Fairness of AI in Medical Imaging"},{"id":"http://arxiv.org/abs/2302.05765v3","updated":"2024-10-22T13:31:16Z","published":"2023-02-11T19:30:55Z","title":"Adversarial Online Collaborative Filtering","summary":"  We investigate the problem of online collaborative filtering under\nno-repetition constraints, whereby users need to be served content in an online\nfashion and a given user cannot be recommended the same content item more than\nonce. We start by designing and analyzing an algorithm that works under\nbiclustering assumptions on the user-item preference matrix, and show that this\nalgorithm exhibits an optimal regret guarantee, while being fully adaptive, in\nthat it is oblivious to any prior knowledge about the sequence of users, the\nuniverse of items, as well as the biclustering parameters of the preference\nmatrix. We then propose a more robust version of this algorithm which operates\nwith general matrices. Also this algorithm is parameter free, and we prove\nregret guarantees that scale with the amount by which the preference matrix\ndeviates from a biclustered structure. To our knowledge, these are the first\nresults on online collaborative filtering that hold at this level of generality\nand adaptivity under no-repetition constraints. Finally, we complement our\ntheoretical findings with simple experiments on real-world datasets aimed at\nboth validating the theory and empirically comparing to standard baselines.\nThis comparison shows the competitive advantage of our approach over these\nbaselines.\n","authors":["Stephen Pasteris","Fabio Vitale","Mark Herbster","Claudio Gentile","Andre' Panisson"],"pdf_url":"https://arxiv.org/pdf/2302.05765v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17704v2","updated":"2024-10-22T13:19:36Z","published":"2024-02-27T17:30:33Z","title":"Transfer Learning Bayesian Optimization to Design Competitor DNA\n  Molecules for Use in Diagnostic Assays","summary":"  With the rise in engineered biomolecular devices, there is an increased need\nfor tailor-made biological sequences. Often, many similar biological sequences\nneed to be made for a specific application meaning numerous, sometimes\nprohibitively expensive, lab experiments are necessary for their optimization.\nThis paper presents a transfer learning design of experiments workflow to make\nthis development feasible. By combining a transfer learning surrogate model\nwith Bayesian optimization, we show how the total number of experiments can be\nreduced by sharing information between optimization tasks. We demonstrate the\nreduction in the number of experiments using data from the development of DNA\ncompetitors for use in an amplification-based diagnostic assay. We use\ncross-validation to compare the predictive accuracy of different transfer\nlearning models, and then compare the performance of the models for both single\nobjective and penalized optimization tasks.\n","authors":["Ruby Sedgwick","John P. Goertz","Molly M. Stevens","Ruth Misener","Mark van der Wilk"],"pdf_url":"https://arxiv.org/pdf/2402.17704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10851v2","updated":"2024-10-22T13:08:02Z","published":"2024-10-06T12:53:07Z","title":"LLM Gesticulator: Leveraging Large Language Models for Scalable and\n  Controllable Co-Speech Gesture Synthesis","summary":"  In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.\n","authors":["Haozhou Pang","Tianwei Ding","Lanshan He","Ming Tao","Lu Zhang","Qi Gan"],"pdf_url":"https://arxiv.org/pdf/2410.10851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16982v1","updated":"2024-10-22T13:02:12Z","published":"2024-10-22T13:02:12Z","title":"Sample-Efficient Geometry Reconstruction from Euclidean Distances using\n  Non-Convex Optimization","summary":"  The problem of finding suitable point embedding or geometric configurations\ngiven only Euclidean distance information of point pairs arises both as a core\ntask and as a sub-problem in a variety of machine learning applications. In\nthis paper, we aim to solve this problem given a minimal number of distance\nsamples. To this end, we leverage continuous and non-convex rank minimization\nformulations of the problem and establish a local convergence guarantee for a\nvariant of iteratively reweighted least squares (IRLS), which applies if a\nminimal random set of observed distances is provided. As a technical tool, we\nestablish a restricted isometry property (RIP) restricted to a tangent space of\nthe manifold of symmetric rank-$r$ matrices given random Euclidean distance\nmeasurements, which might be of independent interest for the analysis of other\nnon-convex approaches. Furthermore, we assess data efficiency, scalability and\ngeneralizability of different reconstruction algorithms through numerical\nexperiments with simulated data as well as real-world data, demonstrating the\nproposed algorithm's ability to identify the underlying geometry from fewer\ndistance samples compared to the state-of-the-art.\n","authors":["Ipsita Ghosh","Abiy Tasissa","Christian Kümmerle"],"pdf_url":"https://arxiv.org/pdf/2410.16982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16975v1","updated":"2024-10-22T12:55:02Z","published":"2024-10-22T12:55:02Z","title":"Publishing Neural Networks in Drug Discovery Might Compromise Training\n  Data Privacy","summary":"  This study investigates the risks of exposing confidential chemical\nstructures when machine learning models trained on these structures are made\npublicly available. We use membership inference attacks, a common method to\nassess privacy that is largely unexplored in the context of drug discovery, to\nexamine neural networks for molecular property prediction in a black-box\nsetting. Our results reveal significant privacy risks across all evaluated\ndatasets and neural network architectures. Combining multiple attacks increases\nthese risks. Molecules from minority classes, often the most valuable in drug\ndiscovery, are particularly vulnerable. We also found that representing\nmolecules as graphs and using message-passing neural networks may mitigate\nthese risks. We provide a framework to assess privacy risks of classification\nmodels and molecular representations. Our findings highlight the need for\ncareful consideration when sharing neural networks trained on proprietary\nchemical structures, informing organisations and researchers about the\ntrade-offs between data confidentiality and model openness.\n","authors":["Fabian P. Krüger","Johan Östman","Lewis Mervin","Igor V. Tetko","Ola Engkvist"],"pdf_url":"https://arxiv.org/pdf/2410.16975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16973v1","updated":"2024-10-22T12:51:51Z","published":"2024-10-22T12:51:51Z","title":"Learning Mathematical Rules with Large Language Models","summary":"  In this paper, we study the ability of large language models to learn\nspecific mathematical rules such as distributivity or simplifying equations. We\npresent an empirical analysis of their ability to generalize these rules, as\nwell as to reuse them in the context of word problems. For this purpose, we\nprovide a rigorous methodology to build synthetic data incorporating such\nrules, and perform fine-tuning of large language models on such data. Our\nexperiments show that our model can learn and generalize these rules to some\nextent, as well as suitably reuse them in the context of word problems.\n","authors":["Antoine Gorceix","Bastien Le Chenadec","Ahmad Rammal","Nelson Vadori","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2410.16973v1.pdf","comment":"4th MATH-AI Workshop at NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.16972v1","updated":"2024-10-22T12:51:46Z","published":"2024-10-22T12:51:46Z","title":"Sample-efficient Bayesian Optimisation Using Known Invariances","summary":"  Bayesian optimisation (BO) is a powerful framework for global optimisation of\ncostly functions, using predictions from Gaussian process models (GPs). In this\nwork, we apply BO to functions that exhibit invariance to a known group of\ntransformations. We show that vanilla and constrained BO algorithms are\ninefficient when optimising such invariant objectives, and provide a method for\nincorporating group invariances into the kernel of the GP to produce\ninvariance-aware algorithms that achieve significant improvements in sample\nefficiency. We derive a bound on the maximum information gain of these\ninvariant kernels, and provide novel upper and lower bounds on the number of\nobservations required for invariance-aware BO algorithms to achieve\n$\\epsilon$-optimality. We demonstrate our method's improved performance on a\nrange of synthetic invariant and quasi-invariant functions. We also apply our\nmethod in the case where only some of the invariance is incorporated into the\nkernel, and find that these kernels achieve similar gains in sample efficiency\nat significantly reduced computational cost. Finally, we use invariant BO to\ndesign a current drive system for a nuclear fusion reactor, finding a\nhigh-performance solution where non-invariant methods failed.\n","authors":["Theodore Brown","Alexandru Cioba","Ilija Bogunovic"],"pdf_url":"https://arxiv.org/pdf/2410.16972v1.pdf","comment":"Accepted as a poster at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2202.08967v2","updated":"2024-10-22T12:46:44Z","published":"2022-02-12T21:39:29Z","title":"Beyond Trading Data: The Hidden Influence of Public Awareness and\n  Interest on Cryptocurrency Volatility","summary":"  Since Bitcoin first appeared on the scene in 2009, cryptocurrencies have\nbecome a worldwide phenomenon as important decentralized financial assets.\nTheir decentralized nature, however, leads to notable volatility against\ntraditional fiat currencies, making the task of accurately forecasting the\ncrypto-fiat exchange rate complex. This study examines the various independent\nfactors that affect the volatility of the Bitcoin-Dollar exchange rate. To this\nend, we propose CoMForE, a multimodal AdaBoost-LSTM ensemble model, which not\nonly utilizes historical trading data but also incorporates public sentiments\nfrom related tweets, public interest demonstrated by search volumes, and\nblockchain hash-rate data. Our developed model goes a step further by\npredicting fluctuations in the overall cryptocurrency value distribution, thus\nincreasing its value for investment decision-making. We have subjected this\nmethod to extensive testing via comprehensive experiments, thereby validating\nthe importance of multimodal combination over exclusive reliance on trading\ndata. Further experiments show that our method significantly surpasses existing\nforecasting tools and methodologies, demonstrating a 19.29% improvement. This\nresult underscores the influence of external independent factors on\ncryptocurrency volatility.\n","authors":["Zeyd Boukhers","Azeddine Bouabdallah","Cong Yang","Jan Jürjens"],"pdf_url":"https://arxiv.org/pdf/2202.08967v2.pdf","comment":"Published at the 32nd ACM International Conference on Information and\n  Knowledge Management (CIKM 2023)"},{"id":"http://arxiv.org/abs/2402.02017v2","updated":"2024-10-22T12:46:09Z","published":"2024-02-03T04:17:09Z","title":"Adaptive $Q$-Aid for Conditional Supervised Learning in Offline\n  Reinforcement Learning","summary":"  Offline reinforcement learning (RL) has progressed with return-conditioned\nsupervised learning (RCSL), but its lack of stitching ability remains a\nlimitation. We introduce $Q$-Aided Conditional Supervised Learning (QCS), which\neffectively combines the stability of RCSL with the stitching capability of\n$Q$-functions. By analyzing $Q$-function over-generalization, which impairs\nstable stitching, QCS adaptively integrates $Q$-aid into RCSL's loss function\nbased on trajectory return. Empirical results show that QCS significantly\noutperforms RCSL and value-based methods, consistently achieving or exceeding\nthe maximum trajectory returns across diverse offline RL benchmarks.\n","authors":["Jeonghye Kim","Suyoung Lee","Woojun Kim","Youngchul Sung"],"pdf_url":"https://arxiv.org/pdf/2402.02017v2.pdf","comment":"Accepted to NeurIPS2024. The project page is available at\n  https://beanie00.com/publications/qcs"},{"id":"http://arxiv.org/abs/2209.12835v4","updated":"2024-10-22T12:38:35Z","published":"2022-09-26T16:41:16Z","title":"Targeted Separation and Convergence with Kernel Discrepancies","summary":"  Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD)\nhave grown central to a wide range of applications, including hypothesis\ntesting, sampler selection, distribution approximation, and variational\ninference. In each setting, these kernel-based discrepancy measures are\nrequired to (i) separate a target P from other probability measures or even\n(ii) control weak convergence to P. In this article we derive new sufficient\nand necessary conditions to ensure (i) and (ii). For MMDs on separable metric\nspaces, we characterize those kernels that separate Bochner embeddable measures\nand introduce simple conditions for separating all measures with unbounded\nkernels and for controlling convergence with bounded kernels. We use these\nresults on $\\mathbb{R}^d$ to substantially broaden the known conditions for KSD\nseparation and convergence control and to develop the first KSDs known to\nexactly metrize weak convergence to P. Along the way, we highlight the\nimplications of our results for hypothesis testing, measuring and improving\nsample quality, and sampling with Stein variational gradient descent.\n","authors":["Alessandro Barp","Carl-Johann Simon-Gabriel","Mark Girolami","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2209.12835v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07063v4","updated":"2024-10-22T12:28:13Z","published":"2023-04-14T11:35:35Z","title":"Rethinking Complex Queries on Knowledge Graphs with Neural Link\n  Predictors","summary":"  Reasoning on knowledge graphs is a challenging task because it utilizes\nobserved information to predict the missing one. Particularly, answering\ncomplex queries based on first-order logic is one of the crucial tasks to\nverify learning to reason abilities for generalization and composition.\nRecently, the prevailing method is query embedding which learns the embedding\nof a set of entities and treats logic operations as set operations and has\nshown great empirical success. Though there has been much research following\nthe same formulation, many of its claims lack a formal and systematic\ninspection. In this paper, we rethink this formulation and justify many of the\nprevious claims by characterizing the scope of queries investigated previously\nand precisely identifying the gap between its formulation and its goal, as well\nas providing complexity analysis for the currently investigated queries.\nMoreover, we develop a new dataset containing ten new types of queries with\nfeatures that have never been considered and therefore can provide a thorough\ninvestigation of complex queries. Finally, we propose a new neural-symbolic\nmethod, Fuzzy Inference with Truth value (FIT), where we equip the neural link\npredictors with fuzzy logic theory to support end-to-end learning using complex\nqueries with provable reasoning capability. Empirical results show that our\nmethod outperforms previous methods significantly in the new dataset and also\nsurpasses previous methods in the existing dataset at the same time.\n","authors":["Hang Yin","Zihao Wang","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2304.07063v4.pdf","comment":"Received in ICLR 2024"},{"id":"http://arxiv.org/abs/2410.16947v1","updated":"2024-10-22T12:21:39Z","published":"2024-10-22T12:21:39Z","title":"ISImed: A Framework for Self-Supervised Learning using Intrinsic Spatial\n  Information in Medical Images","summary":"  This paper demonstrates that spatial information can be used to learn\ninterpretable representations in medical images using Self-Supervised Learning\n(SSL). Our proposed method, ISImed, is based on the observation that medical\nimages exhibit a much lower variability among different images compared to\nclassic data vision benchmarks. By leveraging this resemblance of human body\nstructures across multiple images, we establish a self-supervised objective\nthat creates a latent representation capable of capturing its location in the\nphysical realm. More specifically, our method involves sampling image crops and\ncreating a distance matrix that compares the learned representation vectors of\nall possible combinations of these crops to the true distance between them. The\nintuition is, that the learned latent space is a positional encoding for a\ngiven image crop. We hypothesize, that by learning these positional encodings,\ncomprehensive image representations have to be generated. To test this\nhypothesis and evaluate our method, we compare our learned representation with\ntwo state-of-the-art SSL benchmarking methods on two publicly available medical\nimaging datasets. We show that our method can efficiently learn representations\nthat capture the underlying structure of the data and can be used to transfer\nto a downstream classification task.\n","authors":["Nabil Jabareen","Dongsheng Yuan","Sören Lukassen"],"pdf_url":"https://arxiv.org/pdf/2410.16947v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.16941v1","updated":"2024-10-22T12:16:11Z","published":"2024-10-22T12:16:11Z","title":"Business Process Simulation: Probabilistic Modeling of Intermittent\n  Resource Availability and Multitasking Behavior","summary":"  In business process simulation, resource availability is typically modeled by\nassigning a calendar to each resource, e.g., Monday-Friday, 9:00-18:00.\nResources are assumed to be always available during each time slot in their\navailability calendar. This assumption often becomes invalid due to\ninterruptions, breaks, or time-sharing across processes. In other words,\nexisting approaches fail to capture intermittent availability. Another\nlimitation of existing approaches is that they either do not consider\nmultitasking behavior, or if they do, they assume that resources always\nmultitask (up to a maximum capacity) whenever available. However, studies have\nshown that the multitasking patterns vary across days. This paper introduces a\nprobabilistic approach to model resource availability and multitasking behavior\nfor business process simulation. In this approach, each time slot in a resource\ncalendar has an associated availability probability and a multitasking\nprobability per multitasking level. For example, a resource may be available on\nFridays between 14:00-15:00 with 90\\% probability, and given that they are\nperforming one task during this slot, they may take on a second concurrent task\nwith 60\\% probability. We propose algorithms to discover probabilistic\ncalendars and probabilistic multitasking capacities from event logs. An\nevaluation shows that, with these enhancements, simulation models discovered\nfrom event logs better replicate the distribution of activities and cycle\ntimes, relative to approaches with crisp calendars and monotasking assumptions.\n","authors":["Orlenys López-Pintado","Marlon Dumas"],"pdf_url":"https://arxiv.org/pdf/2410.16941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16935v1","updated":"2024-10-22T12:12:43Z","published":"2024-10-22T12:12:43Z","title":"Graph Neural Networks for Edge Signals: Orientation Equivariance and\n  Invariance","summary":"  Many applications in traffic, civil engineering, or electrical engineering\nrevolve around edge-level signals. Such signals can be categorized as\ninherently directed, for example, the water flow in a pipe network, and\nundirected, like the diameter of a pipe. Topological methods model edge signals\nwith inherent direction by representing them relative to a so-called\norientation assigned to each edge. These approaches can neither model\nundirected edge signals nor distinguish if an edge itself is directed or\nundirected. We address these shortcomings by (i) revising the notion of\norientation equivariance to enable edge direction-aware topological models,\n(ii) proposing orientation invariance as an additional requirement to describe\nsignals without inherent direction, and (iii) developing EIGN, an architecture\ncomposed of novel direction-aware edge-level graph shift operators, that\nprovably fulfills the aforementioned desiderata. It is the first\ngeneral-purpose topological GNN for edge-level signals that can model directed\nand undirected signals while distinguishing between directed and undirected\nedges. A comprehensive evaluation shows that EIGN outperforms prior work in\nedge-level tasks, for example, improving in RMSE on flow simulation tasks by up\nto 43.5%.\n","authors":["Dominik Fuchsgruber","Tim Poštuvan","Stephan Günnemann","Simon Geisler"],"pdf_url":"https://arxiv.org/pdf/2410.16935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04157v2","updated":"2024-10-22T12:04:49Z","published":"2024-07-04T21:23:12Z","title":"Finite Operator Learning: Bridging Neural Operators and Numerical\n  Methods for Efficient Parametric Solution and Optimization of PDEs","summary":"  We introduce a method that combines neural operators, physics-informed\nmachine learning, and standard numerical methods for solving PDEs. The proposed\napproach extends each of the aforementioned methods and unifies them within a\nsingle framework. We can parametrically solve partial differential equations in\na data-free manner and provide accurate sensitivities, meaning the derivatives\nof the solution space with respect to the design space. These capabilities\nenable gradient-based optimization without the typical sensitivity analysis\ncosts, unlike adjoint methods that scale directly with the number of response\nfunctions. Our Finite Operator Learning (FOL) approach uses an uncomplicated\nfeed-forward neural network model to directly map the discrete design space\n(i.e. parametric input space) to the discrete solution space (i.e. finite\nnumber of sensor points in the arbitrary shape domain) ensuring compliance with\nphysical laws by designing them into loss functions. The discretized governing\nequations, as well as the design and solution spaces, can be derived from any\nwell-established numerical techniques. In this work, we employ the Finite\nElement Method (FEM) to approximate fields and their spatial derivatives.\nSubsequently, we conduct Sobolev training to minimize a multi-objective loss\nfunction, which includes the discretized weak form of the energy functional,\nboundary conditions violations, and the stationarity of the residuals with\nrespect to the design variables. Our study focuses on the steady-state heat\nequation within heterogeneous materials that exhibits significant phase\ncontrast and possibly temperature-dependent conductivity. The network's tangent\nmatrix is directly used for gradient-based optimization to improve the\nmicrostructure's heat transfer characteristics. ...\n","authors":["Shahed Rezaei","Reza Najian Asl","Kianoosh Taghikhani","Ahmad Moeineddin","Michael Kaliske","Markus Apel"],"pdf_url":"https://arxiv.org/pdf/2407.04157v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2401.02363"},{"id":"http://arxiv.org/abs/2405.15330v2","updated":"2024-10-22T12:01:45Z","published":"2024-05-24T08:12:41Z","title":"Towards Understanding the Working Mechanism of Text-to-Image Diffusion\n  Model","summary":"  Recently, the strong latent Diffusion Probabilistic Model (DPM) has been\napplied to high-quality Text-to-Image (T2I) generation (e.g., Stable\nDiffusion), by injecting the encoded target text prompt into the gradually\ndenoised diffusion image generator. Despite the success of DPM in practice, the\nmechanism behind it remains to be explored. To fill this blank, we begin by\nexamining the intermediate statuses during the gradual denoising generation\nprocess in DPM. The empirical observations indicate, the shape of image is\nreconstructed after the first few denoising steps, and then the image is filled\nwith details (e.g., texture). The phenomenon is because the low-frequency\nsignal (shape relevant) of the noisy image is not corrupted until the final\nstage in the forward process (initial stage of generation) of adding noise in\nDPM. Inspired by the observations, we proceed to explore the influence of each\ntoken in the text prompt during the two stages. After a series of experiments\nof T2I generations conditioned on a set of text prompts. We conclude that in\nthe earlier generation stage, the image is mostly decided by the special token\n[\\texttt{EOS}] in the text prompt, and the information in the text prompt is\nalready conveyed in this stage. After that, the diffusion model completes the\ndetails of generated images by information from themselves. Finally, we propose\nto apply this observation to accelerate the process of T2I generation by\nproperly removing text guidance, which finally accelerates the sampling up to\n25\\%+.\n","authors":["Mingyang Yi","Aoxue Li","Yi Xin","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2405.15330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16928v1","updated":"2024-10-22T11:59:36Z","published":"2024-10-22T11:59:36Z","title":"xLSTM-Mixer: Multivariate Time Series Forecasting by Mixing via Scalar\n  Memories","summary":"  Time series data is prevalent across numerous fields, necessitating the\ndevelopment of robust and accurate forecasting models. Capturing patterns both\nwithin and between temporal and multivariate components is crucial for reliable\npredictions. We introduce xLSTM-Mixer, a model designed to effectively\nintegrate temporal sequences, joint time-variate information, and multiple\nperspectives for robust forecasting. Our approach begins with a linear forecast\nshared across variates, which is then refined by xLSTM blocks. These blocks\nserve as key elements for modeling the complex dynamics of challenging time\nseries data. xLSTM-Mixer ultimately reconciles two distinct views to produce\nthe final forecast. Our extensive evaluations demonstrate xLSTM-Mixer's\nsuperior long-term forecasting performance compared to recent state-of-the-art\nmethods. A thorough model analysis provides further insights into its key\ncomponents and confirms its robustness and effectiveness. This work contributes\nto the resurgence of recurrent models in time series forecasting.\n","authors":["Maurice Kraus","Felix Divo","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2410.16928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16926v1","updated":"2024-10-22T11:57:32Z","published":"2024-10-22T11:57:32Z","title":"Pyramid Vector Quantization for LLMs","summary":"  Recent works on compression of large language models (LLM) using quantization\nconsidered reparameterizing the architecture such that weights are distributed\non the sphere. This demonstratively improves the ability to quantize by\nincreasing the mathematical notion of coherence, resulting in fewer weight\noutliers without affecting the network output. In this work, we aim to further\nexploit this spherical geometry of the weights when performing quantization by\nconsidering Pyramid Vector Quantization (PVQ) for large language models.\nArranging points evenly on the sphere is notoriously difficult, especially in\nhigh dimensions, and in case approximate solutions exists, representing points\nexplicitly in a codebook is typically not feasible due to its additional memory\ncost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting\npoints onto the 1-sphere, which allows for efficient encoding and decoding\nwithout requiring an explicit codebook in memory. To obtain a practical\nalgorithm, we propose to combine PVQ with scale quantization for which we\nderive theoretically optimal quantizations, under empirically verified\nassumptions. Further, we extend pyramid vector quantization to use Hessian\ninformation to minimize quantization error under expected feature activations,\ninstead of only relying on weight magnitudes. Experimentally, we achieves\nstate-of-the-art quantization performance with pareto-optimal trade-off between\nperformance and bits per weight and bits per activation, compared to compared\nmethods. On weight-only, we find that we can quantize a Llama-3 70B model to\n3.25 bits per weight and retain 98\\% accuracy on downstream tasks.\n","authors":["Tycho F. A. van der Ouderaa","Maximilian L. Croci","Agrin Hilmkil","James Hensman"],"pdf_url":"https://arxiv.org/pdf/2410.16926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16747v2","updated":"2024-10-22T11:53:58Z","published":"2024-05-27T01:31:40Z","title":"Understanding Linear Probing then Fine-tuning Language Models from NTK\n  Perspective","summary":"  The two-stage fine-tuning (FT) method, linear probing (LP) then fine-tuning\n(LP-FT), outperforms linear probing and FT alone. This holds true for both\nin-distribution (ID) and out-of-distribution (OOD) data. One key reason for its\nsuccess is the preservation of pre-trained features, achieved by obtaining a\nnear-optimal linear head during LP. However, despite the widespread use of\nlarge language models, there has been limited exploration of more complex\narchitectures such as Transformers. In this paper, we analyze the training\ndynamics of LP-FT for classification tasks on the basis of the neural tangent\nkernel (NTK) theory. Our analysis decomposes the NTK matrix into two\ncomponents. This decomposition highlights the importance of the linear head\nnorm alongside the prediction accuracy at the start of the FT stage. We also\nobserve a significant increase in the linear head norm during LP, which stems\nfrom training with the cross-entropy (CE) loss. This increase in the linear\nhead norm effectively reduces changes in learned features. Furthermore, we find\nthat this increased norm can adversely affect model calibration, which can be\ncorrected using temperature scaling. Additionally, we extend our analysis with\nthe NTK to the low-rank adaptation (LoRA) method and validate its\neffectiveness. Our experiments using a Transformer-based model on multiple\nnatural language processing datasets confirm our theoretical analysis. Our\nstudy demonstrates the effectiveness of LP-FT for fine-tuning language models.\nCode is available at https://github.com/tom4649/lp-ft_ntk.\n","authors":["Akiyoshi Tomihari","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2405.16747v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16919v1","updated":"2024-10-22T11:52:22Z","published":"2024-10-22T11:52:22Z","title":"EnvBridge: Bridging Diverse Environments with Cross-Environment\n  Knowledge Transfer for Embodied AI","summary":"  In recent years, Large Language Models (LLMs) have demonstrated high\nreasoning capabilities, drawing attention for their applications as agents in\nvarious decision-making processes. One notably promising application of LLM\nagents is robotic manipulation. Recent research has shown that LLMs can\ngenerate text planning or control code for robots, providing substantial\nflexibility and interaction capabilities. However, these methods still face\nchallenges in terms of flexibility and applicability across different\nenvironments, limiting their ability to adapt autonomously. Current approaches\ntypically fall into two categories: those relying on environment-specific\npolicy training, which restricts their transferability, and those generating\ncode actions based on fixed prompts, which leads to diminished performance when\nconfronted with new environments. These limitations significantly constrain the\ngeneralizability of agents in robotic manipulation. To address these\nlimitations, we propose a novel method called EnvBridge. This approach involves\nthe retention and transfer of successful robot control codes from source\nenvironments to target environments. EnvBridge enhances the agent's\nadaptability and performance across diverse settings by leveraging insights\nfrom multiple environments. Notably, our approach alleviates environmental\nconstraints, offering a more flexible and generalizable solution for robotic\nmanipulation tasks. We validated the effectiveness of our method using robotic\nmanipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments\ndemonstrate that LLM agents can successfully leverage diverse knowledge sources\nto solve complex tasks. Consequently, our approach significantly enhances the\nadaptability and robustness of robotic manipulation agents in planning across\ndiverse environments.\n","authors":["Tomoyuki Kagaya","Yuxuan Lou","Thong Jing Yuan","Subramanian Lakshmi","Jayashree Karlekar","Sugiri Pranata","Natsuki Murakami","Akira Kinose","Koki Oguri","Felix Wick","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.16919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16917v1","updated":"2024-10-22T11:51:09Z","published":"2024-10-22T11:51:09Z","title":"DNAHLM -- DNA sequence and Human Language mixed large language Model","summary":"  There are already many DNA large language models, but most of them still\nfollow traditional uses, such as extracting sequence features for\nclassification tasks. More innovative applications of large language models,\nsuch as prompt engineering, RAG, and zero-shot or few-shot prediction, remain\nchallenging for DNA-based models. The key issue lies in the fact that DNA\nmodels and human natural language models are entirely separate; however,\ntechniques like prompt engineering require the use of natural language, thereby\nsignificantly limiting the application of DNA large language models. This paper\nintroduces a hybrid model trained on the GPT-2 network, combining DNA sequences\nand English text to explore the potential of using prompts and fine-tuning in\nDNA models. The model has demonstrated its effectiveness in DNA related\nzero-shot prediction and multitask application.\n","authors":["Wang Liang"],"pdf_url":"https://arxiv.org/pdf/2410.16917v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.09805v3","updated":"2024-10-22T11:33:36Z","published":"2023-06-16T12:43:47Z","title":"Mimicking Better by Matching the Approximate Action Distribution","summary":"  In this paper, we introduce MAAD, a novel, sample-efficient on-policy\nalgorithm for Imitation Learning from Observations. MAAD utilizes a surrogate\nreward signal, which can be derived from various sources such as adversarial\ngames, trajectory matching objectives, or optimal transport criteria. To\ncompensate for the non-availability of expert actions, we rely on an inverse\ndynamics model that infers plausible actions distribution given the expert's\nstate-state transitions; we regularize the imitator's policy by aligning it to\nthe inferred action distribution. MAAD leads to significantly improved sample\nefficiency and stability. We demonstrate its effectiveness in a number of\nMuJoCo environments, both int the OpenAI Gym and the DeepMind Control Suite. We\nshow that it requires considerable fewer interactions to achieve expert\nperformance, outperforming current state-of-the-art on-policy methods.\nRemarkably, MAAD often stands out as the sole method capable of attaining\nexpert performance levels, underscoring its simplicity and efficacy.\n","authors":["João A. Cândido Ramos","Lionel Blondé","Naoya Takeishi","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2306.09805v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13467v2","updated":"2024-10-22T11:32:23Z","published":"2024-09-20T12:55:43Z","title":"Higher-Order Message Passing for Glycan Representation Learning","summary":"  Glycans are the most complex biological sequence, with monosaccharides\nforming extended, non-linear sequences. As post-translational modifications,\nthey modulate protein structure, function, and interactions. Due to their\ndiversity and complexity, predictive models of glycan properties and functions\nare still insufficient. Graph Neural Networks (GNNs) are deep learning models\ndesigned to process and analyze graph-structured data. These architectures\nleverage the connectivity and relational information in graphs to learn\neffective representations of nodes, edges, and entire graphs. Iteratively\naggregating information from neighboring nodes, GNNs capture complex patterns\nwithin graph data, making them particularly well-suited for tasks such as link\nprediction or graph classification across domains. This work presents a new\nmodel architecture based on combinatorial complexes and higher-order message\npassing to extract features from glycan structures into a latent space\nrepresentation. The architecture is evaluated on an improved GlycanML benchmark\nsuite, establishing a new state-of-the-art performance. We envision that these\nimprovements will spur further advances in computational glycosciences and\nreveal the roles of glycans in biology.\n","authors":["Roman Joeres","Daniel Bojar"],"pdf_url":"https://arxiv.org/pdf/2409.13467v2.pdf","comment":"Accepted to MLSB Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.11488v2","updated":"2024-10-22T11:18:29Z","published":"2024-10-15T10:46:03Z","title":"Advancing Training Efficiency of Deep Spiking Neural Networks through\n  Rate-based Backpropagation","summary":"  Recent insights have revealed that rate-coding is a primary form of\ninformation representation captured by surrogate-gradient-based Backpropagation\nThrough Time (BPTT) in training deep Spiking Neural Networks (SNNs). Motivated\nby these findings, we propose rate-based backpropagation, a training strategy\nspecifically designed to exploit rate-based representations to reduce the\ncomplexity of BPTT. Our method minimizes reliance on detailed temporal\nderivatives by focusing on averaged dynamics, streamlining the computational\ngraph to reduce memory and computational demands of SNNs training. We\nsubstantiate the rationality of the gradient approximation between BPTT and the\nproposed method through both theoretical analysis and empirical observations.\nComprehensive experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS\nvalidate that our method achieves comparable performance to BPTT counterparts,\nand surpasses state-of-the-art efficient training techniques. By leveraging the\ninherent benefits of rate-coding, this work sets the stage for more scalable\nand efficient SNNs training within resource-constrained environments. Our code\nis available at https://github.com/Tab-ct/rate-based-backpropagation.\n","authors":["Chengting Yu","Lei Liu","Gaoang Wang","Erping Li","Aili Wang"],"pdf_url":"https://arxiv.org/pdf/2410.11488v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16901v1","updated":"2024-10-22T11:15:07Z","published":"2024-10-22T11:15:07Z","title":"Bayes without Underfitting: Fully Correlated Deep Learning Posteriors\n  via Alternating Projections","summary":"  Bayesian deep learning all too often underfits so that the Bayesian\nprediction is less accurate than a simple point estimate. Uncertainty\nquantification then comes at the cost of accuracy. For linearized models, the\nnull space of the generalized Gauss-Newton matrix corresponds to parameters\nthat preserve the training predictions of the point estimate. We propose to\nbuild Bayesian approximations in this null space, thereby guaranteeing that the\nBayesian predictive does not underfit. We suggest a matrix-free algorithm for\nprojecting onto this null space, which scales linearly with the number of\nparameters and quadratically with the number of output dimensions. We further\npropose an approximation that only scales linearly with parameters to make the\nmethod applicable to generative models. An extensive empirical evaluation shows\nthat the approach scales to large models, including vision transformers with 28\nmillion parameters.\n","authors":["Marco Miani","Hrittik Roy","Søren Hauberg"],"pdf_url":"https://arxiv.org/pdf/2410.16901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16898v1","updated":"2024-10-22T11:03:06Z","published":"2024-10-22T11:03:06Z","title":"MBD: Multi b-value Denoising of Diffusion Magnetic Resonance Images","summary":"  We propose a novel approach to denoising diffusion magnetic resonance images\n(dMRI) using convolutional neural networks, that exploits the benefits of data\nacquired at multiple b-values to offset the need for many redundant\nobservations. Denoising is especially relevant in dMRI since noise can have a\ndeleterious impact on both quantification accuracy and image preprocessing. The\nmost successful methods proposed to date, like Marchenko-Pastur Principal\nComponent Analysis (MPPCA) denoising, are tailored to diffusion-weighting\nrepeated for many encoding directions. They exploit high redundancy of the\ndataset that oversamples the diffusion-encoding direction space, since many\ndirections have collinear components.\n  However, there are many dMRI techniques that do not entail a large number of\nencoding directions or repetitions, and are therefore less suited to this\napproach. For example, clinical dMRI exams may include as few as three encoding\ndirections, with low or negligible data redundancy across directions. Moreover,\npromising new dMRI approaches, like spherical b-tensor encoding (STE), benefit\nfrom high b-values while sensitizing the signal to diffusion along all\ndirections in just a single shot.\n  We introduce a convolutional neural network approach that we call\nmulti-b-value-based denoising (MBD). MBD exploits the similarity in\ndiffusion-weighted images (DWI) across different b-values but along the same\ndiffusion encoding direction. It allows denoising of diffusion images with high\nnoise variance while avoiding blurring, and using just a small number input\nimages.\n","authors":["Jakub Jurek","Andrzej Materka","Kamil Ludwisiak","Agata Majos","Filip Szczepankiewicz"],"pdf_url":"https://arxiv.org/pdf/2410.16898v1.pdf","comment":"this is a biomedical engineering work using machine learning to\n  enhance medical images"},{"id":"http://arxiv.org/abs/2410.16893v1","updated":"2024-10-22T10:56:52Z","published":"2024-10-22T10:56:52Z","title":"Global Optimization of Gaussian Process Acquisition Functions Using a\n  Piecewise-Linear Kernel Approximation","summary":"  Bayesian optimization relies on iteratively constructing and optimizing an\nacquisition function. The latter turns out to be a challenging, non-convex\noptimization problem itself. Despite the relative importance of this step, most\nalgorithms employ sampling- or gradient-based methods, which do not provably\nconverge to global optima. This work investigates mixed-integer programming\n(MIP) as a paradigm for \\textit{global} acquisition function optimization.\nSpecifically, our Piecewise-linear Kernel Mixed Integer Quadratic Programming\n(PK-MIQP) formulation introduces a piecewise-linear approximation for Gaussian\nprocess kernels and admits a corresponding MIQP representation for acquisition\nfunctions. We analyze the theoretical regret bounds of the proposed\napproximation, and empirically demonstrate the framework on synthetic\nfunctions, constrained benchmarks, and a hyperparameter tuning task.\n","authors":["Yilin Xie","Shiqiang Zhang","Joel Paulson","Calvin Tsay"],"pdf_url":"https://arxiv.org/pdf/2410.16893v1.pdf","comment":"16 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.07457v3","updated":"2024-10-22T10:54:15Z","published":"2024-07-10T08:20:47Z","title":"GLBench: A Comprehensive Benchmark for Graph with Large Language Models","summary":"  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n","authors":["Yuhan Li","Peisong Wang","Xiao Zhu","Aochuan Chen","Haiyun Jiang","Deng Cai","Victor Wai Kin Chan","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.07457v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16888v1","updated":"2024-10-22T10:46:36Z","published":"2024-10-22T10:46:36Z","title":"Unsupervised Time Series Anomaly Prediction with Importance-based\n  Generative Contrastive Learning","summary":"  Time series anomaly prediction plays an essential role in many real-world\nscenarios, such as environmental prevention and prompt maintenance of\ncyber-physical systems. However, existing time series anomaly prediction\nmethods mainly require supervised training with plenty of manually labeled\ndata, which are difficult to obtain in practice. Besides, unseen anomalies can\noccur during inference, which could differ from the labeled training data and\nmake these models fail to predict such new anomalies. In this paper, we study a\nnovel problem of unsupervised time series anomaly prediction. We provide a\ntheoretical analysis and propose Importance-based Generative Contrastive\nLearning (IGCL) to address the aforementioned problems. IGCL distinguishes\nbetween normal and anomaly precursors, which are generated by our anomaly\nprecursor pattern generation module. To address the efficiency issues caused by\nthe potential complex anomaly precursor combinations, we propose a memory bank\nwith importance-based scores to adaptively store representative anomaly\nprecursors and generate more complicated anomaly precursors. Extensive\nexperiments on seven benchmark datasets show our method outperforms\nstate-of-the-art baselines on unsupervised time series anomaly prediction\nproblems.\n","authors":["Kai Zhao","Zhihao Zhuang","Chenjuan Guo","Hao Miao","Yunyao Cheng","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16888v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2409.15955v4","updated":"2024-10-22T10:41:58Z","published":"2024-09-24T10:36:40Z","title":"A Historical Trajectory Assisted Optimization Method for Zeroth-Order\n  Federated Learning","summary":"  Federated learning heavily relies on distributed gradient descent techniques.\nIn the situation where gradient information is not available, the gradients\nneed to be estimated from zeroth-order information, which typically involves\ncomputing finite-differences along isotropic random directions. This method\nsuffers from high estimation errors, as the geometric features of the objective\nlandscape may be overlooked during the isotropic sampling. In this work, we\npropose a non-isotropic sampling method to improve the gradient estimation\nprocedure. Gradients in our method are estimated in a subspace spanned by\nhistorical trajectories of solutions, aiming to encourage the exploration of\npromising regions and hence improve the convergence. The proposed method uses a\ncovariance matrix for sampling which is a convex combination of two parts. The\nfirst part is a thin projection matrix containing the basis of the subspace\nwhich is designed to improve the exploitation ability. The second part is the\nhistorical trajectories. We implement this method in zeroth-order federated\nsettings, and show that the convergence rate aligns with existing ones while\nintroducing no significant overheads in communication or local computation. The\neffectiveness of our proposal is verified on several numerical experiments in\ncomparison to several commonly-used zeroth-order federated optimization\nalgorithms.\n","authors":["Chenlin Wu","Xiaoyu He","Zike Li","Jing Gong","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.15955v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16882v1","updated":"2024-10-22T10:36:15Z","published":"2024-10-22T10:36:15Z","title":"Large Language Model-based Augmentation for Imbalanced Node\n  Classification on Text-Attributed Graphs","summary":"  Node classification on graphs frequently encounters the challenge of class\nimbalance, leading to biased performance and posing significant risks in\nreal-world applications. Although several data-centric solutions have been\nproposed, none of them focus on Text-Attributed Graphs (TAGs), and therefore\noverlook the potential of leveraging the rich semantics encoded in textual\nfeatures for boosting the classification of minority nodes. Given this crucial\ngap, we investigate the possibility of augmenting graph data in the text space,\nleveraging the textual generation power of Large Language Models (LLMs) to\nhandle imbalanced node classification on TAGs. Specifically, we propose a novel\napproach called LA-TAG (LLM-based Augmentation on Text-Attributed Graphs),\nwhich prompts LLMs to generate synthetic texts based on existing node texts in\nthe graph. Furthermore, to integrate these synthetic text-attributed nodes into\nthe graph, we introduce a text-based link predictor to connect the synthesized\nnodes with the existing nodes. Our experiments across multiple datasets and\nevaluation metrics show that our framework significantly outperforms\ntraditional non-textual-based data augmentation strategies and specific node\nimbalance solutions. This highlights the promise of using LLMs to resolve\nimbalance issues on TAGs.\n","authors":["Leyao Wang","Yu Wang","Bo Ni","Yuying Zhao","Tyler Derr"],"pdf_url":"https://arxiv.org/pdf/2410.16882v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.16881v1","updated":"2024-10-22T10:33:00Z","published":"2024-10-22T10:33:00Z","title":"Just In Time Transformers","summary":"  Precise energy load forecasting in residential households is crucial for\nmitigating carbon emissions and enhancing energy efficiency; indeed, accurate\nforecasting enables utility companies and policymakers, who advocate\nsustainable energy practices, to optimize resource utilization. Moreover, smart\nmeters provide valuable information by allowing for granular insights into\nconsumption patterns. Building upon available smart meter data, our study aims\nto cluster consumers into distinct groups according to their energy usage\nbehaviours, effectively capturing a diverse spectrum of consumption patterns.\nNext, we design JITtrans (Just In Time transformer), a novel transformer deep\nlearning model that significantly improves energy consumption forecasting\naccuracy, with respect to traditional forecasting methods. Extensive\nexperimental results validate our claims using proprietary smart meter data.\nOur findings highlight the potential of advanced predictive technologies to\nrevolutionize energy management and advance sustainable power systems: the\ndevelopment of efficient and eco-friendly energy solutions critically depends\non such technologies.\n","authors":["Ahmed Ala Eddine Benali","Massimo Cafaro","Italo Epicoco","Marco Pulimeno","Enrico Junior Schioppa"],"pdf_url":"https://arxiv.org/pdf/2410.16881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16879v1","updated":"2024-10-22T10:31:23Z","published":"2024-10-22T10:31:23Z","title":"Contrasting Attitudes Towards Current and Future AI Applications for\n  Computerised Interpretation of ECG: A Clinical Stakeholder Interview Study","summary":"  Objectives: To investigate clinicians' attitudes towards current automated\ninterpretation of ECG and novel AI technologies and their perception of\ncomputer-assisted interpretation. Materials and Methods: We conducted a series\nof interviews with clinicians in the UK. Our study: (i) explores the potential\nfor AI, specifically future 'human-like' computing approaches, to facilitate\nECG interpretation and support clinical decision making, and (ii) elicits their\nopinions about the importance of explainability and trustworthiness of AI\nalgorithms. Results: We performed inductive thematic analysis on interview\ntranscriptions from 23 clinicians and identified the following themes: (i) a\nlack of trust in current systems, (ii) positive attitudes towards future AI\napplications and requirements for these, (iii) the relationship between the\naccuracy and explainability of algorithms, and (iv) opinions on education,\npossible deskilling, and the impact of AI on clinical competencies. Discussion:\nClinicians do not trust current computerised methods, but welcome future 'AI'\ntechnologies. Where clinicians trust future AI interpretation to be accurate,\nthey are less concerned that it is explainable. They also preferred ECG\ninterpretation that demonstrated the results of the algorithm visually. Whilst\nclinicians do not fear job losses, they are concerned about deskilling and the\nneed to educate the workforce to use AI responsibly. Conclusion: Clinicians are\npositive about the future application of AI in clinical decision-making.\nAccuracy is a key factor of uptake and visualisations are preferred over\ncurrent computerised methods. This is viewed as a potential means of training\nand upskilling, in contrast to the deskilling that automation might be\nperceived to bring.\n","authors":["Lukas Hughes-Noehrer","Leda Channer","Gabriel Strain","Gregory Yates","Richard Body","Caroline Jay"],"pdf_url":"https://arxiv.org/pdf/2410.16879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16872v1","updated":"2024-10-22T10:20:20Z","published":"2024-10-22T10:20:20Z","title":"CK4Gen: A Knowledge Distillation Framework for Generating High-Utility\n  Synthetic Survival Datasets in Healthcare","summary":"  Access to real clinical data is heavily restricted by privacy regulations,\nhindering both healthcare research and education. These constraints slow\nprogress in developing new treatments and data-driven healthcare solutions,\nwhile also limiting students' access to real-world datasets, leaving them\nwithout essential practical skills. High-utility synthetic datasets are\ntherefore critical for advancing research and providing meaningful training\nmaterial. However, current generative models -- such as Variational\nAutoencoders (VAEs) and Generative Adversarial Networks (GANs) -- produce\nsurface-level realism at the expense of healthcare utility, blending distinct\npatient profiles and producing synthetic data of limited practical relevance.\nTo overcome these limitations, we introduce CK4Gen (Cox Knowledge for\nGeneration), a novel framework that leverages knowledge distillation from Cox\nProportional Hazards (CoxPH) models to create synthetic survival datasets that\npreserve key clinical characteristics, including hazard ratios and survival\ncurves. CK4Gen avoids the interpolation issues seen in VAEs and GANs by\nmaintaining distinct patient risk profiles, ensuring realistic and reliable\noutputs for research and educational use. Validated across four benchmark\ndatasets -- GBSG2, ACTG320, WHAS500, and FLChain -- CK4Gen outperforms\ncompeting techniques by better aligning real and synthetic data, enhancing\nsurvival model performance in both discrimination and calibration via data\naugmentation. As CK4Gen is scalable across clinical conditions, and with code\nto be made publicly available, future researchers can apply it to their own\ndatasets to generate synthetic versions suitable for open sharing.\n","authors":["Nicholas I-Hsien Kuo","Blanca Gallego","Louisa Jorm"],"pdf_url":"https://arxiv.org/pdf/2410.16872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16871v1","updated":"2024-10-22T10:19:27Z","published":"2024-10-22T10:19:27Z","title":"Error Feedback under $(L_0,L_1)$-Smoothness: Normalization and Momentum","summary":"  We provide the first proof of convergence for normalized error feedback\nalgorithms across a wide range of machine learning problems. Despite their\npopularity and efficiency in training deep neural networks, traditional\nanalyses of error feedback algorithms rely on the smoothness assumption that\ndoes not capture the properties of objective functions in these problems.\nRather, these problems have recently been shown to satisfy generalized\nsmoothness assumptions, and the theoretical understanding of error feedback\nalgorithms under these assumptions remains largely unexplored. Moreover, to the\nbest of our knowledge, all existing analyses under generalized smoothness\neither i) focus on single-node settings or ii) make unrealistically strong\nassumptions for distributed settings, such as requiring data heterogeneity, and\nalmost surely bounded stochastic gradient noise variance. In this paper, we\npropose distributed error feedback algorithms that utilize normalization to\nachieve the $O(1/\\sqrt{K})$ convergence rate for nonconvex problems under\ngeneralized smoothness. Our analyses apply for distributed settings without\ndata heterogeneity conditions, and enable stepsize tuning that is independent\nof problem parameters. Additionally, we provide strong convergence guarantees\nof normalized error feedback algorithms for stochastic settings. Finally, we\nshow that due to their larger allowable stepsizes, our new normalized error\nfeedback algorithms outperform their non-normalized counterparts on various\ntasks, including the minimization of polynomial functions, logistic regression,\nand ResNet-20 training.\n","authors":["Sarit Khirirat","Abdurakhmon Sadiev","Artem Riabinin","Eduard Gorbunov","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2410.16871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16870v1","updated":"2024-10-22T10:19:17Z","published":"2024-10-22T10:19:17Z","title":"Federated Causal Inference: Multi-Centric ATE Estimation beyond\n  Meta-Analysis","summary":"  We study Federated Causal Inference, an approach to estimate treatment\neffects from decentralized data across centers. We compare three classes of\nAverage Treatment Effect (ATE) estimators derived from the Plug-in G-Formula,\nranging from simple meta-analysis to one-shot and multi-shot federated\nlearning, the latter leveraging the full data to learn the outcome model\n(albeit requiring more communication). Focusing on Randomized Controlled Trials\n(RCTs), we derive the asymptotic variance of these estimators for linear\nmodels. Our results provide practical guidance on selecting the appropriate\nestimator for various scenarios, including heterogeneity in sample sizes,\ncovariate distributions, treatment assignment schemes, and center effects. We\nvalidate these findings with a simulation study.\n","authors":["Rémi Khellaf","Aurélien Bellet","Julie Josse"],"pdf_url":"https://arxiv.org/pdf/2410.16870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16868v1","updated":"2024-10-22T10:12:57Z","published":"2024-10-22T10:12:57Z","title":"Rethinking generalization of classifiers in separable classes scenarios\n  and over-parameterized regimes","summary":"  We investigate the learning dynamics of classifiers in scenarios where\nclasses are separable or classifiers are over-parameterized. In both cases,\nEmpirical Risk Minimization (ERM) results in zero training error. However,\nthere are many global minima with a training error of zero, some of which\ngeneralize well and some of which do not. We show that in separable classes\nscenarios the proportion of \"bad\" global minima diminishes exponentially with\nthe number of training data n. Our analysis provides bounds and learning curves\ndependent solely on the density distribution of the true error for the given\nclassifier function set, irrespective of the set's size or complexity (e.g.,\nnumber of parameters). This observation may shed light on the unexpectedly good\ngeneralization of over-parameterized Neural Networks. For the\nover-parameterized scenario, we propose a model for the density distribution of\nthe true error, yielding learning curves that align with experiments on MNIST\nand CIFAR-10.\n","authors":["Julius Martinetz","Christoph Linse","Thomas Martinetz"],"pdf_url":"https://arxiv.org/pdf/2410.16868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16858v1","updated":"2024-10-22T09:52:15Z","published":"2024-10-22T09:52:15Z","title":"Dynamic graph neural networks for enhanced volatility prediction in\n  financial markets","summary":"  Volatility forecasting is essential for risk management and decision-making\nin financial markets. Traditional models like Generalized Autoregressive\nConditional Heteroskedasticity (GARCH) effectively capture volatility\nclustering but often fail to model complex, non-linear interdependencies\nbetween multiple indices. This paper proposes a novel approach using Graph\nNeural Networks (GNNs) to represent global financial markets as dynamic graphs.\nThe Temporal Graph Attention Network (Temporal GAT) combines Graph\nConvolutional Networks (GCNs) and Graph Attention Networks (GATs) to capture\nthe temporal and structural dynamics of volatility spillovers. By utilizing\ncorrelation-based and volatility spillover indices, the Temporal GAT constructs\ndirected graphs that enhance the accuracy of volatility predictions. Empirical\nresults from a 15-year study of eight major global indices show that the\nTemporal GAT outperforms traditional GARCH models and other machine learning\nmethods, particularly in short- to mid-term forecasts. The sensitivity and\nscenario-based analysis over a range of parameters and hyperparameters further\ndemonstrate the significance of the proposed technique. Hence, this work\nhighlights the potential of GNNs in modeling complex market behaviors,\nproviding valuable insights for financial analysts and investors.\n","authors":["Pulikandala Nithish Kumar","Nneka Umeorah","Alex Alochukwu"],"pdf_url":"https://arxiv.org/pdf/2410.16858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02500v3","updated":"2024-10-22T09:42:39Z","published":"2024-02-04T14:18:45Z","title":"Point Cloud Matters: Rethinking the Impact of Different Observation\n  Spaces on Robot Learning","summary":"  In robot learning, the observation space is crucial due to the distinct\ncharacteristics of different modalities, which can potentially become a\nbottleneck alongside policy design. In this study, we explore the influence of\nvarious observation spaces on robot learning, focusing on three predominant\nmodalities: RGB, RGB-D, and point cloud. We introduce OBSBench, a benchmark\ncomprising two simulators and 125 tasks, along with standardized pipelines for\nvarious encoders and policy baselines. Extensive experiments on diverse\ncontact-rich manipulation tasks reveal a notable trend: point cloud-based\nmethods, even those with the simplest designs, frequently outperform their RGB\nand RGB-D counterparts. This trend persists in both scenarios: training from\nscratch and utilizing pre-training. Furthermore, our findings demonstrate that\npoint cloud observations often yield better policy performance and\nsignificantly stronger generalization capabilities across various geometric and\nvisual conditions. These outcomes suggest that the 3D point cloud is a valuable\nobservation modality for intricate robotic tasks. We also suggest that\nincorporating both appearance and coordinate information can enhance the\nperformance of point cloud methods. We hope our work provides valuable insights\nand guidance for designing more generalizable and robust robotic models. Codes\nare available at https://github.com/HaoyiZhu/PointCloudMatters.\n","authors":["Haoyi Zhu","Yating Wang","Di Huang","Weicai Ye","Wanli Ouyang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2402.02500v3.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.16849v1","updated":"2024-10-22T09:35:47Z","published":"2024-10-22T09:35:47Z","title":"Polyak's Heavy Ball Method Achieves Accelerated Local Rate of\n  Convergence under Polyak-Lojasiewicz Inequality","summary":"  In this work, we consider the convergence of Polyak's heavy ball method, both\nin continuous and discrete time, on a non-convex objective function. We recover\nthe convergence rates derived in [Polyak, U.S.S.R. Comput. Math. and Math.\nPhys., 1964] for strongly convex objective functions, assuming only validity of\nthe Polyak-Lojasiewicz inequality. In continuous time our result holds for all\ninitializations, whereas in the discrete time setting we conduct a local\nanalysis around the global minima. Our results demonstrate that the heavy ball\nmethod does, in fact, accelerate on the class of objective functions satisfying\nthe Polyak-Lojasiewicz inequality. This holds even in the discrete time\nsetting, provided the method reaches a neighborhood of the global minima.\nInstead of the usually employed Lyapunov-type arguments, our approach leverages\na new differential geometric perspective of the Polyak-Lojasiewicz inequality\nproposed in [Rebjock and Boumal, Math. Program., 2024].\n","authors":["Sebastian Kassing","Simon Weissmann"],"pdf_url":"https://arxiv.org/pdf/2410.16849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16846v1","updated":"2024-10-22T09:34:22Z","published":"2024-10-22T09:34:22Z","title":"Safe Load Balancing in Software-Defined-Networking","summary":"  High performance, reliability and safety are crucial properties of any\nSoftware-Defined-Networking (SDN) system. Although the use of Deep\nReinforcement Learning (DRL) algorithms has been widely studied to improve\nperformance, their practical applications are still limited as they fail to\nensure safe operations in exploration and decision-making. To fill this gap, we\nexplore the design of a Control Barrier Function (CBF) on top of Deep\nReinforcement Learning (DRL) algorithms for load-balancing. We show that our\nDRL-CBF approach is capable of meeting safety requirements during training and\ntesting while achieving near-optimal performance in testing. We provide results\nusing two simulators: a flow-based simulator, which is used for\nproof-of-concept and benchmarking, and a packet-based simulator that implements\nreal protocols and scheduling. Thanks to the flow-based simulator, we compared\nthe performance against the optimal policy, solving a Non Linear Programming\n(NLP) problem with the SCIP solver. Furthermore, we showed that pre-trained\nmodels in the flow-based simulator, which is faster, can be transferred to the\npacket simulator, which is slower but more accurate, with some fine-tuning.\nOverall, the results suggest that near-optimal Quality-of-Service (QoS)\nperformance in terms of end-to-end delay can be achieved while safety\nrequirements related to link capacity constraints are guaranteed. In the\npacket-based simulator, we also show that our DRL-CBF algorithms outperform\nnon-RL baseline algorithms. When the models are fine-tuned over a few episodes,\nwe achieved smoother QoS and safety in training, and similar performance in\ntesting compared to the case where models have been trained from scratch.\n","authors":["Lam Dinh","Pham Tran Anh Quang","Jérémie Leguay"],"pdf_url":"https://arxiv.org/pdf/2410.16846v1.pdf","comment":"Accepted to Computer Communications 2024. arXiv admin note: text\n  overlap with arXiv:2401.05525"},{"id":"http://arxiv.org/abs/2410.16845v1","updated":"2024-10-22T09:33:29Z","published":"2024-10-22T09:33:29Z","title":"Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating\n  Few-Shot Node Classification","summary":"  Graph Neural Networks (GNNs) have shown superior performance in node\nclassification. However, GNNs perform poorly in the Few-Shot Node\nClassification (FSNC) task that requires robust generalization to make accurate\npredictions for unseen classes with limited labels. To tackle the challenge, we\npropose the integration of Sharpness-Aware Minimization (SAM)--a technique\ndesigned to enhance model generalization by finding a flat minimum of the loss\nlandscape--into GNN training. The standard SAM approach, however, consists of\ntwo forward-backward steps in each training iteration, doubling the\ncomputational cost compared to the base optimizer (e.g., Adam). To mitigate\nthis drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware\nMinimization (FGSAM), that integrates the rapid training of Multi-Layer\nPerceptrons (MLPs) with the superior performance of GNNs. Specifically, we\nutilize GNNs for parameter perturbation while employing MLPs to minimize the\nperturbed loss so that we can find a flat minimum with good generalization more\nefficiently. Moreover, our method reutilizes the gradient from the perturbation\nphase to incorporate graph topology into the minimization process at almost\nzero additional cost. To further enhance training efficiency, we develop FGSAM+\nthat executes exact perturbations periodically. Extensive experiments\ndemonstrate that our proposed algorithm outperforms the standard SAM with lower\ncomputational costs in FSNC tasks. In particular, our FGSAM+ as a SAM variant\noffers a faster optimization than the base optimizer in most cases. In addition\nto FSNC, our proposed methods also demonstrate competitive performance in the\nstandard node classification task for heterophilic graphs, highlighting the\nbroad applicability. The code is available at\nhttps://github.com/draym28/FGSAM_NeurIPS24.\n","authors":["Yihong Luo","Yuhan Chen","Siya Qiu","Yiwei Wang","Chen Zhang","Yan Zhou","Xiaochun Cao","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2410.16845v1.pdf","comment":"NeurIPS24; The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2406.05335v2","updated":"2024-10-22T09:32:17Z","published":"2024-06-08T03:37:05Z","title":"Critical Phase Transition in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated impressive performance. To\nunderstand their behaviors, we need to consider the fact that LLMs sometimes\nshow qualitative changes. The natural world also presents such changes called\nphase transitions, which are defined by singular, divergent statistical\nquantities. Therefore, an intriguing question is whether qualitative changes in\nLLMs are phase transitions. In this work, we have conducted extensive analysis\non texts generated by LLMs and suggested that a phase transition occurs in LLMs\nwhen varying the temperature parameter. Specifically, statistical quantities\nhave divergent properties just at the point between the low-temperature regime,\nwhere LLMs generate sentences with clear repetitive structures, and the\nhigh-temperature regime, where generated sentences are often incomprehensible.\nIn addition, critical behaviors near the phase transition point, such as a\npower-law decay of correlation and slow convergence toward the stationary\nstate, are similar to those in natural languages. Our results suggest a\nmeaningful analogy between LLMs and natural phenomena.\n","authors":["Kai Nakaishi","Yoshihiko Nishikawa","Koji Hukushima"],"pdf_url":"https://arxiv.org/pdf/2406.05335v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.03348v2","updated":"2024-10-22T09:31:49Z","published":"2024-04-04T10:28:55Z","title":"Knowledge Distillation-Based Model Extraction Attack using GAN-based\n  Private Counterfactual Explanations","summary":"  In recent years, there has been a notable increase in the deployment of\nmachine learning (ML) models as services (MLaaS) across diverse production\nsoftware applications. In parallel, explainable AI (XAI) continues to evolve,\naddressing the necessity for transparency and trustworthiness in ML models. XAI\ntechniques aim to enhance the transparency of ML models by providing insights,\nin terms of model's explanations, into their decision-making process.\nSimultaneously, some MLaaS platforms now offer explanations alongside the ML\nprediction outputs. This setup has elevated concerns regarding vulnerabilities\nin MLaaS, particularly in relation to privacy leakage attacks such as model\nextraction attacks (MEA). This is due to the fact that explanations can unveil\ninsights about the inner workings of the model which could be exploited by\nmalicious users. In this work, we focus on investigating how model\nexplanations, particularly counterfactual explanations (CFs), can be exploited\nfor performing MEA within the MLaaS platform. We also delve into assessing the\neffectiveness of incorporating differential privacy (DP) as a mitigation\nstrategy. To this end, we first propose a novel approach for MEA based on\nKnowledge Distillation (KD) to enhance the efficiency of extracting a\nsubstitute model of a target model exploiting CFs, without any knowledge about\nthe training data distribution by the attacker. Then, we advise an approach for\ntraining CF generators incorporating DP to generate private CFs. We conduct\nthorough experimental evaluations on real-world datasets and demonstrate that\nour proposed KD-based MEA can yield a high-fidelity substitute model with a\nreduced number of queries with respect to baseline approaches. Furthermore, our\nfindings reveal that including a privacy layer can allow mitigating the MEA.\nHowever, on the account of the quality of CFs, impacts the performance of the\nexplanations.\n","authors":["Fatima Ezzeddine","Omran Ayoub","Silvia Giordano"],"pdf_url":"https://arxiv.org/pdf/2404.03348v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2404.16656v2","updated":"2024-10-22T09:30:36Z","published":"2024-04-25T14:48:29Z","title":"A Self-Organizing Clustering System for Unsupervised Distribution Shift\n  Detection","summary":"  Modeling non-stationary data is a challenging problem in the field of\ncontinual learning, and data distribution shifts may result in negative\nconsequences on the performance of a machine learning model. Classic learning\ntools are often vulnerable to perturbations of the input covariates, and are\nsensitive to outliers and noise, and some tools are based on rigid algebraic\nassumptions. Distribution shifts are frequently occurring due to changes in raw\nmaterials for production, seasonality, a different user base, or even\nadversarial attacks. Therefore, there is a need for more effective distribution\nshift detection techniques. In this work, we propose a continual learning\nframework for monitoring and detecting distribution changes. We explore the\nproblem in a latent space generated by a bio-inspired self-organizing\nclustering and statistical aspects of the latent space. In particular, we\ninvestigate the projections made by two topology-preserving maps: the\nSelf-Organizing Map and the Scale Invariant Map. Our method can be applied in\nboth a supervised and an unsupervised context. We construct the assessment of\nchanges in the data distribution as a comparison of Gaussian signals, making\nthe proposed method fast and robust. We compare it to other unsupervised\ntechniques, specifically Principal Component Analysis (PCA) and Kernel-PCA. Our\ncomparison involves conducting experiments using sequences of images (based on\nMNIST and injected shifts with adversarial samples), chemical sensor\nmeasurements, and the environmental variable related to ozone levels. The\nempirical study reveals the potential of the proposed approach.\n","authors":["Sebastián Basterrech","Line Clemmensen","Gerardo Rubino"],"pdf_url":"https://arxiv.org/pdf/2404.16656v2.pdf","comment":"Revised version of the accepted manuscript to IJCNN'2024. Main\n  corrections were in Section 2.2 and Section 3.3. In Section 2.2 was corrected\n  expression (3), and in Section 3.3 in the definition of the elements of the\n  matrix $D$ it was a typo where $\\phi(x)$ was written instead of $x$"},{"id":"http://arxiv.org/abs/2402.08180v3","updated":"2024-10-22T09:23:43Z","published":"2024-02-13T02:36:41Z","title":"Online Structured Prediction with Fenchel--Young Losses and Improved\n  Surrogate Regret for Online Multiclass Classification with Logistic Loss","summary":"  This paper studies online structured prediction with full-information\nfeedback. For online multiclass classification, Van der Hoeven (2020)\nestablished \\emph{finite} surrogate regret bounds, which are independent of the\ntime horizon, by introducing an elegant \\emph{exploit-the-surrogate-gap}\nframework. However, this framework has been limited to multiclass\nclassification primarily because it relies on a classification-specific\nprocedure for converting estimated scores to outputs. We extend the\nexploit-the-surrogate-gap framework to online structured prediction with\n\\emph{Fenchel--Young losses}, a large family of surrogate losses that includes\nthe logistic loss for multiclass classification as a special case, obtaining\nfinite surrogate regret bounds in various structured prediction problems. To\nthis end, we propose and analyze \\emph{randomized decoding}, which converts\nestimated scores to general structured outputs. Moreover, by applying our\ndecoding to online multiclass classification with the logistic loss, we obtain\na surrogate regret bound of $O(\\| \\mathbf{U} \\|_\\mathrm{F}^2)$, where\n$\\mathbf{U}$ is the best offline linear estimator and $\\| \\cdot \\|_\\mathrm{F}$\ndenotes the Frobenius norm. This bound is tight up to logarithmic factors and\nimproves the previous bound of $O(d\\| \\mathbf{U} \\|_\\mathrm{F}^2)$ due to Van\nder Hoeven (2020) by a factor of $d$, the number of classes.\n","authors":["Shinsaku Sakaue","Han Bao","Taira Tsuchiya","Taihei Oki"],"pdf_url":"https://arxiv.org/pdf/2402.08180v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01344v2","updated":"2024-10-22T09:22:33Z","published":"2023-12-03T10:40:07Z","title":"Enhancing Algorithm Performance Understanding through tsMorph:\n  Generating Semi-Synthetic Time Series for Robust Forecasting Evaluation","summary":"  Time series forecasting is a subject of significant scientific and industrial\nimportance. Despite the widespread utilization of forecasting methods, there is\na dearth of research aimed at comprehending the conditions under which these\nmethods yield favorable or unfavorable performances. Empirical studies,\nalthough common, are challenged by the limited availability of time series\ndatasets, restricting the extraction of reliable insights. To address this\nlimitation, we present tsMorph, a tool for generating semi-synthetic time\nseries through dataset morphing. tsMorph works by creating a sequence of\ndatasets from two original datasets. The characteristics of the generated\ndatasets progressively depart from those of one of the datasets and converge\ntoward the attributes of the other dataset. This method provides a valuable\nalternative for obtaining substantial datasets. In this paper, we show the\nbenefits of tsMorph by assessing the predictive performance of the Long\nShort-Term Memory Network and DeepAR forecasting algorithms. The time series\nused for the experiments comes from the NN5 Competition. The experimental\nresults provide important insights. Notably, the performances of the two\nalgorithms improve proportionally with the frequency of the time series. These\nexperiments confirm that tsMorph can be an effective tool for better\nunderstanding the behavior of forecasting algorithms, delivering a pathway to\novercoming the limitations posed by empirical studies and enabling more\nextensive and reliable experiments.\n","authors":["Moisés Santos","André de Carvalho","Carlos Soares"],"pdf_url":"https://arxiv.org/pdf/2312.01344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07563v2","updated":"2024-10-22T09:06:38Z","published":"2024-10-10T02:59:36Z","title":"PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency","summary":"  We introduce PLaMo-100B, a large-scale language model designed for Japanese\nproficiency. The model was trained from scratch using 2 trillion tokens, with\narchitecture such as QK Normalization and Z-Loss to ensure training stability\nduring the training process. Post-training techniques, including Supervised\nFine-Tuning and Direct Preference Optimization, were applied to refine the\nmodel's performance. Benchmark evaluations suggest that PLaMo-100B performs\nwell, particularly in Japanese-specific tasks, achieving results that are\ncompetitive with frontier models like GPT-4. The base model is available at\nhttps://huggingface.co/pfnet/plamo-100b.\n","authors":["Preferred Elements"," :","Kenshin Abe","Kaizaburo Chubachi","Yasuhiro Fujita","Yuta Hirokawa","Kentaro Imajo","Toshiki Kataoka","Hiroyoshi Komatsu","Hiroaki Mikami","Tsuguo Mogami","Shogo Murai","Kosuke Nakago","Daisuke Nishino","Toru Ogawa","Daisuke Okanohara","Yoshihiko Ozaki","Shotaro Sano","Shuji Suzuki","Tianqi Xu","Toshihiko Yanase"],"pdf_url":"https://arxiv.org/pdf/2410.07563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16826v1","updated":"2024-10-22T08:58:44Z","published":"2024-10-22T08:58:44Z","title":"Guarantees of a Preconditioned Subgradient Algorithm for\n  Overparameterized Asymmetric Low-rank Matrix Recovery","summary":"  In this paper, we focus on a matrix factorization-based approach for robust\nlow-rank and asymmetric matrix recovery from corrupted measurements. We address\nthe challenging scenario where the rank of the sought matrix is unknown and\nemploy an overparameterized approach using the variational form of the nuclear\nnorm as a regularizer. We propose a subgradient algorithm that inherits the\nmerits of preconditioned algorithms, whose rate of convergence does not depend\non the condition number of the sought matrix, and addresses their current\nlimitation, i.e., the lack of convergence guarantees in the case of asymmetric\nmatrices with unknown rank. In this setting, we provide, for the first time in\nthe literature, linear convergence guarantees for the derived overparameterized\npreconditioned subgradient algorithm in the presence of gross corruptions.\nAdditionally, by applying our approach to matrix sensing, we highlight its\nmerits when the measurement operator satisfies the mixed-norm restricted\nisometry properties. Lastly, we present numerical experiments that validate our\ntheoretical results and demonstrate the effectiveness of our approach.\n","authors":["Paris Giampouras","HanQin Cai","Rene Vidal"],"pdf_url":"https://arxiv.org/pdf/2410.16826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16813v1","updated":"2024-10-22T08:40:05Z","published":"2024-10-22T08:40:05Z","title":"Klein Model for Hyperbolic Neural Networks","summary":"  Hyperbolic neural networks (HNNs) have been proved effective in modeling\ncomplex data structures. However, previous works mainly focused on the\nPoincar\\'e ball model and the hyperboloid model as coordinate representations\nof the hyperbolic space, often neglecting the Klein model. Despite this, the\nKlein model offers its distinct advantages thanks to its straight-line\ngeodesics, which facilitates the well-known Einstein midpoint construction,\npreviously leveraged to accompany HNNs in other models. In this work, we\nintroduce a framework for hyperbolic neural networks based on the Klein model.\nWe provide detailed formulation for representing useful operations using the\nKlein model. We further study the Klein linear layer and prove that the\n\"tangent space construction\" of the scalar multiplication and parallel\ntransport are exactly the Einstein scalar multiplication and the Einstein\naddition, analogous to the M\\\"obius operations used in the Poincar\\'e ball\nmodel. We show numerically that the Klein HNN performs on par with the\nPoincar\\'e ball model, providing a third option for HNN that works as a\nbuilding block for more complicated architectures.\n","authors":["Yidan Mao","Jing Gu","Marcus C. Werner","Dongmian Zou"],"pdf_url":"https://arxiv.org/pdf/2410.16813v1.pdf","comment":"Accepted to NeurIPS 2024 Symmetry and Geometry in Neural\n  Representations Workshop"},{"id":"http://arxiv.org/abs/2410.16811v1","updated":"2024-10-22T08:38:46Z","published":"2024-10-22T08:38:46Z","title":"Masked Clinical Modelling: A Framework for Synthetic and Augmented\n  Survival Data Generation","summary":"  Access to real clinical data is often restricted due to privacy obligations,\ncreating significant barriers for healthcare research. Synthetic datasets\nprovide a promising solution, enabling secure data sharing and model\ndevelopment. However, most existing approaches focus on data realism rather\nthan utility -- ensuring that models trained on synthetic data yield clinically\nmeaningful insights comparable to those trained on real data. In this paper, we\npresent Masked Clinical Modelling (MCM), a framework inspired by masked\nlanguage modelling, designed for both data synthesis and conditional data\naugmentation. We evaluate this prototype on the WHAS500 dataset using Cox\nProportional Hazards models, focusing on the preservation of hazard ratios as\nkey clinical metrics. Our results show that data generated using the MCM\nframework improves both discrimination and calibration in survival analysis,\noutperforming existing methods. MCM demonstrates strong potential to support\nsurvival data analysis and broader healthcare applications.\n","authors":["Nicholas I-Hsien Kuo","Blanca Gallego","Louisa Jorm"],"pdf_url":"https://arxiv.org/pdf/2410.16811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16805v1","updated":"2024-10-22T08:32:17Z","published":"2024-10-22T08:32:17Z","title":"Test-time Adversarial Defense with Opposite Adversarial Path and High\n  Attack Time Cost","summary":"  Deep learning models are known to be vulnerable to adversarial attacks by\ninjecting sophisticated designed perturbations to input data. Training-time\ndefenses still exhibit a significant performance gap between natural accuracy\nand robust accuracy. In this paper, we investigate a new test-time adversarial\ndefense method via diffusion-based recovery along opposite adversarial paths\n(OAPs). We present a purifier that can be plugged into a pre-trained model to\nresist adversarial attacks. Different from prior arts, the key idea is\nexcessive denoising or purification by integrating the opposite adversarial\ndirection with reverse diffusion to push the input image further toward the\nopposite adversarial direction. For the first time, we also exemplify the\npitfall of conducting AutoAttack (Rand) for diffusion-based defense methods.\nThrough the lens of time complexity, we examine the trade-off between the\neffectiveness of adaptive attack and its computation complexity against our\ndefense. Experimental evaluation along with time cost analysis verifies the\neffectiveness of the proposed method.\n","authors":["Cheng-Han Yeh","Kuanchun Yu","Chun-Shien Lu"],"pdf_url":"https://arxiv.org/pdf/2410.16805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16802v1","updated":"2024-10-22T08:27:43Z","published":"2024-10-22T08:27:43Z","title":"Evaluating the Effectiveness of Attack-Agnostic Features for Morphing\n  Attack Detection","summary":"  Morphing attacks have diversified significantly over the past years, with new\nmethods based on generative adversarial networks (GANs) and diffusion models\nposing substantial threats to face recognition systems. Recent research has\ndemonstrated the effectiveness of features extracted from large vision models\npretrained on bonafide data only (attack-agnostic features) for detecting deep\ngenerative images. Building on this, we investigate the potential of these\nimage representations for morphing attack detection (MAD). We develop\nsupervised detectors by training a simple binary linear SVM on the extracted\nfeatures and one-class detectors by modeling the distribution of bonafide\nfeatures with a Gaussian Mixture Model (GMM). Our method is evaluated across a\ncomprehensive set of attacks and various scenarios, including generalization to\nunseen attacks, different source datasets, and print-scan data. Our results\nindicate that attack-agnostic features can effectively detect morphing attacks,\noutperforming traditional supervised and one-class detectors from the\nliterature in most scenarios. Additionally, we provide insights into the\nstrengths and limitations of each considered representation and discuss\npotential future research directions to further enhance the robustness and\ngeneralizability of our approach.\n","authors":["Laurent Colbois","Sébastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2410.16802v1.pdf","comment":"Published in the 2024 IEEE International Joint Conference on\n  Biometrics (IJCB)"},{"id":"http://arxiv.org/abs/2207.09185v3","updated":"2024-10-22T08:17:21Z","published":"2022-07-19T10:46:02Z","title":"Multimodal hierarchical Variational AutoEncoders with Factor Analysis\n  latent space","summary":"  Purpose: Handling heterogeneous and mixed data types has become increasingly\ncritical with the exponential growth in real-world databases. While deep\ngenerative models attempt to merge diverse data views into a common latent\nspace, they often sacrifice interpretability, flexibility, and modularity. This\nstudy proposes a novel method to address these limitations by combining\nVariational AutoEncoders (VAEs) with a Factor Analysis latent space (FA-VAE).\n  Methods: The proposed FA-VAE method employs multiple VAEs to learn a private\nrepresentation for each heterogeneous data view in a continuous latent space.\nInformation is shared between views using a low-dimensional latent space,\ngenerated via a linear projection matrix. This modular design creates a\nhierarchical dependency between private and shared latent spaces, allowing for\nthe flexible addition of new views and conditioning of pre-trained models.\n  Results: The FA-VAE approach facilitates cross-generation of data from\ndifferent domains and enables transfer learning between generative models. This\nallows for effective integration of information across diverse data views while\npreserving their distinct characteristics.\n  Conclusions: By overcoming the limitations of existing methods, the FA-VAE\nprovides a more interpretable, flexible, and modular solution for managing\nheterogeneous data types. It offers a pathway to more efficient and scalable\ndata-handling strategies, enhancing the potential for cross-domain data\nsynthesis and model transferability.\n","authors":["Alejandro Guerrero-López","Carlos Sevilla-Salcedo","Vanessa Gómez-Verdejo","Pablo M. Olmos"],"pdf_url":"https://arxiv.org/pdf/2207.09185v3.pdf","comment":"21 pages main work, 2 pages supplementary, 14 figures"},{"id":"http://arxiv.org/abs/2410.16794v1","updated":"2024-10-22T08:17:20Z","published":"2024-10-22T08:17:20Z","title":"One-Step Diffusion Distillation through Score Implicit Matching","summary":"  Despite their strong performances on many generative tasks, diffusion models\nrequire a large number of sampling steps in order to generate realistic\nsamples. This has motivated the community to develop effective methods to\ndistill pre-trained diffusion models into more efficient models, but these\nmethods still typically require few-step inference or perform substantially\nworse than the underlying model. In this paper, we present Score Implicit\nMatching (SIM) a new approach to distilling pre-trained diffusion models into\nsingle-step generator models, while maintaining almost the same sample\ngeneration ability as the original model as well as being data-free with no\nneed of training samples for distillation. The method rests upon the fact that,\nalthough the traditional score-based loss is intractable to minimize for\ngenerator models, under certain conditions we can efficiently compute the\ngradients for a wide class of score-based divergences between a diffusion model\nand a generator. SIM shows strong empirical performances for one-step\ngenerators: on the CIFAR10 dataset, it achieves an FID of 2.06 for\nunconditional generation and 1.96 for class-conditional generation. Moreover,\nby applying SIM to a leading transformer-based diffusion model, we distill a\nsingle-step generator for text-to-image (T2I) generation that attains an\naesthetic score of 6.42 with no performance decline over the original\nmulti-step counterpart, clearly outperforming the other one-step generators\nincluding SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85. We\nwill release this industry-ready one-step transformer-based T2I generator along\nwith this paper.\n","authors":["Weijian Luo","Zemin Huang","Zhengyang Geng","J. Zico Kolter","Guo-jun Qi"],"pdf_url":"https://arxiv.org/pdf/2410.16794v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16790v1","updated":"2024-10-22T08:07:44Z","published":"2024-10-22T08:07:44Z","title":"Sample-Efficient Curriculum Reinforcement Learning for Complex Reward\n  Functions","summary":"  Reinforcement learning (RL) shows promise in control problems, but its\npractical application is often hindered by the complexity arising from\nintricate reward functions with constraints. While the reward hypothesis\nsuggests these competing demands can be encapsulated in a single scalar reward\nfunction, designing such functions remains challenging. Building on existing\nwork, we start by formulating preferences over trajectories to derive a\nrealistic reward function that balances goal achievement with constraint\nsatisfaction in the application of mobile robotics with dynamic obstacles. To\nmitigate reward exploitation in such complex settings, we propose a novel\ntwo-stage reward curriculum combined with a flexible replay buffer that\nadaptively samples experiences. Our approach first learns on a subset of\nrewards before transitioning to the full reward, allowing the agent to learn\ntrade-offs between objectives and constraints. After transitioning to a new\nstage, our method continues to make use of past experiences by updating their\nrewards for sample-efficient learning. We investigate the efficacy of our\napproach in robot navigation tasks and demonstrate superior performance\ncompared to baselines in terms of true reward achievement and task completion,\nunderlining its effectiveness.\n","authors":["Kilian Freitag","Kristian Ceder","Rita Laezza","Knut Åkesson","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2410.16790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16780v1","updated":"2024-10-22T07:53:41Z","published":"2024-10-22T07:53:41Z","title":"Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems","summary":"  The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task.\n","authors":["Krishna Sayana","Raghavendra Vasudeva","Yuri Vasilevski","Kun Su","Liam Hebert","Hubert Pham","Ambarish Jash","Sukhdeep Sodhi"],"pdf_url":"https://arxiv.org/pdf/2410.16780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10787v3","updated":"2024-10-22T07:51:43Z","published":"2024-08-20T12:27:53Z","title":"A Lightweight Modular Framework for Low-Cost Open-Vocabulary Object\n  Detection Training","summary":"  Object detection is a fundamental challenge in computer vision, centered on\nrecognizing objects within images, with diverse applications in areas like\nimage analysis, robotics, and autonomous vehicles. Although existing methods\nhave achieved great success, they are often constrained by a fixed vocabulary\nof objects. To overcome this limitation, approaches like MDETR have redefined\nobject detection by incorporating region-level vision-language pre-training,\nenabling open-vocabulary object detectors. However, these methods are\ncomputationally heavy due to the simultaneous training of large models for both\nvision and language representations. To address this, we introduce a\nlightweight framework that significantly reduces the number of parameters while\npreserving, or even improving, performance. Our solution is applied to MDETR,\nresulting in the development of Lightweight MDETR (LightMDETR), an optimized\nversion of MDETR designed to enhance computational efficiency without\nsacrificing accuracy. The core of our approach involves freezing the MDETR\nbackbone and training only the Universal Projection module (UP), which bridges\nvision and language representations. A learnable modality token parameter\nallows the UP to seamlessly switch between modalities. Evaluations on tasks\nlike phrase grounding, referring expression comprehension, and segmentation\nshow that LightMDETR not only reduces computational costs but also outperforms\nseveral state-of-the-art methods in terms of accuracy.\n","authors":["Bilal Faye","Binta Sow","Hanane Azzag","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2408.10787v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2004.04454v2","updated":"2024-10-22T07:40:17Z","published":"2020-04-09T09:52:49Z","title":"TensorProjection Layer: A Tensor-Based Dimension Reduction Method in\n  Deep Neural Networks","summary":"  In this paper, we propose a dimension reduction method specifically designed\nfor tensor-structured feature data in deep neural networks. The method is\nimplemented as a hidden layer, called the TensorProjection layer, which\ntransforms input tensors into output tensors with reduced dimensions through\nmode-wise projections. The projection directions are treated as model\nparameters of the layer and are optimized during model training. Our method can\nserve as an alternative to pooling layers for summarizing image data, or to\nconvolutional layers as a technique for reducing the number of channels. We\nconduct experiments on tasks such as medical image classification and\nsegmentation, integrating the TensorProjection layer into commonly used\nbaseline architectures to evaluate its effectiveness. Numerical experiments\nindicate that the proposed method can outperform traditional downsampling\nmethods, such as pooling layers, in our tasks, suggesting it as a promising\nalternative for feature summarization.\n","authors":["Toshinari Morimoto","Su-Yun Huang"],"pdf_url":"https://arxiv.org/pdf/2004.04454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16765v1","updated":"2024-10-22T07:33:34Z","published":"2024-10-22T07:33:34Z","title":"Survival Models: Proper Scoring Rule and Stochastic Optimization with\n  Competing Risks","summary":"  When dealing with right-censored data, where some outcomes are missing due to\na limited observation period, survival analysis -- known as time-to-event\nanalysis -- focuses on predicting the time until an event of interest occurs.\nMultiple classes of outcomes lead to a classification variant: predicting the\nmost likely event, a less explored area known as competing risks. Classic\ncompeting risks models couple architecture and loss, limiting scalability.To\naddress these issues, we design a strictly proper censoring-adjusted separable\nscoring rule, allowing optimization on a subset of the data as each observation\nis evaluated independently. The loss estimates outcome probabilities and\nenables stochastic optimization for competing risks, which we use for efficient\ngradient boosting trees. SurvivalBoost not only outperforms 12 state-of-the-art\nmodels across several metrics on 4 real-life datasets, both in competing risks\nand survival settings, but also provides great calibration, the ability to\npredict across any time horizon, and computation times faster than existing\nmethods.\n","authors":["Julie Alberge","Vincent Maladière","Olivier Grisel","Judith Abécassis","Gaël Varoquaux"],"pdf_url":"https://arxiv.org/pdf/2410.16765v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.14085"},{"id":"http://arxiv.org/abs/2410.16760v1","updated":"2024-10-22T07:27:20Z","published":"2024-10-22T07:27:20Z","title":"Efficient Frequency Selective Surface Analysis via End-to-End\n  Model-Based Learning","summary":"  This paper introduces an innovative end-to-end model-based deep learning\napproach for efficient electromagnetic analysis of high-dimensional frequency\nselective surfaces (FSS). Unlike traditional data-driven methods that require\nlarge datasets, this approach combines physical insights from equivalent\ncircuit models with deep learning techniques to significantly reduce model\ncomplexity and enhance prediction accuracy. Compared to previously introduced\nmodel-based learning approaches, the proposed method is trained end-to-end from\nthe physical structure of the FSS (geometric parameters) to its electromagnetic\nresponse (S-parameters). Additionally, an improvement in phase prediction\naccuracy through a modified loss function is presented. Comparisons with direct\nmodels, including deep neural networks (DNN) and radial basis function networks\n(RBFN), demonstrate the superiority of the model-based approach in terms of\ncomputational efficiency, model size, and generalization capability.\n","authors":["Cheima Hammami","Lucas Polo-López","Luc Le Magoarou"],"pdf_url":"https://arxiv.org/pdf/2410.16760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04183v2","updated":"2024-10-22T07:24:05Z","published":"2024-10-05T14:57:52Z","title":"Unsupervised Assessment of Landscape Shifts Based on Persistent Entropy\n  and Topological Preservation","summary":"  In Continual Learning (CL) contexts, concept drift typically refers to the\nanalysis of changes in data distribution. A drift in the input data can have\nnegative consequences on a learning predictor and the system's stability. The\nmajority of concept drift methods emphasize the analysis of statistical changes\nin non-stationary data over time. In this context, we consider another\nperspective, where the concept drift also integrates substantial changes in the\ntopological characteristics of the data stream. In this article, we introduce a\nnovel framework for monitoring changes in multi-dimensional data streams. We\nexplore variations in the topological structures of the data, presenting\nanother angle on the standard concept drift. Our developed approach is based on\npersistent entropy and topology-preserving projections in a continual learning\nscenario. The framework operates in both unsupervised and supervised\nenvironments. To show the utility of the proposed framework, we analyze the\nmodel across three scenarios using data streams generated with MNIST samples.\nThe obtained results reveal the potential of applying topological data analysis\nfor shift detection and encourage further research in this area.\n","authors":["Sebastian Basterrech"],"pdf_url":"https://arxiv.org/pdf/2410.04183v2.pdf","comment":"KDD'2024. Workshop on Drift Detection and Landscape Shifts"},{"id":"http://arxiv.org/abs/2410.16750v1","updated":"2024-10-22T07:12:38Z","published":"2024-10-22T07:12:38Z","title":"Theoretical Convergence Guarantees for Variational Autoencoders","summary":"  Variational Autoencoders (VAE) are popular generative models used to sample\nfrom complex data distributions. Despite their empirical success in various\nmachine learning tasks, significant gaps remain in understanding their\ntheoretical properties, particularly regarding convergence guarantees. This\npaper aims to bridge that gap by providing non-asymptotic convergence\nguarantees for VAE trained using both Stochastic Gradient Descent and Adam\nalgorithms.We derive a convergence rate of $\\mathcal{O}(\\log n / \\sqrt{n})$,\nwhere $n$ is the number of iterations of the optimization algorithm, with\nexplicit dependencies on the batch size, the number of variational samples, and\nother key hyperparameters. Our theoretical analysis applies to both Linear VAE\nand Deep Gaussian VAE, as well as several VAE variants, including $\\beta$-VAE\nand IWAE. Additionally, we empirically illustrate the impact of hyperparameters\non convergence, offering new insights into the theoretical understanding of VAE\ntraining.\n","authors":["Sobihan Surendran","Antoine Godichon-Baggioni","Sylvain Le Corff"],"pdf_url":"https://arxiv.org/pdf/2410.16750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16726v2","updated":"2024-10-22T07:05:57Z","published":"2024-05-26T23:48:30Z","title":"Exploring Edge Probability Graph Models Beyond Edge Independency:\n  Concepts, Analyses, and Algorithms","summary":"  Desirable random graph models (RGMs) should (i) generate realistic structures\nsuch as high clustering (i.e., high subgraph densities), (ii) generate variable\n(i.e., not overly similar) graphs, and (iii) remain tractable to compute and\ncontrol graph statistics. A common class of RGMs (e.g., Erd\\H{o}s-R'{e}nyi and\nstochastic Kronecker) outputs edge probabilities, and we need to realize (i.e.,\nsample from) the edge probabilities to generate graphs. Typically, each edge's\nexistence is assumed to be determined independently for simplicity and\ntractability. However, with edge independency, RGMs theoretically cannot\nproduce high subgraph densities and high output variability simultaneously. In\nthis work, we explore realization beyond edge independence that can produce\nmore realistic structures while maintaining high traceability and variability.\nTheoretically, we propose an edge-dependent realization framework called\nbinding that provably preserves output variability, and derive closed-form\ntractability results on subgraph (e.g., triangle) densities in generated\ngraphs. Practically, we propose algorithms for graph generation with binding\nand parameter fitting of binding. Our empirical results demonstrate that\nbinding exhibits high tractability and generates realistic graphs with high\nclustering, significantly improving upon existing RGMs assuming edge\nindependency.\n","authors":["Fanchen Bu","Ruochen Yang","Paul Bogdan","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2405.16726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16739v1","updated":"2024-10-22T06:46:28Z","published":"2024-10-22T06:46:28Z","title":"Corrected Soft Actor Critic for Continuous Control","summary":"  The Soft Actor-Critic (SAC) algorithm is known for its stability and high\nsample efficiency in deep reinforcement learning. However, the tanh\ntransformation applied to sampled actions in SAC distorts the action\ndistribution, hindering the selection of the most probable actions. This paper\npresents a novel action sampling method that directly identifies and selects\nthe most probable actions within the transformed distribution, thereby\naddressing this issue. Extensive experiments on standard continuous control\nbenchmarks demonstrate that the proposed method significantly enhances SAC's\nperformance, resulting in faster convergence and higher cumulative rewards\ncompared to the original algorithm.\n","authors":["Yanjun Chen","Xinming Zhang","Xianghui Wang","Zhiqiang Xu","Xiaoyu Shen","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16738v1","updated":"2024-10-22T06:46:09Z","published":"2024-10-22T06:46:09Z","title":"LLM-Assisted Red Teaming of Diffusion Models through \"Failures Are\n  Fated, But Can Be Faded\"","summary":"  In large deep neural networks that seem to perform surprisingly well on many\ntasks, we also observe a few failures related to accuracy, social biases, and\nalignment with human values, among others. Therefore, before deploying these\nmodels, it is crucial to characterize this failure landscape for engineers to\ndebug or audit models. Nevertheless, it is infeasible to exhaustively test for\nall possible combinations of factors that could lead to a model's failure. In\nthis paper, we improve the \"Failures are fated, but can be faded\" framework\n(arXiv:2406.07145)--a post-hoc method to explore and construct the failure\nlandscape in pre-trained generative models--with a variety of deep\nreinforcement learning algorithms, screening tests, and LLM-based rewards and\nstate generation. With the aid of limited human feedback, we then demonstrate\nhow to restructure the failure landscape to be more desirable by moving away\nfrom the discovered failure modes. We empirically demonstrate the effectiveness\nof the proposed method on diffusion models. We also highlight the strengths and\nweaknesses of each algorithm in identifying failure modes.\n","authors":["Som Sagar","Aditya Taparia","Ransalu Senanayake"],"pdf_url":"https://arxiv.org/pdf/2410.16738v1.pdf","comment":"13 pages, 11 figures. arXiv admin note: substantial text overlap with\n  arXiv:2406.07145"},{"id":"http://arxiv.org/abs/2410.16737v1","updated":"2024-10-22T06:46:05Z","published":"2024-10-22T06:46:05Z","title":"Interactive Residual Domain Adaptation Networks for Partial Transfer\n  Industrial Fault Diagnosis","summary":"  The partial domain adaptation (PDA) challenge is a prevalent issue in\nindustrial fault diagnosis. Current PDA approaches primarily rely on\nadversarial learning for domain adaptation and use reweighting strategies to\nexclude source samples deemed outliers. However, the transferability of\nfeatures diminishes from general feature extraction layers to higher\ntask-specific layers in adversarial learning-based adaptation modules, leading\nto significant negative transfer in PDA settings. We term this issue the\nadaptation-discrimination paradox (ADP). Furthermore, reweighting strategies\noften suffer from unreliable pseudo-labels, compromising their effectiveness.\nDrawing inspiration from traditional classification settings where such partial\nchallenge is not a concern, we propose a novel PDA framework called Interactive\nResidual Domain Adaptation Networks (IRDAN), which introduces domain-wise\nmodels for each domain to provide a new perspective for the PDA challenge. Each\ndomain-wise model is equipped with a residual domain adaptation (RDA) block to\nmitigate the ADP problem. Additionally, we introduce a confident information\nflow via an interactive learning strategy, training the modules of IRDAN\nsequentially to avoid cross-interference. We also establish a reliable stopping\ncriterion for selecting the best-performing model, ensuring practical usability\nin real-world applications. Experiments have demonstrated the superior\nperformance of the proposed IRDAN.\n","authors":["Gecheng Chen","Chengwen Luo","Jianqiang Li","Xinkai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03876v2","updated":"2024-10-22T06:36:46Z","published":"2023-12-06T19:46:06Z","title":"Scaling transformer neural networks for skillful and reliable\n  medium-range weather forecasting","summary":"  Weather forecasting is a fundamental problem for anticipating and mitigating\nthe impacts of climate change. Recently, data-driven approaches for weather\nforecasting based on deep learning have shown great promise, achieving\naccuracies that are competitive with operational systems. However, those\nmethods often employ complex, customized architectures without sufficient\nablation analysis, making it difficult to understand what truly contributes to\ntheir success. Here we introduce Stormer, a simple transformer model that\nachieves state-of-the-art performance on weather forecasting with minimal\nchanges to the standard transformer backbone. We identify the key components of\nStormer through careful empirical analyses, including weather-specific\nembedding, randomized dynamics forecast, and pressure-weighted loss. At the\ncore of Stormer is a randomized forecasting objective that trains the model to\nforecast the weather dynamics over varying time intervals. During inference,\nthis allows us to produce multiple forecasts for a target lead time and combine\nthem to obtain better forecast accuracy. On WeatherBench 2, Stormer performs\ncompetitively at short to medium-range forecasts and outperforms current\nmethods beyond 7 days, while requiring orders-of-magnitude less training data\nand compute. Additionally, we demonstrate Stormer's favorable scaling\nproperties, showing consistent improvements in forecast accuracy with increases\nin model size and training tokens. Code and checkpoints are available at\nhttps://github.com/tung-nd/stormer.\n","authors":["Tung Nguyen","Rohan Shah","Hritik Bansal","Troy Arcomano","Romit Maulik","Veerabhadra Kotamarthi","Ian Foster","Sandeep Madireddy","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2312.03876v2.pdf","comment":"Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2401.16635v3","updated":"2024-10-22T06:19:20Z","published":"2024-01-30T00:17:37Z","title":"Improving Reinforcement Learning from Human Feedback with Efficient\n  Reward Model Ensemble","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a widely adopted\napproach for aligning large language models with human values. However, RLHF\nrelies on a reward model that is trained with a limited amount of human\npreference data, which could lead to inaccurate predictions. As a result, RLHF\nmay produce outputs that are misaligned with human values. To mitigate this\nissue, we contribute a reward ensemble method that allows the reward model to\nmake more accurate predictions. As using an ensemble of large language\nmodel-based reward models can be computationally and resource-expensive, we\nexplore efficient ensemble methods including linear-layer ensemble and\nLoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy\nOptimization with our ensembled reward models, and verify that our ensemble\nmethods help improve the alignment performance of RLHF outputs.\n","authors":["Shun Zhang","Zhenfang Chen","Sunli Chen","Yikang Shen","Zhiqing Sun","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2401.16635v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16719v1","updated":"2024-10-22T05:59:29Z","published":"2024-10-22T05:59:29Z","title":"Progressive Compositionality In Text-to-Image Generative Models","summary":"  Despite the impressive text-to-image (T2I) synthesis capabilities of\ndiffusion models, they often struggle to understand compositional relationships\nbetween objects and attributes, especially in complex settings. Existing\nsolutions have tackled these challenges by optimizing the cross-attention\nmechanism or learning from the caption pairs with minimal semantic changes.\nHowever, can we generate high-quality complex contrastive images that diffusion\nmodels can directly discriminate based on visual representations? In this work,\nwe leverage large-language models (LLMs) to compose realistic, complex\nscenarios and harness Visual-Question Answering (VQA) systems alongside\ndiffusion models to automatically curate a contrastive dataset, ConPair,\nconsisting of 15k pairs of high-quality contrastive images. These pairs feature\nminimal visual discrepancies and cover a wide range of attribute categories,\nespecially complex and natural scenarios. To learn effectively from these error\ncases, i.e., hard negative images, we propose EvoGen, a new multi-stage\ncurriculum for contrastive learning of diffusion models. Through extensive\nexperiments across a wide range of compositional scenarios, we showcase the\neffectiveness of our proposed framework on compositional T2I benchmarks.\n","authors":["Xu Han","Linghao Jin","Xiaofeng Liu","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2410.16719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16718v1","updated":"2024-10-22T05:56:57Z","published":"2024-10-22T05:56:57Z","title":"Optimal Partial Graph Matching","summary":"  Partial graph matching addresses the limitations of traditional graph\nmatching by allowing some nodes to remain unmatched, making it applicable to\nmore complex scenarios. However, this flexibility introduces additional\ncomplexity, as both the subset of nodes to match and the optimal mapping must\nbe determined. While recent studies have explored deep learning techniques for\npartial graph matching, a significant limitation remains: the absence of an\noptimization objective that fully captures the problem's intrinsic nature while\nenabling efficient solutions. In this paper, we propose a novel optimization\nframework for partial graph matching, inspired by optimal partial transport.\nOur approach formulates an objective that enables partial assignments while\nincorporating matching biases, using weighted total variation as the divergence\nfunction to guarantee optimal partial assignments. We employ the Hungarian\nalgorithm to achieve efficient, exact solutions with cubic time complexity. Our\ncontributions are threefold: (i) we introduce a robust optimization objective\nthat balances matched and unmatched nodes; (ii) we establish a connection\nbetween partial graph matching and the linear sum assignment problem, enabling\nefficient solutions; (iii) we propose a deep graph matching architecture with a\nnovel partial matching loss, providing an end-to-end solution. The empirical\nevaluations on standard graph matching benchmarks demonstrate the efficacy of\nthe proposed approach.\n","authors":["Gathika Ratnayaka","James Nichols","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16713v1","updated":"2024-10-22T05:49:24Z","published":"2024-10-22T05:49:24Z","title":"Collapse or Thrive? Perils and Promises of Synthetic Data in a\n  Self-Generating World","summary":"  The increasing presence of AI-generated content on the internet raises a\ncritical question: What happens when generative machine learning models are\npretrained on web-scale datasets containing data created by earlier models?\nSome authors prophesy $\\textit{model collapse}$ under a \"$\\textit{replace}$\"\nscenario: a sequence of models, the first trained with real data and each later\none trained only on synthetic data from its preceding model. In this scenario,\nmodels successively degrade. Others see collapse as easily avoidable; in an\n\"$\\textit{accumulate}$' scenario, a sequence of models is trained, but each\ntraining uses all real and synthetic data generated so far. In this work, we\ndeepen and extend the study of these contrasting scenarios. First, collapse\nversus avoidance of collapse is studied by comparing the replace and accumulate\nscenarios on each of three prominent generative modeling settings; we find the\nsame contrast emerges in all three settings. Second, we study a compromise\nscenario; the available data remains the same as in the accumulate scenario --\nbut unlike $\\textit{accumulate}$ and like $\\textit{replace}$, each model is\ntrained using a fixed compute budget; we demonstrate that model test loss on\nreal data is larger than in the $\\textit{accumulate}$ scenario, but apparently\nplateaus, unlike the divergence seen with $\\textit{replace}$. Third, we study\nthe relative importance of cardinality and proportion of real data for avoiding\nmodel collapse. Surprisingly, we find a non-trivial interaction between real\nand synthetic data, where the value of synthetic data for reducing test loss\ndepends on the absolute quantity of real data. Our insights are particularly\nimportant when forecasting whether future frontier generative models will\ncollapse or thrive, and our results open avenues for empirically and\nmathematically studying the context-dependent value of synthetic data.\n","authors":["Joshua Kazdan","Rylan Schaeffer","Apratim Dey","Matthias Gerstgrasser","Rafael Rafailov","David L. Donoho","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2410.16713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11216v2","updated":"2024-10-22T05:45:46Z","published":"2024-04-17T10:00:56Z","title":"Position Engineering: Boosting Large Language Models through Positional\n  Information Manipulation","summary":"  The performance of large language models (LLMs) is significantly influenced\nby the quality of the prompts provided. In response, researchers have developed\nenormous prompt engineering strategies aimed at modifying the prompt text to\nenhance task performance. In this paper, we introduce a novel technique termed\nposition engineering, which offers a more efficient way to guide large language\nmodels. Unlike prompt engineering, which requires substantial effort to modify\nthe text provided to LLMs, position engineering merely involves altering the\npositional information in the prompt without modifying the text itself. We have\nevaluated position engineering in two widely-used LLM scenarios:\nretrieval-augmented generation (RAG) and in-context learning (ICL). Our\nfindings show that position engineering substantially improves upon the\nbaseline in both cases. Position engineering thus represents a promising new\nstrategy for exploiting the capabilities of large language models.\n","authors":["Zhiyuan He","Huiqiang Jiang","Zilong Wang","Yuqing Yang","Luna Qiu","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2404.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16710v1","updated":"2024-10-22T05:32:40Z","published":"2024-10-22T05:32:40Z","title":"Influential Language Data Selection via Gradient Trajectory Pursuit","summary":"  Curating a desirable dataset for training has been the core of building\nhighly capable large language models (Touvron et al., 2023; Achiam et al.,\n2023; Team et al.,2024). Gradient influence scores (Pruthi et al., 2020; Xia et\nal., 2024) are shown to be correlated with model performance and are commonly\nused as the criterion for data selection. However, existing methods are built\nupon either individual sample rankings or inefficient matching process, leading\nto suboptimal performance or scaling up issues.In this paper, we propose\nGradient Trajectory Pursuit (GTP), an algorithm that performs pursuit of\ngradient trajectories via jointly selecting data points under an L0-norm\nregularized objective. The proposed algorithm highlights: (1) joint selection\ninstead of independent top-k selection, which automatically de-duplicates\nsamples; (2) higher efficiency with compressive sampling processes, which can\nbe further sped up using a distributed framework. In the experiments, we\ndemonstrate the algorithm in both in-domain and target-domain selection\nbenchmarks and show that it outperforms top-k selection and competitive\nalgorithms consistently, for example, our algorithm chooses as low as 0.5% data\nto achieve full performance on the targeted instruction tuning tasks\n","authors":["Zhiwei Deng","Tao Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2410.16710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16709v1","updated":"2024-10-22T05:27:01Z","published":"2024-10-22T05:27:01Z","title":"Universal approximation property of ODENet and ResNet with a single\n  activation function","summary":"  We study a universal approximation property of ODENet and ResNet. The ODENet\nis a map from an initial value to the final value of an ODE system in a finite\ninterval. It is considered a mathematical model of a ResNet-type deep learning\nsystem. We consider dynamical systems with vector fields given by a single\ncomposition of the activation function and an affine mapping, which is the most\ncommon choice of the ODENet or ResNet vector field in actual machine learning\nsystems. We show that such an ODENet and ResNet with a restricted vector field\ncan uniformly approximate ODENet with a general vector field.\n","authors":["Masato Kimura","Kazunori Matsui","Yosuke Mizuno"],"pdf_url":"https://arxiv.org/pdf/2410.16709v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.16705v1","updated":"2024-10-22T05:20:21Z","published":"2024-10-22T05:20:21Z","title":"Privacy-hardened and hallucination-resistant synthetic data generation\n  with logic-solvers","summary":"  Machine-generated data is a valuable resource for training Artificial\nIntelligence algorithms, evaluating rare workflows, and sharing data under\nstricter data legislations. The challenge is to generate data that is accurate\nand private. Current statistical and deep learning methods struggle with large\ndata volumes, are prone to hallucinating scenarios incompatible with reality,\nand seldom quantify privacy meaningfully. Here we introduce Genomator, a logic\nsolving approach (SAT solving), which efficiently produces private and\nrealistic representations of the original data. We demonstrate the method on\ngenomic data, which arguably is the most complex and private information.\nSynthetic genomes hold great potential for balancing underrepresented\npopulations in medical research and advancing global data exchange. We\nbenchmark Genomator against state-of-the-art methodologies (Markov generation,\nRestricted Boltzmann Machine, Generative Adversarial Network and Conditional\nRestricted Boltzmann Machines), demonstrating an 84-93% accuracy improvement\nand 95-98% higher privacy. Genomator is also 1000-1600 times more efficient,\nmaking it the only tested method that scales to whole genomes. We show the\nuniversal trade-off between privacy and accuracy, and use Genomator's tuning\ncapability to cater to all applications along the spectrum, from provable\nprivate representations of sensitive cohorts, to datasets with\nindistinguishable pharmacogenomic profiles. Demonstrating the production-scale\ngeneration of tuneable synthetic data can increase trust and pave the way into\nthe clinic.\n","authors":["Mark A. Burgess","Brendan Hosking","Roc Reguant","Anubhav Kaphle","Mitchell J. O'Brien","Letitia M. F. Sng","Yatish Jain","Denis C. Bauer"],"pdf_url":"https://arxiv.org/pdf/2410.16705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06682v2","updated":"2024-10-22T05:12:24Z","published":"2023-12-09T07:08:00Z","title":"Learning to Denoise Biomedical Knowledge Graph for Robust Molecular\n  Interaction Prediction","summary":"  Molecular interaction prediction plays a crucial role in forecasting unknown\ninteractions between molecules, such as drug-target interaction (DTI) and\ndrug-drug interaction (DDI), which are essential in the field of drug discovery\nand therapeutics. Although previous prediction methods have yielded promising\nresults by leveraging the rich semantics and topological structure of\nbiomedical knowledge graphs (KGs), they have primarily focused on enhancing\npredictive performance without addressing the presence of inevitable noise and\ninconsistent semantics. This limitation has hindered the advancement of\nKG-based prediction methods. To address this limitation, we propose BioKDN\n(Biomedical Knowledge Graph Denoising Network) for robust molecular interaction\nprediction. BioKDN refines the reliable structure of local subgraphs by\ndenoising noisy links in a learnable manner, providing a general module for\nextracting task-relevant interactions. To enhance the reliability of the\nrefined structure, BioKDN maintains consistent and robust semantics by\nsmoothing relations around the target interaction. By maximizing the mutual\ninformation between reliable structure and smoothed relations, BioKDN\nemphasizes informative semantics to enable precise predictions. Experimental\nresults on real-world datasets show that BioKDN surpasses state-of-the-art\nmodels in DTI and DDI prediction tasks, confirming the effectiveness and\nrobustness of BioKDN in denoising unreliable interactions within contaminated\nKGs\n","authors":["Tengfei Ma","Yujie Chen","Wen Tao","Dashun Zheng","Xuan Lin","Patrick Cheong-lao Pang","Yiping Liu","Yijun Wang","Longyue Wang","Bosheng Song","Xiangxiang Zeng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.06682v2.pdf","comment":"13 pages, Accepted at TKDE"},{"id":"http://arxiv.org/abs/2410.16701v1","updated":"2024-10-22T05:12:19Z","published":"2024-10-22T05:12:19Z","title":"ClimaQA: An Automated Evaluation Framework for Climate Foundation Models","summary":"  The use of foundation models in climate science has recently gained\nsignificant attention. However, a critical issue remains: the lack of a\ncomprehensive evaluation framework capable of assessing the quality and\nscientific validity of model outputs. To address this issue, we develop\nClimaGen (Climate QA Generator), an automated algorithmic framework that\ngenerates question-answer pairs from graduate textbooks with climate scientists\nin the loop. As a result, we present ClimaQA-Gold, an expert-annotated\nbenchmark dataset alongside ClimaQA-Silver, a large-scale, comprehensive\nsynthetic QA dataset for climate science. Finally, we develop evaluation\nstrategies and compare different Large Language Models (LLMs) on our\nbenchmarks. Our results offer novel insights into various approaches used to\nenhance climate foundation models.\n","authors":["Veeramakali Vignesh Manivannan","Yasaman Jafari","Srikar Eranky","Spencer Ho","Rose Yu","Duncan Watson-Parris","Yian Ma","Leon Bergen","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2410.16701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16699v1","updated":"2024-10-22T05:11:45Z","published":"2024-10-22T05:11:45Z","title":"Graph Transformers Dream of Electric Flow","summary":"  We show theoretically and empirically that the linear Transformer, when\napplied to graph data, can implement algorithms that solve canonical problems\nsuch as electric flow and eigenvector decomposition. The input to the\nTransformer is simply the graph incidence matrix; no other explicit positional\nencoding information is provided. We present explicit weight configurations for\nimplementing each such graph algorithm, and we bound the errors of the\nconstructed Transformers by the errors of the underlying algorithms. Our\ntheoretical findings are corroborated by experiments on synthetic data.\nAdditionally, on a real-world molecular regression task, we observe that the\nlinear Transformer is capable of learning a more effective positional encoding\nthan the default one based on Laplacian eigenvectors. Our work is an initial\nstep towards elucidating the inner-workings of the Transformer for graph data.\n","authors":["Xiang Cheng","Lawrence Carin","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2410.16699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16698v1","updated":"2024-10-22T05:07:30Z","published":"2024-10-22T05:07:30Z","title":"Hyperboloid GPLVM for Discovering Continuous Hierarchies via\n  Nonparametric Estimation","summary":"  Dimensionality reduction (DR) offers a useful representation of complex\nhigh-dimensional data. Recent DR methods focus on hyperbolic geometry to derive\na faithful low-dimensional representation of hierarchical data. However,\nexisting methods are based on neighbor embedding, frequently ruining the\ncontinual relation of the hierarchies. This paper presents hyperboloid Gaussian\nprocess (GP) latent variable models (hGP-LVMs) to embed high-dimensional\nhierarchical data with implicit continuity via nonparametric estimation. We\nadopt generative modeling using the GP, which brings effective hierarchical\nembedding and executes ill-posed hyperparameter tuning. This paper presents\nthree variants that employ original point, sparse point, and Bayesian\nestimations. We establish their learning algorithms by incorporating the\nRiemannian optimization and active approximation scheme of GP-LVM. For Bayesian\ninference, we further introduce the reparameterization trick to realize\nBayesian latent variable learning. In the last part of this paper, we apply\nhGP-LVMs to several datasets and show their ability to represent\nhigh-dimensional hierarchies in low-dimensional spaces.\n","authors":["Koshi Watanabe","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2410.16698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13253v2","updated":"2024-10-22T04:58:21Z","published":"2024-10-17T06:20:43Z","title":"FDF: Flexible Decoupled Framework for Time Series Forecasting with\n  Conditional Denoising and Polynomial Modeling","summary":"  Time series forecasting is vital in numerous web applications, influencing\ncritical decision-making across industries. While diffusion models have\nrecently gained increasing popularity for this task, we argue they suffer from\na significant drawback: indiscriminate noise addition to the original time\nseries followed by denoising, which can obscure underlying dynamic evolving\ntrend and complicate forecasting. To address this limitation, we propose a\nnovel flexible decoupled framework (FDF) that learns high-quality time series\nrepresentations for enhanced forecasting performance. A key characteristic of\nour approach leverages the inherent inductive bias of time series data by\ndecomposing it into trend and seasonal components, each modeled separately to\nenable decoupled analysis and modeling. Specifically, we propose an innovative\nConditional Denoising Seasonal Module (CDSM) within the diffusion model, which\nleverages statistical information from the historical window to conditionally\nmodel the complex seasonal component. Notably, we incorporate a Polynomial\nTrend Module (PTM) to effectively capture the smooth trend component, thereby\nenhancing the model's ability to represent temporal dependencies. Extensive\nexperiments validate the effectiveness of our framework, demonstrating superior\nperformance over existing methods and higlighting its flexibility in time\nseries forecasting. We hope our work can bring a new perspective for time\nseries forecasting. We intend to make our code publicly available as\nopen-source in the future.\n","authors":["Jintao Zhang","Mingyue Cheng","Xiaoyu Tao","Zhiding Liu","Daoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13253v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16694v1","updated":"2024-10-22T04:55:12Z","published":"2024-10-22T04:55:12Z","title":"Governing equation discovery of a complex system from snapshots","summary":"  Complex systems in physics, chemistry, and biology that evolve over time with\ninherent randomness are typically described by stochastic differential\nequations (SDEs). A fundamental challenge in science and engineering is to\ndetermine the governing equations of a complex system from snapshot data.\nTraditional equation discovery methods often rely on stringent assumptions,\nsuch as the availability of the trajectory information or time-series data, and\nthe presumption that the underlying system is deterministic. In this work, we\nintroduce a data-driven, simulation-free framework, called Sparse\nIdentification of Differential Equations from Snapshots (SpIDES), that\ndiscovers the governing equations of a complex system from snapshots by\nutilizing the advanced machine learning techniques to perform three essential\nsteps: probability flow reconstruction, probability density estimation, and\nBayesian sparse identification. We validate the effectiveness and robustness of\nSpIDES by successfully identifying the governing equation of an over-damped\nLangevin system confined within two potential wells. By extracting\ninterpretable drift and diffusion terms from the SDEs, our framework provides\ndeeper insights into system dynamics, enhances predictive accuracy, and\nfacilitates more effective strategies for managing and simulating stochastic\nsystems.\n","authors":["Qunxi Zhu","Bolin Zhao","Jingdong Zhang","Peiyang Li","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2410.16694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16692v1","updated":"2024-10-22T04:45:47Z","published":"2024-10-22T04:45:47Z","title":"Lower Bounds for Time-Varying Kernelized Bandits","summary":"  The optimization of black-box functions with noisy observations is a\nfundamental problem with widespread applications, and has been widely studied\nunder the assumption that the function lies in a reproducing kernel Hilbert\nspace (RKHS). This problem has been studied extensively in the stationary\nsetting, and near-optimal regret bounds are known via developments in both\nupper and lower bounds. In this paper, we consider non-stationary scenarios,\nwhich are crucial for certain applications but are currently less\nwell-understood. Specifically, we provide the first algorithm-independent lower\nbounds, where the time variations are subject satisfying a total variation\nbudget according to some function norm. Under $\\ell_{\\infty}$-norm variations,\nour bounds are found to be close to the state-of-the-art upper bound (Hong\n\\emph{et al.}, 2023). Under RKHS norm variations, the upper and lower bounds\nare still reasonably close but with more of a gap, raising the interesting open\nquestion of whether non-minor improvements in the upper bound are possible.\n","authors":["Xu Cai","Jonathan Scarlett"],"pdf_url":"https://arxiv.org/pdf/2410.16692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14568v4","updated":"2024-10-22T04:40:48Z","published":"2022-11-26T13:48:05Z","title":"BeGin: Extensive Benchmark Scenarios and An Easy-to-use Framework for\n  Graph Continual Learning","summary":"  Continual Learning (CL) is the process of learning ceaselessly a sequence of\ntasks. Most existing CL methods deal with independent data (e.g., images and\ntext) for which many benchmark frameworks and results under standard\nexperimental settings are available. Compared to them, however, CL methods for\ngraph data (graph CL) are relatively underexplored because of (a) the lack of\nstandard experimental settings, especially regarding how to deal with the\ndependency between instances, (b) the lack of benchmark datasets and scenarios,\nand (c) high complexity in implementation and evaluation due to the dependency.\nIn this paper, regarding (a) we define four standard incremental settings\n(task-, class-, domain-, and time-incremental) for node-, link-, and\ngraph-level problems, extending the previously explored scope. Regarding (b),\nwe provide 35 benchmark scenarios based on 24 real-world graphs. Regarding (c),\nwe develop BeGin, an easy and fool-proof framework for graph CL. BeGin is\neasily extended since it is modularized with reusable modules for data\nprocessing, algorithm design, and evaluation. Especially, the evaluation module\nis completely separated from user code to eliminate potential mistakes.\nRegarding benchmark results, we cover 3x more combinations of incremental\nsettings and levels of problems than the latest benchmark. All assets for the\nbenchmark framework are publicly available at\nhttps://github.com/ShinhwanKang/BeGin.\n","authors":["Jihoon Ko","Shinhwan Kang","Taehyung Kwon","Heechan Moon","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2211.14568v4.pdf","comment":"Full version of the ACM TIST paper with the same title"},{"id":"http://arxiv.org/abs/2406.07658v2","updated":"2024-10-22T04:29:19Z","published":"2024-06-11T18:59:24Z","title":"Treeffuser: Probabilistic Predictions via Conditional Diffusions with\n  Gradient-Boosted Trees","summary":"  Probabilistic prediction aims to compute predictive distributions rather than\nsingle point predictions. These distributions enable practitioners to quantify\nuncertainty, compute risk, and detect outliers. However, most probabilistic\nmethods assume parametric responses, such as Gaussian or Poisson distributions.\nWhen these assumptions fail, such models lead to bad predictions and poorly\ncalibrated uncertainty. In this paper, we propose Treeffuser, an easy-to-use\nmethod for probabilistic prediction on tabular data. The idea is to learn a\nconditional diffusion model where the score function is estimated using\ngradient-boosted trees. The conditional diffusion model makes Treeffuser\nflexible and non-parametric, while the gradient-boosted trees make it robust\nand easy to train on CPUs. Treeffuser learns well-calibrated predictive\ndistributions and can handle a wide range of regression tasks -- including\nthose with multivariate, multimodal, and skewed responses. We study Treeffuser\non synthetic and real data and show that it outperforms existing methods,\nproviding better calibrated probabilistic predictions. We further demonstrate\nits versatility with an application to inventory allocation under uncertainty\nusing sales data from Walmart. We implement Treeffuser in\nhttps://github.com/blei-lab/treeffuser.\n","authors":["Nicolas Beltran-Velez","Alessandro Antonio Grande","Achille Nazaret","Alp Kucukelbir","David Blei"],"pdf_url":"https://arxiv.org/pdf/2406.07658v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16682v1","updated":"2024-10-22T04:27:03Z","published":"2024-10-22T04:27:03Z","title":"Methods of improving LLM training stability","summary":"  Training stability of large language models(LLMs) is an important research\ntopic. Reproducing training instabilities can be costly, so we use a small\nlanguage model with 830M parameters and experiment with higher learning rates\nto force models to diverge. One of the sources of training instability is the\ngrowth of logits in attention layers. We extend the focus of the previous work\nand look not only at the magnitude of the logits but at all outputs of linear\nlayers in the Transformer block. We observe that with a high learning rate the\nL2 norm of all linear layer outputs can grow with each training step and the\nmodel diverges. Specifically we observe that QKV, Proj and FC2 layers have the\nlargest growth of the output magnitude. This prompts us to explore several\noptions: 1) apply layer normalization not only after QK layers but also after\nProj and FC2 layers too; 2) apply layer normalization after the QKV layer (and\nremove pre normalization). 3) apply QK layer normalization together with\nsoftmax capping. We show that with the last two methods we can increase\nlearning rate by 1.5x (without model divergence) in comparison to an approach\nbased on QK layer normalization only. Also we observe significant perplexity\nimprovements for all three methods in comparison to the baseline model.\n","authors":["Oleg Rybakov","Mike Chrzanowski","Peter Dykas","Jinze Xue","Ben Lanir"],"pdf_url":"https://arxiv.org/pdf/2410.16682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11793v4","updated":"2024-10-22T04:15:49Z","published":"2024-02-19T02:48:40Z","title":"Generative Kaleidoscopic Networks","summary":"  We discovered that the neural networks, especially the deep ReLU networks,\ndemonstrate an `over-generalization' phenomenon. That is, the output values for\nthe inputs that were not seen during training are mapped close to the output\nrange that were observed during the learning process. In other words, the\nneural networks learn a many-to-one mapping and this effect is more prominent\nas we increase the number of layers or the depth of the neural network. We\nutilize this property of neural networks to design a dataset kaleidoscope,\ntermed as `Generative Kaleidoscopic Networks'. Succinctly, if we learn a model\nto map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$,\nthe proposed `Kaleidoscopic sampling' procedure starts with a random input\nnoise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots\nf_\\mathcal{N}(z)\\cdots )$. After a burn-in period duration, we start observing\nsamples from the input distribution and the quality of samples recovered\nimproves as we increase the depth of the model. Scope: We observed this\nphenomenon to various degrees for the other deep learning architectures like\nCNNs, Transformers & U-Nets and we are currently investigating them further.\n","authors":["Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2402.11793v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16673v1","updated":"2024-10-22T04:13:55Z","published":"2024-10-22T04:13:55Z","title":"Efficient Antibody Structure Refinement Using Energy-Guided SE(3) Flow\n  Matching","summary":"  Antibodies are proteins produced by the immune system that recognize and bind\nto specific antigens, and their 3D structures are crucial for understanding\ntheir binding mechanism and designing therapeutic interventions. The\nspecificity of antibody-antigen binding predominantly depends on the\ncomplementarity-determining regions (CDR) within antibodies. Despite recent\nadvancements in antibody structure prediction, the quality of predicted CDRs\nremains suboptimal. In this paper, we develop a novel antibody structure\nrefinement method termed FlowAB based on energy-guided flow matching. FlowAB\nadopts the powerful deep generative method SE(3) flow matching and\nsimultaneously incorporates important physical prior knowledge into the flow\nmodel to guide the generation process. The extensive experiments demonstrate\nthat FlowAB can significantly improve the antibody CDR structures. It achieves\nnew state-of-the-art performance on the antibody structure prediction task when\nused in conjunction with an appropriate prior model while incurring only\nmarginal computational overhead. This advantage makes FlowAB a practical tool\nin antibody engineering.\n","authors":["Jiying Zhang","Zijing Liu","Shengyuan Bai","He Cao","Yu Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16673v1.pdf","comment":"BIBM 2024 regular paper"},{"id":"http://arxiv.org/abs/2404.01617v2","updated":"2024-10-22T04:09:15Z","published":"2024-04-02T03:43:55Z","title":"Designing Network Algorithms via Large Language Models","summary":"  We introduce NADA, the first framework to autonomously design network\nalgorithms by leveraging the generative capabilities of large language models\n(LLMs). Starting with an existing algorithm implementation, NADA enables LLMs\nto create a wide variety of alternative designs in the form of code blocks. It\nthen efficiently identifies the top-performing designs through a series of\nfiltering techniques, minimizing the need for full-scale evaluations and\nsignificantly reducing computational costs. Using adaptive bitrate (ABR)\nstreaming as a case study, we demonstrate that NADA produces novel ABR\nalgorithms -- previously unknown to human developers -- that consistently\noutperform the original algorithm in diverse network environments, including\nbroadband, satellite, 4G, and 5G.\n","authors":["Zhiyuan He","Aashish Gottipati","Lili Qiu","Xufang Luo","Kenuo Xu","Yuqing Yang","Francis Y. Yan"],"pdf_url":"https://arxiv.org/pdf/2404.01617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14932v2","updated":"2024-10-22T04:03:38Z","published":"2024-10-19T01:36:51Z","title":"Can AI weather models predict out-of-distribution gray swan tropical\n  cyclones?","summary":"  Predicting gray swan weather extremes, which are possible but so rare that\nthey are absent from the training dataset, is a major concern for AI\nweather/climate models. An important open question is whether AI models can\nextrapolate from weaker weather events present in the training set to stronger,\nunseen weather extremes. To test this, we train independent versions of the AI\nmodel FourCastNet on the 1979-2015 ERA5 dataset with all data, or with Category\n3-5 tropical cyclones (TCs) removed, either globally or only over the North\nAtlantic or Western Pacific basin. We then test these versions of FourCastNet\non 2018-2023 Category 5 TCs (gray swans). All versions yield similar accuracy\nfor global weather, but the one trained without Category 3-5 TCs cannot\naccurately forecast Category 5 TCs, indicating that these models cannot\nextrapolate from weaker storms. The versions trained without Category 3-5 TCs\nin one basin show some skill forecasting Category 5 TCs in that basin,\nsuggesting that FourCastNet can generalize across tropical basins. This is\nencouraging and surprising because regional information is implicitly encoded\nin inputs. No version satisfies gradient-wind balance, implying that enforcing\nsuch physical constraints may not improve generalizability to gray swans. Given\nthat current state-of-the-art AI weather/climate models have similar learning\nstrategies, we expect our findings to apply to other models and extreme events.\nOur work demonstrates that novel learning strategies are needed for AI\nweather/climate models to provide early warning or estimated statistics for the\nrarest, most impactful weather extremes.\n","authors":["Y. Qiang Sun","Pedram Hassanzadeh","Mohsen Zand","Ashesh Chattopadhyay","Jonathan Weare","Dorian S. Abbot"],"pdf_url":"https://arxiv.org/pdf/2410.14932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16670v1","updated":"2024-10-22T03:59:53Z","published":"2024-10-22T03:59:53Z","title":"CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing","summary":"  Sequential reasoning in agent systems has been significantly advanced by\nlarge language models (LLMs), yet existing approaches face limitations.\nReflection-driven reasoning relies solely on knowledge in pretrained models,\nlimiting performance in novel scenarios, while experience-assisted reasoning\noften depends on external experiences and lacks clear principles for selecting\nrepresentative experiences. We address these limitations by proposing CoPS\n(Cross-Task Experience Sharing), a generalizable algorithm that enhances\nsequential reasoning by cross-task experience sharing and selection. In detail,\nCoPS leverages agents' experiences on previous tasks, selecting\ndistribution-matched experiences via a provable pessimism-based strategy to\nmaximize utility while minimizing risks from distribution shifts. Extensive\nexperimental results on benchmarks like Alfworld, Webshop, and HotPotQA\ndemonstrate that CoPS consistently outperforms state-of-the-art baselines, with\nsuperior sample efficiency suitable for resource-constrained scenarios.\nTheoretically, we show that the performance of our algorithm depends on both\nthe quality of the pretrained LLM and the matching between the agent's\ntask-dependent trial distribution and that generated by the LLM. Our work\nbridges the gap between existing sequential reasoning paradigms and validates\nthe effectiveness of leveraging cross-task experiences, shedding light on the\npotential to improve agents' generalization and adaptability across diverse\ntasks. Our codes are available at\n$\\href{https://github.com/uclaml/COPS}{\\text{https://github.com/uclaml/COPS}}$.\n","authors":["Chen Yang","Chenyang Zhao","Quanquan Gu","Dongruo Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.16670v1.pdf","comment":"25 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2410.16669v1","updated":"2024-10-22T03:54:52Z","published":"2024-10-22T03:54:52Z","title":"Linear Partial Gromov-Wasserstein Embedding","summary":"  The Gromov Wasserstein (GW) problem, a variant of the classical optimal\ntransport (OT) problem, has attracted growing interest in the machine learning\nand data science communities due to its ability to quantify similarity between\nmeasures in different metric spaces. However, like the classical OT problem, GW\nimposes an equal mass constraint between measures, which restricts its\napplication in many machine learning tasks. To address this limitation, the\npartial Gromov-Wasserstein (PGW) problem has been introduced, which relaxes the\nequal mass constraint, enabling the comparison of general positive Radon\nmeasures. Despite this, both GW and PGW face significant computational\nchallenges due to their non-convex nature. To overcome these challenges, we\npropose the linear partial Gromov-Wasserstein (LPGW) embedding, a linearized\nembedding technique for the PGW problem. For $K$ different metric measure\nspaces, the pairwise computation of the PGW distance requires solving the PGW\nproblem $\\mathcal{O}(K^2)$ times. In contrast, the proposed linearization\ntechnique reduces this to $\\mathcal{O}(K)$ times. Similar to the linearization\ntechnique for the classical OT problem, we prove that LPGW defines a valid\nmetric for metric measure spaces. Finally, we demonstrate the effectiveness of\nLPGW in practical applications such as shape retrieval and learning with\ntransport-based embeddings, showing that LPGW preserves the advantages of PGW\nin partial matching while significantly enhancing computational efficiency.\n","authors":["Yikun Bai","Abihith Kothapalli","Hengrong Du","Rocio Diaz Martin","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2410.16669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11680v3","updated":"2024-10-22T03:46:41Z","published":"2023-09-20T23:24:22Z","title":"Federated Learning with Neural Graphical Models","summary":"  Federated Learning (FL) addresses the need to create models based on\nproprietary data in such a way that multiple clients retain exclusive control\nover their data, while all benefit from improved model accuracy due to pooled\nresources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic\nGraphical models that utilize the expressive power of neural networks to learn\ncomplex non-linear dependencies between the input features. They learn to\ncapture the underlying data distribution and have efficient algorithms for\ninference and sampling. We develop a FL framework which maintains a global NGM\nmodel that learns the averaged information from the local NGM models while\nkeeping the training data within the client's environment. Our design, FedNGMs,\navoids the pitfalls and shortcomings of neuron matching frameworks like\nFederated Matched Averaging that suffers from model parameter explosion. Our\nglobal model size remains constant throughout the process. In the cases where\nclients have local variables that are not part of the combined global\ndistribution, we propose a `Stitching' algorithm, which personalizes the global\nNGM models by merging the additional variables using the client's data. FedNGM\nis robust to data heterogeneity, large number of participants, and limited\ncommunication bandwidth. We experimentally demonstrated the use of FedNGMs for\nextracting insights from CDC's Infant Mortality dataset and discuss interesting\nfuture applications.\n","authors":["Urszula Chajewska","Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2309.11680v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16666v1","updated":"2024-10-22T03:39:21Z","published":"2024-10-22T03:39:21Z","title":"QuasiNav: Asymmetric Cost-Aware Navigation Planning with Constrained\n  Quasimetric Reinforcement Learning","summary":"  Autonomous navigation in unstructured outdoor environments is inherently\nchallenging due to the presence of asymmetric traversal costs, such as varying\nenergy expenditures for uphill versus downhill movement. Traditional\nreinforcement learning methods often assume symmetric costs, which can lead to\nsuboptimal navigation paths and increased safety risks in real-world scenarios.\nIn this paper, we introduce QuasiNav, a novel reinforcement learning framework\nthat integrates quasimetric embeddings to explicitly model asymmetric costs and\nguide efficient, safe navigation. QuasiNav formulates the navigation problem as\na constrained Markov decision process (CMDP) and employs quasimetric embeddings\nto capture directionally dependent costs, allowing for a more accurate\nrepresentation of the terrain. This approach is combined with adaptive\nconstraint tightening within a constrained policy optimization framework to\ndynamically enforce safety constraints during learning. We validate QuasiNav\nacross three challenging navigation scenarios-undulating terrains, asymmetric\nhill traversal, and directionally dependent terrain traversal-demonstrating its\neffectiveness in both simulated and real-world environments. Experimental\nresults show that QuasiNav significantly outperforms conventional methods,\nachieving higher success rates, improved energy efficiency, and better\nadherence to safety constraints.\n","authors":["Jumman Hossain","Abu-Zaher Faridee","Derrik Asher","Jade Freeman","Theron Trout","Timothy Gregory","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2410.16666v1.pdf","comment":"Under Review for ICRA 2025"},{"id":"http://arxiv.org/abs/2410.06648v4","updated":"2024-10-22T03:35:54Z","published":"2024-10-09T08:00:12Z","title":"Q-WSL: Optimizing Goal-Conditioned RL with Weighted Supervised Learning\n  via Dynamic Programming","summary":"  A novel class of advanced algorithms, termed Goal-Conditioned Weighted\nSupervised Learning (GCWSL), has recently emerged to tackle the challenges\nposed by sparse rewards in goal-conditioned reinforcement learning (RL). GCWSL\nconsistently delivers strong performance across a diverse set of goal-reaching\ntasks due to its simplicity, effectiveness, and stability. However, GCWSL\nmethods lack a crucial capability known as trajectory stitching, which is\nessential for learning optimal policies when faced with unseen skills during\ntesting. This limitation becomes particularly pronounced when the replay buffer\nis predominantly filled with sub-optimal trajectories. In contrast, traditional\nTD-based RL methods, such as Q-learning, which utilize Dynamic Programming, do\nnot face this issue but often experience instability due to the inherent\ndifficulties in value function approximation. In this paper, we propose\nQ-learning Weighted Supervised Learning (Q-WSL), a novel framework designed to\novercome the limitations of GCWSL by incorporating the strengths of Dynamic\nProgramming found in Q-learning. Q-WSL leverages Dynamic Programming results to\noutput the optimal action of (state, goal) pairs across different trajectories\nwithin the replay buffer. This approach synergizes the strengths of both\nQ-learning and GCWSL, effectively mitigating their respective weaknesses and\nenhancing overall performance. Empirical evaluations on challenging\ngoal-reaching tasks demonstrate that Q-WSL surpasses other goal-conditioned\napproaches in terms of both performance and sample efficiency. Additionally,\nQ-WSL exhibits notable robustness in environments characterized by binary\nreward structures and environmental stochasticity.\n","authors":["Xing Lei","Xuetao Zhang","Zifeng Zhuang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06648v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16663v1","updated":"2024-10-22T03:29:33Z","published":"2024-10-22T03:29:33Z","title":"FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs","summary":"  FlashAttention series has been widely applied in the inference of large\nlanguage models (LLMs). However, FlashAttention series only supports the\nhigh-level GPU architectures, e.g., Ampere and Hopper. At present,\nFlashAttention series is not easily transferrable to NPUs and low-resource\nGPUs. Moreover, FlashAttention series is inefficient for multi- NPUs or GPUs\ninference scenarios. In this work, we propose FastAttention which pioneers the\nadaptation of FlashAttention series for NPUs and low-resource GPUs to boost LLM\ninference efficiency. Specifically, we take Ascend NPUs and Volta-based GPUs as\nrepresentatives for designing our FastAttention. We migrate FlashAttention\nseries to Ascend NPUs by proposing a novel two-level tiling strategy for\nruntime speedup, tiling-mask strategy for memory saving and the\ntiling-AllReduce strategy for reducing communication overhead, respectively.\nBesides, we adapt FlashAttention for Volta-based GPUs by redesigning the\noperands layout in shared memory and introducing a simple yet effective CPU-GPU\ncooperative strategy for efficient memory utilization. On Ascend NPUs, our\nFastAttention can achieve a 10.7$\\times$ speedup compared to the standard\nattention implementation. Llama-7B within FastAttention reaches up to\n5.16$\\times$ higher throughput than within the standard attention. On Volta\narchitecture GPUs, FastAttention yields 1.43$\\times$ speedup compared to its\nequivalents in \\texttt{xformers}. Pangu-38B within FastAttention brings\n1.46$\\times$ end-to-end speedup using FasterTransformer. Coupled with the\npropose CPU-GPU cooperative strategy, FastAttention supports a maximal input\nlength of 256K on 8 V100 GPUs. All the codes will be made available soon.\n","authors":["Haoran Lin","Xianzhi Yu","Kang Zhao","Lu Hou","Zongyuan Zhan","Stanislav Kamenev","Han Bao","Ting Hu","Mingkai Wang","Qixin Chang","Siyue Sui","Weihao Sun","Jiaxin Hu","Jun Yao","Zekun Yin","Cheng Qian","Ying Zhang","Yinfei Pan","Yu Yang","Weiguo Liu"],"pdf_url":"https://arxiv.org/pdf/2410.16663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16659v1","updated":"2024-10-22T03:21:59Z","published":"2024-10-22T03:21:59Z","title":"RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary\n  Detection in Partially Machine Generated Texts","summary":"  With increasing usage of generative models for text generation and widespread\nuse of machine generated texts in various domains, being able to distinguish\nbetween human written and machine generated texts is a significant challenge.\nWhile existing models and proprietary systems focus on identifying whether\ngiven text is entirely human written or entirely machine generated, only a few\nsystems provide insights at sentence or paragraph level at likelihood of being\nmachine generated at a non reliable accuracy level, working well only for a set\nof domains and generators. This paper introduces few reliable approaches for\nthe novel task of identifying which part of a given text is machine generated\nat a word level while comparing results from different approaches and methods.\nWe present a comparison with proprietary systems , performance of our model on\nunseen domains' and generators' texts. The findings reveal significant\nimprovements in detection accuracy along with comparison on other aspects of\ndetection capabilities. Finally we discuss potential avenues for improvement\nand implications of our work. The proposed model is also well suited for\ndetecting which parts of a text are machine generated in outputs of Instruct\nvariants of many LLMs.\n","authors":["Ram Mohan Rao Kadiyala"],"pdf_url":"https://arxiv.org/pdf/2410.16659v1.pdf","comment":"published at naacl 2024"},{"id":"http://arxiv.org/abs/2410.15212v2","updated":"2024-10-22T03:19:43Z","published":"2024-10-19T20:57:16Z","title":"Boardwalk Empire: How Generative AI is Revolutionizing Economic\n  Paradigms","summary":"  The relentless pursuit of technological advancements has ushered in a new era\nwhere artificial intelligence (AI) is not only a powerful tool but also a\ncritical economic driver. At the forefront of this transformation is Generative\nAI, which is catalyzing a paradigm shift across industries. Deep generative\nmodels, an integration of generative and deep learning techniques, excel in\ncreating new data beyond analyzing existing ones, revolutionizing sectors from\nproduction and manufacturing to finance. By automating design, optimization,\nand innovation cycles, Generative AI is reshaping core industrial processes. In\nthe financial sector, it is transforming risk assessment, trading strategies,\nand forecasting, demonstrating its profound impact. This paper explores the\nsweeping changes driven by deep learning models like Large Language Models\n(LLMs), highlighting their potential to foster innovative business models,\ndisruptive technologies, and novel economic landscapes. As we stand at the\nthreshold of an AI-driven economic era, Generative AI is emerging as a pivotal\nforce, driving innovation, disruption, and economic evolution on a global\nscale.\n","authors":["Subramanyam Sahoo","Kamlesh Dutta"],"pdf_url":"https://arxiv.org/pdf/2410.15212v2.pdf","comment":"25 pages, 8 figures, Accepted at National Conference on Advances in\n  Marketing Paradigms for Research, Innovation and Technology (AMRIT 2023)"},{"id":"http://arxiv.org/abs/2302.13582v3","updated":"2024-10-22T03:19:38Z","published":"2023-02-27T08:40:45Z","title":"Neural Graph Revealers","summary":"  Sparse graph recovery methods work well where the data follows their\nassumptions but often they are not designed for doing downstream probabilistic\nqueries. This limits their adoption to only identifying connections among the\ninput variables. On the other hand, the Probabilistic Graphical Models (PGMs)\nassume an underlying base graph between variables and learns a distribution\nover them. PGM design choices are carefully made such that the inference \\&\nsampling algorithms are efficient. This brings in certain restrictions and\noften simplifying assumptions. In this work, we propose Neural Graph Revealers\n(NGRs), that are an attempt to efficiently merge the sparse graph recovery\nmethods with PGMs into a single flow. The problem setting consists of an input\ndata X with D features and M samples and the task is to recover a sparse graph\nshowing connection between the features and jointly learn a probability\ndistribution over them. NGRs view the neural networks as a `glass box' or more\nspecifically as a multitask learning framework. We introduce `Graph-constrained\npath norm' that NGRs leverage to learn a graphical model that captures complex\nnon-linear functional dependencies between the features in the form of an\nundirected sparse graph. Furthermore, NGRs can handle multimodal inputs like\nimages, text, categorical data, embeddings etc. which is not straightforward to\nincorporate in the existing methods. We show experimental results of doing\nsparse graph recovery and probabilistic inference on data from Gaussian\ngraphical models and a multimodal infant mortality dataset by Centers for\nDisease Control and Prevention.\n","authors":["Harsh Shrivastava","Urszula Chajewska"],"pdf_url":"https://arxiv.org/pdf/2302.13582v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06412v2","updated":"2024-10-22T03:11:31Z","published":"2024-10-08T22:48:39Z","title":"Stochastic Sparse Sampling: A Framework for Variable-Length Medical Time\n  Series Classification","summary":"  While the majority of time series classification research has focused on\nmodeling fixed-length sequences, variable-length time series classification\n(VTSC) remains critical in healthcare, where sequence length may vary among\npatients and events. To address this challenge, we propose\n$\\textbf{S}$tochastic $\\textbf{S}$parse $\\textbf{S}$ampling (SSS), a novel VTSC\nframework developed for medical time series. SSS manages variable-length\nsequences by sparsely sampling fixed windows to compute local predictions,\nwhich are then aggregated and calibrated to form a global prediction. We apply\nSSS to the task of seizure onset zone (SOZ) localization, a critical VTSC\nproblem requiring identification of seizure-inducing brain regions from\nvariable-length electrophysiological time series. We evaluate our method on the\nEpilepsy iEEG Multicenter Dataset, a heterogeneous collection of intracranial\nelectroencephalography (iEEG) recordings obtained from four independent medical\ncenters. SSS demonstrates superior performance compared to state-of-the-art\n(SOTA) baselines across most medical centers, and superior performance on all\nout-of-distribution (OOD) unseen medical centers. Additionally, SSS naturally\nprovides post-hoc insights into local signal characteristics related to the\nSOZ, by visualizing temporally averaged local predictions throughout the\nsignal.\n","authors":["Xavier Mootoo","Alan A. Díaz-Montiel","Milad Lankarany","Hina Tabassum"],"pdf_url":"https://arxiv.org/pdf/2410.06412v2.pdf","comment":"20 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.16657v1","updated":"2024-10-22T03:02:29Z","published":"2024-10-22T03:02:29Z","title":"Dual-Model Defense: Safeguarding Diffusion Models from Membership\n  Inference Attacks through Disjoint Data Splitting","summary":"  Diffusion models have demonstrated remarkable capabilities in image\nsynthesis, but their recently proven vulnerability to Membership Inference\nAttacks (MIAs) poses a critical privacy concern. This paper introduces two\nnovel and efficient approaches (DualMD and DistillMD) to protect diffusion\nmodels against MIAs while maintaining high utility. Both methods are based on\ntraining two separate diffusion models on disjoint subsets of the original\ndataset. DualMD then employs a private inference pipeline that utilizes both\nmodels. This strategy significantly reduces the risk of black-box MIAs by\nlimiting the information any single model contains about individual training\nsamples. The dual models can also generate \"soft targets\" to train a private\nstudent model in DistillMD, enhancing privacy guarantees against all types of\nMIAs. Extensive evaluations of DualMD and DistillMD against state-of-the-art\nMIAs across various datasets in white-box and black-box settings demonstrate\ntheir effectiveness in substantially reducing MIA success rates while\npreserving competitive image generation performance. Notably, our experiments\nreveal that DistillMD not only defends against MIAs but also mitigates model\nmemorization, indicating that both vulnerabilities stem from overfitting and\ncan be addressed simultaneously with our unified approach.\n","authors":["Bao Q. Tran","Viet Nguyen","Anh Tran","Toan Tran"],"pdf_url":"https://arxiv.org/pdf/2410.16657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16656v1","updated":"2024-10-22T03:00:11Z","published":"2024-10-22T03:00:11Z","title":"Parsimonious Dynamic Mode Decomposition: A Robust and Automated Approach\n  for Optimally Sparse Mode Selection in Complex Systems","summary":"  This paper introduces the Parsimonious Dynamic Mode Decomposition (parsDMD),\na novel algorithm designed to automatically select an optimally sparse subset\nof dynamic modes for both spatiotemporal and purely temporal data. By\nincorporating time-delay embedding and leveraging Orthogonal Matching Pursuit\n(OMP), parsDMD ensures robustness against noise and effectively handles\ncomplex, nonlinear dynamics. The algorithm is validated on a diverse range of\ndatasets, including standing wave signals, identifying hidden dynamics, fluid\ndynamics simulations (flow past a cylinder and transonic buffet), and\natmospheric sea-surface temperature (SST) data. ParsDMD addresses a significant\nlimitation of the traditional sparsity-promoting DMD (spDMD), which requires\nmanual tuning of sparsity parameters through a rigorous trial-and-error process\nto balance between single-mode and all-mode solutions. In contrast, parsDMD\nautonomously determines the optimally sparse subset of modes without user\nintervention, while maintaining minimal computational complexity. Comparative\nanalyses demonstrate that parsDMD consistently outperforms spDMD by providing\nmore accurate mode identification and effective reconstruction in noisy\nenvironments. These advantages render parsDMD an effective tool for real-time\ndiagnostics, forecasting, and reduced-order model construction across various\ndisciplines.\n","authors":["Arpan Das","Pier Marzocca","Oleg Levinski"],"pdf_url":"https://arxiv.org/pdf/2410.16656v1.pdf","comment":"42 pages, 16 Figures"},{"id":"http://arxiv.org/abs/2410.15472v2","updated":"2024-10-22T02:59:51Z","published":"2024-10-20T19:02:41Z","title":"Multi-Layer Feature Fusion with Cross-Channel Attention-Based U-Net for\n  Kidney Tumor Segmentation","summary":"  Renal tumors, especially renal cell carcinoma (RCC), show significant\nheterogeneity, posing challenges for diagnosis using radiology images such as\nMRI, echocardiograms, and CT scans. U-Net based deep learning techniques are\nemerging as a promising approach for automated medical image segmentation for\nminimally invasive diagnosis of renal tumors. However, current techniques need\nfurther improvements in accuracy to become clinically useful to radiologists.\nIn this study, we present an improved U-Net based model for end-to-end\nautomated semantic segmentation of CT scan images to identify renal tumors. The\nmodel uses residual connections across convolution layers, integrates a\nmulti-layer feature fusion (MFF) and cross-channel attention (CCA) within\nencoder blocks, and incorporates skip connections augmented with additional\ninformation derived using MFF and CCA. We evaluated our model on the KiTS19\ndataset, which contains data from 210 patients. For kidney segmentation, our\nmodel achieves a Dice Similarity Coefficient (DSC) of 0.97 and a Jaccard index\n(JI) of 0.95. For renal tumor segmentation, our model achieves a DSC of 0.96\nand a JI of 0.91. Based on a comparison of available DSC scores, our model\noutperforms the current leading models.\n","authors":["Fnu Neha","Arvind K. Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.15472v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.16653v1","updated":"2024-10-22T02:57:44Z","published":"2024-10-22T02:57:44Z","title":"Enhancing Two-Player Performance Through Single-Player Knowledge\n  Transfer: An Empirical Study on Atari 2600 Games","summary":"  Playing two-player games using reinforcement learning and self-play can be\nchallenging due to the complexity of two-player environments and the possible\ninstability in the training process. We propose that a reinforcement learning\nalgorithm can train more efficiently and achieve improved performance in a\ntwo-player game if it leverages the knowledge from the single-player version of\nthe same game. This study examines the proposed idea in ten different Atari\n2600 environments using the Atari 2600 RAM as the input state. We discuss the\nadvantages of using transfer learning from a single-player training process\nover training in a two-player setting from scratch, and demonstrate our results\nin a few measures such as training time and average total reward. We also\ndiscuss a method of calculating RAM complexity and its relationship to\nperformance.\n","authors":["Kimiya Saadat","Richard Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13228v2","updated":"2024-10-22T02:53:15Z","published":"2024-10-17T05:30:59Z","title":"From PINNs to PIKANs: Recent Advances in Physics-Informed Machine\n  Learning","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a key tool in\nScientific Machine Learning since their introduction in 2017, enabling the\nefficient solution of ordinary and partial differential equations using sparse\nmeasurements. Over the past few years, significant advancements have been made\nin the training and optimization of PINNs, covering aspects such as network\narchitectures, adaptive refinement, domain decomposition, and the use of\nadaptive weights and activation functions. A notable recent development is the\nPhysics-Informed Kolmogorov-Arnold Networks (PIKANS), which leverage a\nrepresentation model originally proposed by Kolmogorov in 1957, offering a\npromising alternative to traditional PINNs. In this review, we provide a\ncomprehensive overview of the latest advancements in PINNs, focusing on\nimprovements in network design, feature expansion, optimization techniques,\nuncertainty quantification, and theoretical insights. We also survey key\napplications across a range of fields, including biomedicine, fluid and solid\nmechanics, geophysics, dynamical systems, heat transfer, chemical engineering,\nand beyond. Finally, we review computational frameworks and software tools\ndeveloped by both academia and industry to support PINN research and\napplications.\n","authors":["Juan Diego Toscano","Vivek Oommen","Alan John Varghese","Zongren Zou","Nazanin Ahmadi Daryakenari","Chenxi Wu","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2410.13228v2.pdf","comment":"physics-informed neural networks, Kolmogorov-Arnold networks,\n  optimization algorithms, separable PINNs, self-adaptive weights, uncertainty\n  quantification"},{"id":"http://arxiv.org/abs/2410.16647v1","updated":"2024-10-22T02:45:59Z","published":"2024-10-22T02:45:59Z","title":"GE2E-KWS: Generalized End-to-End Training and Evaluation for Zero-shot\n  Keyword Spotting","summary":"  We propose GE2E-KWS -- a generalized end-to-end training and evaluation\nframework for customized keyword spotting. Specifically, enrollment utterances\nare separated and grouped by keywords from the training batch and their\nembedding centroids are compared to all other test utterance embeddings to\ncompute the loss. This simulates runtime enrollment and verification stages,\nand improves convergence stability and training speed by optimizing matrix\noperations compared to SOTA triplet loss approaches. To benchmark different\nmodels reliably, we propose an evaluation process that mimics the production\nenvironment and compute metrics that directly measure keyword matching\naccuracy. Trained with GE2E loss, our 419KB quantized conformer model beats a\n7.5GB ASR encoder by 23.6% relative AUC, and beats a same size triplet loss\nmodel by 60.7% AUC. Our KWS models are natively streamable with low memory\nfootprints, and designed to continuously run on-device with no retraining\nneeded for new keywords (zero-shot).\n","authors":["Pai Zhu","Jacob W. Bartel","Dhruuv Agarwal","Kurt Partridge","Hyun Jin Park","Quan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16647v1.pdf","comment":"8 pages, 6 figures, 2 tables The paper is accepted in IEEE Spoken\n  Language Technology (SLT) 2024"},{"id":"http://arxiv.org/abs/2303.11647v2","updated":"2024-10-22T02:45:44Z","published":"2023-03-21T07:46:28Z","title":"Are uGLAD? Time will tell!","summary":"  We frequently encounter multiple series that are temporally correlated in our\nsurroundings, such as EEG data to examine alterations in brain activity or\nsensors to monitor body movements. Segmentation of multivariate time series\ndata is a technique for identifying meaningful patterns or changes in the time\nseries that can signal a shift in the system's behavior. However, most\nsegmentation algorithms have been designed primarily for univariate time\nseries, and their performance on multivariate data remains largely\nunsatisfactory, making this a challenging problem. In this work, we introduce a\nnovel approach for multivariate time series segmentation using conditional\nindependence (CI) graphs. CI graphs are probabilistic graphical models that\nrepresents the partial correlations between the nodes. We propose a domain\nagnostic multivariate segmentation framework $\\texttt{tGLAD}$ which draws a\nparallel between the CI graph nodes and the variables of the time series.\nConsider applying a graph recovery model $\\texttt{uGLAD}$ to a short interval\nof the time series, it will result in a CI graph that shows partial\ncorrelations among the variables. We extend this idea to the entire time series\nby utilizing a sliding window to create a batch of time intervals and then run\na single $\\texttt{uGLAD}$ model in multitask learning mode to recover all the\nCI graphs simultaneously. As a result, we obtain a corresponding temporal CI\ngraphs representation. We then designed a first-order and second-order based\ntrajectory tracking algorithms to study the evolution of these graphs across\ndistinct intervals. Finally, an `Allocation' algorithm is used to determine a\nsuitable segmentation of the temporal graph sequence. $\\texttt{tGLAD}$ provides\na competitive time complexity of $O(N)$ for settings where number of variables\n$D<<N$. We demonstrate successful empirical results on a Physical Activity\nMonitoring data.\n","authors":["Shima Imani","Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2303.11647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05857v2","updated":"2024-10-22T02:41:48Z","published":"2023-08-10T21:06:18Z","title":"Knowledge Propagation over Conditional Independence Graphs","summary":"  Conditional Independence (CI) graph is a special type of a Probabilistic\nGraphical Model (PGM) where the feature connections are modeled using an\nundirected graph and the edge weights show the partial correlation strength\nbetween the features. Since the CI graphs capture direct dependence between\nfeatures, they have been garnering increasing interest within the research\ncommunity for gaining insights into the systems from various domains, in\nparticular discovering the domain topology. In this work, we propose algorithms\nfor performing knowledge propagation over the CI graphs. Our experiments\ndemonstrate that our techniques improve upon the state-of-the-art on the\npublicly available Cora and PubMed datasets.\n","authors":["Urszula Chajewska","Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2308.05857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02216v2","updated":"2024-10-22T02:33:25Z","published":"2023-06-03T23:53:57Z","title":"Forgettable Federated Linear Learning with Certified Data Unlearning","summary":"  The advent of Federated Learning (FL) has revolutionized the way distributed\nsystems handle collaborative model training while preserving user privacy.\nRecently, Federated Unlearning (FU) has emerged to address demands for the\n\"right to be forgotten\"\" and unlearning of the impact of poisoned clients\nwithout requiring retraining in FL. Most FU algorithms require the cooperation\nof retained or target clients (clients to be unlearned), introducing additional\ncommunication overhead and potential security risks. In addition, some FU\nmethods need to store historical models to execute the unlearning process.\nThese challenges hinder the efficiency and memory constraints of the current FU\nmethods. Moreover, due to the complexity of nonlinear models and their training\nstrategies, most existing FU methods for deep neural networks (DNN) lack\ntheoretical certification. In this work, we introduce a novel FL training and\nunlearning strategy in DNN, termed Forgettable Federated Linear Learning\n(F^2L^2). F^2L^2 considers a common practice of using pre-trained models to\napproximate DNN linearly, allowing them to achieve similar performance as the\noriginal networks via Federated Linear Training (FLT). We then present\nFedRemoval, a certified, efficient, and secure unlearning strategy that enables\nthe server to unlearn a target client without requiring client communication or\nadding additional storage. We have conducted extensive empirical validation on\nsmall- to large-scale datasets, using both convolutional neural networks and\nmodern foundation models. These experiments demonstrate the effectiveness of\nF^2L^2 in balancing model accuracy with the successful unlearning of target\nclients. F^2L^2 represents a promising pipeline for efficient and trustworthy\nFU. The code is available here.\n","authors":["Ruinan Jin","Minghui Chen","Qiong Zhang","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2306.02216v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.11493v2","updated":"2024-10-22T02:31:19Z","published":"2024-10-15T10:57:02Z","title":"Towards Fair Graph Representation Learning in Social Networks","summary":"  With the widespread use of Graph Neural Networks (GNNs) for representation\nlearning from network data, the fairness of GNN models has raised great\nattention lately. Fair GNNs aim to ensure that node representations can be\naccurately classified, but not easily associated with a specific group.\nExisting advanced approaches essentially enhance the generalisation of node\nrepresentation in combination with data augmentation strategy, and do not\ndirectly impose constraints on the fairness of GNNs. In this work, we identify\nthat a fundamental reason for the unfairness of GNNs in social network learning\nis the phenomenon of social homophily, i.e., users in the same group are more\ninclined to congregate. The message-passing mechanism of GNNs can cause users\nin the same group to have similar representations due to social homophily,\nleading model predictions to establish spurious correlations with sensitive\nattributes. Inspired by this reason, we propose a method called Equity-Aware\nGNN (EAGNN) towards fair graph representation learning. Specifically, to ensure\nthat model predictions are independent of sensitive attributes while\nmaintaining prediction performance, we introduce constraints for fair\nrepresentation learning based on three principles: sufficiency, independence,\nand separation. We theoretically demonstrate that our EAGNN method can\neffectively achieve group fairness. Extensive experiments on three datasets\nwith varying levels of social homophily illustrate that our EAGNN method\nachieves the state-of-the-art performance across two fairness metrics and\noffers competitive effectiveness.\n","authors":["Guixian Zhang","Guan Yuan","Debo Cheng","Lin Liu","Jiuyong Li","Shichao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.11493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04061v3","updated":"2024-10-22T02:31:13Z","published":"2024-02-06T15:05:25Z","title":"TopoNav: Topological Navigation for Efficient Exploration in Sparse\n  Reward Environments","summary":"  Autonomous robots exploring unknown environments face a significant\nchallenge: navigating effectively without prior maps and with limited external\nfeedback. This challenge intensifies in sparse reward environments, where\ntraditional exploration techniques often fail. In this paper, we present\nTopoNav, a novel topological navigation framework that integrates active\nmapping, hierarchical reinforcement learning, and intrinsic motivation to\nenable efficient goal-oriented exploration and navigation in sparse-reward\nsettings. TopoNav dynamically constructs a topological map of the environment,\ncapturing key locations and pathways. A two-level hierarchical policy\narchitecture, comprising a high-level graph traversal policy and low-level\nmotion control policies, enables effective navigation and obstacle avoidance\nwhile maintaining focus on the overall goal. Additionally, TopoNav incorporates\nintrinsic motivation to guide exploration toward relevant regions and frontier\nnodes in the topological map, addressing the challenges of sparse extrinsic\nrewards. We evaluate TopoNav both in the simulated and real-world off-road\nenvironments using a Clearpath Jackal robot, across three challenging\nnavigation scenarios: goal-reaching, feature-based navigation, and navigation\nin complex terrains. We observe an increase in exploration coverage by 7- 20%,\nin success rates by 9-19%, and reductions in navigation times by 15-36% across\nvarious scenarios, compared to state-of-the-art methods\n","authors":["Jumman Hossain","Abu-Zaher Faridee","Nirmalya Roy","Jade Freeman","Timothy Gregory","Theron T. Trout"],"pdf_url":"https://arxiv.org/pdf/2402.04061v3.pdf","comment":"Accepted at the 37th IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2409.16167v3","updated":"2024-10-22T02:29:22Z","published":"2024-09-24T15:08:41Z","title":"Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering","summary":"  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.\n","authors":["Ziyu Zhao","Tao Shen","Didi Zhu","Zexi Li","Jing Su","Xuwu Wang","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16167v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16638v1","updated":"2024-10-22T02:27:57Z","published":"2024-10-22T02:27:57Z","title":"LLMScan: Causal Scan for LLM Misbehavior Detection","summary":"  Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.\n","authors":["Mengdi Zhang","Kai Kiat Goh","Peixin Zhang","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.16638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16636v1","updated":"2024-10-22T02:27:32Z","published":"2024-10-22T02:27:32Z","title":"General Frameworks for Conditional Two-Sample Testing","summary":"  We study the problem of conditional two-sample testing, which aims to\ndetermine whether two populations have the same distribution after accounting\nfor confounding factors. This problem commonly arises in various applications,\nsuch as domain adaptation and algorithmic fairness, where comparing two groups\nis essential while controlling for confounding variables. We begin by\nestablishing a hardness result for conditional two-sample testing,\ndemonstrating that no valid test can have significant power against any single\nalternative without proper assumptions. We then introduce two general\nframeworks that implicitly or explicitly target specific classes of\ndistributions for their validity and power. Our first framework allows us to\nconvert any conditional independence test into a conditional two-sample test in\na black-box manner, while preserving the asymptotic properties of the original\nconditional independence test. The second framework transforms the problem into\ncomparing marginal distributions with estimated density ratios, which allows us\nto leverage existing methods for marginal two-sample testing. We demonstrate\nthis idea in a concrete manner with classification and kernel-based methods.\nFinally, simulation studies are conducted to illustrate the proposed frameworks\nin finite-sample scenarios.\n","authors":["Seongchan Lee","Suman Cha","Ilmun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.16636v1.pdf","comment":"39 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16632v1","updated":"2024-10-22T02:21:30Z","published":"2024-10-22T02:21:30Z","title":"Benchmarking Smoothness and Reducing High-Frequency Oscillations in\n  Continuous Control Policies","summary":"  Reinforcement learning (RL) policies are prone to high-frequency\noscillations, especially undesirable when deploying to hardware in the\nreal-world. In this paper, we identify, categorize, and compare methods from\nthe literature that aim to mitigate high-frequency oscillations in deep RL. We\ndefine two broad classes: loss regularization and architectural methods. At\ntheir core, these methods incentivize learning a smooth mapping, such that\nnearby states in the input space produce nearby actions in the output space. We\npresent benchmarks in terms of policy performance and control smoothness on\ntraditional RL environments from the Gymnasium and a complex manipulation task,\nas well as three robotics locomotion tasks that include deployment and\nevaluation with real-world hardware. Finally, we also propose hybrid methods\nthat combine elements from both loss regularization and architectural methods.\nWe find that the best-performing hybrid outperforms other methods, and improves\ncontrol smoothness by 26.8% over the baseline, with a worst-case performance\ndegradation of just 2.8%.\n","authors":["Guilherme Christmann","Ying-Sheng Luo","Hanjaya Mandala","Wei-Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16632v1.pdf","comment":"Presented in IROS 2024"},{"id":"http://arxiv.org/abs/2406.09564v2","updated":"2024-10-22T02:14:24Z","published":"2024-06-13T20:12:46Z","title":"Towards Domain Adaptive Neural Contextual Bandits","summary":"  Contextual bandit algorithms are essential for solving real-world decision\nmaking problems. In practice, collecting a contextual bandit's feedback from\ndifferent domains may involve different costs. For example, measuring drug\nreaction from mice (as a source domain) and humans (as a target domain).\nUnfortunately, adapting a contextual bandit algorithm from a source domain to a\ntarget domain with distribution shift still remains a major challenge and\nlargely unexplored. In this paper, we introduce the first general domain\nadaptation method for contextual bandits. Our approach learns a bandit model\nfor the target domain by collecting feedback from the source domain. Our\ntheoretical analysis shows that our algorithm maintains a sub-linear regret\nbound even adapting across domains. Empirical results show that our approach\noutperforms the state-of-the-art contextual bandit algorithms on real-world\ndatasets.\n","authors":["Ziyan Wang","Xiaoming Huo","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15357v2","updated":"2024-10-22T02:10:57Z","published":"2024-10-20T10:53:42Z","title":"Wireless Link Quality Estimation Using LSTM Model","summary":"  In recent years, various services have been provided through high-speed and\nhigh-capacity wireless networks on mobile communication devices, necessitating\nstable communication regardless of indoor or outdoor environments. To achieve\nstable communication, it is essential to implement proactive measures, such as\nswitching to an alternative path and ensuring data buffering before the\ncommunication quality becomes unstable. The technology of Wireless Link Quality\nEstimation (WLQE), which predicts the communication quality of wireless\nnetworks in advance, plays a crucial role in this context. In this paper, we\npropose a novel WLQE model for estimating the communication quality of wireless\nnetworks by leveraging sequential information. Our proposed method is based on\nLong Short-Term Memory (LSTM), enabling highly accurate estimation by\nconsidering the sequential information of link quality. We conducted a\ncomparative evaluation with the conventional model, stacked autoencoder-based\nlink quality estimator (LQE-SAE), using a dataset recorded in real-world\nenvironmental conditions. Our LSTM-based LQE model demonstrates its\nsuperiority, achieving a 4.0% higher accuracy and a 4.6% higher macro-F1 score\nthan the LQE-SAE model in the evaluation.\n","authors":["Yuki Kanto","Kohei Watabe"],"pdf_url":"https://arxiv.org/pdf/2410.15357v2.pdf","comment":"This paper was submitted to IEEE Network Operations and Management\n  Symposium"},{"id":"http://arxiv.org/abs/2410.16618v1","updated":"2024-10-22T02:06:38Z","published":"2024-10-22T02:06:38Z","title":"SoK: Dataset Copyright Auditing in Machine Learning Systems","summary":"  As the implementation of machine learning (ML) systems becomes more\nwidespread, especially with the introduction of larger ML models, we perceive a\nspring demand for massive data. However, it inevitably causes infringement and\nmisuse problems with the data, such as using unauthorized online artworks or\nface images to train ML models. To address this problem, many efforts have been\nmade to audit the copyright of the model training dataset. However, existing\nsolutions vary in auditing assumptions and capabilities, making it difficult to\ncompare their strengths and weaknesses. In addition, robustness evaluations\nusually consider only part of the ML pipeline and hardly reflect the\nperformance of algorithms in real-world ML applications. Thus, it is essential\nto take a practical deployment perspective on the current dataset copyright\nauditing tools, examining their effectiveness and limitations. Concretely, we\ncategorize dataset copyright auditing research into two prominent strands:\nintrusive methods and non-intrusive methods, depending on whether they require\nmodifications to the original dataset. Then, we break down the intrusive\nmethods into different watermark injection options and examine the\nnon-intrusive methods using various fingerprints. To summarize our results, we\noffer detailed reference tables, highlight key points, and pinpoint unresolved\nissues in the current literature. By combining the pipeline in ML systems and\nanalyzing previous studies, we highlight several future directions to make\nauditing tools more suitable for real-world copyright protection requirements.\n","authors":["Linkang Du","Xuanru Zhou","Min Chen","Chusong Zhang","Zhou Su","Peng Cheng","Jiming Chen","Zhikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16618v1.pdf","comment":"To appear in the IEEE Symposium on Security and Privacy 2025, San\n  Francisco, CA, USA"},{"id":"http://arxiv.org/abs/2405.16194v2","updated":"2024-10-22T02:06:35Z","published":"2024-05-25T11:53:23Z","title":"Diffusion-Reward Adversarial Imitation Learning","summary":"  Imitation learning aims to learn a policy from observing expert\ndemonstrations without access to reward signals from environments. Generative\nadversarial imitation learning (GAIL) formulates imitation learning as\nadversarial learning, employing a generator policy learning to imitate expert\nbehaviors and discriminator learning to distinguish the expert demonstrations\nfrom agent trajectories. Despite its encouraging results, GAIL training is\noften brittle and unstable. Inspired by the recent dominance of diffusion\nmodels in generative modeling, we propose Diffusion-Reward Adversarial\nImitation Learning (DRAIL), which integrates a diffusion model into GAIL,\naiming to yield more robust and smoother rewards for policy learning.\nSpecifically, we propose a diffusion discriminative classifier to construct an\nenhanced discriminator, and design diffusion rewards based on the classifier's\noutput for policy learning. Extensive experiments are conducted in navigation,\nmanipulation, and locomotion, verifying DRAIL's effectiveness compared to prior\nimitation learning methods. Moreover, additional experimental results\ndemonstrate the generalizability and data efficiency of DRAIL. Visualized\nlearned reward functions of GAIL and DRAIL suggest that DRAIL can produce more\nrobust and smoother rewards. Project page:\nhttps://nturobotlearninglab.github.io/DRAIL/\n","authors":["Chun-Mao Lai","Hsiang-Chun Wang","Ping-Chun Hsieh","Yu-Chiang Frank Wang","Min-Hung Chen","Shao-Hua Sun"],"pdf_url":"https://arxiv.org/pdf/2405.16194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16613v1","updated":"2024-10-22T01:55:02Z","published":"2024-10-22T01:55:02Z","title":"Real-time Sub-milliwatt Epilepsy Detection Implemented on a Spiking\n  Neural Network Edge Inference Processor","summary":"  Analyzing electroencephalogram (EEG) signals to detect the epileptic seizure\nstatus of a subject presents a challenge to existing technologies aimed at\nproviding timely and efficient diagnosis. In this study, we aimed to detect\ninterictal and ictal periods of epileptic seizures using a spiking neural\nnetwork (SNN). Our proposed approach provides an online and real-time\npreliminary diagnosis of epileptic seizures and helps to detect possible\npathological conditions.To validate our approach, we conducted experiments\nusing multiple datasets. We utilized a trained SNN to identify the presence of\nepileptic seizures and compared our results with those of related studies. The\nSNN model was deployed on Xylo, a digital SNN neuromorphic processor designed\nto process temporal signals. Xylo efficiently simulates spiking leaky\nintegrate-and-fire neurons with exponential input synapses. Xylo has much lower\nenergy requirments than traditional approaches to signal processing, making it\nan ideal platform for developing low-power seizure detection systems.Our\nproposed method has a high test accuracy of 93.3% and 92.9% when classifying\nictal and interictal periods. At the same time, the application has an average\npower consumption of 87.4 uW(IO power) + 287.9 uW(computational power) when\ndeployed to Xylo. Our method demonstrates excellent low-latency performance\nwhen tested on multiple datasets. Our work provides a new solution for seizure\ndetection, and it is expected to be widely used in portable and wearable\ndevices in the future.\n","authors":["Ruixin Lia","Guoxu Zhaoa","Dylan Richard Muir","Yuya Ling","Karla Burelo","Mina Khoei","Dong Wang","Yannan Xing","Ning Qiao"],"pdf_url":"https://arxiv.org/pdf/2410.16613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10006v2","updated":"2024-10-22T01:51:15Z","published":"2023-10-16T02:02:56Z","title":"Soft ascent-descent as a stable and flexible alternative to flooding","summary":"  As a heuristic for improving test accuracy in classification, the \"flooding\"\nmethod proposed by Ishida et al. (2020) sets a threshold for the average\nsurrogate loss at training time; above the threshold, gradient descent is run\nas usual, but below the threshold, a switch to gradient ascent is made. While\nsetting the threshold is non-trivial and is usually done with validation data,\nthis simple technique has proved remarkably effective in terms of accuracy. On\nthe other hand, what if we are also interested in other metrics such as model\ncomplexity or average surrogate loss at test time? As an attempt to achieve\nbetter overall performance with less fine-tuning, we propose a softened,\npointwise mechanism called SoftAD (soft ascent-descent) that downweights points\non the borderline, limits the effects of outliers, and retains the\nascent-descent effect of flooding, with no additional computational overhead.\nWe contrast formal stationarity guarantees with those for flooding, and\nempirically demonstrate how SoftAD can realize classification accuracy\ncompetitive with flooding (and the more expensive alternative SAM) while\nenjoying a much smaller loss generalization gap and model norm.\n","authors":["Matthew J. Holland","Kosuke Nakatani"],"pdf_url":"https://arxiv.org/pdf/2310.10006v2.pdf","comment":"Revised version accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2311.02622v2","updated":"2024-10-22T01:41:21Z","published":"2023-11-05T11:27:03Z","title":"Hierarchical Simplicity Bias of Neural Networks","summary":"  Neural networks often exhibit simplicity bias, favoring simpler features over\nmore complex ones, even when both are equally predictive. We introduce a novel\nmethod called imbalanced label coupling to explore and extend this simplicity\nbias across multiple hierarchical levels. Our approach demonstrates that\ntrained networks sequentially consider features of increasing complexity based\non their correlation with labels in the training set, regardless of their\nactual predictive power. For example, in CIFAR-10, simple spurious features can\ncause misclassifications where most cats are predicted as dogs and most trucks\nas automobiles. We empirically show that last-layer retraining with target data\ndistribution \\citep{kirichenko2022last} is insufficient to fully recover core\nfeatures when spurious features perfectly correlate with target labels in our\nsynthetic datasets. Our findings deepen the understanding of the implicit\nbiases inherent in neural networks.\n","authors":["Zhehang Du"],"pdf_url":"https://arxiv.org/pdf/2311.02622v2.pdf","comment":"20 pages, 21 figures, revised version, accepted at OPT2024: 16th\n  Annual Workshop on Optimization for Machine Learning"},{"id":"http://arxiv.org/abs/2410.16608v1","updated":"2024-10-22T01:40:43Z","published":"2024-10-22T01:40:43Z","title":"Assessing and improving reliability of neighbor embedding methods: a\n  map-continuity perspective","summary":"  Visualizing high-dimensional data is an important routine for understanding\nbiomedical data and interpreting deep learning models. Neighbor embedding\nmethods, such as t-SNE, UMAP, and LargeVis, among others, are a family of\npopular visualization methods which reduce high-dimensional data to two\ndimensions. However, recent studies suggest that these methods often produce\nvisual artifacts, potentially leading to incorrect scientific conclusions.\nRecognizing that the current limitation stems from a lack of data-independent\nnotions of embedding maps, we introduce a novel conceptual and computational\nframework, LOO-map, that learns the embedding maps based on a classical\nstatistical idea known as the leave-one-out. LOO-map extends the embedding over\na discrete set of input points to the entire input space, enabling a systematic\nassessment of map continuity, and thus the reliability of the visualizations.\nWe find for many neighbor embedding methods, their embedding maps can be\nintrinsically discontinuous. The discontinuity induces two types of observed\nmap distortion: ``overconfidence-inducing discontinuity,\" which exaggerates\ncluster separation, and ``fracture-inducing discontinuity,\" which creates\nspurious local structures. Building upon LOO-map, we propose two diagnostic\npoint-wise scores -- perturbation score and singularity score -- to address\nthese limitations. These scores can help identify unreliable embedding points,\ndetect out-of-distribution data, and guide hyperparameter selection. Our\napproach is flexible and works as a wrapper around many neighbor embedding\nalgorithms. We test our methods across multiple real-world datasets from\ncomputer vision and single-cell omics to demonstrate their effectiveness in\nenhancing the interpretability and accuracy of visualizations.\n","authors":["Zhexuan Liu","Rong Ma","Yiqiao Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.16608v1.pdf","comment":"43 pages, 15 figures"},{"id":"http://arxiv.org/abs/2211.16327v5","updated":"2024-10-22T01:35:27Z","published":"2022-11-29T16:10:11Z","title":"On the Power of Foundation Models","summary":"  With infinitely many high-quality data points, infinite computational power,\nan infinitely large foundation model with a perfect training algorithm and\nguaranteed zero generalization error on the pretext task, can the model be used\nfor everything? This question cannot be answered by the existing theory of\nrepresentation, optimization or generalization, because the issues they mainly\ninvestigate are assumed to be nonexistent here. In this paper, we show that\ncategory theory provides powerful machinery to answer this question. We have\nproved three results. The first one limits the power of prompt-based learning,\nsaying that the model can solve a downstream task with prompts if and only if\nthe task is representable. The second one says fine tuning does not have this\nlimit, as a foundation model with the minimum required power (up to symmetry)\ncan theoretically solve downstream tasks for the category defined by pretext\ntask, with fine tuning and enough resources. Our final result can be seen as a\nnew type of generalization theorem, showing that the foundation model can\ngenerate unseen objects from the target category (e.g., images) using the\nstructural information from the source category (e.g., texts). Along the way,\nwe provide a categorical framework for supervised and self-supervised learning,\nwhich might be of independent interest.\n","authors":["Yang Yuan"],"pdf_url":"https://arxiv.org/pdf/2211.16327v5.pdf","comment":"ICML'23. This version fixed a bug when applying prompt tuning theorem\n  to LLM"},{"id":"http://arxiv.org/abs/2310.17712v3","updated":"2024-10-22T01:35:02Z","published":"2023-10-26T18:16:23Z","title":"Community Detection Guarantees Using Embeddings Learned by Node2Vec","summary":"  Embedding the nodes of a large network into an Euclidean space is a common\nobjective in modern machine learning, with a variety of tools available. These\nembeddings can then be used as features for tasks such as community\ndetection/node clustering or link prediction, where they achieve state of the\nart performance. With the exception of spectral clustering methods, there is\nlittle theoretical understanding for commonly used approaches to learning\nembeddings. In this work we examine the theoretical properties of the\nembeddings learned by node2vec. Our main result shows that the use of $k$-means\nclustering on the embedding vectors produced by node2vec gives weakly\nconsistent community recovery for the nodes in (degree corrected) stochastic\nblock models. We also discuss the use of these embeddings for node and link\nprediction tasks. We demonstrate this result empirically, and examine how this\nrelates to other embedding tools for network data.\n","authors":["Andrew Davison","S. Carlyle Morgan","Owen G. Ward"],"pdf_url":"https://arxiv.org/pdf/2310.17712v3.pdf","comment":"Camera ready version for Neurips 2024"},{"id":"http://arxiv.org/abs/2410.16606v1","updated":"2024-10-22T01:32:46Z","published":"2024-10-22T01:32:46Z","title":"GALA: Graph Diffusion-based Alignment with Jigsaw for Source-free Domain\n  Adaptation","summary":"  Source-free domain adaptation is a crucial machine learning topic, as it\ncontains numerous applications in the real world, particularly with respect to\ndata privacy. Existing approaches predominantly focus on Euclidean data, such\nas images and videos, while the exploration of non-Euclidean graph data remains\nscarce. Recent graph neural network (GNN) approaches can suffer from serious\nperformance decline due to domain shift and label scarcity in source-free\nadaptation scenarios. In this study, we propose a novel method named Graph\nDiffusion-based Alignment with Jigsaw (GALA), tailored for source-free graph\ndomain adaptation. To achieve domain alignment, GALA employs a graph diffusion\nmodel to reconstruct source-style graphs from target data. Specifically, a\nscore-based graph diffusion model is trained using source graphs to learn the\ngenerative source styles. Then, we introduce perturbations to target graphs via\na stochastic differential equation instead of sampling from a prior, followed\nby the reverse process to reconstruct source-style graphs. We feed the\nsource-style graphs into an off-the-shelf GNN and introduce class-specific\nthresholds with curriculum learning, which can generate accurate and unbiased\npseudo-labels for target graphs. Moreover, we develop a simple yet effective\ngraph-mixing strategy named graph jigsaw to combine confident graphs and\nunconfident graphs, which can enhance generalization capabilities and\nrobustness via consistency learning. Extensive experiments on benchmark\ndatasets validate the effectiveness of GALA.\n","authors":["Junyu Luo","Yiyang Gu","Xiao Luo","Wei Ju","Zhiping Xiao","Yusheng Zhao","Jingyang Yuan","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16606v1.pdf","comment":"IEEE TPAMI"},{"id":"http://arxiv.org/abs/2409.13705v2","updated":"2024-10-22T01:01:56Z","published":"2024-09-05T14:35:35Z","title":"Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble","summary":"  Increasing use of large language models (LLMs) demand performant guardrails\nto ensure the safety of inputs and outputs of LLMs. When these safeguards are\ntrained on imbalanced data, they can learn the societal biases. We present a\nlight-weight, post-processing method for mitigating counterfactual fairness in\nclosed-source text safety classifiers. Our approach involves building an\nensemble that not only outperforms the input classifiers and policy-aligns\nthem, but also acts as a debiasing regularizer. We introduce two\nthreshold-agnostic metrics to assess the counterfactual fairness of a model,\nand demonstrate how combining these metrics with Fair Data Reweighting (FDW)\nhelps mitigate biases. We create an expanded Open AI dataset, and a new\ntemplated LLM-generated dataset based on user-prompts, both of which are\ncounterfactually balanced across identity groups and cover four key areas of\nsafety; we will work towards publicly releasing these datasets. Our results\nshow that our approach improves counterfactual fairness with minimal impact on\nmodel performance.\n","authors":["Olivia Sturman","Aparna Joshi","Bhaktipriya Radharapu","Piyush Kumar","Renee Shelby"],"pdf_url":"https://arxiv.org/pdf/2409.13705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16593v1","updated":"2024-10-22T00:30:31Z","published":"2024-10-22T00:30:31Z","title":"Graph Sampling for Scalable and Expressive Graph Neural Networks on\n  Homophilic Graphs","summary":"  Graph Neural Networks (GNNs) excel in many graph machine learning tasks but\nface challenges when scaling to large networks. GNN transferability allows\ntraining on smaller graphs and applying the model to larger ones, but existing\nmethods often rely on random subsampling, leading to disconnected subgraphs and\nreduced model expressivity. We propose a novel graph sampling algorithm that\nleverages feature homophily to preserve graph structure. By minimizing the\ntrace of the data correlation matrix, our method better preserves the graph\nLaplacian's rank than random sampling while achieving lower complexity than\nspectral methods. Experiments on citation networks show improved performance in\npreserving graph rank and GNN transferability compared to random sampling.\n","authors":["Haolin Li","Luana Ruiz"],"pdf_url":"https://arxiv.org/pdf/2410.16593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16592v1","updated":"2024-10-22T00:30:08Z","published":"2024-10-22T00:30:08Z","title":"ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding","summary":"  The rise of social media and short-form video (SFV) has facilitated a\nbreeding ground for misinformation. With the emergence of large language\nmodels, significant research has gone into curbing this misinformation problem\nwith automatic false claim detection for text. Unfortunately, the automatic\ndetection of misinformation in SFV is a more complex problem that remains\nlargely unstudied. While text samples are monomodal (only containing words),\nSFVs comprise three different modalities: words, visuals, and non-linguistic\naudio. In this work, we introduce Video Masked Autoencoders for Misinformation\nGuarding (ViMGuard), the first deep-learning architecture capable of\nfact-checking an SFV through analysis of all three of its constituent\nmodalities. ViMGuard leverages a dual-component system. First, Video and Audio\nMasked Autoencoders analyze the visual and non-linguistic audio elements of a\nvideo to discern its intention; specifically whether it intends to make an\ninformative claim. If it is deemed that the SFV has informative intent, it is\npassed through our second component: a Retrieval Augmented Generation system\nthat validates the factual accuracy of spoken words. In evaluation, ViMGuard\noutperformed three cutting-edge fact-checkers, thus setting a new standard for\nSFV fact-checking and marking a significant stride toward trustworthy news on\nsocial platforms. To promote further testing and iteration, VimGuard was\ndeployed into a Chrome extension and all code was open-sourced on GitHub.\n","authors":["Andrew Kan","Christopher Kan","Zaid Nabulsi"],"pdf_url":"https://arxiv.org/pdf/2410.16592v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.05841v2","updated":"2024-10-22T00:16:21Z","published":"2024-07-08T11:38:49Z","title":"An Empirical Comparison of Vocabulary Expansion and Initialization\n  Approaches for Language Models","summary":"  Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods. We release our code publicly\n(https://github.com/AI4Bharat/VocabAdaptation_LLM/tree/CW2V).\n","authors":["Nandini Mundra","Aditya Nanda Kishore","Raj Dabre","Ratish Puduppully","Anoop Kunchukuttan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.05841v2.pdf","comment":"CONLL 2024 (EMNLP 2024)"},{"id":"http://arxiv.org/abs/2405.15050v2","updated":"2024-10-22T23:57:34Z","published":"2024-05-23T20:58:33Z","title":"Reinforcement Learning for Infinite-Horizon Average-Reward Linear MDPs\n  via Approximation by Discounted-Reward MDPs","summary":"  We study the infinite-horizon average-reward reinforcement learning with\nlinear MDPs. Previous approaches either suffer from computational inefficiency\nor require strong assumptions on dynamics, such as ergodicity, for achieving a\nregret bound of $\\widetilde{O}(\\sqrt{T})$. In this paper, we propose an\nalgorithm that achieves the regret bound of $\\widetilde{O}(\\sqrt{T})$ and is\ncomputationally efficient in the sense that the time complexity is polynomial\nin problem parameters. Our algorithm runs an optimistic value iteration on a\ndiscounted-reward MDP that approximates the average-reward setting. With an\nappropriately tuned discounting factor $\\gamma$, the algorithm attains the\ndesired $\\widetilde{O}(\\sqrt{T})$ regret. The challenge in our approximation\napproach is to get a regret bound with a sharp dependency on the effective\nhorizon $1 / (1 - \\gamma)$. We address this challenge by clipping the value\nfunction obtained at each value iteration step to limit the span of the value\nfunction.\n","authors":["Kihyuk Hong","Woojin Chae","Yufan Zhang","Dabeen Lee","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2405.15050v2.pdf","comment":"Fixes an error in the analysis in the previous version by modifying\n  the algorithm and analysis"},{"id":"http://arxiv.org/abs/2410.17477v1","updated":"2024-10-22T23:24:15Z","published":"2024-10-22T23:24:15Z","title":"Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination","summary":"  The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to \\textit{hallucinate} false or misleading information, limiting\ntheir reliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.\n","authors":["Jerry Huang","Prasanna Parthasarathi","Mehdi Rezagholizadeh","Boxing Chen","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2410.17477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17473v1","updated":"2024-10-22T23:14:09Z","published":"2024-10-22T23:14:09Z","title":"DROP: Distributional and Regular Optimism and Pessimism for\n  Reinforcement Learning","summary":"  In reinforcement learning (RL), temporal difference (TD) error is known to be\nrelated to the firing rate of dopamine neurons. It has been observed that each\ndopamine neuron does not behave uniformly, but each responds to the TD error in\nan optimistic or pessimistic manner, interpreted as a kind of distributional\nRL. To explain such a biological data, a heuristic model has also been designed\nwith learning rates asymmetric for the positive and negative TD errors.\nHowever, this heuristic model is not theoretically-grounded and unknown whether\nit can work as a RL algorithm. This paper therefore introduces a novel\ntheoretically-grounded model with optimism and pessimism, which is derived from\ncontrol as inference. In combination with ensemble learning, a distributional\nvalue function as a critic is estimated from regularly introduced optimism and\npessimism. Based on its central value, a policy in an actor is improved. This\nproposed algorithm, so-called DROP (distributional and regular optimism and\npessimism), is compared on dynamic tasks. Although the heuristic model showed\npoor learning performance, DROP showed excellent one in all tasks with high\ngenerality. In other words, it was suggested that DROP is a new model that can\nelicit the potential contributions of optimism and pessimism.\n","authors":["Taisuke Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2410.17473v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.11402v2","updated":"2024-10-22T23:13:34Z","published":"2024-09-17T17:59:06Z","title":"NVLM: Open Frontier-Class Multimodal LLMs","summary":"  We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B\nand will open-source the training code for the community soon.\n","authors":["Wenliang Dai","Nayeon Lee","Boxin Wang","Zhuolin Yang","Zihan Liu","Jon Barker","Tuomas Rintamaki","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2409.11402v2.pdf","comment":"Fixed the typos. For more information, please visit our project page\n  at: https://research.nvidia.com/labs/adlr/NVLM-1"},{"id":"http://arxiv.org/abs/2410.17467v1","updated":"2024-10-22T22:49:31Z","published":"2024-10-22T22:49:31Z","title":"Multi-modal Machine Learning Analysis of X-ray Absorption Near-Edge\n  Spectra and Pair Distribution Functions: Performance and Interpretability\n  towards Experimental Design","summary":"  We used off-the-shelf interpretable ML techniques to combine information from\nmultiple heterogeneous spectra: X-ray absorption near-edge spectra (XANES) and\natomic pair distribution functions (PDFs), to extract information about local\nstructure and chemistry of transition metal oxides. This approach enabled us to\nanalyze the relative contributions of the different spectra to different\nprediction tasks. Specifically, we trained random forest models on XANES, PDF,\nand both of them combined, to extract charge (oxidation) state, coordination\nnumber, and mean nearest-neighbor bond length of transition metal cations in\noxides. We find that XANES-only models tend to outperform the PDF-only models\nfor all the tasks, and information from XANES often dominated when the two\ninputs were combined. This was even true for structural tasks where we might\nexpect PDF to dominate. However, the performance gap closes when we used\nspecies-specific differential PDFs (dPDFs) as the inputs instead of total PDFs.\nOur results highlight that XANES contains rich structural information and may\nbe further developed as a structural probe. Our interpretable, multimodal\napproach is quick and easy to implement when suitable structural and\nspectroscopic databases are available. This approach provides valuable insights\ninto the relative strengths of different modalities for a practical scientific\ngoal, guiding researchers in their experiment design tasks such as deciding\nwhen it is useful to combine complementary techniques in a scientific\ninvestigation.\n","authors":["Tanaporn Na Narong","Zoe N. Zachko","Steven B. Torrisi","Simon J. L. Billinge"],"pdf_url":"https://arxiv.org/pdf/2410.17467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17466v1","updated":"2024-10-22T22:49:04Z","published":"2024-10-22T22:49:04Z","title":"Evolution with Opponent-Learning Awareness","summary":"  The universe involves many independent co-learning agents as an ever-evolving\npart of our observed environment. Yet, in practice, Multi-Agent Reinforcement\nLearning (MARL) applications are usually constrained to small, homogeneous\npopulations and remain computationally intensive. In this paper, we study how\nlarge heterogeneous populations of learning agents evolve in normal-form games.\nWe show how, under assumptions commonly made in the multi-armed bandit\nliterature, Multi-Agent Policy Gradient closely resembles the Replicator\nDynamic, and we further derive a fast, parallelizable implementation of\nOpponent-Learning Awareness tailored for evolutionary simulations. This enables\nus to simulate the evolution of very large populations made of heterogeneous\nco-learning agents, under both naive and advanced learning strategies. We\ndemonstrate our approach in simulations of 200,000 agents, evolving in the\nclassic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game\nhighlights distinct ways in which Opponent-Learning Awareness affects\nevolution.\n","authors":["Yann Bouteiller","Karthik Soma","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2410.17466v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.17465v1","updated":"2024-10-22T22:49:01Z","published":"2024-10-22T22:49:01Z","title":"Bauplan: zero-copy, scale-up FaaS for data pipelines","summary":"  Chaining functions for longer workloads is a key use case for FaaS platforms\nin data applications. However, modern data pipelines differ significantly from\ntypical serverless use cases (e.g., webhooks and microservices); this makes it\ndifficult to retrofit existing pipeline frameworks due to structural\nconstraints. In this paper, we describe these limitations in detail and\nintroduce bauplan, a novel FaaS programming model and serverless runtime\ndesigned for data practitioners. bauplan enables users to declaratively define\nfunctional Directed Acyclic Graphs (DAGs) along with their runtime\nenvironments, which are then efficiently executed on cloud-based workers. We\nshow that bauplan achieves both better performance and a superior developer\nexperience for data workloads by making the trade-off of reducing generality in\nfavor of data-awareness\n","authors":["Jacopo Tagliabue","Tyler Caraza-Harter","Ciro Greco"],"pdf_url":"https://arxiv.org/pdf/2410.17465v1.pdf","comment":"Accepted for the 10th International Workshop on Serverless Computing\n  (pre-print)"},{"id":"http://arxiv.org/abs/2405.15473v2","updated":"2024-10-22T22:48:15Z","published":"2024-05-24T11:51:08Z","title":"Encoder Embedding for General Graph and Node Classification","summary":"  Graph encoder embedding, a recent technique for graph data, offers speed and\nscalability in producing vertex-level representations from binary graphs. In\nthis paper, we extend the applicability of this method to a general graph\nmodel, which includes weighted graphs, distance matrices, and kernel matrices.\nWe prove that the encoder embedding satisfies the law of large numbers and the\ncentral limit theorem on a per-observation basis. Under certain condition, it\nachieves asymptotic normality on a per-class basis, enabling optimal\nclassification through discriminant analysis. These theoretical findings are\nvalidated through a series of experiments involving weighted graphs, as well as\ntext and image data transformed into general graph representations using\nappropriate distance metrics.\n","authors":["Cencheng Shen"],"pdf_url":"https://arxiv.org/pdf/2405.15473v2.pdf","comment":"16 pages"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2407.02273v4","updated":"2024-10-22T06:48:54Z","published":"2024-07-02T14:02:53Z","title":"Language Model Alignment in Multilingual Trolley Problems","summary":"  We evaluate the moral alignment of large language models (LLMs) with human\npreferences in multilingual trolley problems. Building on the Moral Machine\nexperiment, which captures over 40 million human judgments across 200+\ncountries, we develop a cross-lingual corpus of moral dilemma vignettes in over\n100 languages called MultiTP. This dataset enables the assessment of LLMs'\ndecision-making processes in diverse linguistic contexts. Our analysis explores\nthe alignment of 19 different LLMs with human judgments, capturing preferences\nacross six moral dimensions: species, gender, fitness, status, age, and the\nnumber of lives involved. By correlating these preferences with the demographic\ndistribution of language speakers and examining the consistency of LLM\nresponses to various prompt paraphrasings, our findings provide insights into\ncross-lingual and ethical biases of LLMs and their intersection. We discover\nsignificant variance in alignment across languages, challenging the assumption\nof uniform moral reasoning in AI systems and highlighting the importance of\nincorporating diverse perspectives in AI ethics. The results underscore the\nneed for further research on the integration of multilingual dimensions in\nresponsible AI research to ensure fair and equitable AI interactions worldwide.\nOur code and data are at https://github.com/causalNLP/moralmachine\n","authors":["Zhijing Jin","Max Kleiman-Weiner","Giorgio Piatti","Sydney Levine","Jiarui Liu","Fernando Gonzalez","Francesco Ortu","András Strausz","Mrinmaya Sachan","Rada Mihalcea","Yejin Choi","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2407.02273v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16077v2","updated":"2024-10-22T09:37:45Z","published":"2024-10-21T14:55:59Z","title":"CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian\n  Product Routing in Mixture-of-Experts","summary":"  Large language models (LLM) have been attracting much attention from the\ncommunity recently, due to their remarkable performance in all kinds of\ndownstream tasks. According to the well-known scaling law, scaling up a dense\nLLM enhances its capabilities, but also significantly increases the\ncomputational complexity. Mixture-of-Experts (MoE) models address that by\nallowing the model size to grow without substantially raising training or\ninference costs. Yet MoE models face challenges regarding knowledge sharing\namong experts, making their performance somehow sensitive to routing accuracy.\nTo tackle that, previous works introduced shared experts and combined their\noutputs with those of the top $K$ routed experts in an ``addition'' manner. In\nthis paper, inspired by collective matrix factorization to learn shared\nknowledge among data, we propose CartesianMoE, which implements more effective\nknowledge sharing among experts in more like a ``multiplication'' manner.\nExtensive experimental results indicate that CartesianMoE outperforms previous\nMoE models for building LLMs, in terms of both perplexity and downstream task\nperformance. And we also find that CartesianMoE achieves better expert routing\nrobustness.\n","authors":["Zhenpeng Su","Xing Wu","Zijia Lin","Yizhe Xiong","Minxuan Lv","Guangyuan Ma","Hui Chen","Songlin Hu","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2410.16077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16070v2","updated":"2024-10-22T13:40:18Z","published":"2024-10-21T14:48:35Z","title":"On-Device LLMs for SMEs: Challenges and Opportunities","summary":"  This paper presents a systematic review of the infrastructure requirements\nfor deploying Large Language Models (LLMs) on-device within the context of\nsmall and medium-sized enterprises (SMEs), focusing on both hardware and\nsoftware perspectives. From the hardware viewpoint, we discuss the utilization\nof processing units like GPUs and TPUs, efficient memory and storage solutions,\nand strategies for effective deployment, addressing the challenges of limited\ncomputational resources typical in SME settings. From the software perspective,\nwe explore framework compatibility, operating system optimization, and the use\nof specialized libraries tailored for resource-constrained environments. The\nreview is structured to first identify the unique challenges faced by SMEs in\ndeploying LLMs on-device, followed by an exploration of the opportunities that\nboth hardware innovations and software adaptations offer to overcome these\nobstacles. Such a structured review provides practical insights, contributing\nsignificantly to the community by enhancing the technological resilience of\nSMEs in integrating LLMs.\n","authors":["Jeremy Stephen Gabriel Yee","Pai Chet Ng","Zhengkui Wang","Ian McLoughlin","Aik Beng Ng","Simon See"],"pdf_url":"https://arxiv.org/pdf/2410.16070v2.pdf","comment":"9 pages, 1 figure. The work is supported by the SIT-NVIDIA Joint AI\n  Centre"},{"id":"http://arxiv.org/abs/2410.15639v2","updated":"2024-10-22T03:14:46Z","published":"2024-10-21T04:57:09Z","title":"Can Large Language Models Invent Algorithms to Improve Themselves?","summary":"  Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and learns model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. In mathematical reasoning tasks, Self-Developing not only creates\nmodels that surpass the seed model but also consistently outperforms models\ncreated using human-designed algorithms. Additionally, these LLM-discovered\nalgorithms demonstrate strong effectiveness, including transferability to\nout-of-domain models.\n","authors":["Yoichi Ishibashi","Taro Yano","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2410.15639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15608v2","updated":"2024-10-22T13:55:26Z","published":"2024-10-21T03:13:20Z","title":"Moonshine: Speech Recognition for Live Transcription and Voice Commands","summary":"  This paper introduces Moonshine, a family of speech recognition models\noptimized for live transcription and voice command processing. Moonshine is\nbased on an encoder-decoder transformer architecture and employs Rotary\nPosition Embedding (RoPE) instead of traditional absolute position embeddings.\nThe model is trained on speech segments of various lengths, but without using\nzero-padding, leading to greater efficiency for the encoder during inference\ntime. When benchmarked against OpenAI's Whisper tiny-en, Moonshine Tiny\ndemonstrates a 5x reduction in compute requirements for transcribing a\n10-second speech segment while incurring no increase in word error rates across\nstandard evaluation datasets. These results highlight Moonshine's potential for\nreal-time and resource-constrained applications.\n","authors":["Nat Jeffries","Evan King","Manjunath Kudlur","Guy Nicholson","James Wang","Pete Warden"],"pdf_url":"https://arxiv.org/pdf/2410.15608v2.pdf","comment":"7 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.17251v1","updated":"2024-10-22T17:59:57Z","published":"2024-10-22T17:59:57Z","title":"Altogether: Image Captioning via Re-aligning Alt-text","summary":"  This paper focuses on creating synthetic data to improve the quality of image\ncaptions. Existing works typically have two shortcomings. First, they caption\nimages from scratch, ignoring existing alt-text metadata, and second, lack\ntransparency if the captioners' training data (e.g. GPT) is unknown. In this\npaper, we study a principled approach Altogether based on the key idea to edit\nand re-align existing alt-texts associated with the images. To generate\ntraining data, we perform human annotation where annotators start with the\nexisting alt-text and re-align it to the image content in multiple rounds,\nconsequently constructing captions with rich visual concepts. This differs from\nprior work that carries out human annotation as a one-time description task\nsolely based on images and annotator knowledge. We train a captioner on this\ndata that generalizes the process of re-aligning alt-texts at scale. Our\nresults show our Altogether approach leads to richer image captions that also\nimprove text-to-image generation and zero-shot image classification tasks.\n","authors":["Hu Xu","Po-Yao Huang","Xiaoqing Ellen Tan","Ching-Feng Yeh","Jacob Kahn","Christine Jou","Gargi Ghosh","Omer Levy","Luke Zettlemoyer","Wen-tau Yih","Shang-Wen Li","Saining Xie","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2410.17251v1.pdf","comment":"accepted by EMNLP 2024; MetaCLIPv2"},{"id":"http://arxiv.org/abs/2410.17250v1","updated":"2024-10-22T17:59:56Z","published":"2024-10-22T17:59:56Z","title":"JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding\n  Benchmark for Culture-aware Evaluation","summary":"  Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.\n","authors":["Shota Onohara","Atsuyuki Miyai","Yuki Imajuku","Kazuki Egashira","Jeonghun Baek","Xiang Yue","Graham Neubig","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2410.17250v1.pdf","comment":"Project page: https://mmmu-japanese-benchmark.github.io/JMMMU/"},{"id":"http://arxiv.org/abs/2410.17247v1","updated":"2024-10-22T17:59:53Z","published":"2024-10-22T17:59:53Z","title":"PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid\n  Visual Redundancy Reduction","summary":"  In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. We hope that the insights and approach\nintroduced by PyramidDrop will inspire future research to further investigate\nthe role of image tokens in LVLMs.\n","authors":["Long Xing","Qidong Huang","Xiaoyi Dong","Jiajie Lu","Pan Zhang","Yuhang Zang","Yuhang Cao","Conghui He","Jiaqi Wang","Feng Wu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.17247v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.17245v1","updated":"2024-10-22T17:59:39Z","published":"2024-10-22T17:59:39Z","title":"Towards Reliable Evaluation of Behavior Steering Interventions in LLMs","summary":"  Representation engineering methods have recently shown promise for enabling\nefficient steering of model behavior. However, evaluation pipelines for these\nmethods have primarily relied on subjective demonstrations, instead of\nquantitative, objective metrics. We aim to take a step towards addressing this\nissue by advocating for four properties missing from current evaluations: (i)\ncontexts sufficiently similar to downstream tasks should be used for assessing\nintervention quality; (ii) model likelihoods should be accounted for; (iii)\nevaluations should allow for standardized comparisons across different target\nbehaviors; and (iv) baseline comparisons should be offered. We introduce an\nevaluation pipeline grounded in these criteria, offering both a quantitative\nand visual analysis of how effectively a given method works. We use this\npipeline to evaluate two representation engineering methods on how effectively\nthey can steer behaviors such as truthfulness and corrigibility, finding that\nsome interventions are less effective than previously reported.\n","authors":["Itamar Pres","Laura Ruis","Ekdeep Singh Lubana","David Krueger"],"pdf_url":"https://arxiv.org/pdf/2410.17245v1.pdf","comment":"Accepted to the NeurIPS 2024 - Workshop on Foundation Model\n  Interventions"},{"id":"http://arxiv.org/abs/2410.17238v1","updated":"2024-10-22T17:56:08Z","published":"2024-10-22T17:56:08Z","title":"SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning","summary":"  Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.\n","authors":["Yizhou Chi","Yizhang Lin","Sirui Hong","Duyi Pan","Yaying Fei","Guanghao Mei","Bangbang Liu","Tianqi Pang","Jacky Kwok","Ceyao Zhang","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17238v1.pdf","comment":"The code is available at https://github.com/geekan/MetaGPT"},{"id":"http://arxiv.org/abs/2410.17236v1","updated":"2024-10-22T17:54:45Z","published":"2024-10-22T17:54:45Z","title":"Large Language Models Empowered Personalized Web Agents","summary":"  Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB.\n","authors":["Hongru Cai","Yongqi Li","Wenjie Wang","Fengbin Zhu","Xiaoyu Shen","Wenjie Li","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.17236v1.pdf","comment":"The code and data are available on the project website\n  https://hongrucai.github.io/PersonalWAB/"},{"id":"http://arxiv.org/abs/2410.17235v1","updated":"2024-10-22T17:54:07Z","published":"2024-10-22T17:54:07Z","title":"Automated Spinal MRI Labelling from Reports Using a Large Language Model","summary":"  We propose a general pipeline to automate the extraction of labels from\nradiology reports using large language models, which we validate on spinal MRI\nreports. The efficacy of our labelling method is measured on five distinct\nconditions: spinal cancer, stenosis, spondylolisthesis, cauda equina\ncompression and herniation. Using open-source models, our method equals or\nsurpasses GPT-4 on a held-out set of reports. Furthermore, we show that the\nextracted labels can be used to train imaging models to classify the identified\nconditions in the accompanying MR scans. All classifiers trained using\nautomated labels achieve comparable performance to models trained using scans\nmanually annotated by clinicians. Code can be found at\nhttps://github.com/robinyjpark/AutoLabelClassifier.\n","authors":["Robin Y. Park","Rhydian Windsor","Amir Jamaludin","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2410.17235v1.pdf","comment":"Accepted to Medical Image Computing and Computer Assisted\n  Intervention (MICCAI 2024, Spotlight). 11 pages plus appendix"},{"id":"http://arxiv.org/abs/2410.17234v1","updated":"2024-10-22T17:54:03Z","published":"2024-10-22T17:54:03Z","title":"Fine-Tuning Large Language Models to Appropriately Abstain with Semantic\n  Entropy","summary":"  Large Language Models (LLMs) are known to hallucinate, whereby they generate\nplausible but inaccurate text. This phenomenon poses significant risks in\ncritical applications, such as medicine or law, necessitating robust\nhallucination mitigation strategies. While recent works have proposed\nfine-tuning methods to teach LLMs to abstain from answering questions beyond\ntheir knowledge or capabilities, these methods rely on the existence of\nground-truth labels or are limited to short-form responses. To address these\nlimitations, we propose fine-tuning using semantic entropy, an uncertainty\nmeasure derived from introspection into the model which does not require\nexternal labels. We demonstrate that our approach matches or outperforms models\nfine-tuned using prior work and achieves strong performance for both short and\nlong-form generations on a range of datasets.\n","authors":["Benedict Aaron Tjandra","Muhammed Razzak","Jannik Kossen","Kunal Handa","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2410.17234v1.pdf","comment":"Accepted to NeurIPS Safe Generative AI Workshop 2024"},{"id":"http://arxiv.org/abs/2407.12883v2","updated":"2024-10-22T17:49:31Z","published":"2024-07-16T17:58:27Z","title":"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive\n  Retrieval","summary":"  Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. Our dataset consists of 1,384\nreal-world queries spanning diverse domains, such as economics, psychology,\nmathematics, and coding. These queries are drawn from naturally occurring and\ncarefully curated human data. Extensive evaluation reveals that even\nstate-of-the-art retrieval models perform poorly on BRIGHT. The leading model\non the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of\n59.0 nDCG@10, produces a score of nDCG@10 of 18.3 on BRIGHT. We show that\nincorporating explicit reasoning about the query improves retrieval performance\nby up to 12.2 points. Moreover, incorporating retrieved documents from the\ntop-performing retriever boosts question-answering performance by over 6.6\npoints. We believe that BRIGHT paves the way for future research on retrieval\nsystems in more realistic and challenging settings.\n","authors":["Hongjin Su","Howard Yen","Mengzhou Xia","Weijia Shi","Niklas Muennighoff","Han-yu Wang","Haisu Liu","Quan Shi","Zachary S. Siegel","Michael Tang","Ruoxi Sun","Jinsung Yoon","Sercan O. Arik","Danqi Chen","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2407.12883v2.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2410.17225v1","updated":"2024-10-22T17:47:05Z","published":"2024-10-22T17:47:05Z","title":"Dhoroni: Exploring Bengali Climate Change and Environmental Views with a\n  Multi-Perspective News Dataset and Natural Language Processing","summary":"  Climate change poses critical challenges globally, disproportionately\naffecting low-income countries that often lack resources and linguistic\nrepresentation on the international stage. Despite Bangladesh's status as one\nof the most vulnerable nations to climate impacts, research gaps persist in\nBengali-language studies related to climate change and NLP. To address this\ndisparity, we introduce Dhoroni, a novel Bengali (Bangla) climate change and\nenvironmental news dataset, comprising a 2300 annotated Bangla news articles,\noffering multiple perspectives such as political influence,\nscientific/statistical data, authenticity, stance detection, and stakeholder\ninvolvement. Furthermore, we present an in-depth exploratory analysis of\nDhoroni and introduce BanglaBERT-Dhoroni family, a novel baseline model family\nfor climate and environmental opinion detection in Bangla, fine-tuned on our\ndataset. This research contributes significantly to enhancing accessibility and\nanalysis of climate discourse in Bengali (Bangla), addressing crucial\ncommunication and research gaps in climate-impacted regions like Bangladesh\nwith 180 million people.\n","authors":["Azmine Toushik Wasi","Wahid Faisal","Taj Ahmad","Abdur Rahman","Mst Rafia Islam"],"pdf_url":"https://arxiv.org/pdf/2410.17225v1.pdf","comment":"In Review"},{"id":"http://arxiv.org/abs/2410.17222v1","updated":"2024-10-22T17:45:47Z","published":"2024-10-22T17:45:47Z","title":"Context-aware Prompt Tuning: Advancing In-Context Learning with\n  Adversarial Methods","summary":"  Fine-tuning Large Language Models (LLMs) typically involves updating at least\na few billions of parameters. A more parameter-efficient approach is Prompt\nTuning (PT), which updates only a few learnable tokens, and differently,\nIn-Context Learning (ICL) adapts the model to a new task by simply including\nexamples in the input without any training. When applying optimization-based\nmethods, such as fine-tuning and PT for few-shot learning, the model is\nspecifically adapted to the small set of training examples, whereas ICL leaves\nthe model unchanged. This distinction makes traditional learning methods more\nprone to overfitting; in contrast, ICL is less sensitive to the few-shot\nscenario. While ICL is not prone to overfitting, it does not fully extract the\ninformation that exists in the training examples. This work introduces\nContext-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and\nadversarial attacks. We build on the ICL strategy of concatenating examples\nbefore the input, but we extend this by PT-like learning, refining the context\nembedding through iterative optimization to extract deeper insights from the\ntraining examples. We carefully modify specific context tokens, considering the\nunique structure of input and output formats. Inspired by adversarial attacks,\nwe adjust the input based on the labels present in the context, focusing on\nminimizing, rather than maximizing, the loss. Moreover, we apply a projected\ngradient descent algorithm to keep token embeddings close to their original\nvalues, under the assumption that the user-provided data is inherently\nvaluable. Our method has been shown to achieve superior accuracy across\nmultiple classification tasks using various LLM models.\n","authors":["Tsachi Blau","Moshe Kimhi","Yonatan Belinkov","Alexander Bronstein","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2410.17222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17218v1","updated":"2024-10-22T17:43:39Z","published":"2024-10-22T17:43:39Z","title":"Creativity in AI: Progresses and Challenges","summary":"  Creativity is the ability to produce novel, useful, and surprising ideas, and\nhas been widely studied as a crucial aspect of human cognition. Machine\ncreativity on the other hand has been a long-standing challenge. With the rise\nof advanced generative AI, there has been renewed interest and debate regarding\nAI's creative capabilities. Therefore, it is imperative to revisit the state of\ncreativity in AI and identify key progresses and remaining challenges. In this\nwork, we survey leading works studying the creative capabilities of AI systems,\nfocusing on creative problem-solving, linguistic, artistic, and scientific\ncreativity. Our review suggests that while the latest AI models are largely\ncapable of producing linguistically and artistically creative outputs such as\npoems, images, and musical pieces, they struggle with tasks that require\ncreative problem-solving, abstract thinking and compositionality and their\ngenerations suffer from a lack of diversity, originality, long-range\nincoherence and hallucinations. We also discuss key questions concerning\ncopyright and authorship issues with generative models. Furthermore, we\nhighlight the need for a comprehensive evaluation of creativity that is\nprocess-driven and considers several dimensions of creativity. Finally, we\npropose future research directions to improve the creativity of AI outputs,\ndrawing inspiration from cognitive science and psychology.\n","authors":["Mete Ismayilzada","Debjit Paul","Antoine Bosselut","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2410.17218v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2410.17215v1","updated":"2024-10-22T17:40:32Z","published":"2024-10-22T17:40:32Z","title":"MiniPLM: Knowledge Distillation for Pre-Training Language Models","summary":"  Knowledge distillation (KD) is widely used to train small, high-performing\nstudent language models (LMs) using large teacher LMs. While effective in\nfine-tuning, KD during pre-training faces challenges in efficiency,\nflexibility, and effectiveness. Existing methods either incur high\ncomputational costs due to online teacher inference, require tokenization\nmatching between teacher and student LMs, or risk losing the difficulty and\ndiversity of the teacher-generated training data. To address these issues, we\npropose MiniPLM, a KD framework for pre-training LMs by refining the training\ndata distribution with the teacher's knowledge. For efficiency, MiniPLM\nperforms offline teacher LM inference, allowing KD for multiple student LMs\nwithout adding training-time costs. For flexibility, MiniPLM operates solely on\nthe training corpus, enabling KD across model families. For effectiveness,\nMiniPLM leverages the differences between large and small LMs to enhance the\ndifficulty and diversity of the training data, helping student LMs acquire\nversatile and sophisticated knowledge. Extensive experiments demonstrate that\nMiniPLM boosts the student LMs' performance on 9 widely used downstream tasks,\nimproves the language modeling capabilities, and reduces pre-training\ncomputation. The benefit of MiniPLM extends to large pre-training scales,\nevidenced by the extrapolation of the scaling curves. Further analysis reveals\nthat MiniPLM supports KD across model families and enhances the utilization of\npre-training data. Our model, code, and data are available at\nhttps://github.com/thu-coai/MiniPLM.\n","authors":["Yuxian Gu","Hao Zhou","Fandong Meng","Jie Zhou","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.17215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10796v2","updated":"2024-10-22T17:35:03Z","published":"2024-10-14T17:57:09Z","title":"Context-Parametric Inversion: Why Instruction Finetuning May Not\n  Actually Improve Context Reliance","summary":"  A standard practice when using large language models is for users to\nsupplement their instruction with an input context containing new information\nfor the model to process. However, models struggle to reliably follow the input\ncontext, especially when it conflicts with their parametric knowledge from\npretraining. In-principle, one would expect models to adapt to the user context\nbetter after instruction finetuning, particularly when handling knowledge\nconflicts. However, we observe a surprising failure mode: during instruction\ntuning, the context reliance under knowledge conflicts initially increases as\nexpected, but then gradually decreases as instruction finetuning progresses.\nThis happens while the performance on standard benchmarks keeps on increasing\nfar after this drop. We call this phenomenon context-parametric inversion and\nobserve it across multiple general purpose instruction tuning datasets such as\nTULU, Alpaca and Ultrachat, across different model families like Llama,\nMistral, and Pythia. We perform various controlled studies and theoretical\nanalysis to show that context-parametric inversion occurs due to examples in\nthe instruction finetuning data where the input context provides information\nthat aligns with model's parametric knowledge. Our analysis suggests some\nnatural mitigation strategies with limited but insightful gains, and serves as\na useful starting point in addressing this deficiency in instruction\nfinetuning.\n","authors":["Sachin Goyal","Christina Baek","J. Zico Kolter","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2410.10796v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.17210v1","updated":"2024-10-22T17:34:59Z","published":"2024-10-22T17:34:59Z","title":"Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh\n  through Large Language Modeling","summary":"  Purpose: Bangladesh's legal system struggles with major challenges like\ndelays, complexity, high costs, and millions of unresolved cases, which deter\nmany from pursuing legal action due to lack of knowledge or financial\nconstraints. This research seeks to develop a specialized Large Language Model\n(LLM) to assist in the Bangladeshi legal system. Methods: We created\nUKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and\nscraping data on various legal acts. We fine-tuned the GPT-2 model on this\ndataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance\nin English. Results: The model was rigorously evaluated using semantic\nassessments, including case studies supported by expert opinions. The\nevaluation provided promising results, demonstrating the potential for the\nmodel to assist in legal matters within Bangladesh. Conclusion: Our work\nrepresents the first structured effort toward building an AI-based legal\nassistant for Bangladesh. While the results are encouraging, further\nrefinements are necessary to improve the model's accuracy, credibility, and\nsafety. This is a significant step toward creating a legal AI capable of\nserving the needs of a population of 180 million.\n","authors":["Azmine Toushik Wasi","Wahid Faisal","Mst Rafia Islam","Mahathir Mohammad Bappy"],"pdf_url":"https://arxiv.org/pdf/2410.17210v1.pdf","comment":"In Review"},{"id":"http://arxiv.org/abs/2410.17209v1","updated":"2024-10-22T17:31:37Z","published":"2024-10-22T17:31:37Z","title":"Audio-to-Score Conversion Model Based on Whisper methodology","summary":"  This thesis develops a Transformer model based on Whisper, which extracts\nmelodies and chords from music audio and records them into ABC notation. A\ncomprehensive data processing workflow is customized for ABC notation,\nincluding data cleansing, formatting, and conversion, and a mutation mechanism\nis implemented to increase the diversity and quality of training data. This\nthesis innovatively introduces the \"Orpheus' Score\", a custom notation system\nthat converts music information into tokens, designs a custom vocabulary\nlibrary, and trains a corresponding custom tokenizer. Experiments show that\ncompared to traditional algorithms, the model has significantly improved\naccuracy and performance. While providing a convenient audio-to-score tool for\nmusic enthusiasts, this work also provides new ideas and tools for research in\nmusic information processing.\n","authors":["Hongyao Zhang","Bohang Sun"],"pdf_url":"https://arxiv.org/pdf/2410.17209v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17196v1","updated":"2024-10-22T17:15:20Z","published":"2024-10-22T17:15:20Z","title":"VoiceBench: Benchmarking LLM-Based Voice Assistants","summary":"  Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.\n","authors":["Yiming Chen","Xianghu Yue","Chen Zhang","Xiaoxue Gao","Robby T. Tan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.17196v1.pdf","comment":"Work in progress. Data is available at\n  https://github.com/MatthewCYM/VoiceBench"},{"id":"http://arxiv.org/abs/2410.17195v1","updated":"2024-10-22T17:13:38Z","published":"2024-10-22T17:13:38Z","title":"Language Model Non-myopic Generation for Reasoning and Planning","summary":"  Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.\n","authors":["Chang Ma","Haiteng Zhao","Junlei Zhang","Junxian He","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.17195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08213v2","updated":"2024-10-22T17:07:14Z","published":"2024-03-13T03:22:02Z","title":"Can Large Language Models Identify Authorship?","summary":"  The ability to accurately identify authorship is crucial for verifying\ncontent authenticity and mitigating misinformation. Large Language Models\n(LLMs) have demonstrated an exceptional capacity for reasoning and\nproblem-solving. However, their potential in authorship analysis remains\nunder-explored. Traditional studies have depended on hand-crafted stylistic\nfeatures, whereas state-of-the-art approaches leverage text embeddings from\npre-trained language models. These methods, which typically require fine-tuning\non labeled data, often suffer from performance degradation in cross-domain\napplications and provide limited explainability. This work seeks to address\nthree research questions: (1) Can LLMs perform zero-shot, end-to-end authorship\nverification effectively? (2) Are LLMs capable of accurately attributing\nauthorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs\nprovide explainability in authorship analysis, particularly through the role of\nlinguistic features? Moreover, we investigate the integration of explicit\nlinguistic features to guide LLMs in their reasoning processes. Our assessment\ndemonstrates LLMs' proficiency in both tasks without the need for\ndomain-specific fine-tuning, providing explanations into their decision making\nvia a detailed analysis of linguistic features. This establishes a new\nbenchmark for future research on LLM-based authorship analysis.\n","authors":["Baixiang Huang","Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2403.08213v2.pdf","comment":"Accepted to EMNLP 2024 Findings. The main paper is 9 pages long, with\n  16 pages total. The code, results, dataset, and additional resources are\n  available on the project website: https://llm-authorship.github.io/"},{"id":"http://arxiv.org/abs/2409.13686v2","updated":"2024-10-22T17:06:17Z","published":"2024-09-20T17:54:16Z","title":"The Impact of Large Language Models in Academia: from Writing to\n  Speaking","summary":"  Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.\n","authors":["Mingmeng Geng","Caixi Chen","Yanru Wu","Dongping Chen","Yao Wan","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.13686v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2405.06643v2","updated":"2024-10-22T17:05:17Z","published":"2024-03-06T23:02:30Z","title":"Levels of AI Agents: from Rules to Large Language Models","summary":"  AI agents are defined as artificial entities to perceive the environment,\nmake decisions and take actions. Inspired by the 6 levels of autonomous driving\nby Society of Automotive Engineers, the AI agents are also categorized based on\nutilities and strongness, as the following levels: L0, no AI, with tools taking\ninto account perception plus actions; L1, using rule-based AI; L2, making\nrule-based AI replaced by IL/RL-based AI, with additional reasoning & decision\nmaking; L3, applying LLM-based AI instead of IL/RL-based AI, additionally\nsetting up memory & reflection; L4, based on L3, facilitating autonomous\nlearning & generalization; L5, based on L4, appending personality of emotion\nand character and collaborative behavior with multi-agents.\n","authors":["Yu Huang"],"pdf_url":"https://arxiv.org/pdf/2405.06643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14344v2","updated":"2024-10-22T16:59:12Z","published":"2024-07-19T14:28:07Z","title":"LLMs left, right, and center: Assessing GPT's capabilities to label\n  political bias from web domains","summary":"  This research investigates whether OpenAI's GPT-4, a state-of-the-art large\nlanguage model, can accurately classify the political bias of news sources\nbased solely on their URLs. Given the subjective nature of political labels,\nthird-party bias ratings like those from Ad Fontes Media, AllSides, and Media\nBias/Fact Check (MBFC) are often used in research to analyze news source\ndiversity. This study aims to determine if GPT-4 can replicate these human\nratings on a seven-degree scale (\"far-left\" to \"far-right\"). The analysis\ncompares GPT-4's classifications against MBFC's, and controls for website\npopularity using Open PageRank scores. Findings reveal a high correlation\n($\\text{Spearman's } \\rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and\nMBFC's ratings, indicating the model's potential reliability. However, GPT-4\nabstained from classifying approximately $\\frac{2}{3}$ of the dataset. It is\nmore likely to abstain from rating unpopular websites, which also suffer from\nless accurate assessments. The LLM tends to avoid classifying sources that MBFC\nconsiders to be centrist, resulting in more polarized outputs. Finally, this\nanalysis shows a slight leftward skew in GPT's classifications compared to\nMBFC's. Therefore, while this paper suggests that while GPT-4 can be a\nscalable, cost-effective tool for political bias classification of news\nwebsites, its use should be as a complement to human judgment to mitigate\nbiases.\n","authors":["Raphael Hernandes","Giulio Corsi"],"pdf_url":"https://arxiv.org/pdf/2407.14344v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.17174v1","updated":"2024-10-22T16:51:27Z","published":"2024-10-22T16:51:27Z","title":"From Attention to Activation: Unravelling the Enigmas of Large Language\n  Models","summary":"  We study two strange phenomena in auto-regressive Transformers: (1) the\ndominance of the first token in attention heads; (2) the occurrence of large\noutlier activations in the hidden states. We find that popular large language\nmodels, such as Llama attend maximally to the first token in 98% of attention\nheads, a behaviour we attribute to the softmax function. To mitigate this\nissue, we propose a reformulation of softmax to softmax-1. Furthermore, we\nidentify adaptive optimisers, e.g. Adam, as the primary contributor to the\nlarge outlier activations and introduce OrthoAdam, a novel optimiser that\nutilises orthogonal matrices to transform gradients, to address this issue.\nFinally, not only do our methods prevent these phenomena from occurring, but\nadditionally, they enable Transformers to sustain their performance when\nquantised using basic algorithms, something that standard methods are unable to\ndo. In summary, our methods reduce the attention proportion on the first token\nfrom 65% to 3.3%, the activation kurtosis in the hidden states from 1657 to\n3.1, and perplexity penalty under 4-bit weight quantisation from 3565 to 0.3.\n","authors":["Prannay Kaul","Chengcheng Ma","Ismail Elezi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2410.17174v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.17170v1","updated":"2024-10-22T16:50:00Z","published":"2024-10-22T16:50:00Z","title":"Self-calibration for Language Model Quantization and Pruning","summary":"  Quantization and pruning are fundamental approaches for model compression,\nenabling efficient inference for language models. In a post-training setting,\nstate-of-the-art quantization and pruning methods require calibration data, a\nsmall set of unlabeled examples. Conventionally, randomly sampled web text is\nused, aiming to reflect the model training data. However, this poses two key\nproblems: (1) unrepresentative calibration examples can harm model performance,\nand (2) organizations increasingly avoid releasing model training data. In this\npaper, we propose self-calibration as a solution. Our approach requires no\nexternal data, instead leveraging the model itself to generate synthetic\ncalibration data as a better approximation of the pre-training data\ndistribution. We extensively compare the performance of self-calibration with\nseveral baselines, across a variety of models, compression methods, and tasks.\nOur approach proves consistently competitive in maximizing downstream task\nperformance, frequently outperforming even using real data.\n","authors":["Miles Williams","George Chrysostomou","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2410.17170v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.17161v1","updated":"2024-10-22T16:34:36Z","published":"2024-10-22T16:34:36Z","title":"Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence","summary":"  We propose a novel approach for learning interchangeable tokens in language\nmodels to obtain an extendable vocabulary that can generalize to new tokens.\nOur method is designed to address alpha-equivalence, the principle that\nrenaming bound variables in a syntactic expression preserves semantics. This\nproperty arises in many formal languages such as temporal logics, in which all\nproposition symbols represent the same concept but are distinguishable from\neach other. To handle such tokens, we develop a dual-part embedding approach.\nThe first part is shared across all interchangeable tokens, thereby enforcing\nthat they represent the same core concept. The second part is randomly\ngenerated for each token, which enables distinguishability. We evaluate our\nmethod in a Transformer encoder-decoder model on two tasks: solving linear\ntemporal logic formulae and copying with extendable vocabulary. Our method\ndemonstrates promising generalization capabilities in addition to introducing a\nfavorable inductive bias for alpha-equivalence.\n","authors":["İlker Işık","Ramazan Gokberk Cinbis","Ebru Aydin Gol"],"pdf_url":"https://arxiv.org/pdf/2410.17161v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.17152v1","updated":"2024-10-22T16:29:33Z","published":"2024-10-22T16:29:33Z","title":"Improving Pinterest Search Relevance Using Large Language Models","summary":"  To improve relevance scoring on Pinterest Search, we integrate Large Language\nModels (LLMs) into our search relevance model, leveraging carefully designed\ntext representations to predict the relevance of Pins effectively. Our approach\nuses search queries alongside content representations that include captions\nextracted from a generative visual language model. These are further enriched\nwith link-based text data, historically high-quality engaged queries,\nuser-curated boards, Pin titles and Pin descriptions, creating robust models\nfor predicting search relevance. We use a semi-supervised learning approach to\nefficiently scale up the amount of training data, expanding beyond the\nexpensive human labeled data available. By utilizing multilingual LLMs, our\nsystem extends training data to include unseen languages and domains, despite\ninitial data and annotator expertise being confined to English. Furthermore, we\ndistill from the LLM-based model into real-time servable model architectures\nand features. We provide comprehensive offline experimental validation for our\nproposed techniques and demonstrate the gains achieved through the final\ndeployed system at scale.\n","authors":["Han Wang","Mukuntha Narayanan Sundararaman","Onur Gungor","Yu Xu","Krishna Kamath","Rakesh Chalasani","Kurchi Subhra Hazra","Jinfeng Rao"],"pdf_url":"https://arxiv.org/pdf/2410.17152v1.pdf","comment":"CIKM 2024 Workshop on Industrial Recommendation Systems"},{"id":"http://arxiv.org/abs/2410.14594v2","updated":"2024-10-22T16:27:12Z","published":"2024-10-18T16:44:22Z","title":"Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases","summary":"  Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5).\n","authors":["Elias Lumer","Vamse Kumar Subbiah","James A. Burke","Pradeep Honaganahalli Basavaraju","Austin Huber"],"pdf_url":"https://arxiv.org/pdf/2410.14594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17145v1","updated":"2024-10-22T16:26:03Z","published":"2024-10-22T16:26:03Z","title":"Can General-Purpose Large Language Models Generalize to English-Thai\n  Machine Translation ?","summary":"  Large language models (LLMs) perform well on common tasks but struggle with\ngeneralization in low-resource and low-computation settings. We examine this\nlimitation by testing various LLMs and specialized translation models on\nEnglish-Thai machine translation and code-switching datasets. Our findings\nreveal that under more strict computational constraints, such as 4-bit\nquantization, LLMs fail to translate effectively. In contrast, specialized\nmodels, with comparable or lower computational requirements, consistently\noutperform LLMs. This underscores the importance of specialized models for\nmaintaining performance under resource constraints.\n","authors":["Jirat Chiaranaipanich","Naiyarat Hanmatheekuna","Jitkapat Sawatphol","Krittamate Tiankanon","Jiramet Kinchagawat","Amrest Chinkamol","Parinthapat Pengpun","Piyalitt Ittichaiwong","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2410.17145v1.pdf","comment":"Accepted in GenBench EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14669v2","updated":"2024-10-22T16:07:22Z","published":"2024-10-18T17:58:21Z","title":"NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples","summary":"  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n","authors":["Baiqi Li","Zhiqiu Lin","Wenxuan Peng","Jean de Dieu Nyandwi","Daniel Jiang","Zixian Ma","Simran Khanuja","Ranjay Krishna","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.14669v2.pdf","comment":"Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench ; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/"},{"id":"http://arxiv.org/abs/2410.17131v1","updated":"2024-10-22T16:04:03Z","published":"2024-10-22T16:04:03Z","title":"Aligning Large Language Models via Self-Steering Optimization","summary":"  Automated alignment develops alignment systems with minimal human\nintervention. The key to automated alignment lies in providing learnable and\naccurate preference signals for preference learning without human annotation.\nIn this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm\nthat autonomously generates high-quality preference signals based on predefined\nprinciples during iterative training, eliminating the need for manual\nannotation. $SSO$ maintains the accuracy of signals by ensuring a consistent\ngap between chosen and rejected responses while keeping them both on-policy to\nsuit the current policy model's learning capacity. $SSO$ can benefit the online\nand offline training of the policy model, as well as enhance the training of\nreward models. We validate the effectiveness of $SSO$ with two foundation\nmodels, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy\npreference signals throughout iterative training. Without any manual annotation\nor external models, $SSO$ leads to significant performance improvements across\nsix subjective or objective benchmarks. Besides, the preference data generated\nby $SSO$ significantly enhanced the performance of the reward model on\nRewardbench. Our work presents a scalable approach to preference optimization,\npaving the way for more efficient and effective automated alignment.\n","authors":["Hao Xiang","Bowen Yu","Hongyu Lin","Keming Lu","Yaojie Lu","Xianpei Han","Le Sun","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2410.17131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17127v1","updated":"2024-10-22T16:00:26Z","published":"2024-10-22T16:00:26Z","title":"PAPILLON: PrivAcy Preservation from Internet-based and Local Language\n  MOdel ENsembles","summary":"  Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code will be available at https://github.com/siyan-sylvia-li/PAPILLON.\n","authors":["Li Siyan","Vethavikashini Chithrra Raghuram","Omar Khattab","Julia Hirschberg","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.17127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17126v1","updated":"2024-10-22T15:59:58Z","published":"2024-10-22T15:59:58Z","title":"Exploring RL-based LLM Training for Formal Language Tasks with\n  Programmed Rewards","summary":"  Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning\nfrom Human Feedback to align large language models (LLMs) with downstream\ntasks. This paper investigates the feasibility of using PPO for direct\nreinforcement learning (RL) from explicitly programmed reward signals, as\nopposed to indirect learning from human feedback via an intermediary reward\nmodel. We focus on tasks expressed through formal languages, such as\nmathematics and programming, where explicit reward functions can be programmed\nto automatically assess the quality of generated outputs. We apply this\napproach to a sentiment alignment task, a simple arithmetic task, and a more\ncomplex game synthesis task. The sentiment alignment task replicates prior\nresearch and serves to validate our experimental setup. Our results show that\npure RL-based training for the two formal language tasks is challenging, with\nsuccess being limited even for the simple arithmetic task. We propose a novel\nbatch-entropy regularization term to aid exploration, although training is not\nyet entirely stable. Our findings suggest that direct RL training of LLMs may\nbe more suitable for relatively minor changes, such as alignment, than for\nlearning new tasks altogether, even if an informative reward signal can be\nexpressed programmatically.\n","authors":["Alexander G. Padula","Dennis J. N. J. Soemers"],"pdf_url":"https://arxiv.org/pdf/2410.17126v1.pdf","comment":"Accepted at BNAIC 2024"},{"id":"http://arxiv.org/abs/2410.17112v1","updated":"2024-10-22T15:37:46Z","published":"2024-10-22T15:37:46Z","title":"Enhancing Answer Attribution for Faithful Text Generation with Large\n  Language Models","summary":"  The increasing popularity of Large Language Models (LLMs) in recent years has\nchanged the way users interact with and pose questions to AI-based\nconversational systems. An essential aspect for increasing the trustworthiness\nof generated LLM answers is the ability to trace the individual claims from\nresponses back to relevant sources that support them, the process known as\nanswer attribution. While recent work has started exploring the task of answer\nattribution in LLMs, some challenges still remain. In this work, we first\nperform a case study analyzing the effectiveness of existing answer attribution\nmethods, with a focus on subtasks of answer segmentation and evidence\nretrieval. Based on the observed shortcomings, we propose new methods for\nproducing more independent and contextualized claims for better retrieval and\nattribution. The new methods are evaluated and shown to improve the performance\nof answer attribution components. We end with a discussion and outline of\nfuture directions for the task.\n","authors":["Juraj Vladika","Luca Mülln","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2410.17112v1.pdf","comment":"Accepted to KDIR 2024 (part of IC3K 2024)"},{"id":"http://arxiv.org/abs/2410.17099v1","updated":"2024-10-22T15:22:58Z","published":"2024-10-22T15:22:58Z","title":"Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations","summary":"  The quality is a crucial issue for crowd annotations. Answer aggregation is\nan important type of solution. The aggregated answers estimated from multiple\ncrowd answers to the same instance are the eventually collected annotations,\nrather than the individual crowd answers themselves. Recently, the capability\nof Large Language Models (LLMs) on data annotation tasks has attracted interest\nfrom researchers. Most of the existing studies mainly focus on the average\nperformance of individual crowd workers; several recent works studied the\nscenarios of aggregation on categorical labels and LLMs used as label creators.\nHowever, the scenario of aggregation on text answers and the role of LLMs as\naggregators are not yet well-studied. In this paper, we investigate the\ncapability of LLMs as aggregators in the scenario of close-ended crowd text\nanswer aggregation. We propose a human-LLM hybrid text answer aggregation\nmethod with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We\nmake the experiments based on public crowdsourcing datasets. The results show\nthe effectiveness of our approach based on the collaboration of crowd workers\nand LLMs.\n","authors":["Jiyi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17099v1.pdf","comment":"Accepted in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14516v2","updated":"2024-10-22T15:20:00Z","published":"2024-10-18T14:55:14Z","title":"Do LLMs \"know\" internally when they follow instructions?","summary":"  Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.\n","authors":["Juyeon Heo","Christina Heinze-Deml","Oussama Elachqar","Shirley Ren","Udhay Nallasamy","Andy Miller","Kwan Ho Ryan Chan","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13023v2","updated":"2024-10-22T15:18:55Z","published":"2022-09-26T20:49:18Z","title":"Lex2Sent: A bagging approach to unsupervised sentiment analysis","summary":"  Unsupervised text classification, with its most common form being sentiment\nanalysis, used to be performed by counting words in a text that were stored in\na lexicon, which assigns each word to one class or as a neutral word. In recent\nyears, these lexicon-based methods fell out of favor and were replaced by\ncomputationally demanding fine-tuning techniques for encoder-only models such\nas BERT and zero-shot classification using decoder-only models such as GPT-4.\nIn this paper, we propose an alternative approach: Lex2Sent, which provides\nimprovement over classic lexicon methods but does not require any GPU or\nexternal hardware. To classify texts, we train embedding models to determine\nthe distances between document embeddings and the embeddings of the parts of a\nsuitable lexicon. We employ resampling, which results in a bagging effect,\nboosting the performance of the classification. We show that our model\noutperforms lexica and provides a basis for a high performing few-shot\nfine-tuning approach in the task of binary sentiment analysis.\n","authors":["Kai-Robin Lange","Jonas Rieger","Carsten Jentsch"],"pdf_url":"https://arxiv.org/pdf/2209.13023v2.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.14582v2","updated":"2024-10-22T15:16:14Z","published":"2024-10-18T16:32:10Z","title":"Do LLMs estimate uncertainty well in instruction-following?","summary":"  Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.\n","authors":["Juyeon Heo","Miao Xiong","Christina Heinze-Deml","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17088v1","updated":"2024-10-22T15:14:54Z","published":"2024-10-22T15:14:54Z","title":"Science Out of Its Ivory Tower: Improving Accessibility with\n  Reinforcement Learning","summary":"  A vast amount of scholarly work is published daily, yet much of it remains\ninaccessible to the general public due to dense jargon and complex language. To\naddress this challenge in science communication, we introduce a reinforcement\nlearning framework that fine-tunes a language model to rewrite scholarly\nabstracts into more comprehensible versions. Guided by a carefully balanced\ncombination of word- and sentence-level accessibility rewards, our language\nmodel effectively substitutes technical terms with more accessible\nalternatives, a task which models supervised fine-tuned or guided by\nconventional readability measures struggle to accomplish. Our best model\nadjusts the readability level of scholarly abstracts by approximately six U.S.\ngrade levels -- in other words, from a postgraduate to a high school level.\nThis translates to roughly a 90% relative boost over the supervised fine-tuning\nbaseline, all while maintaining factual accuracy and high-quality language. An\nin-depth analysis of our approach shows that balanced rewards lead to\nsystematic modifications in the base model, likely contributing to smoother\noptimization and superior performance. We envision this work as a step toward\nbridging the gap between scholarly research and the general public,\nparticularly younger readers and those without a college degree.\n","authors":["Haining Wang","Jason Clark","Hannah McKelvey","Leila Sterman","Zheng Gao","Zuoyu Tian","Sandra Kübler","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16264v3","updated":"2024-10-22T15:09:58Z","published":"2024-06-24T02:03:57Z","title":"One Thousand and One Pairs: A \"novel\" challenge for long-context\n  language models","summary":"  Synthetic long-context LLM benchmarks (e.g., \"needle-in-the-haystack\") test\nonly surface-level retrieval capabilities, but how well can long-context LLMs\nretrieve, synthesize, and reason over information across book-length inputs? We\naddress this question by creating NoCha, a dataset of 1,001 minimally different\npairs of true and false claims about 67 recently-published English fictional\nbooks, written by human readers of those books. In contrast to existing\nlong-context benchmarks, our annotators confirm that the largest share of pairs\nin NoCha require global reasoning over the entire book to verify. Our\nexperiments show that while human readers easily perform this task, it is\nenormously challenging for all ten long-context LLMs that we evaluate: no\nopen-weight model performs above random chance (despite their strong\nperformance on synthetic benchmarks), while GPT-4o achieves the highest\naccuracy at 55.8%. Further analysis reveals that (1) on average, models perform\nmuch better on pairs that require only sentence-level retrieval vs. global\nreasoning; (2) model-generated explanations for their decisions are often\ninaccurate even for correctly-labeled claims; and (3) models perform\nsubstantially worse on speculative fiction books that contain extensive\nworld-building. The methodology proposed in NoCha allows for the evolution of\nthe benchmark dataset and the easy analysis of future models.\n","authors":["Marzena Karpinska","Katherine Thai","Kyle Lo","Tanya Goyal","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2406.16264v3.pdf","comment":"EMNLP 2024, camera ready"},{"id":"http://arxiv.org/abs/2408.10943v2","updated":"2024-10-22T15:07:35Z","published":"2024-08-20T15:33:16Z","title":"SysBench: Can Large Language Models Follow System Messages?","summary":"  Large Language Models (LLMs) have become instrumental across various\napplications, with the customization of these models to specific scenarios\nbecoming increasingly critical. System message, a fundamental component of\nLLMs, is consist of carefully crafted instructions that guide the behavior of\nmodel to meet intended goals. Despite the recognized potential of system\nmessages to optimize AI-driven solutions, there is a notable absence of a\ncomprehensive benchmark for evaluating how well LLMs follow system messages. To\nfill this gap, we introduce SysBench, a benchmark that systematically analyzes\nsystem message following ability in terms of three limitations of existing\nLLMs: constraint violation, instruction misjudgement and multi-turn\ninstability. Specifically, we manually construct evaluation dataset based on\nsix prevalent types of constraints, including 500 tailor-designed system\nmessages and multi-turn user conversations covering various interaction\nrelationships. Additionally, we develop a comprehensive evaluation protocol to\nmeasure model performance. Finally, we conduct extensive evaluation across\nvarious existing LLMs, measuring their ability to follow specified constraints\ngiven in system messages. The results highlight both the strengths and\nweaknesses of existing models, offering key insights and directions for future\nresearch. The open source library SysBench is available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/SysBench.\n","authors":["Yanzhao Qin","Tao Zhang","Tao Zhang","Yanjun Shen","Wenjing Luo","Haoze Sun","Yan Zhang","Yujing Qiao","Weipeng Chen","Zenan Zhou","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2408.10943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17081v1","updated":"2024-10-22T15:02:37Z","published":"2024-10-22T15:02:37Z","title":"Continuous Speech Tokenizer in Text To Speech","summary":"  The fusion of speech and language in the era of large language models has\ngarnered significant attention. Discrete speech token is often utilized in\ntext-to-speech tasks for speech compression and portability, which is\nconvenient for joint training with text and have good compression efficiency.\nHowever, we found that the discrete speech tokenizer still suffers from\ninformation loss. Therefore, we propose a simple yet effective continuous\nspeech tokenizer and a text-to-speech model based on continuous speech tokens.\nOur results show that the speech language model based on the continuous speech\ntokenizer has better continuity and higher estimated Mean Opinion Scores (MoS).\nThis enhancement is attributed to better information preservation rate of the\ncontinuous speech tokenizer across both low and high frequencies in the\nfrequency domain.\n","authors":["Yixing Li","Ruobing Xie","Xingwu Sun","Yu Cheng","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2410.17081v1.pdf","comment":"4 pages. Under review"},{"id":"http://arxiv.org/abs/2410.17051v1","updated":"2024-10-22T14:30:40Z","published":"2024-10-22T14:30:40Z","title":"Data-driven Coreference-based Ontology Building","summary":"  While coreference resolution is traditionally used as a component in\nindividual document understanding, in this work we take a more global view and\nexplore what can we learn about a domain from the set of all document-level\ncoreference relations that are present in a large corpus. We derive coreference\nchains from a corpus of 30 million biomedical abstracts and construct a graph\nbased on the string phrases within these chains, establishing connections\nbetween phrases if they co-occur within the same coreference chain. We then use\nthe graph structure and the betweeness centrality measure to distinguish\nbetween edges denoting hierarchy, identity and noise, assign directionality to\nedges denoting hierarchy, and split nodes (strings) that correspond to multiple\ndistinct concepts. The result is a rich, data-driven ontology over concepts in\nthe biomedical domain, parts of which overlaps significantly with\nhuman-authored ontologies. We release the coreference chains and resulting\nontology under a creative-commons license, along with the code.\n","authors":["Shir Ashury-Tahan","Amir David Nissan Cohen","Nadav Cohen","Yoram Louzoun","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2410.17051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17050v1","updated":"2024-10-22T14:30:03Z","published":"2024-10-22T14:30:03Z","title":"UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs","summary":"  The key components of machine learning are data samples for training, model\nfor learning patterns, and loss function for optimizing accuracy. Analogously,\nunlearning can potentially be achieved through anti-data samples (or\nanti-samples), unlearning method, and reversed loss function. While prior\nresearch has explored unlearning methods and reversed loss functions, the\npotential of anti-samples remains largely untapped. In this paper, we introduce\nUnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language\nmodels (LLMs). Our contributions are threefold; first, we propose a novel\nconcept of anti-sample-induced unlearning; second, we generate anti-samples by\nleveraging misleading rationales, which help reverse learned associations and\naccelerate the unlearning process; and third, we enable fine-grained targeted\nunlearning, allowing for the selective removal of specific associations without\nimpacting related knowledge - something not achievable by previous works.\nResults demonstrate that anti-samples offer an efficient, targeted unlearning\nstrategy for LLMs, opening new avenues for privacy-preserving machine learning\nand model modification.\n","authors":["Yash Sinha","Murari Mandal","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2410.17050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17040v1","updated":"2024-10-22T14:12:43Z","published":"2024-10-22T14:12:43Z","title":"Arabic Dataset for LLM Safeguard Evaluation","summary":"  The growing use of large language models (LLMs) has raised concerns regarding\ntheir safety. While many studies have focused on English, the safety of LLMs in\nArabic, with its linguistic and cultural complexities, remains under-explored.\nHere, we aim to bridge this gap. In particular, we present an\nArab-region-specific safety evaluation dataset consisting of 5,799 questions,\nincluding direct attacks, indirect attacks, and harmless requests with\nsensitive words, adapted to reflect the socio-cultural context of the Arab\nworld. To uncover the impact of different stances in handling sensitive and\ncontroversial topics, we propose a dual-perspective evaluation framework. It\nassesses the LLM responses from both governmental and opposition viewpoints.\nExperiments over five leading Arabic-centric and multilingual LLMs reveal\nsubstantial disparities in their safety performance. This reinforces the need\nfor culturally specific datasets to ensure the responsible deployment of LLMs.\n","authors":["Yasser Ashraf","Yuxia Wang","Bin Gu","Preslav Nakov","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2410.17040v1.pdf","comment":"17 pages, 6 figures, 10 tables"},{"id":"http://arxiv.org/abs/2404.18923v4","updated":"2024-10-22T14:08:52Z","published":"2024-04-29T17:58:36Z","title":"Holmes: A Benchmark to Assess the Linguistic Competence of Language\n  Models","summary":"  We introduce Holmes, a new benchmark designed to assess language models (LMs)\nlinguistic competence - their unconscious understanding of linguistic\nphenomena. Specifically, we use classifier-based probing to examine LMs'\ninternal representations regarding distinct linguistic phenomena (e.g.,\npart-of-speech tagging). As a result, we meet recent calls to disentangle LMs'\nlinguistic competence from other cognitive abilities, such as following\ninstructions in prompting-based evaluations. Composing Holmes, we review over\n270 probing studies and include more than 200 datasets to assess syntax,\nmorphology, semantics, reasoning, and discourse phenomena. Analyzing over 50\nLMs reveals that, aligned with known trends, their linguistic competence\ncorrelates with model size. However, surprisingly, model architecture and\ninstruction tuning also significantly influence performance, particularly in\nmorphology and syntax. Finally, we propose FlashHolmes, a streamlined version\nthat reduces the computation load while maintaining high-ranking precision.\n","authors":["Andreas Waldis","Yotam Perlitz","Leshem Choshen","Yufang Hou","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2404.18923v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13948v2","updated":"2024-10-22T14:07:57Z","published":"2024-04-22T07:49:36Z","title":"Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by\n  Simulating Documents in the Wild via Low-level Perturbations","summary":"  The robustness of recent Large Language Models (LLMs) has become increasingly\ncrucial as their applicability expands across various domains and real-world\napplications. Retrieval-Augmented Generation (RAG) is a promising solution for\naddressing the limitations of LLMs, yet existing studies on the robustness of\nRAG often overlook the interconnected relationships between RAG components or\nthe potential threats prevalent in real-world databases, such as minor textual\nerrors. In this work, we investigate two underexplored aspects when assessing\nthe robustness of RAG: 1) vulnerability to noisy documents through low-level\nperturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we\nintroduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}),\nwhich targets these aspects. Specifically, GARAG is designed to reveal\nvulnerabilities within each component and test the overall system functionality\nagainst noisy documents. We validate RAG robustness by applying our\n\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and\nLLMs. The experimental results show that GARAG consistently achieves high\nattack success rates. Also, it significantly devastates the performance of each\ncomponent and their synergy, highlighting the substantial risk that minor\ntextual inaccuracies pose in disrupting RAG systems in the real world.\n","authors":["Sukmin Cho","Soyeong Jeong","Jeongyeon Seo","Taeho Hwang","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2404.13948v2.pdf","comment":"Findings of EMNLP Camera-ready version"},{"id":"http://arxiv.org/abs/2410.17035v1","updated":"2024-10-22T14:06:31Z","published":"2024-10-22T14:06:31Z","title":"DIRI: Adversarial Patient Reidentification with Large Language Models\n  for Evaluating Clinical Text Anonymization","summary":"  Sharing protected health information (PHI) is critical for furthering\nbiomedical research. Before data can be distributed, practitioners often\nperform deidentification to remove any PHI contained in the text. Contemporary\ndeidentification methods are evaluated on highly saturated datasets (tools\nachieve near-perfect accuracy) which may not reflect the full variability or\ncomplexity of real-world clinical text and annotating them is resource\nintensive, which is a barrier to real-world applications. To address this gap,\nwe developed an adversarial approach using a large language model (LLM) to\nre-identify the patient corresponding to a redacted clinical note and evaluated\nthe performance with a novel De-Identification/Re-Identification (DIRI) method.\nOur method uses a large language model to reidentify the patient corresponding\nto a redacted clinical note. We demonstrate our method on medical data from\nWeill Cornell Medicine anonymized with three deidentification tools: rule-based\nPhilter and two deep-learning-based models, BiLSTM-CRF and ClinicalBERT.\nAlthough ClinicalBERT was the most effective, masking all identified PII, our\ntool still reidentified 9% of clinical notes Our study highlights significant\nweaknesses in current deidentification technologies while providing a tool for\niterative development and improvement.\n","authors":["John X. Morris","Thomas R. Campion","Sri Laasya Nutheti","Yifan Peng","Akhil Raj","Ramin Zabih","Curtis L. Cole"],"pdf_url":"https://arxiv.org/pdf/2410.17035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17028v1","updated":"2024-10-22T13:52:51Z","published":"2024-10-22T13:52:51Z","title":"Can a Machine Distinguish High and Low Amount of Social Creak in Speech?","summary":"  Objectives: ncreased prevalence of social creak particularly among female\nspeakers has been reported in several studies. The study of social creak has\nbeen previously conducted by combining perceptual evaluation of speech with\nconventional acoustical parameters such as the harmonic-to-noise ratio and\ncepstral peak prominence. In the current study, machine learning (ML) was used\nto automatically distinguish speech of low amount of social creak from speech\nof high amount of social creak.\n  Methods: The amount of creak in continuous speech samples produced in Finnish\nby 90 female speakers was first perceptually assessed by two voice specialists.\nBased on their assessments, the speech samples were divided into two categories\n(low $vs$. high amount of creak). Using the speech signals and their creak\nlabels, seven different ML models were trained. Three spectral representations\nwere used as feature for each model.\n  Results: The results show that the best performance (accuracy of 71.1\\%) was\nobtained by the following two systems: an Adaboost classifier using the\nmel-spectrogram feature and a decision tree classifier using the mel-frequency\ncepstral coefficient feature.\n  Conclusions: The study of social creak is becoming increasingly popular in\nsociolinguistic and vocological research. The conventional human perceptual\nassessment of the amount of creak is laborious and therefore ML technology\ncould be used to assist researchers studying social creak. The classification\nsystems reported in this study could be considered as baselines in future\nML-based studies on social creak.\n","authors":["Anne-Maria Laukkanen","Sudarsana Reddy Kadiri","Shrikanth Narayanan","Paavo Alku"],"pdf_url":"https://arxiv.org/pdf/2410.17028v1.pdf","comment":"Accepted in Journal of Voice"},{"id":"http://arxiv.org/abs/2410.17021v1","updated":"2024-10-22T13:47:38Z","published":"2024-10-22T13:47:38Z","title":"SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop\n  Question Answering Based on Finite State Machine","summary":"  Large Language Models with chain-of-thought prompting, such as OpenAI-o1,\nhave shown impressive capabilities in natural language inference tasks.\nHowever, Multi-hop Question Answering (MHQA) remains challenging for many\nexisting models due to issues like hallucination, error propagation, and\nlimited context length. To address these challenges and enhance LLMs'\nperformance on MHQA, we propose the Self-Guiding prompting Finite State Machine\n(SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike\ntraditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively\nbreaking down complex questions into sub-questions, correcting itself to\nimprove accuracy. It processes one sub-question at a time, dynamically deciding\nthe next step based on the current context and results, functioning much like\nan automaton. Experiments across various benchmarks demonstrate the\neffectiveness of our approach, outperforming strong baselines on challenging\ndatasets such as Musique. SG-FSM reduces hallucination, enabling recovery of\nthe correct final answer despite intermediate errors. It also improves\nadherence to specified output formats, simplifying evaluation significantly.\n","authors":["Xiaochen Wang","Junqing He","Liang Chen","Reza Haf Zhe Yang","Yiru Wang","Xiangdi Meng","Kunhao Pan","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2410.17021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17018v1","updated":"2024-10-22T13:39:47Z","published":"2024-10-22T13:39:47Z","title":"Exploring Forgetting in Large Language Model Pre-Training","summary":"  Catastrophic forgetting remains a formidable obstacle to building an\nomniscient model in large language models (LLMs). Despite the pioneering\nresearch on task-level forgetting in LLM fine-tuning, there is scant focus on\nforgetting during pre-training. We systematically explored the existence and\nmeasurement of forgetting in pre-training, questioning traditional metrics such\nas perplexity (PPL) and introducing new metrics to better detect entity memory\nretention. Based on our revised assessment of forgetting metrics, we explored\nlow-cost, straightforward methods to mitigate forgetting during the\npre-training phase. Further, we carefully analyzed the learning curves,\noffering insights into the dynamics of forgetting. Extensive evaluations and\nanalyses on forgetting of pre-training could facilitate future research on\nLLMs.\n","authors":["Chonghua Liao","Ruobing Xie","Xingwu Sun","Haowen Sun","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2410.17018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10851v2","updated":"2024-10-22T13:08:02Z","published":"2024-10-06T12:53:07Z","title":"LLM Gesticulator: Leveraging Large Language Models for Scalable and\n  Controllable Co-Speech Gesture Synthesis","summary":"  In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.\n","authors":["Haozhou Pang","Tianwei Ding","Lanshan He","Ming Tao","Lu Zhang","Qi Gan"],"pdf_url":"https://arxiv.org/pdf/2410.10851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16977v1","updated":"2024-10-22T12:56:04Z","published":"2024-10-22T12:56:04Z","title":"IPL: Leveraging Multimodal Large Language Models for Intelligent Product\n  Listing","summary":"  Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g.,\nAmazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are\nmainly targeting individual sellers who usually lack sufficient experience in\ne-commerce. Individual sellers often struggle to compose proper descriptions\nfor selling products. With the recent advancement of Multimodal Large Language\nModels (MLLMs), we attempt to integrate such state-of-the-art generative AI\ntechnologies into the product listing process. To this end, we develop IPL, an\nIntelligent Product Listing tool tailored to generate descriptions using\nvarious product attributes such as category, brand, color, condition, etc. IPL\nenables users to compose product descriptions by merely uploading photos of the\nselling product. More importantly, it can imitate the content style of our C2C\nplatform Xianyu. This is achieved by employing domain-specific instruction\ntuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation\n(RAG) process. A comprehensive empirical evaluation demonstrates that the\nunderlying model of IPL significantly outperforms the base model in\ndomain-specific tasks while producing less hallucination. IPL has been\nsuccessfully deployed in our production system, where 72% of users have their\npublished product listings based on the generated content, and those product\nlistings are shown to have a quality score 5.6% higher than those without AI\nassistance.\n","authors":["Kang Chen","Qingheng Zhang","Chengbao Lian","Yixin Ji","Xuwei Liu","Shuguang Han","Guoqiang Wu","Fei Huang","Jufeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16973v1","updated":"2024-10-22T12:51:51Z","published":"2024-10-22T12:51:51Z","title":"Learning Mathematical Rules with Large Language Models","summary":"  In this paper, we study the ability of large language models to learn\nspecific mathematical rules such as distributivity or simplifying equations. We\npresent an empirical analysis of their ability to generalize these rules, as\nwell as to reuse them in the context of word problems. For this purpose, we\nprovide a rigorous methodology to build synthetic data incorporating such\nrules, and perform fine-tuning of large language models on such data. Our\nexperiments show that our model can learn and generalize these rules to some\nextent, as well as suitably reuse them in the context of word problems.\n","authors":["Antoine Gorceix","Bastien Le Chenadec","Ahmad Rammal","Nelson Vadori","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2410.16973v1.pdf","comment":"4th MATH-AI Workshop at NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.16930v1","updated":"2024-10-22T12:00:58Z","published":"2024-10-22T12:00:58Z","title":"Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities\n  Using Only Forward Passes","summary":"  Math reasoning is a highly active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence. However, few works have\nexplored how math reasoning is encoded within LLM parameters and if it is a\nskill that can be isolated within a model. Doing so could allow targeted\nintervention to improve math performance without altering non-math behavior and\nfoster understanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a method for isolating math-specific parameters in\nLLMs using only forward passes. MathNeuro builds on existing work by using\nweights and activations to calculate parameter importance, but isolates\nmath-specific parameters by removing those important for general language\ntasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning\nability without destroying its general language ability. Scaling these\nparameters by a small constant improves a pretrained or instruction-tuned LLM's\nperformance by 4-17% on GSM8K while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.\n","authors":["Bryan R. Christ","Zack Gottesman","Jonathan Kropko","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2410.16930v1.pdf","comment":"21 pages, 29 figures"},{"id":"http://arxiv.org/abs/2410.07114v3","updated":"2024-10-22T11:55:46Z","published":"2024-09-19T19:48:31Z","title":"System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam","summary":"  The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch average of 40.63 points. Neither model had access to the\nexam figures. Since there was a risk of model contamination (i.e., the\nknowledge cutoff of o1-preview and GPT-4o was after the exam was published\nonline), we repeated the procedure with a new Mathematics B exam that was\npublished after the cutoff date. The results again indicated that o1-preview\nperformed strongly (97.8th percentile), which suggests that contamination was\nnot a factor. We also show that there is some variability in the output of\no1-preview, which means that sometimes there is 'luck' (the answer is correct)\nor 'bad luck' (the output has diverged into something that is incorrect). We\ndemonstrate that a self-consistency approach, where repeated prompts are given\nand the most common answer is selected, is a useful strategy for identifying\nthe correct answer. It is concluded that while OpenAI's new model series holds\ngreat potential, certain risks must be considered.\n","authors":["Joost de Winter","Dimitra Dodou","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2410.07114v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16919v1","updated":"2024-10-22T11:52:22Z","published":"2024-10-22T11:52:22Z","title":"EnvBridge: Bridging Diverse Environments with Cross-Environment\n  Knowledge Transfer for Embodied AI","summary":"  In recent years, Large Language Models (LLMs) have demonstrated high\nreasoning capabilities, drawing attention for their applications as agents in\nvarious decision-making processes. One notably promising application of LLM\nagents is robotic manipulation. Recent research has shown that LLMs can\ngenerate text planning or control code for robots, providing substantial\nflexibility and interaction capabilities. However, these methods still face\nchallenges in terms of flexibility and applicability across different\nenvironments, limiting their ability to adapt autonomously. Current approaches\ntypically fall into two categories: those relying on environment-specific\npolicy training, which restricts their transferability, and those generating\ncode actions based on fixed prompts, which leads to diminished performance when\nconfronted with new environments. These limitations significantly constrain the\ngeneralizability of agents in robotic manipulation. To address these\nlimitations, we propose a novel method called EnvBridge. This approach involves\nthe retention and transfer of successful robot control codes from source\nenvironments to target environments. EnvBridge enhances the agent's\nadaptability and performance across diverse settings by leveraging insights\nfrom multiple environments. Notably, our approach alleviates environmental\nconstraints, offering a more flexible and generalizable solution for robotic\nmanipulation tasks. We validated the effectiveness of our method using robotic\nmanipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments\ndemonstrate that LLM agents can successfully leverage diverse knowledge sources\nto solve complex tasks. Consequently, our approach significantly enhances the\nadaptability and robustness of robotic manipulation agents in planning across\ndiverse environments.\n","authors":["Tomoyuki Kagaya","Yuxuan Lou","Thong Jing Yuan","Subramanian Lakshmi","Jayashree Karlekar","Sugiri Pranata","Natsuki Murakami","Akira Kinose","Koki Oguri","Felix Wick","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.16919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07457v3","updated":"2024-10-22T10:54:15Z","published":"2024-07-10T08:20:47Z","title":"GLBench: A Comprehensive Benchmark for Graph with Large Language Models","summary":"  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n","authors":["Yuhan Li","Peisong Wang","Xiao Zhu","Aochuan Chen","Haiyun Jiang","Deng Cai","Victor Wai Kin Chan","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.07457v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15689v2","updated":"2024-10-22T10:49:57Z","published":"2024-08-28T10:25:53Z","title":"TempoFormer: A Transformer for Temporally-aware Representations in\n  Change Detection","summary":"  Dynamic representation learning plays a pivotal role in understanding the\nevolution of linguistic content over time. On this front both context and time\ndynamics as well as their interplay are of prime importance. Current approaches\nmodel context via pre-trained representations, which are typically temporally\nagnostic. Previous work on modelling context and temporal dynamics has used\nrecurrent methods, which are slow and prone to overfitting. Here we introduce\nTempoFormer, the first task-agnostic transformer-based and temporally-aware\nmodel for dynamic representation learning. Our approach is jointly trained on\ninter and intra context dynamics and introduces a novel temporal variation of\nrotary positional embeddings. The architecture is flexible and can be used as\nthe temporal representation foundation of other models or applied to different\ntransformer-based architectures. We show new SOTA performance on three\ndifferent real-time change detection tasks.\n","authors":["Talia Tseriotou","Adam Tsakalidis","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2408.15689v2.pdf","comment":"Accepted by EMNLP Main 2024"},{"id":"http://arxiv.org/abs/2405.15319v2","updated":"2024-10-22T10:31:59Z","published":"2024-05-24T08:00:00Z","title":"Stacking Your Transformers: A Closer Look at Model Growth for Efficient\n  LLM Pre-Training","summary":"  LLMs are computationally expensive to pre-train due to their large scale.\nModel growth emerges as a promising approach by leveraging smaller models to\naccelerate the training of larger ones. However, the viability of these model\ngrowth methods in efficient LLM pre-training remains underexplored. This work\nidentifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1)\nlack of comprehensive evaluation, ($\\textit{O}$2) untested viability for\nscaling, and ($\\textit{O}$3) lack of empirical guidelines. To tackle\n$\\textit{O}$1, we summarize existing approaches into four atomic growth\noperators and systematically evaluate them in a standardized LLM pre-training\nsetting. Our findings reveal that a depthwise stacking operator, called\n$G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to\ndecreased loss and improved overall performance on eight standard NLP\nbenchmarks compared to strong baselines. Motivated by these promising results,\nwe conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to\naddress $\\textit{O}$2 and $\\textit{O}$3. For $\\textit{O}$2 (untested\nscalability), our study shows that $G_{\\text{stack}}$ is scalable and\nconsistently performs well, with experiments up to 7B LLMs after growth and\npre-training LLMs with 750B tokens. For example, compared to a conventionally\ntrained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to\nthe same loss with 194B tokens, resulting in a 54.6\\% speedup. We further\naddress $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines\nto determine growth timing and growth factor for $G_{\\text{stack}}$, making it\npractical in general LLM pre-training. We also provide in-depth discussions and\ncomprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained\nmodel are available at https://llm-stacking.github.io.\n","authors":["Wenyu Du","Tongxu Luo","Zihan Qiu","Zeyu Huang","Yikang Shen","Reynold Cheng","Yike Guo","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2405.15319v2.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.14262v2","updated":"2024-10-22T10:12:00Z","published":"2024-10-18T08:18:18Z","title":"Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation","summary":"  This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.\n","authors":["Ted Kwartler","Matthew Berman","Alan Aqrawi"],"pdf_url":"https://arxiv.org/pdf/2410.14262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16855v1","updated":"2024-10-22T09:43:39Z","published":"2024-10-22T09:43:39Z","title":"Tracing the Development of the Virtual Particle Concept Using Semantic\n  Change Detection","summary":"  Virtual particles are peculiar objects. They figure prominently in much of\ntheoretical and experimental research in elementary particle physics. But\nexactly what they are is far from obvious. In particular, to what extent they\nshould be considered \"real\" remains a matter of controversy in philosophy of\nscience. Also their origin and development has only recently come into focus of\nscholarship in the history of science. In this study, we propose using the\nintriguing case of virtual particles to discuss the efficacy of Semantic Change\nDetection (SCD) based on contextualized word embeddings from a domain-adapted\nBERT model in studying specific scientific concepts. We find that the SCD\nmetrics align well with qualitative research insights in the history and\nphilosophy of science, as well as with the results obtained from Dependency\nParsing to determine the frequency and connotations of the term \"virtual.\"\nStill, the metrics of SCD provide additional insights over and above the\nqualitative research and the Dependency Parsing. Among other things, the\nmetrics suggest that the concept of the virtual particle became more stable\nafter 1950 but at the same time also more polysemous.\n","authors":["Michael Zichert","Adrian Wüthrich"],"pdf_url":"https://arxiv.org/pdf/2410.16855v1.pdf","comment":"CHR 2024: Computational Humanities Research Conference"},{"id":"http://arxiv.org/abs/2410.16848v1","updated":"2024-10-22T09:35:42Z","published":"2024-10-22T09:35:42Z","title":"ETHIC: Evaluating Large Language Models on Long-Context Tasks with High\n  Information Coverage","summary":"  Recent advancements in large language models (LLM) capable of processing\nextremely long texts highlight the need for a dedicated evaluation benchmark to\nassess their long-context capabilities. However, existing methods, like the\nneedle-in-a-haystack test, do not effectively assess whether these models fully\nutilize contextual information, raising concerns about the reliability of\ncurrent evaluation techniques. To thoroughly examine the effectiveness of\nexisting benchmarks, we introduce a new metric called information coverage\n(IC), which quantifies the proportion of the input context necessary for\nanswering queries. Our findings indicate that current benchmarks exhibit low\nIC; although the input context may be extensive, the actual usable context is\noften limited. To address this, we present ETHIC, a novel benchmark designed to\nassess LLMs' ability to leverage the entire context. Our benchmark comprises\n2,648 test instances spanning four long-context tasks with high IC scores in\nthe domains of books, debates, medicine, and law. Our evaluations reveal\nsignificant performance drops in contemporary LLMs, highlighting a critical\nchallenge in managing long contexts. Our benchmark is available at\nhttps://github.com/dmis-lab/ETHIC.\n","authors":["Taewhoo Lee","Chanwoong Yoon","Kyochul Jang","Donghyeon Lee","Minju Song","Hyunjae Kim","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2410.16848v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.16843v1","updated":"2024-10-22T09:25:21Z","published":"2024-10-22T09:25:21Z","title":"Trustworthy Alignment of Retrieval-Augmented Large Language Models via\n  Reinforcement Learning","summary":"  Trustworthiness is an essential prerequisite for the real-world application\nof large language models. In this paper, we focus on the trustworthiness of\nlanguage models with respect to retrieval augmentation. Despite being supported\nwith external evidence, retrieval-augmented generation still suffers from\nhallucinations, one primary cause of which is the conflict between contextual\nand parametric knowledge. We deem that retrieval-augmented language models have\nthe inherent capabilities of supplying response according to both contextual\nand parametric knowledge. Inspired by aligning language models with human\npreference, we take the first step towards aligning retrieval-augmented\nlanguage models to a status where it responds relying merely on the external\nevidence and disregards the interference of parametric knowledge. Specifically,\nwe propose a reinforcement learning based algorithm Trustworthy-Alignment,\ntheoretically and experimentally demonstrating large language models'\ncapability of reaching a trustworthy status without explicit supervision on how\nto respond. Our work highlights the potential of large language models on\nexploring its intrinsic abilities by its own and expands the application\nscenarios of alignment from fulfilling human preference to creating trustworthy\nagents.\n","authors":["Zongmeng Zhang","Yufeng Shi","Jinhua Zhu","Wengang Zhou","Xiang Qi","Peng Zhang","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2410.16843v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2410.16842v1","updated":"2024-10-22T09:25:04Z","published":"2024-10-22T09:25:04Z","title":"Assessment of Transformer-Based Encoder-Decoder Model for Human-Like\n  Summarization","summary":"  In recent times, extracting valuable information from large text is making\nsignificant progress. Especially in the current era of social media, people\nexpect quick bites of information. Automatic text summarization seeks to tackle\nthis by slimming large texts down into more manageable summaries. This\nimportant research area can aid in decision-making by digging out salient\ncontent from large text. With the progress in deep learning models, significant\nwork in language models has emerged. The encoder-decoder framework in deep\nlearning has become the central approach for automatic text summarization. This\nwork leverages transformer-based BART model for human-like summarization which\nis an open-ended problem with many challenges. On training and fine-tuning the\nencoder-decoder model, it is tested with diverse sample articles and the\nquality of summaries of diverse samples is assessed based on human evaluation\nparameters. Further, the finetuned model performance is compared with the\nbaseline pretrained model based on evaluation metrics like ROUGE score and\nBERTScore. Additionally, domain adaptation of the model is required for\nimproved performance of abstractive summarization of dialogues between\ninterlocutors. On investigating, the above popular evaluation metrics are found\nto be insensitive to factual errors. Further investigation of the summaries\ngenerated by finetuned model is done using the contemporary evaluation metrics\nof factual consistency like WeCheck and SummaC. Empirical results on BBC News\narticles highlight that the gold standard summaries written by humans are more\nfactually consistent by 17% than the abstractive summaries generated by\nfinetuned model.\n","authors":["Sindhu Nair","Y. S. Rao","Radha Shankarmani"],"pdf_url":"https://arxiv.org/pdf/2410.16842v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2404.04846v2","updated":"2024-10-22T09:16:49Z","published":"2024-04-07T07:39:45Z","title":"F-MALLOC: Feed-forward Memory Allocation for Continual Learning in\n  Neural Machine Translation","summary":"  In the evolving landscape of Neural Machine Translation (NMT), the\npretrain-then-finetune paradigm has yielded impressive results. However, the\npersistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While\nprevious work has introduced Continual Learning (CL) methods to address CF,\nthese approaches grapple with the delicate balance between avoiding forgetting\nand maintaining system extensibility. To address this, we propose a CL method,\nnamed $\\textbf{F-MALLOC}$ ($\\textbf{F}$eed-forward $\\textbf{M}$emory\n$\\textbf{ALLOC}ation)$. F-MALLOC is inspired by recent insights highlighting\nthat feed-forward layers emulate neural memories and encapsulate crucial\ntranslation knowledge. It decomposes feed-forward layers into discrete memory\ncells and allocates these memories to different tasks. By learning to allocate\nand safeguard these memories, our method effectively alleviates CF while\nensuring robust extendability. Besides, we propose a comprehensive assessment\nprotocol for multi-stage CL of NMT systems. Experiments conducted following\nthis new protocol showcase the superior performance of F-MALLOC, evidenced by\nhigher BLEU scores and almost zero forgetting.\n","authors":["Junhong Wu","Yuchen Liu","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2404.04846v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2410.16834v1","updated":"2024-10-22T09:14:21Z","published":"2024-10-22T09:14:21Z","title":"Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation","summary":"  The correlation between NLG automatic evaluation metrics and human evaluation\nis often regarded as a critical criterion for assessing the capability of an\nevaluation metric. However, different grouping methods and correlation\ncoefficients result in various types of correlation measures used in\nmeta-evaluation. In specific evaluation scenarios, prior work often directly\nfollows conventional measure settings, but the characteristics and differences\nbetween these measures have not gotten sufficient attention. Therefore, this\npaper analyzes 12 common correlation measures using a large amount of\nreal-world data from six widely-used NLG evaluation datasets and 32 evaluation\nmetrics, revealing that different measures indeed impact the meta-evaluation\nresults. Furthermore, we propose three perspectives that reflect the capability\nof meta-evaluation and find that the measure using global grouping and Pearson\ncorrelation exhibits the best overall performance, involving the discriminative\npower, ranking consistency, and sensitivity to score granularity.\n","authors":["Mingqi Gao","Xinyu Hu","Li Lin","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2410.16834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07563v2","updated":"2024-10-22T09:06:38Z","published":"2024-10-10T02:59:36Z","title":"PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency","summary":"  We introduce PLaMo-100B, a large-scale language model designed for Japanese\nproficiency. The model was trained from scratch using 2 trillion tokens, with\narchitecture such as QK Normalization and Z-Loss to ensure training stability\nduring the training process. Post-training techniques, including Supervised\nFine-Tuning and Direct Preference Optimization, were applied to refine the\nmodel's performance. Benchmark evaluations suggest that PLaMo-100B performs\nwell, particularly in Japanese-specific tasks, achieving results that are\ncompetitive with frontier models like GPT-4. The base model is available at\nhttps://huggingface.co/pfnet/plamo-100b.\n","authors":["Preferred Elements"," :","Kenshin Abe","Kaizaburo Chubachi","Yasuhiro Fujita","Yuta Hirokawa","Kentaro Imajo","Toshiki Kataoka","Hiroyoshi Komatsu","Hiroaki Mikami","Tsuguo Mogami","Shogo Murai","Kosuke Nakago","Daisuke Nishino","Toru Ogawa","Daisuke Okanohara","Yoshihiko Ozaki","Shotaro Sano","Shuji Suzuki","Tianqi Xu","Toshihiko Yanase"],"pdf_url":"https://arxiv.org/pdf/2410.07563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14710v2","updated":"2024-10-22T09:00:19Z","published":"2024-09-23T05:12:13Z","title":"ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning","summary":"  Role-playing is an emerging application in the field of Human-Computer\nInteraction (HCI), primarily implemented through the alignment training of a\nlarge language model (LLM) with assigned characters. Despite significant\nprogress, role-playing agents (RPLAs) still struggle with maintaining\nrole-consistency across conversations, particularly when confronted with\nboundary queries subtly related to character attributes. In this paper, we\npresent ERABAL, a framework aimed at enhancing RPLAs' role-playing capabilities\nthrough boundary-aware learning. ERABAL encompasses a generation pipeline for\nrole-specific dialogues and a concomitant methodology for alignment training.\nThrough comprehensive evaluations, we demonstrate that ERABAL is both efficient\nand effective. By training with significantly fewer dialogues than those used\nin leading approaches, ERABAL achieves notable improvements across\nWikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared\nto the generalist baseline models. Our code and datasets will be made publicly\navailable to support further research.\n","authors":["Yihong Tang","Jiao Ou","Che Liu","Fuzheng Zhang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2409.14710v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.10618"},{"id":"http://arxiv.org/abs/2407.04185v3","updated":"2024-10-22T08:53:02Z","published":"2024-07-04T23:26:56Z","title":"HAF-RM: A Hybrid Alignment Framework for Reward Model Training","summary":"  The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io.\n","authors":["Shujun Liu","Xiaoyu Shen","Yuhang Lai","Siyuan Wang","Shengbin Yue","Zengfeng Huang","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.04185v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16812v1","updated":"2024-10-22T08:38:50Z","published":"2024-10-22T08:38:50Z","title":"Optimizing Chain-of-Thought Reasoning: Tackling Arranging Bottleneck via\n  Plan Augmentation","summary":"  Multi-step reasoning ability of large language models is crucial in tasks\nsuch as math and tool utilization. Current researches predominantly focus on\nenhancing model performance in these multi-step reasoning tasks through\nfine-tuning with Chain-of-Thought (CoT) steps, yet these methods tend to be\nheuristic, without exploring nor resolving the bottleneck. In this study, we\nsubdivide CoT reasoning into two parts: arranging and executing, and identify\nthat the bottleneck of models mainly lies in arranging rather than executing.\nBased on this finding, we propose a plan-based training and reasoning method\nthat guides models to generate arranging steps through abstract plans. We\nexperiment on both math (GSM8k) and tool utilization (ToolBench) benchmarks.\nResults show that compared to fine-tuning directly with CoT data, our approach\nachieves a better performance on alleviating arranging bottleneck, particularly\nexcelling in long-distance reasoning generalization.\n","authors":["Yuli Qiu","Jiashu Yao","Heyan Huang","Yuhang Guo"],"pdf_url":"https://arxiv.org/pdf/2410.16812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16803v1","updated":"2024-10-22T08:28:05Z","published":"2024-10-22T08:28:05Z","title":"Context-aware Inductive Knowledge Graph Completion with Latent Type\n  Constraints and Subgraph Reasoning","summary":"  Inductive knowledge graph completion (KGC) aims to predict missing triples\nwith unseen entities. Recent works focus on modeling reasoning paths between\nthe head and tail entity as direct supporting evidence. However, these methods\ndepend heavily on the existence and quality of reasoning paths, which limits\ntheir general applicability in different scenarios. In addition, we observe\nthat latent type constraints and neighboring facts inherent in KGs are also\nvital in inferring missing triples. To effectively utilize all useful\ninformation in KGs, we introduce CATS, a novel context-aware inductive KGC\nsolution. With sufficient guidance from proper prompts and supervised\nfine-tuning, CATS activates the strong semantic understanding and reasoning\ncapabilities of large language models to assess the existence of query triples,\nwhich consist of two modules. First, the type-aware reasoning module evaluates\nwhether the candidate entity matches the latent entity type as required by the\nquery relation. Then, the subgraph reasoning module selects relevant reasoning\npaths and neighboring facts, and evaluates their correlation to the query\ntriple. Experiment results on three widely used datasets demonstrate that CATS\nsignificantly outperforms state-of-the-art methods in 16 out of 18\ntransductive, inductive, and few-shot settings with an average absolute MRR\nimprovement of 7.2%.\n","authors":["Muzhi Li","Cehao Yang","Chengjin Xu","Zixing Song","Xuhui Jiang","Jian Guo","Ho-fung Leung","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2410.16803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16801v1","updated":"2024-10-22T08:27:23Z","published":"2024-10-22T08:27:23Z","title":"Controlled Low-Rank Adaptation with Subspace Regularization for\n  Continued Training on Large Language Models","summary":"  Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsubspace regularization method on LoRA structure. Aiming to reduce the scale of\noutput change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix null space. Experimental\nresults on commonly used LLM finetuning tasks reveal that CLoRA significantly\noutperforms existing LoRA subsequent methods on both in-domain and outdomain\nevaluations, highlighting the superority of CLoRA as a effective\nparameter-efficient finetuning method with catastrophic forgetting mitigating.\nFurther investigation for model parameters indicates that CLoRA effectively\nbalances the trade-off between model capacity and degree of forgetting.\n","authors":["Yuheng Lu","Bingshuo Qian","Caixia Yuan","Huixing Jiang","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16788v1","updated":"2024-10-22T08:04:32Z","published":"2024-10-22T08:04:32Z","title":"Correct after Answer: Enhancing Multi-Span Question Answering with\n  Post-Processing Method","summary":"  Multi-Span Question Answering (MSQA) requires models to extract one or\nmultiple answer spans from a given context to answer a question. Prior work\nmainly focuses on designing specific methods or applying heuristic strategies\nto encourage models to predict more correct predictions. However, these models\nare trained on gold answers and fail to consider the incorrect predictions.\nThrough a statistical analysis, we observe that models with stronger abilities\ndo not predict less incorrect predictions compared with other models. In this\nwork, we propose Answering-Classifying-Correcting (ACC) framework, which\nemploys a post-processing strategy to handle incorrect predictions.\nSpecifically, the ACC framework first introduces a classifier to classify the\npredictions into three types and exclude \"wrong predictions\", then introduces a\ncorrector to modify \"partially correct predictions\". Experiments on several\nMSQA datasets show that ACC framework significantly improves the Exact Match\n(EM) scores, and further analysis demostrates that ACC framework efficiently\nreduces the number of incorrect predictions, improving the quality of\npredictions.\n","authors":["Jiayi Lin","Chenyang Zhang","Haibo Tong","Dongyu Zhang","Qingqing Hong","Bingxuan Hou","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16788v1.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.16780v1","updated":"2024-10-22T07:53:41Z","published":"2024-10-22T07:53:41Z","title":"Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems","summary":"  The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task.\n","authors":["Krishna Sayana","Raghavendra Vasudeva","Yuri Vasilevski","Kun Su","Liam Hebert","Hubert Pham","Ambarish Jash","Sukhdeep Sodhi"],"pdf_url":"https://arxiv.org/pdf/2410.16780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16775v1","updated":"2024-10-22T07:45:18Z","published":"2024-10-22T07:45:18Z","title":"Context-Aware LLM Translation System Using Conversation Summarization\n  and Dialogue History","summary":"  Translating conversational text, particularly in customer support contexts,\npresents unique challenges due to its informal and unstructured nature. We\npropose a context-aware LLM translation system that leverages conversation\nsummarization and dialogue history to enhance translation quality for the\nEnglish-Korean language pair. Our approach incorporates the two most recent\ndialogues as raw data and a summary of earlier conversations to manage context\nlength effectively. We demonstrate that this method significantly improves\ntranslation accuracy, maintaining coherence and consistency across\nconversations. This system offers a practical solution for customer support\ntranslation tasks, addressing the complexities of conversational text.\n","authors":["Mingi Sung","Seungmin Lee","Jiwon Kim","Sejoon Kim"],"pdf_url":"https://arxiv.org/pdf/2410.16775v1.pdf","comment":"Accepted to WMT 2024"},{"id":"http://arxiv.org/abs/2410.14748v2","updated":"2024-10-22T07:19:40Z","published":"2024-10-17T19:38:55Z","title":"ETF: An Entity Tracing Framework for Hallucination Detection in Code\n  Summaries","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced their ability to understand both natural language and code, driving\ntheir use in tasks like natural language-to-code (NL2Code) and code\nsummarization. However, LLMs are prone to hallucination-outputs that stray from\nintended meanings. Detecting hallucinations in code summarization is especially\ndifficult due to the complex interplay between programming and natural\nlanguages. We introduce a first-of-its-kind dataset with $\\sim$10K samples,\ncurated specifically for hallucination detection in code summarization. We\nfurther propose a novel Entity Tracing Framework (ETF) that a) utilizes static\nprogram analysis to identify code entities from the program and b) uses LLMs to\nmap and verify these entities and their intents within generated code\nsummaries. Our experimental analysis demonstrates the effectiveness of the\nframework, leading to a 0.73 F1 score. This approach provides an interpretable\nmethod for detecting hallucinations by grounding entities, allowing us to\nevaluate summary accuracy.\n","authors":["Kishan Maharaj","Vitobha Munigala","Srikanth G. Tamilselvam","Prince Kumar","Sayandeep Sen","Palani Kodeswaran","Abhijit Mishra","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2410.14748v2.pdf","comment":"11 pages, 6 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2410.14059v2","updated":"2024-10-22T06:47:43Z","published":"2024-10-17T22:03:52Z","title":"UCFE: A User-Centric Financial Expertise Benchmark for Large Language\n  Models","summary":"  This paper introduces the UCFE: User-Centric Financial Expertise benchmark,\nan innovative framework designed to evaluate the ability of large language\nmodels (LLMs) to handle complex real-world financial tasks. UCFE benchmark\nadopts a hybrid approach that combines human expert evaluations with dynamic,\ntask-specific interactions to simulate the complexities of evolving financial\nscenarios. Firstly, we conducted a user study involving 804 participants,\ncollecting their feedback on financial tasks. Secondly, based on this feedback,\nwe created our dataset that encompasses a wide range of user intents and\ninteractions. This dataset serves as the foundation for benchmarking 12 LLM\nservices using the LLM-as-Judge methodology. Our results show a significant\nalignment between benchmark scores and human preferences, with a Pearson\ncorrelation coefficient of 0.78, confirming the effectiveness of the UCFE\ndataset and our evaluation approach. UCFE benchmark not only reveals the\npotential of LLMs in the financial sector but also provides a robust framework\nfor assessing their performance and user satisfaction. The benchmark dataset\nand evaluation code are available.\n","authors":["Yuzhe Yang","Yifei Zhang","Yan Hu","Yilin Guo","Ruoli Gan","Yueru He","Mingcong Lei","Xiao Zhang","Haining Wang","Qianqian Xie","Jimin Huang","Honghai Yu","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16736v1","updated":"2024-10-22T06:43:28Z","published":"2024-10-22T06:43:28Z","title":"Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through\n  Failure-Inducing Exploration","summary":"  Large language models (LLMs) have significantly benefited from training on\ndiverse, high-quality task-specific data, leading to impressive performance\nacross a range of downstream applications. Current methods often rely on\nhuman-annotated data or predefined task templates to direct powerful LLMs in\nsynthesizing task-relevant data for effective model training. However, this\ndependence on manually designed components may constrain the scope of generated\ndata, potentially overlooking critical edge cases or novel scenarios that could\nchallenge the model. In this paper, we present a novel approach, ReverseGen,\ndesigned to automatically generate effective training samples that expose the\nweaknesses of LLMs. Specifically, we introduce a dedicated proposer trained to\nproduce queries that lead target models to generate unsatisfactory responses.\nThese failure-inducing queries are then used to construct training data,\nhelping to address the models' shortcomings and improve overall performance.\nOur approach is flexible and can be applied to models of various scales (3B,\n7B, and 8B). We evaluate ReverseGen on three key applications (safety, honesty,\nand math), demonstrating that our generated data is both highly effective and\ndiverse. Models fine-tuned with ReverseGen-generated data consistently\noutperform those trained on human-annotated or general model-generated data,\noffering a new perspective on data synthesis for task-specific LLM enhancement.\n","authors":["Qintong Li","Jiahui Gao","Sheng Wang","Renjie Pi","Xueliang Zhao","Chuan Wu","Xin Jiang","Zhenguo Li","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.16736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12074v3","updated":"2024-10-22T06:38:07Z","published":"2024-06-17T20:20:47Z","title":"COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for\n  Aligning Large Language Models to Online Communities","summary":"  Social scientists use surveys to probe the opinions and beliefs of\npopulations, but these methods are slow, costly, and prone to biases. Recent\nadvances in large language models (LLMs) enable the creating of computational\nrepresentations or \"digital twins\" of populations that generate human-like\nresponses mimicking the population's language, styles, and attitudes. We\nintroduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs\nto online communities to elicit their beliefs. Given a corpus of a community's\nonline discussions, Community-Cross-Instruct automatically generates\ninstruction-output pairs by an advanced LLM to (1) finetune a foundational LLM\nto faithfully represent that community, and (2) evaluate the alignment of the\nfinetuned model to the community. We demonstrate the method's utility in\naccurately representing political and diet communities on Reddit. Unlike prior\nmethods requiring human-authored instructions, Community-Cross-Instruct\ngenerates instructions in a fully unsupervised manner, enhancing scalability\nand generalization across domains. This work enables cost-effective and\nautomated surveying of diverse online communities.\n","authors":["Zihao He","Minh Duc Chu","Rebecca Dorn","Siyi Guo","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2406.12074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17378v2","updated":"2024-10-22T06:32:10Z","published":"2024-06-25T08:55:12Z","title":"A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens","summary":"  Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nembedding LLMs, the obtained text embedding will be able to be aligned with the\nkey tokens in the input text. We first fully analyze this phenomenon on eight\nembedding LLMs and show that this phenomenon is universal and is not affected\nby model architecture, training strategy, and embedding method. With a deeper\nanalysis, we then find that the main change in embedding space between the\nembedding LLMs and their original generative LLMs is in the first principal\ncomponent. By adjusting the first principal component, we can align text\nembedding with the key tokens. Finally, we give several examples to demonstrate\nthe vast application potential of this finding: (1) we propose a simple and\npractical sparse retrieval method based on the aligned tokens, which can\nachieve 80\\% of the dense retrieval effect of the same model while reducing the\ncomputation significantly; (2) we show that our findings provide a fresh\nperspective to help understand fuzzy concepts (e.g., semantic relatedness vs.\nsemantic similarity) and emerging technologies (e.g., instruction-following\nembedding) in this field.\n","authors":["Zhijie Nie","Richong Zhang","Zhanyu Wu"],"pdf_url":"https://arxiv.org/pdf/2406.17378v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.16726v1","updated":"2024-10-22T06:25:16Z","published":"2024-10-22T06:25:16Z","title":"Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap","summary":"  While automatic speech recognition (ASR) systems have achieved remarkable\nperformance with large-scale datasets, their efficacy remains inadequate in\nlow-resource settings, encompassing dialects, accents, minority languages, and\nlong-tail hotwords, domains with significant practical relevance. With the\nadvent of versatile and powerful text-to-speech (TTS) models, capable of\ngenerating speech with human-level naturalness, expressiveness, and diverse\nspeaker profiles, leveraging TTS for ASR data augmentation provides a\ncost-effective and practical approach to enhancing ASR performance.\nComprehensive experiments on an unprecedentedly rich variety of low-resource\ndatasets demonstrate consistent and substantial performance improvements,\nproving that the proposed method of enhancing low-resource ASR through a\nversatile TTS model is highly effective and has broad application prospects.\nFurthermore, we delve deeper into key characteristics of synthesized speech\ndata that contribute to ASR improvement, examining factors such as text\ndiversity, speaker diversity, and the volume of synthesized data, with text\ndiversity being studied for the first time in this work. We hope our findings\nprovide helpful guidance and reference for the practical application of\nTTS-based data augmentation and push the advancement of low-resource ASR one\nstep further.\n","authors":["Guanrou Yang","Fan Yu","Ziyang Ma","Zhihao Du","Zhifu Gao","Shiliang Zhang","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16635v3","updated":"2024-10-22T06:19:20Z","published":"2024-01-30T00:17:37Z","title":"Improving Reinforcement Learning from Human Feedback with Efficient\n  Reward Model Ensemble","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a widely adopted\napproach for aligning large language models with human values. However, RLHF\nrelies on a reward model that is trained with a limited amount of human\npreference data, which could lead to inaccurate predictions. As a result, RLHF\nmay produce outputs that are misaligned with human values. To mitigate this\nissue, we contribute a reward ensemble method that allows the reward model to\nmake more accurate predictions. As using an ensemble of large language\nmodel-based reward models can be computationally and resource-expensive, we\nexplore efficient ensemble methods including linear-layer ensemble and\nLoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy\nOptimization with our ensembled reward models, and verify that our ensemble\nmethods help improve the alignment performance of RLHF outputs.\n","authors":["Shun Zhang","Zhenfang Chen","Sunli Chen","Yikang Shen","Zhiqing Sun","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2401.16635v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16714v1","updated":"2024-10-22T05:51:34Z","published":"2024-10-22T05:51:34Z","title":"Magnetic Preference Optimization: Achieving Last-iterate Convergence for\n  Language Models Alignment","summary":"  Self-play methods have demonstrated remarkable success in enhancing model\ncapabilities across various domains. In the context of Reinforcement Learning\nfrom Human Feedback (RLHF), self-play not only boosts Large Language Model\n(LLM) performance but also overcomes the limitations of traditional\nBradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a\npreference-based, two-player constant-sum game. However, existing methods\neither guarantee only average-iterate convergence, incurring high storage and\ninference costs, or converge to the NE of a regularized game, failing to\naccurately reflect true human preferences. In this paper, we introduce Magnetic\nPreference Optimization (MPO), a novel approach capable of achieving\nlast-iterate convergence to the NE of the original game, effectively overcoming\nthe limitations of existing methods. Building upon Magnetic Mirror Descent\n(MMD), MPO attains a linear convergence rate, making it particularly suitable\nfor fine-tuning LLMs. To ensure our algorithm is both theoretically sound and\npractically viable, we present a simple yet effective implementation that\nadapts the theoretical insights to the RLHF setting. Empirical results\ndemonstrate that MPO can significantly enhance the performance of LLMs,\nhighlighting the potential of self-play methods in alignment.\n","authors":["Mingzhi Wang","Chengdong Ma","Qizhi Chen","Linjian Meng","Yang Han","Jiancong Xiao","Zhaowei Zhang","Jing Huo","Weijie J. Su","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16714v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.11216v2","updated":"2024-10-22T05:45:46Z","published":"2024-04-17T10:00:56Z","title":"Position Engineering: Boosting Large Language Models through Positional\n  Information Manipulation","summary":"  The performance of large language models (LLMs) is significantly influenced\nby the quality of the prompts provided. In response, researchers have developed\nenormous prompt engineering strategies aimed at modifying the prompt text to\nenhance task performance. In this paper, we introduce a novel technique termed\nposition engineering, which offers a more efficient way to guide large language\nmodels. Unlike prompt engineering, which requires substantial effort to modify\nthe text provided to LLMs, position engineering merely involves altering the\npositional information in the prompt without modifying the text itself. We have\nevaluated position engineering in two widely-used LLM scenarios:\nretrieval-augmented generation (RAG) and in-context learning (ICL). Our\nfindings show that position engineering substantially improves upon the\nbaseline in both cases. Position engineering thus represents a promising new\nstrategy for exploiting the capabilities of large language models.\n","authors":["Zhiyuan He","Huiqiang Jiang","Zilong Wang","Yuqing Yang","Luna Qiu","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2404.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02246v3","updated":"2024-10-22T05:44:04Z","published":"2024-03-04T17:34:34Z","title":"PHAnToM: Persona-based Prompting Has An Effect on Theory-of-Mind\n  Reasoning in Large Language Models","summary":"  The use of LLMs in natural language reasoning has shown mixed results,\nsometimes rivaling or even surpassing human performance in simpler\nclassification tasks while struggling with social-cognitive reasoning, a domain\nwhere humans naturally excel. These differences have been attributed to many\nfactors, such as variations in prompting and the specific LLMs used. However,\nno reasons appear conclusive, and no clear mechanisms have been established in\nprior work. In this study, we empirically evaluate how role-playing prompting\ninfluences Theory-of-Mind (ToM) reasoning capabilities. Grounding our rsearch\nin psychological theory, we propose the mechanism that, beyond the inherent\nvariance in the complexity of reasoning tasks, performance differences arise\nbecause of socially-motivated prompting differences. In an era where prompt\nengineering with role-play is a typical approach to adapt LLMs to new contexts,\nour research advocates caution as models that adopt specific personas might\npotentially result in errors in social-cognitive reasoning.\n","authors":["Fiona Anting Tan","Gerard Christopher Yeo","Kokil Jaidka","Fanyou Wu","Weijie Xu","Vinija Jain","Aman Chadha","Yang Liu","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.02246v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.16712v1","updated":"2024-10-22T05:39:24Z","published":"2024-10-22T05:39:24Z","title":"DENOASR: Debiasing ASRs through Selective Denoising","summary":"  Automatic Speech Recognition (ASR) systems have been examined and shown to\nexhibit biases toward particular groups of individuals, influenced by factors\nsuch as demographic traits, accents, and speech styles. Noise can\ndisproportionately impact speakers with certain accents, dialects, or speaking\nstyles, leading to biased error rates. In this work, we introduce a novel\nframework DENOASR, which is a selective denoising technique to reduce the\ndisparity in the word error rates between the two gender groups, male and\nfemale. We find that a combination of two popular speech denoising techniques,\nviz. DEMUCS and LE, can be effectively used to mitigate ASR disparity without\ncompromising their overall performance. Experiments using two state-of-the-art\nopen-source ASRs - OpenAI WHISPER and NVIDIA NEMO - on multiple benchmark\ndatasets, including TIE, VOX-POPULI, TEDLIUM, and FLEURS, show that there is a\npromising reduction in the average word error rate gap across the two gender\ngroups. For a given dataset, the denoising is selectively applied on speech\nsamples having speech intelligibility below a certain threshold, estimated\nusing a small validation sample, thus ameliorating the need for large-scale\nhuman-written ground-truth transcripts. Our findings suggest that selective\ndenoising can be an elegant approach to mitigate biases in present-day ASR\nsystems.\n","authors":["Anand Kumar Rai","Siddharth D Jaiswal","Shubham Prakash","Bendi Pragnya Sree","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.16712v1.pdf","comment":"Paper accepted at IEEE ICKG 2024"},{"id":"http://arxiv.org/abs/2410.16710v1","updated":"2024-10-22T05:32:40Z","published":"2024-10-22T05:32:40Z","title":"Influential Language Data Selection via Gradient Trajectory Pursuit","summary":"  Curating a desirable dataset for training has been the core of building\nhighly capable large language models (Touvron et al., 2023; Achiam et al.,\n2023; Team et al.,2024). Gradient influence scores (Pruthi et al., 2020; Xia et\nal., 2024) are shown to be correlated with model performance and are commonly\nused as the criterion for data selection. However, existing methods are built\nupon either individual sample rankings or inefficient matching process, leading\nto suboptimal performance or scaling up issues.In this paper, we propose\nGradient Trajectory Pursuit (GTP), an algorithm that performs pursuit of\ngradient trajectories via jointly selecting data points under an L0-norm\nregularized objective. The proposed algorithm highlights: (1) joint selection\ninstead of independent top-k selection, which automatically de-duplicates\nsamples; (2) higher efficiency with compressive sampling processes, which can\nbe further sped up using a distributed framework. In the experiments, we\ndemonstrate the algorithm in both in-domain and target-domain selection\nbenchmarks and show that it outperforms top-k selection and competitive\nalgorithms consistently, for example, our algorithm chooses as low as 0.5% data\nto achieve full performance on the targeted instruction tuning tasks\n","authors":["Zhiwei Deng","Tao Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2410.16710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16708v1","updated":"2024-10-22T05:25:54Z","published":"2024-10-22T05:25:54Z","title":"Atomic Fact Decomposition Helps Attributed Question Answering","summary":"  Attributed Question Answering (AQA) aims to provide both a trustworthy answer\nand a reliable attribution report for a given question. Retrieval is a widely\nadopted approach, including two general paradigms: Retrieval-Then-Read (RTR)\nand post-hoc retrieval. Recently, Large Language Models (LLMs) have shown\nremarkable proficiency, prompting growing interest in AQA among researchers.\nHowever, RTR-based AQA often suffers from irrelevant knowledge and rapidly\nchanging information, even when LLMs are adopted, while post-hoc\nretrieval-based AQA struggles with comprehending long-form answers with complex\nlogic, and precisely identifying the content needing revision and preserving\nthe original intent. To tackle these problems, this paper proposes an Atomic\nfact decomposition-based Retrieval and Editing (ARE) framework, which\ndecomposes the generated long-form answers into molecular clauses and atomic\nfacts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are\nfine-tuned using a well-constructed dataset, generated from large scale\nKnowledge Graphs (KGs). This process involves extracting one-hop neighbors from\na given set of entities and transforming the result into coherent long-form\ntext. Subsequently, ARE leverages a search engine to retrieve evidences related\nto atomic facts, inputting these evidences into an LLM-based verifier to\ndetermine whether the facts require expansion for re-retrieval or editing.\nFurthermore, the edited facts are backtracked into the original answer, with\nevidence aggregated based on the relationship between molecular clauses and\natomic facts. Extensive evaluations demonstrate the superior performance of our\nproposed method over the state-of-the-arts on several datasets, with an\nadditionally proposed new metric $Attr_{p}$ for evaluating the precision of\nevidence attribution.\n","authors":["Zhichao Yan","Jiapu Wang","Jiaoyan Chen","Xiaoli Li","Ru Li","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16703v1","updated":"2024-10-22T05:16:19Z","published":"2024-10-22T05:16:19Z","title":"PLDR-LLM: Large Language Model from Power Law Decoder Representations","summary":"  We present the Large Language Model from Power Law Decoder Representations\n(PLDR-LLM), a language model that leverages non-linear and linear\ntransformations through Power Law Graph Attention mechanism to generate\nwell-defined deductive and inductive outputs. We pretrain the PLDR-LLMs of\nvarying layer sizes with a small batch size of 32 and $\\sim$8B tokens from the\nRefinedWeb dataset, and show that they achieve competitive performance in\nzero-shot and few-shot settings compared to scaled dot-product LLMs of similar\nmodel size reported in the literature. We show that deductive outputs of\nPLDR-LLMs can be used to compare model characteristics or improve the\nperformance by introducing the Directed Acyclic Graph (DAG) loss as a metric\nand regularizer. Our results indicate that the initial maximum learning rate\nand warm-up steps have a lasting impact on deductive outputs throughout the\npretraining. We provide a detailed description of PLDR-LLM architecture, its\nimplementation and the pretraining procedure.\n","authors":["Burc Gokden"],"pdf_url":"https://arxiv.org/pdf/2410.16703v1.pdf","comment":"22 pages, 4 figures, 10 tables"},{"id":"http://arxiv.org/abs/2407.00219v2","updated":"2024-10-22T05:13:15Z","published":"2024-06-28T20:06:30Z","title":"Evaluating Human Alignment and Model Faithfulness of LLM Rationale","summary":"  We study how well large language models (LLMs) explain their generations\nthrough rationales -- a set of tokens extracted from the input text that\nreflect the decision-making process of LLMs. Specifically, we systematically\nstudy rationales derived using two approaches: (1) popular prompting-based\nmethods, where prompts are used to guide LLMs in generating rationales, and (2)\ntechnical attribution-based methods, which leverage attention or gradients to\nidentify important tokens. Our analysis spans three classification datasets\nwith annotated rationales, encompassing tasks with varying performance levels.\nWhile prompting-based self-explanations are widely used, our study reveals that\nthese explanations are not always as \"aligned\" with the human rationale as\nattribution-based explanations. Even more so, fine-tuning LLMs to enhance\nclassification task accuracy does not enhance the alignment of prompting-based\nrationales. Still, it does considerably improve the alignment of\nattribution-based methods (e.g., InputXGradient). More importantly, we show\nthat prompting-based self-explanation is also less \"faithful\" than\nattribution-based explanations, failing to provide a reliable account of the\nmodel's decision-making process. To evaluate faithfulness, unlike prior studies\nthat excluded misclassified examples, we evaluate all instances and also\nexamine the impact of fine-tuning and accuracy on alignment and faithfulness.\nOur findings suggest that inconclusive faithfulness results reported in earlier\nstudies may stem from low classification accuracy. These findings underscore\nthe importance of more rigorous and comprehensive evaluations of LLM\nrationales.\n","authors":["Mohsen Fayyaz","Fan Yin","Jiao Sun","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2407.00219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16682v1","updated":"2024-10-22T04:27:03Z","published":"2024-10-22T04:27:03Z","title":"Methods of improving LLM training stability","summary":"  Training stability of large language models(LLMs) is an important research\ntopic. Reproducing training instabilities can be costly, so we use a small\nlanguage model with 830M parameters and experiment with higher learning rates\nto force models to diverge. One of the sources of training instability is the\ngrowth of logits in attention layers. We extend the focus of the previous work\nand look not only at the magnitude of the logits but at all outputs of linear\nlayers in the Transformer block. We observe that with a high learning rate the\nL2 norm of all linear layer outputs can grow with each training step and the\nmodel diverges. Specifically we observe that QKV, Proj and FC2 layers have the\nlargest growth of the output magnitude. This prompts us to explore several\noptions: 1) apply layer normalization not only after QK layers but also after\nProj and FC2 layers too; 2) apply layer normalization after the QKV layer (and\nremove pre normalization). 3) apply QK layer normalization together with\nsoftmax capping. We show that with the last two methods we can increase\nlearning rate by 1.5x (without model divergence) in comparison to an approach\nbased on QK layer normalization only. Also we observe significant perplexity\nimprovements for all three methods in comparison to the baseline model.\n","authors":["Oleg Rybakov","Mike Chrzanowski","Peter Dykas","Jinze Xue","Ben Lanir"],"pdf_url":"https://arxiv.org/pdf/2410.16682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16676v1","updated":"2024-10-22T04:18:19Z","published":"2024-10-22T04:18:19Z","title":"Improving Causal Reasoning in Large Language Models: A Survey","summary":"  Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning.\n","authors":["Siheng Xiong","Delin Chen","Qingyang Wu","Longxuan Yu","Qingzhen Liu","Dawei Li","Zhikai Chen","Xiaoze Liu","Liangming Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08777v3","updated":"2024-10-22T04:14:08Z","published":"2024-02-13T20:21:29Z","title":"DNABERT-S: Pioneering Species Differentiation with Species-Aware DNA\n  Embeddings","summary":"  We introduce DNABERT-S, a tailored genome model that develops species-aware\nembeddings to naturally cluster and segregate DNA sequences of different\nspecies in the embedding space. Differentiating species from genomic sequences\n(i.e., DNA and RNA) is vital yet challenging, since many real-world species\nremain uncharacterized, lacking known genomes for reference. Embedding-based\nmethods are therefore used to differentiate species in an unsupervised manner.\nDNABERT-S builds upon a pre-trained genome foundation model named DNABERT-2. To\nencourage effective embeddings to error-prone long-read DNA sequences, we\nintroduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes\nthe hidden representations of DNA sequences at randomly selected layers and\ntrains the model to recognize and differentiate these mixed proportions at the\noutput layer. We further enhance it with the proposed Curriculum Contrastive\nLearning (C$^2$LR) strategy. Empirical results on 23 diverse datasets show\nDNABERT-S's effectiveness, especially in realistic label-scarce scenarios. For\nexample, it identifies twice more species from a mixture of unlabeled genomic\nsequences, doubles the Adjusted Rand Index (ARI) in species clustering, and\noutperforms the top baseline's performance in 10-shot species classification\nwith just a 2-shot training. Model, codes, and data are publicly available at\n\\url{https://github.com/MAGICS-LAB/DNABERT_S}.\n","authors":["Zhihan Zhou","Weimin Wu","Harrison Ho","Jiayi Wang","Lizhen Shi","Ramana V Davuluri","Zhong Wang","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2402.08777v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05286v3","updated":"2024-10-22T03:58:20Z","published":"2024-03-08T13:10:59Z","title":"LLM4Decompile: Decompiling Binary Code with Large Language Models","summary":"  Decompilation aims to convert binary code to high-level source code, but\ntraditional tools like Ghidra often produce results that are difficult to read\nand execute. Motivated by the advancements in Large Language Models (LLMs), we\npropose LLM4Decompile, the first and largest open-source LLM series (1.3B to\n33B) trained to decompile binary code. We optimize the LLM training process and\nintroduce the LLM4Decompile-End models to decompile binary directly. The\nresulting models significantly outperform GPT-4o and Ghidra on the HumanEval\nand ExeBench benchmarks by over 100% in terms of re-executability rate.\nAdditionally, we improve the standard refinement approach to fine-tune the\nLLM4Decompile-Ref models, enabling them to effectively refine the decompiled\ncode from Ghidra and achieve a further 16.2% improvement over the\nLLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to\nrevolutionize binary code decompilation, delivering remarkable improvements in\nreadability and executability while complementing conventional tools for\noptimal results. Our code, dataset, and models are released at\nhttps://github.com/albertan017/LLM4Decompile\n","authors":["Hanzhuo Tan","Qi Luo","Jing Li","Yuqun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05286v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16665v1","updated":"2024-10-22T03:38:37Z","published":"2024-10-22T03:38:37Z","title":"SafetyAnalyst: Interpretable, transparent, and steerable LLM safety\n  moderation","summary":"  The ideal LLM content moderation system would be both structurally\ninterpretable (so its decisions can be explained to users) and steerable (to\nreflect a community's values or align to safety standards). However, current\nsystems fall short on both of these dimensions. To address this gap, we present\nSafetyAnalyst, a novel LLM safety moderation framework. Given a prompt,\nSafetyAnalyst creates a structured \"harm-benefit tree,\" which identifies 1) the\nactions that could be taken if a compliant response were provided, 2) the\nharmful and beneficial effects of those actions (along with their likelihood,\nseverity, and immediacy), and 3) the stakeholders that would be impacted by\nthose effects. It then aggregates this structured representation into a\nharmfulness score based on a parameterized set of safety preferences, which can\nbe transparently aligned to particular values. Using extensive harm-benefit\nfeatures generated by SOTA LLMs on 19k prompts, we fine-tuned an open-weight LM\nto specialize in generating harm-benefit trees through symbolic knowledge\ndistillation. On a comprehensive set of prompt safety benchmarks, we show that\nour system (average F1=0.75) outperforms existing LLM safety moderation systems\n(average F1$<$0.72) on prompt harmfulness classification, while offering the\nadditional advantages of interpretability and steerability.\n","authors":["Jing-Jing Li","Valentina Pyatkin","Max Kleiman-Weiner","Liwei Jiang","Nouha Dziri","Anne G. E. Collins","Jana Schaich Borg","Maarten Sap","Yejin Choi","Sydney Levine"],"pdf_url":"https://arxiv.org/pdf/2410.16665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01812v3","updated":"2024-10-22T03:30:01Z","published":"2024-09-14T02:35:29Z","title":"From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice","summary":"  Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.\n","authors":["Qian Niu","Keyu Chen","Ming Li","Pohsun Feng","Ziqian Bi","Lawrence KQ Yan","Yichao Zhang","Caitlyn Heqi Yin","Cheng Fei","Junyu Liu","Benji Peng"],"pdf_url":"https://arxiv.org/pdf/2410.01812v3.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.16659v1","updated":"2024-10-22T03:21:59Z","published":"2024-10-22T03:21:59Z","title":"RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary\n  Detection in Partially Machine Generated Texts","summary":"  With increasing usage of generative models for text generation and widespread\nuse of machine generated texts in various domains, being able to distinguish\nbetween human written and machine generated texts is a significant challenge.\nWhile existing models and proprietary systems focus on identifying whether\ngiven text is entirely human written or entirely machine generated, only a few\nsystems provide insights at sentence or paragraph level at likelihood of being\nmachine generated at a non reliable accuracy level, working well only for a set\nof domains and generators. This paper introduces few reliable approaches for\nthe novel task of identifying which part of a given text is machine generated\nat a word level while comparing results from different approaches and methods.\nWe present a comparison with proprietary systems , performance of our model on\nunseen domains' and generators' texts. The findings reveal significant\nimprovements in detection accuracy along with comparison on other aspects of\ndetection capabilities. Finally we discuss potential avenues for improvement\nand implications of our work. The proposed model is also well suited for\ndetecting which parts of a text are machine generated in outputs of Instruct\nvariants of many LLMs.\n","authors":["Ram Mohan Rao Kadiyala"],"pdf_url":"https://arxiv.org/pdf/2410.16659v1.pdf","comment":"published at naacl 2024"},{"id":"http://arxiv.org/abs/2410.16658v1","updated":"2024-10-22T03:19:16Z","published":"2024-10-22T03:19:16Z","title":"Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent","summary":"  Adsorption energy is a key reactivity descriptor in catalysis, enabling the\nefficient screening of potential catalysts. However, determining adsorption\nenergy involves comparing the energies of multiple adsorbate-catalyst\nconfigurations, which is computationally demanding due to a large number of\npossible configurations. Current algorithmic approaches typically enumerate\nadsorption sites and configurations without leveraging theoretical insights to\nguide the initial setup. In this work, we present Adsorb-Agent, a Large\nLanguage Model (LLM) agent designed to efficiently derive system-specific\nstable adsorption configurations with minimal human intervention. Adsorb-Agent\nleverages built-in knowledge and emergent reasoning capabilities, significantly\nreducing the number of initial configurations required while improving accuracy\nin predicting the minimum adsorption energy. We demonstrate its performance\nusing two example systems, NNH-CuPd3 (111) and NNH-Mo3Pd (111), for the\nNitrogen Reduction Reaction (NRR), a sustainable alternative to the Haber-Bosch\nprocess. Adsorb-Agent outperforms conventional \"heuristic\" and \"random\"\nalgorithms by identifying lower-energy configurations with fewer initial\nsetups, reducing computational costs while enhancing accuracy. This highlights\nits potential to accelerate catalyst discovery.\n","authors":["Janghoon Ock","Tirtha Vinchurkar","Yayati Jadhav","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2410.16658v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.16645v1","updated":"2024-10-22T02:45:09Z","published":"2024-10-22T02:45:09Z","title":"Chatting with Bots: AI, Speech Acts, and the Edge of Assertion","summary":"  This paper addresses the question of whether large language model-powered\nchatbots are capable of assertion. According to what we call the Thesis of\nChatbot Assertion (TCA), chatbots are the kinds of things that can assert, and\nat least some of the output produced by current-generation chatbots qualifies\nas assertion. We provide some motivation for TCA, arguing that it ought to be\ntaken seriously and not simply dismissed. We also review recent objections to\nTCA, arguing that these objections are weighty. We thus confront the following\ndilemma: how can we do justice to both the considerations for and against TCA?\nWe consider two influential responses to this dilemma - the first appeals to\nthe notion of proxy-assertion; the second appeals to fictionalism - and argue\nthat neither is satisfactory. Instead, reflecting on the ontogenesis of\nassertion, we argue that we need to make space for a category of\nproto-assertion. We then apply the category of proto-assertion to chatbots,\narguing that treating chatbots as proto-assertors provides a satisfactory\nresolution to the dilemma of chatbot assertion.\n","authors":["Iwan Williams","Tim Bayne"],"pdf_url":"https://arxiv.org/pdf/2410.16645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16640v1","updated":"2024-10-22T02:38:48Z","published":"2024-10-22T02:38:48Z","title":"A Statistical Analysis of LLMs' Self-Evaluation Using Proverbs","summary":"  Large language models (LLMs) such as ChatGPT, GPT-4, Claude-3, and Llama are\nbeing integrated across a variety of industries. Despite this rapid\nproliferation, experts are calling for caution in the interpretation and\nadoption of LLMs, owing to numerous associated ethical concerns. Research has\nalso uncovered shortcomings in LLMs' reasoning and logical abilities, raising\nquestions on the potential of LLMs as evaluation tools. In this paper, we\ninvestigate LLMs' self-evaluation capabilities on a novel proverb reasoning\ntask. We introduce a novel proverb database consisting of 300 proverb pairs\nthat are similar in intent but different in wordings, across topics spanning\ngender, wisdom, and society. We propose tests to evaluate textual consistencies\nas well as numerical consistencies across similar proverbs, and demonstrate the\neffectiveness of our method and dataset in identifying failures in LLMs'\nself-evaluation which in turn can highlight issues related to gender\nstereotypes and lack of cultural understanding in LLMs.\n","authors":["Ryosuke Sonoda","Ramya Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2410.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16167v3","updated":"2024-10-22T02:29:22Z","published":"2024-09-24T15:08:41Z","title":"Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering","summary":"  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.\n","authors":["Ziyu Zhao","Tao Shen","Didi Zhu","Zexi Li","Jing Su","Xuwu Wang","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16167v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16638v1","updated":"2024-10-22T02:27:57Z","published":"2024-10-22T02:27:57Z","title":"LLMScan: Causal Scan for LLM Misbehavior Detection","summary":"  Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.\n","authors":["Mengdi Zhang","Kai Kiat Goh","Peixin Zhang","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.16638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16633v1","updated":"2024-10-22T02:21:42Z","published":"2024-10-22T02:21:42Z","title":"Graph-Structured Trajectory Extraction from Travelogues","summary":"  Previous studies on sequence-based extraction of human movement trajectories\nhave an issue of inadequate trajectory representation. Specifically, a pair of\nlocations may not be lined up in a sequence especially when one location\nincludes the other geographically. In this study, we propose a graph\nrepresentation that retains information on the geographic hierarchy as well as\nthe temporal order of visited locations, and have constructed a benchmark\ndataset for graph-structured trajectory extraction. The experiments with our\nbaselines have demonstrated that it is possible to accurately predict visited\nlocations and the order among them, but it remains a challenge to predict the\nhierarchical relations.\n","authors":["Aitaro Yamamoto","Hiroyuki Otomo","Hiroki Ouchi","Shohei Higashiyama","Hiroki Teranishi","Hiroyuki Shindo","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.16633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10523v2","updated":"2024-10-22T01:58:40Z","published":"2024-05-17T04:05:05Z","title":"Adaptable and Reliable Text Classification using Large Language Models","summary":"  Text classification is fundamental in Natural Language Processing (NLP), and\nthe advent of Large Language Models (LLMs) has revolutionized the field. This\npaper introduces an adaptable and reliable text classification paradigm, which\nleverages LLMs as the core component to address text classification tasks. Our\nsystem simplifies the traditional text classification workflows, reducing the\nneed for extensive preprocessing and domain-specific expertise to deliver\nadaptable and reliable text classification results. We evaluated the\nperformance of several LLMs, machine learning algorithms, and neural\nnetwork-based architectures on four diverse datasets. Results demonstrate that\ncertain LLMs surpass traditional methods in sentiment analysis, spam SMS\ndetection, and multi-label classification. Furthermore, it is shown that the\nsystem's performance can be further enhanced through few-shot or fine-tuning\nstrategies, making the fine-tuned model the top performer across all datasets.\nSource code and datasets are available in this GitHub repository:\nhttps://github.com/yeyimilk/llm-zero-shot-classifiers.\n","authors":["Zhiqiang Wang","Yiran Pang","Yanbin Lin","Xingquan Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.10523v2.pdf","comment":"ICDM Workshop ARRL 2024"},{"id":"http://arxiv.org/abs/2406.16030v2","updated":"2024-10-22T01:31:31Z","published":"2024-06-23T06:38:56Z","title":"Zero-Shot Cross-Lingual NER Using Phonemic Representations for\n  Low-Resource Languages","summary":"  Existing zero-shot cross-lingual NER approaches require substantial prior\nknowledge of the target language, which is impractical for low-resource\nlanguages. In this paper, we propose a novel approach to NER using phonemic\nrepresentation based on the International Phonetic Alphabet (IPA) to bridge the\ngap between representations of different languages. Our experiments show that\nour method significantly outperforms baseline models in extremely low-resource\nlanguages, with the highest average F1 score (46.38%) and lowest standard\ndeviation (12.67), particularly demonstrating its robustness with non-Latin\nscripts. Our codes are available at\nhttps://github.com/Gabriel819/zeroshot_ner.git\n","authors":["Jimin Sohn","Haeji Jung","Alex Cheng","Jooeon Kang","Yilin Du","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2406.16030v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.14978v2","updated":"2024-10-22T01:29:36Z","published":"2024-10-19T05:01:56Z","title":"Subversive Characters and Stereotyping Readers: Characterizing Queer\n  Relationalities with Dialogue-Based Relation Extraction","summary":"  Television is often seen as a site for subcultural identification and\nsubversive fantasy, including in queer cultures. How might we measure\nsubversion, or the degree to which the depiction of social relationship between\na dyad (e.g. two characters who are colleagues) deviates from its typical\nrepresentation on TV? To explore this question, we introduce the task of\nstereotypic relationship extraction. Built on cognitive stylistics, linguistic\nanthropology, and dialogue relation extraction, in this paper, we attempt to\nmodel the cognitive process of stereotyping TV characters in dialogic\ninteractions. Given a dyad, we want to predict: what social relationship do the\nspeakers exhibit through their words? Subversion is then characterized by the\ndiscrepancy between the distribution of the model's predictions and the ground\ntruth labels. To demonstrate the usefulness of this task and gesture at a\nmethodological intervention, we enclose four case studies to characterize the\nrepresentation of queer relationalities in the Big Bang Theory, Frasier, and\nGilmore Girls, as we explore the suspicious and reparative modes of reading\nwith our computational methods.\n","authors":["Kent K. Chang","Anna Ho","David Bamman"],"pdf_url":"https://arxiv.org/pdf/2410.14978v2.pdf","comment":"CHR 2024: Computational Humanities Research Conference"},{"id":"http://arxiv.org/abs/2409.13705v2","updated":"2024-10-22T01:01:56Z","published":"2024-09-05T14:35:35Z","title":"Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble","summary":"  Increasing use of large language models (LLMs) demand performant guardrails\nto ensure the safety of inputs and outputs of LLMs. When these safeguards are\ntrained on imbalanced data, they can learn the societal biases. We present a\nlight-weight, post-processing method for mitigating counterfactual fairness in\nclosed-source text safety classifiers. Our approach involves building an\nensemble that not only outperforms the input classifiers and policy-aligns\nthem, but also acts as a debiasing regularizer. We introduce two\nthreshold-agnostic metrics to assess the counterfactual fairness of a model,\nand demonstrate how combining these metrics with Fair Data Reweighting (FDW)\nhelps mitigate biases. We create an expanded Open AI dataset, and a new\ntemplated LLM-generated dataset based on user-prompts, both of which are\ncounterfactually balanced across identity groups and cover four key areas of\nsafety; we will work towards publicly releasing these datasets. Our results\nshow that our approach improves counterfactual fairness with minimal impact on\nmodel performance.\n","authors":["Olivia Sturman","Aparna Joshi","Bhaktipriya Radharapu","Piyush Kumar","Renee Shelby"],"pdf_url":"https://arxiv.org/pdf/2409.13705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16597v1","updated":"2024-10-22T00:47:54Z","published":"2024-10-22T00:47:54Z","title":"Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for\n  Improved Coverage and Efficiency","summary":"  Knowledge graphs (KGs) generated by large language models (LLMs) are becoming\nincreasingly valuable for Retrieval-Augmented Generation (RAG) applications\nthat require knowledge-intensive reasoning. However, existing KG extraction\nmethods predominantly rely on prompt-based approaches, which are inefficient\nfor processing large-scale corpora. These approaches often suffer from\ninformation loss, particularly with long documents, due to the lack of\nspecialized design for KG construction. Additionally, there is a gap in\nevaluation datasets and methodologies for ontology-free KG construction. To\novercome these limitations, we propose SynthKG, a multi-step, document-level\nontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM\non the synthesized document-KG pairs, we streamline the multi-step process into\na single-step KG generation approach called Distill-SynthKG, substantially\nreducing the number of LLM inference calls. Furthermore, we re-purpose existing\nquestion-answering datasets to establish KG evaluation datasets and introduce\nnew evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a\nnovel graph-based retrieval framework for RAG. Experimental results demonstrate\nthat Distill-SynthKG not only surpasses all baseline models in KG quality --\nincluding models up to eight times larger -- but also consistently excels in\nretrieval and question-answering tasks. Our proposed graph retrieval framework\nalso outperforms all KG-retrieval methods across multiple benchmark datasets.\nWe release the SynthKG dataset and Distill-SynthKG model publicly to support\nfurther research and development.\n","authors":["Prafulla Kumar Choubey","Xin Su","Man Luo","Xiangyu Peng","Caiming Xiong","Tiep Le","Shachar Rosenman","Vasudev Lal","Phil Mui","Ricky Ho","Phillip Howard","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2410.16597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11020v3","updated":"2024-10-22T00:42:39Z","published":"2024-10-14T19:16:56Z","title":"Improving the Language Understanding Capabilities of Large Language\n  Models Using Reinforcement Learning","summary":"  Large language models (LLMs), built on decoder-only transformers, excel in\nnatural language generation and adapt to diverse tasks using zero-shot and\nfew-shot prompting. However, these prompting methods often struggle on natural\nlanguage understanding (NLU) tasks, where encoder-only models like BERT-base\noutperform LLMs on benchmarks like GLUE and SuperGLUE. This paper explores two\napproaches-supervised fine-tuning (SFT) and proximal policy optimization\n(PPO)-to enhance LLMs' NLU abilities. To reduce the cost of full-model\nfine-tuning, we integrate low-rank adaptation (LoRA) layers, limiting updates\nto these layers during both SFT and PPO. In SFT, task-specific prompts are\nconcatenated with input queries and ground-truth labels, optimizing with\nnext-token prediction. Despite this, LLMs still underperform compared to models\nlike BERT-base on several NLU tasks. To close this gap, we apply PPO, a\nreinforcement learning technique that treats each token generation as an action\nand uses a reward function based on alignment with ground-truth answers. PPO\nthen updates the model to maximize these rewards, aligning outputs with correct\nlabels. Our experiments with LLAMA2-7B show that PPO improves performance, with\na 6.3-point gain over SFT on GLUE. PPO exceeds zero-shot by 38.7 points and\nfew-shot by 26.1 points on GLUE, while surpassing these by 28.8 and 28.5 points\non SuperGLUE. Additionally, PPO outperforms BERT-large by 2.7 points on GLUE\nand 9.3 points on SuperGLUE. The improvements are consistent across models like\nQwen2.5-7B and MPT-7B, highlighting PPO's robustness in enhancing LLMs' NLU\ncapabilities.\n","authors":["Bokai Hu","Sai Ashish Somayajula","Xin Pan","Zihan Huang","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2410.11020v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16592v1","updated":"2024-10-22T00:30:08Z","published":"2024-10-22T00:30:08Z","title":"ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding","summary":"  The rise of social media and short-form video (SFV) has facilitated a\nbreeding ground for misinformation. With the emergence of large language\nmodels, significant research has gone into curbing this misinformation problem\nwith automatic false claim detection for text. Unfortunately, the automatic\ndetection of misinformation in SFV is a more complex problem that remains\nlargely unstudied. While text samples are monomodal (only containing words),\nSFVs comprise three different modalities: words, visuals, and non-linguistic\naudio. In this work, we introduce Video Masked Autoencoders for Misinformation\nGuarding (ViMGuard), the first deep-learning architecture capable of\nfact-checking an SFV through analysis of all three of its constituent\nmodalities. ViMGuard leverages a dual-component system. First, Video and Audio\nMasked Autoencoders analyze the visual and non-linguistic audio elements of a\nvideo to discern its intention; specifically whether it intends to make an\ninformative claim. If it is deemed that the SFV has informative intent, it is\npassed through our second component: a Retrieval Augmented Generation system\nthat validates the factual accuracy of spoken words. In evaluation, ViMGuard\noutperformed three cutting-edge fact-checkers, thus setting a new standard for\nSFV fact-checking and marking a significant stride toward trustworthy news on\nsocial platforms. To promote further testing and iteration, VimGuard was\ndeployed into a Chrome extension and all code was open-sourced on GitHub.\n","authors":["Andrew Kan","Christopher Kan","Zaid Nabulsi"],"pdf_url":"https://arxiv.org/pdf/2410.16592v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.05841v2","updated":"2024-10-22T00:16:21Z","published":"2024-07-08T11:38:49Z","title":"An Empirical Comparison of Vocabulary Expansion and Initialization\n  Approaches for Language Models","summary":"  Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods. We release our code publicly\n(https://github.com/AI4Bharat/VocabAdaptation_LLM/tree/CW2V).\n","authors":["Nandini Mundra","Aditya Nanda Kishore","Raj Dabre","Ratish Puduppully","Anoop Kunchukuttan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.05841v2.pdf","comment":"CONLL 2024 (EMNLP 2024)"},{"id":"http://arxiv.org/abs/2410.16589v1","updated":"2024-10-22T00:14:36Z","published":"2024-10-22T00:14:36Z","title":"Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis\n  with Large Language Models","summary":"  Sentiment analysis has become increasingly important for assessing public\nopinion and informing decision-making. Large language models (LLMs) have\nrevolutionized this field by capturing nuanced language patterns. However,\nadapting LLMs to domain-specific sentiment analysis tasks remains challenging\ndue to computational constraints and the need for optimal fine-tuning. To\naddress these challenges, we propose a novel Dynamic Adaptive Rank Space\nExploration (DARSE) framework for efficient and effective sentiment analysis\nusing LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the\noptimal rank range, a fine-grained exploration algorithm to refine rank\nselection, and a dynamic rank allocation method to determine the optimal rank\ncombination for each LLM layer. Extensive experiments demonstrate that DARSE\nsignificantly improves sentiment analysis accuracy, achieving a 15.1%\nimprovement in MSE and a 4.3% improvement in accuracy compared to previous\nwork. Our framework strikes a balance between computational efficiency and\nmodel performance, making it a promising approach for sentiment analysis with\nLLMs.\n","authors":["Hongcheng Ding","Fuzhen Hu","Xuanze Zhao","Zixiao Jiang","Shamsul Nahar Abdullah","Deshinta Arrova Dewi"],"pdf_url":"https://arxiv.org/pdf/2410.16589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11100v2","updated":"2024-10-22T00:02:28Z","published":"2024-05-17T21:27:32Z","title":"Are Large Language Models Moral Hypocrites? A Study Based on Moral\n  Foundations","summary":"  Large language models (LLMs) have taken centre stage in debates on Artificial\nIntelligence. Yet there remains a gap in how to assess LLMs' conformity to\nimportant human values. In this paper, we investigate whether state-of-the-art\nLLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid\nresults) are moral hypocrites. We employ two research instruments based on the\nMoral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which\ninvestigates which values are considered morally relevant in abstract moral\njudgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate\nmoral cognition in concrete scenarios related to each moral foundation. We\ncharacterise conflicts in values between these different abstractions of moral\nevaluation as hypocrisy. We found that both models displayed reasonable\nconsistency within each instrument compared to humans, but they displayed\ncontradictory and hypocritical behaviour when we compared the abstract values\npresent in the MFQ to the evaluation of concrete moral violations of the MFV.\n","authors":["José Luiz Nunes","Guilherme F. C. F. Almeida","Marcelo de Araujo","Simone D. J. Barbosa"],"pdf_url":"https://arxiv.org/pdf/2405.11100v2.pdf","comment":"Final version available at:\n  https://ojs.aaai.org/index.php/AIES/article/view/31704 13 pages, 4 figures, 2\n  tables"},{"id":"http://arxiv.org/abs/2410.17477v1","updated":"2024-10-22T23:24:15Z","published":"2024-10-22T23:24:15Z","title":"Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination","summary":"  The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to \\textit{hallucinate} false or misleading information, limiting\ntheir reliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.\n","authors":["Jerry Huang","Prasanna Parthasarathi","Mehdi Rezagholizadeh","Boxing Chen","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2410.17477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11402v2","updated":"2024-10-22T23:13:34Z","published":"2024-09-17T17:59:06Z","title":"NVLM: Open Frontier-Class Multimodal LLMs","summary":"  We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B\nand will open-source the training code for the community soon.\n","authors":["Wenliang Dai","Nayeon Lee","Boxin Wang","Zhuolin Yang","Zihan Liu","Jon Barker","Tuomas Rintamaki","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2409.11402v2.pdf","comment":"Fixed the typos. For more information, please visit our project page\n  at: https://research.nvidia.com/labs/adlr/NVLM-1"},{"id":"http://arxiv.org/abs/2410.17462v1","updated":"2024-10-22T22:43:14Z","published":"2024-10-22T22:43:14Z","title":"Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain\n  Annotation","summary":"  Time series data is ubiquitous across various domains, including\nmanufacturing, finance, and healthcare. High-quality annotations are essential\nfor effectively understanding time series and facilitating downstream tasks;\nhowever, obtaining such annotations is challenging, particularly in\nmission-critical domains. In this paper, we propose TESSA, a multi-agent system\ndesigned to automatically generate both general and domain-specific annotations\nfor time series data. TESSA introduces two agents: a general annotation agent\nand a domain-specific annotation agent. The general agent captures common\npatterns and knowledge across multiple source domains, leveraging both\ntime-series-wise and text-wise features to generate general annotations.\nMeanwhile, the domain-specific agent utilizes limited annotations from the\ntarget domain to learn domain-specific terminology and generate targeted\nannotations. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate that TESSA effectively generates high-quality annotations,\noutperforming existing methods.\n","authors":["Minhua Lin","Zhengzhang Chen","Yanchi Liu","Xujiang Zhao","Zongyu Wu","Junxiang Wang","Xiang Zhang","Suhang Wang","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.17462v1.pdf","comment":"23 pages, 9 figures, 24 tables"},{"id":"http://arxiv.org/abs/2410.14872v2","updated":"2024-10-22T22:18:14Z","published":"2024-10-18T21:38:21Z","title":"How to Evaluate Reward Models for RLHF","summary":"  We introduce a new benchmark for reward models that quantifies their ability\nto produce strong language models through RLHF (Reinforcement Learning from\nHuman Feedback). The gold-standard approach is to run a full RLHF training\npipeline and directly probe downstream LLM performance. However, this process\nis prohibitively expensive. To address this, we build a predictive model of\ndownstream LLM performance by evaluating the reward model on proxy tasks. These\nproxy tasks consist of a large-scale human preference and a verifiable\ncorrectness preference dataset, in which we measure 12 metrics across 12\ndomains. To investigate which reward model metrics are most correlated to\ngold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a\nlarge-scale crowdsourced human preference platform to view real reward model\ndownstream performance as ground truth. Ultimately, we compile our data and\nfindings into Preference Proxy Evaluations (PPE), the first reward model\nbenchmark explicitly linked to post-RLHF real-world human preference\nperformance, which we open-source for public use and further development. Our\ncode and evaluations can be found at https://github.com/lmarena/PPE .\n","authors":["Evan Frick","Tianle Li","Connor Chen","Wei-Lin Chiang","Anastasios N. Angelopoulos","Jiantao Jiao","Banghua Zhu","Joseph E. Gonzalez","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.14872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13639v2","updated":"2024-10-22T22:05:16Z","published":"2024-10-17T15:09:03Z","title":"A Comparative Study on Reasoning Patterns of OpenAI's o1 Model","summary":"  Enabling Large Language Models (LLMs) to handle a wider range of complex\ntasks (e.g., coding, math) has drawn great attention from many researchers. As\nLLMs continue to evolve, merely increasing the number of model parameters\nyields diminishing performance improvements and heavy computational costs.\nRecently, OpenAI's o1 model has shown that inference strategies (i.e.,\nTest-time Compute methods) can also significantly enhance the reasoning\ncapabilities of LLMs. However, the mechanisms behind these methods are still\nunexplored. In our work, to investigate the reasoning patterns of o1, we\ncompare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent\nWorkflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general\nreasoning benchmarks in three domains (i.e., math, coding, commonsense\nreasoning). Specifically, first, our experiments show that the o1 model has\nachieved the best performance on most datasets. Second, as for the methods of\nsearching diverse responses (e.g., BoN), we find the reward models' capability\nand the search space both limit the upper boundary of these methods. Third, as\nfor the methods that break the problem into many sub-problems, the Agent\nWorkflow has achieved better performance than Step-wise BoN due to the\ndomain-specific system prompt for planning better reasoning processes. Fourth,\nit is worth mentioning that we have summarized six reasoning patterns of o1,\nand provided a detailed analysis on several reasoning benchmarks.\n","authors":["Siwei Wu","Zhongyuan Peng","Xinrun Du","Tuney Zheng","Minghao Liu","Jialong Wu","Jiachen Ma","Yizhi Li","Jian Yang","Wangchunshu Zhou","Qunshu Lin","Junbo Zhao","Zhaoxiang Zhang","Wenhao Huang","Ge Zhang","Chenghua Lin","J. H. Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17450v1","updated":"2024-10-22T21:55:54Z","published":"2024-10-22T21:55:54Z","title":"Interação entre robôs humanoides: desenvolvendo a\n  colaboração e comunicação autônoma","summary":"  This study investigates the interaction between humanoid robots NAO and\nPepper, emphasizing their potential applications in educational settings. NAO,\nwidely used in education, and Pepper, designed for social interactions, of er\nnew opportunities for autonomous communication and collaboration. Through a\nseries of programmed interactions, the robots demonstrated their ability to\ncommunicate and coordinate actions autonomously, highlighting their potential\nas tools for enhancing learning environments. The research also explores the\nintegration of emerging technologies, such as artificial intelligence, into\nthese systems, allowing robots to learn from each other and adapt their\nbehavior. The findings suggest that NAO and Pepper can significantly contribute\nto both technical learning and the development of social and emotional skills\nin students, of ering innovative pedagogical approaches through the use of\nhumanoid robotics.\n","authors":["Moraes Pablo","Peters Christopher","Rodríguez Mónica","Sodre Hiago","Mazondo Ahilen","Sandin Vincent","Barcelona Sebastian","Moraes William","Fernández Santiago","Assunção Nathalie","de Vargas Bruna","Dörnbach Tobias","Kelbouscas André","Grando Ricardo"],"pdf_url":"https://arxiv.org/pdf/2410.17450v1.pdf","comment":"in Portuguese language"},{"id":"http://arxiv.org/abs/2410.17448v1","updated":"2024-10-22T21:50:52Z","published":"2024-10-22T21:50:52Z","title":"In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models","summary":"  Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We also demonstrate how strategic prompting improves the\nmodel's performance and how the natural language interface simplifies\nintegrating theory with data. Although this approach does not outperform\nestablished SR programs where target equations are more complex, LLMs can\nnonetheless iterate toward improved solutions while following instructions and\nincorporating scientific context in natural language.\n","authors":["Samiha Sharlin","Tyler R. Josephson"],"pdf_url":"https://arxiv.org/pdf/2410.17448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17439v1","updated":"2024-10-22T21:30:58Z","published":"2024-10-22T21:30:58Z","title":"Evaluating AI-Generated Essays with GRE Analytical Writing Assessment","summary":"  The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing GPT-4o received an average score of 4.67, falling between\n\"generally thoughtful, well-developed analysis of the issue and conveys meaning\nclearly\" and \"presents a competent analysis of the issue and conveys meaning\nwith acceptable clarity\" according to the GRE scoring guideline. We also\nevaluated the detection accuracy of these essays, with detectors trained on\nessays generated by the same and different LLMs.\n","authors":["Yang Zhong","Jiangang Hao","Michael Fauss","Chen Li","Yuan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17439v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17423v1","updated":"2024-10-22T20:52:51Z","published":"2024-10-22T20:52:51Z","title":"Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis","summary":"  The current surge in Artificial Intelligence (AI) interest, reflected in\nheightened media coverage since 2009, has sparked significant debate on AI's\nimplications for privacy, social justice, workers' rights, and democracy. The\nmedia plays a crucial role in shaping public perception and acceptance of AI\ntechnologies. However, research into how AI appears in media has primarily\nfocused on anglophone contexts, leaving a gap in understanding how AI is\nrepresented globally. This study addresses this gap by analyzing 3,560 news\narticles from Brazilian media published between July 1, 2023, and February 29,\n2024, from 13 popular online news outlets. Using Computational Grounded Theory\n(CGT), the study applies Latent Dirichlet Allocation (LDA), BERTopic, and\nNamed-Entity Recognition to investigate the main topics in AI coverage and the\nentities represented. The findings reveal that Brazilian news coverage of AI is\ndominated by topics related to applications in the workplace and product\nlaunches, with limited space for societal concerns, which mostly focus on\ndeepfakes and electoral integrity. The analysis also highlights a significant\npresence of industry-related entities, indicating a strong influence of\ncorporate agendas in the country's news. This study underscores the need for a\nmore critical and nuanced discussion of AI's societal impacts in Brazilian\nmedia.\n","authors":["Raphael Hernandes","Giulio Corsi"],"pdf_url":"https://arxiv.org/pdf/2410.17423v1.pdf","comment":"18 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.12843v2","updated":"2024-10-22T20:50:56Z","published":"2024-07-04T15:10:51Z","title":"NutriBench: A Dataset for Evaluating Large Language Models in\n  Carbohydrate Estimation from Meal Descriptions","summary":"  Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide more accurate and faster estimates. Finally, we perform a\nreal-world risk assessment by simulating the effect of carbohydrate predictions\non the blood glucose levels of individuals with diabetes. Our work highlights\nthe opportunities and challenges of using LLMs for nutrition estimation,\ndemonstrating their potential to aid professionals and laypersons and improve\nhealth outcomes. Our benchmark is publicly available at:\nhttps://mehak126.github.io/nutribench.html\n","authors":["Andong Hua","Mehak Preet Dhaliwal","Ryan Burke","Yao Qin"],"pdf_url":"https://arxiv.org/pdf/2407.12843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17413v1","updated":"2024-10-22T20:39:21Z","published":"2024-10-22T20:39:21Z","title":"Scalable Influence and Fact Tracing for Large Language Model Pretraining","summary":"  Training data attribution (TDA) methods aim to attribute model outputs back\nto specific training examples, and the application of these methods to large\nlanguage model (LLM) outputs could significantly advance model transparency and\ndata curation. However, it has been challenging to date to apply these methods\nto the full scale of LLM pretraining. In this paper, we refine existing\ngradient-based methods to work effectively at scale, allowing us to retrieve\ninfluential examples for an 8B-parameter language model from a pretraining\ncorpus of over 160B tokens with no need for subsampling or pre-filtering. Our\nmethod combines several techniques, including optimizer state correction, a\ntask-specific Hessian approximation, and normalized encodings, which we find to\nbe critical for performance at scale. In quantitative evaluations on a fact\ntracing task, our method performs best at identifying examples that influence\nmodel predictions, but classical, model-agnostic retrieval methods such as BM25\nstill perform better at finding passages which explicitly contain relevant\nfacts. These results demonstrate a misalignment between factual attribution and\ncausal influence. With increasing model size and training tokens, we find that\ninfluence more closely aligns with attribution. Finally, we examine different\ntypes of examples identified as influential by our method, finding that while\nmany directly entail a particular fact, others support the same output by\nreinforcing priors on relation types, common entities, and names.\n","authors":["Tyler A. Chang","Dheeraj Rajagopal","Tolga Bolukbasi","Lucas Dixon","Ian Tenney"],"pdf_url":"https://arxiv.org/pdf/2410.17413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17401v1","updated":"2024-10-22T20:18:26Z","published":"2024-10-22T20:18:26Z","title":"AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents","summary":"  Vision Language Models (VLMs) have revolutionized the creation of generalist\nweb agents, empowering them to autonomously complete diverse tasks on\nreal-world websites, thereby boosting human efficiency and productivity.\nHowever, despite their remarkable capabilities, the safety and security of\nthese agents against malicious attacks remain critically underexplored, raising\nsignificant concerns about their safe deployment. To uncover and exploit such\nvulnerabilities in web agents, we provide AdvWeb, a novel black-box attack\nframework designed against web agents. AdvWeb trains an adversarial prompter\nmodel that generates and injects adversarial prompts into web pages, misleading\nweb agents into executing targeted adversarial actions such as inappropriate\nstock purchases or incorrect bank transactions, actions that could lead to\nsevere real-world consequences. With only black-box access to the web agent, we\ntrain and optimize the adversarial prompter model using DPO, leveraging both\nsuccessful and failed attack strings against the target agent. Unlike prior\napproaches, our adversarial string injection maintains stealth and control: (1)\nthe appearance of the website remains unchanged before and after the attack,\nmaking it nearly impossible for users to detect tampering, and (2) attackers\ncan modify specific substrings within the generated adversarial string to\nseamlessly change the attack objective (e.g., purchasing stocks from a\ndifferent company), enhancing attack flexibility and efficiency. We conduct\nextensive evaluations, demonstrating that AdvWeb achieves high success rates in\nattacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings\nexpose critical vulnerabilities in current LLM/VLM-based agents, emphasizing\nthe urgent need for developing more reliable web agents and effective defenses.\nOur code and data are available at https://ai-secure.github.io/AdvWeb/ .\n","authors":["Chejian Xu","Mintong Kang","Jiawei Zhang","Zeyi Liao","Lingbo Mo","Mengqi Yuan","Huan Sun","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.17401v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2405.06626v2","updated":"2024-10-22T20:05:32Z","published":"2024-05-10T17:40:02Z","title":"Characterizing the Accuracy -- Efficiency Trade-off of Low-rank\n  Decomposition in Language Models","summary":"  Recent large language models (LLMs) employ billions of parameters to enable\nbroad problem-solving capabilities. Such language models also tend to be\nmemory-bound because of the dominance of matrix-vector and matrix-matrix\nmultiplications with low arithmetic intensity. Therefore, optimizing the memory\nfootprint and traffic is an important optimization direction for LLMs today.\nModel compression methods such as quantization and parameter pruning have been\nactively explored to achieve memory footprint and traffic optimization.\nHowever, the accuracy-efficiency trade-off of rank pruning (i.e., low-rank\ndecomposition) for LLMs is not well-understood yet. Therefore, in this work, we\ncharacterize the accuracy-efficiency trade-off of a low-rank decomposition\nmethod, specifically Tucker decomposition, on recent language models, including\nan open-source LLM, Llama 2. We formalize the low-rank decomposition design\nspace and show that the decomposition design space is enormous (e.g.,\nO($2^{39}$) for Llama2-7B). To navigate such a vast design space, we formulate\nit and perform thorough case studies of accuracy-efficiency trade-offs using\nsix widely used LLM benchmarks on BERT and Llama 2 models. Our results show\nthat we can achieve a 9\\% model size reduction with minimal accuracy drops,\nwhich range from 4\\%p (\\%p refers to \"percentage point,\" which refers to the\nabsolute difference between two percentage numbers; 74\\% -> 78\\% = 4\\%p\nincrease) to 10\\%p, depending on the difficulty of the benchmark, without any\nretraining to recover accuracy after decomposition. The results show that\nlow-rank decomposition can be a promising direction for LLM-based applications\nthat require real-time service at scale (e.g., AI agent and real-time coding\nassistant), where the latency is as important as the model accuracy.\n","authors":["Chakshu Moar","Faraz Tahmasebi","Michael Pellauer","Hyoukjun Kwon"],"pdf_url":"https://arxiv.org/pdf/2405.06626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12856v4","updated":"2024-10-22T19:53:58Z","published":"2024-05-21T15:13:12Z","title":"LLM Processes: Numerical Predictive Distributions Conditioned on Natural\n  Language","summary":"  Machine learning practitioners often face significant challenges in formally\nintegrating their prior knowledge and beliefs into predictive models, limiting\nthe potential for nuanced and context-aware analyses. Moreover, the expertise\nneeded to integrate this prior knowledge into probabilistic modeling typically\nlimits the application of these models to specialists. Our goal is to build a\nregression model that can process numerical data and make probabilistic\npredictions at arbitrary locations, guided by natural language text which\ndescribes a user's prior knowledge. Large Language Models (LLMs) provide a\nuseful starting point for designing such a tool since they 1) provide an\ninterface where users can incorporate expert insights in natural language and\n2) provide an opportunity for leveraging latent problem-relevant knowledge\nencoded in LLMs that users may not have themselves. We start by exploring\nstrategies for eliciting explicit, coherent numerical predictive distributions\nfrom LLMs. We examine these joint predictive distributions, which we call LLM\nProcesses, over arbitrarily-many quantities in settings such as forecasting,\nmulti-dimensional regression, black-box optimization, and image modeling. We\ninvestigate the practical details of prompting to elicit coherent predictive\ndistributions, and demonstrate their effectiveness at regression. Finally, we\ndemonstrate the ability to usefully incorporate text into numerical\npredictions, improving predictive performance and giving quantitative structure\nthat reflects qualitative descriptions. This lets us begin to explore the rich,\ngrounded hypothesis space that LLMs implicitly encode.\n","authors":["James Requeima","John Bronskill","Dami Choi","Richard E. Turner","David Duvenaud"],"pdf_url":"https://arxiv.org/pdf/2405.12856v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17385v1","updated":"2024-10-22T19:39:15Z","published":"2024-10-22T19:39:15Z","title":"Do Vision-Language Models Represent Space and How? Evaluating Spatial\n  Frame of Reference Under Ambiguities","summary":"  Spatial expressions in situated communication can be ambiguous, as their\nmeanings vary depending on the frames of reference (FoR) adopted by speakers\nand listeners. While spatial language understanding and reasoning by\nvision-language models (VLMs) have gained increasing attention, potential\nambiguities in these models are still under-explored. To address this issue, we\npresent the COnsistent Multilingual Frame Of Reference Test (COMFORT), an\nevaluation protocol to systematically assess the spatial reasoning capabilities\nof VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing\nsome alignment with English conventions in resolving ambiguities, our\nexperiments reveal significant shortcomings of VLMs: notably, the models (1)\nexhibit poor robustness and consistency, (2) lack the flexibility to\naccommodate multiple FoRs, and (3) fail to adhere to language-specific or\nculture-specific conventions in cross-lingual tests, as English tends to\ndominate other languages. With a growing effort to align vision-language models\nwith human cognitive intuitions, we call for more attention to the ambiguous\nnature and cross-cultural diversity of spatial reasoning.\n","authors":["Zheyuan Zhang","Fengyuan Hu","Jayjun Lee","Freda Shi","Parisa Kordjamshidi","Joyce Chai","Ziqiao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.17385v1.pdf","comment":"Accepted to Pluralistic Alignment @ NeurIPS 2024 | Project page:\n  https://spatial-comfort.github.io/"},{"id":"http://arxiv.org/abs/2410.17375v1","updated":"2024-10-22T19:15:35Z","published":"2024-10-22T19:15:35Z","title":"AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM\n  Acceleration","summary":"  Large language models typically generate tokens autoregressively, using each\ntoken as input for the next. Recent work on Speculative Decoding has sought to\naccelerate this process by employing a smaller, faster draft model to more\nquickly generate candidate tokens. These candidates are then verified in\nparallel by the larger (original) verify model, resulting in overall speedup\ncompared to using the larger model by itself in an autoregressive fashion. In\nthis work, we introduce AMUSD (Asynchronous Multi-device Speculative Decoding),\na system that further accelerates generation by decoupling the draft and verify\nphases into a continuous, asynchronous approach. Unlike conventional\nspeculative decoding, where only one model (draft or verify) performs token\ngeneration at a time, AMUSD enables both models to perform predictions\nindependently on separate devices (e.g., GPUs). We evaluate our approach over\nmultiple datasets and show that AMUSD achieves an average 29% improvement over\nspeculative decoding and up to 1.96$\\times$ speedup over conventional\nautoregressive decoding, while achieving identical output quality. Our system\nis open-source and available at https://github.com/BradMcDanel/AMUSD/.\n","authors":["Bradley McDanel"],"pdf_url":"https://arxiv.org/pdf/2410.17375v1.pdf","comment":"4 pages, 5 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2405.11724v2","updated":"2024-10-22T19:07:08Z","published":"2024-05-20T01:57:34Z","title":"Token-wise Influential Training Data Retrieval for Large Language Models","summary":"  Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.\n","authors":["Huawei Lin","Jikai Long","Zhaozhuo Xu","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.11724v2.pdf","comment":"Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution"},{"id":"http://arxiv.org/abs/2402.13459v2","updated":"2024-10-22T19:01:35Z","published":"2024-02-21T01:30:03Z","title":"Learning to Poison Large Language Models During Instruction Tuning","summary":"  The advent of Large Language Models (LLMs) has marked significant\nachievements in language processing and reasoning capabilities. Despite their\nadvancements, LLMs face vulnerabilities to data poisoning attacks, where\nadversaries insert backdoor triggers into training data to manipulate outputs\nfor malicious purposes. This work further identifies additional security risks\nin LLMs by designing a new data poisoning attack tailored to exploit the\ninstruction tuning process. We propose a novel gradient-guided backdoor trigger\nlearning (GBTL) algorithm to identify adversarial triggers efficiently,\nensuring an evasion of detection by conventional defenses while maintaining\ncontent integrity. Through experimental validation across various tasks,\nincluding sentiment analysis, domain generation, and question answering, our\npoisoning strategy demonstrates a high success rate in compromising various\nLLMs' outputs. We further propose two defense strategies against data poisoning\nattacks, including in-context learning (ICL) and continuous learning (CL),\nwhich effectively rectify the behavior of LLMs and significantly reduce the\ndecline in performance. Our work highlights the significant security risks\npresent during the instruction tuning of LLMs and emphasizes the necessity of\nsafeguarding LLMs against data poisoning attacks.\n","authors":["Yao Qiang","Xiangyu Zhou","Saleh Zare Zade","Mohammad Amin Roshani","Prashant Khanduri","Douglas Zytko","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.13459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15226v2","updated":"2024-10-22T18:54:23Z","published":"2024-10-19T22:14:07Z","title":"On the Diversity of Synthetic Data and its Impact on Training Large\n  Language Models","summary":"  The rise of Large Language Models (LLMs) has accentuated the need for\ndiverse, high-quality pre-training data. Synthetic data emerges as a viable\nsolution to the challenges of data scarcity and inaccessibility. While previous\nliterature has focused predominantly on the quality and quantity of real data,\nour work enables the measurement of diversity in synthetic data and explores\nits impact on LLM performance. We study the downstream effects of synthetic\ndata diversity during both the pre-training and fine-tuning stages by\nintroducing a new diversity metric, \\textit{LLM cluster-agent}, designed to\nevaluate the diversity of synthetic datasets. Through a series of controlled\nexperiments with models of 350M and 1.4B parameters, we demonstrate that the\nproposed cluster-based LLM scoring of diversity correlates positively with both\npre-training and supervised fine-tuning performance. Our findings also reveal\nthat synthetic data diversity in pre-training affects supervised fine-tuning\nmore significantly than pre-training itself, even for smaller models. We hope\nthis study advances our understanding of the optimal use of synthetic data in\nLLM training and opens new avenues for efficient data generation processes.\n","authors":["Hao Chen","Abdul Waheed","Xiang Li","Yidong Wang","Jindong Wang","Bhiksha Raj","Marah I. Abdin"],"pdf_url":"https://arxiv.org/pdf/2410.15226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17542v3","updated":"2024-10-22T18:51:01Z","published":"2024-06-25T13:29:14Z","title":"CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization","summary":"  Large language models (LLMs) have recently demonstrated remarkable\nperformance across diverse language tasks. But their deployment is often\nconstrained by their substantial computational and storage requirements.\nQuantization has emerged as a key technique for addressing this challenge,\nenabling the compression of large models with minimal impact on performance.\nThe recent GPTQ algorithm, a post-training quantization (PTQ) method, has\nproven highly effective for compressing LLMs, sparking a wave of research that\nleverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the\nPTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ\nwith improved performance. CDQuant uses greedy coordinate descent to minimize\nthe layer-wise reconstruction loss to achieve high-quality quantized weights.\nOur algorithm is easy to implement and scales efficiently to models with\nhundreds of billions of parameters. We perform extensive evaluation on Gemma,\nand PaLM2 model families, and demonstrate that CDQuant consistently outperforms\nGPTQ in 2-4 bit weight quantization. Moreover, CDQuant improves the performance\nof state-of-the-art PTQ techniques such as QuIP and FrameQuant when used as a\nreplacement for their GPTQ component, resulting in further gains in quality.\n","authors":["Pranav Ajit Nair","Arun Sai Suggala"],"pdf_url":"https://arxiv.org/pdf/2406.17542v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17355v1","updated":"2024-10-22T18:47:46Z","published":"2024-10-22T18:47:46Z","title":"All Entities are Not Created Equal: Examining the Long Tail for\n  Fine-Grained Entity Typing","summary":"  Pre-trained language models (PLMs) are trained on large amounts of data,\nwhich helps capture world knowledge alongside linguistic competence. Due to\nthis, they are extensively used for ultra-fine entity typing tasks, where they\nprovide the entity knowledge held in its parameter space. Given that PLMs learn\nfrom co-occurrence patterns, they likely contain more knowledge or less\nknowledge about entities depending on their how frequent they are in the\npre-training data. In this work, we probe PLMs to elicit encoded entity\nprobabilities and demonstrate that they highly correlate with their frequency\nin large-scale internet data. Then, we demonstrate that entity-typing\napproaches that rely on PLMs struggle with entities at the long tail on the\ndistribution. Our findings suggests that we need to go beyond PLMs to produce\nsolutions that perform well for rare, new or infrequent entities.\n","authors":["Advait Deshmukh","Ashwin Umadi","Dananjay Srinivas","Maria Leonor Pacheco"],"pdf_url":"https://arxiv.org/pdf/2410.17355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17678v5","updated":"2024-10-22T18:26:51Z","published":"2024-07-25T00:27:07Z","title":"S2-Attention: Hardware-Aware Context Sharding Among Attention Heads","summary":"  Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.\n","authors":["Xihui Lin","Yunan Zhang","Suyu Ge","Liliang Ren","Barun Patra","Vishrav Chaudhary","Hao Peng","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2407.17678v5.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.02024v3","updated":"2024-10-22T18:22:11Z","published":"2024-10-02T20:45:51Z","title":"FLAG: Financial Long Document Classification via AMR-based GNN","summary":"  The advent of large language models (LLMs) has initiated much research into\ntheir various financial applications. However, in applying LLMs on long\ndocuments, semantic relations are not explicitly incorporated, and a full or\narbitrarily sparse attention operation is employed. In recent years, progress\nhas been made in Abstract Meaning Representation (AMR), which is a graph-based\nrepresentation of text to preserve its semantic relations. Since AMR can\nrepresent semantic relationships at a deeper level, it can be beneficially\nutilized by graph neural networks (GNNs) for constructing effective\ndocument-level graph representations built upon LLM embeddings to predict\ntarget metrics in the financial domain. We propose FLAG: Financial Long\ndocument classification via AMR-based GNN, an AMR graph based framework to\ngenerate document-level embeddings for long financial document classification.\nWe construct document-level graphs from sentence-level AMR graphs, endow them\nwith specialized LLM word embeddings in the financial domain, apply a deep\nlearning mechanism that utilizes a GNN, and examine the efficacy of our\nAMR-based approach in predicting labeled target data from long financial\ndocuments. Extensive experiments are conducted on a dataset of quarterly\nearnings calls transcripts of companies in various sectors of the economy, as\nwell as on a corpus of more recent earnings calls of companies in the S&P 1500\nComposite Index. We find that our AMR-based approach outperforms fine-tuning\nLLMs directly on text in predicting stock price movement trends at different\ntime horizons in both datasets. Our work also outperforms previous work\nutilizing document graphs and GNNs for text classification.\n","authors":["Bolun \"Namir\" Xia","Aparna Gupta","Mohammed J. Zaki"],"pdf_url":"https://arxiv.org/pdf/2410.02024v3.pdf","comment":"8 pages, 3 figures, to be published in CIFEr Conference 2024 as\n  \"Semantic Graph Learning for Trend Prediction from Long Financial Documents\""},{"id":"http://arxiv.org/abs/2410.17337v1","updated":"2024-10-22T18:11:43Z","published":"2024-10-22T18:11:43Z","title":"Captions Speak Louder than Images (CASLIE): Generalizing Foundation\n  Models for E-commerce from High-quality Multimodal Instruction Data","summary":"  Leveraging multimodal data to drive breakthroughs in e-commerce applications\nthrough Multimodal Foundation Models (MFMs) is gaining increasing attention\nfrom the research community. However, there are significant challenges that\nhinder the optimal use of multimodal e-commerce data by foundation models: (1)\nthe scarcity of large-scale, high-quality multimodal benchmark datasets; and\n(2) the lack of effective multimodal information integration methods. To\naddress these challenges, in this paper, we introduce MMECInstruct, the\nfirst-ever, large-scale, and high-quality multimodal instruction dataset for\ne-commerce. We also develop CASLIE, a simple, lightweight, yet effective\nframework for integrating multimodal information for e-commerce. Leveraging\nMMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted\nas CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models\nsubstantially outperform 5 categories of advanced baseline models in the\nin-domain evaluation. Moreover, CASLIE models show strong generalizability to\nout-of-domain settings. MMECInstruct and CASLIE models are publicly accessible\nthrough https://ninglab.github.io/CASLIE/.\n","authors":["Xinyi Ling","Bo Peng","Hanwen Du","Zhihui Zhu","Xia Ning"],"pdf_url":"https://arxiv.org/pdf/2410.17337v1.pdf","comment":"Xinyi Ling and Bo Peng contributed equally to this paper"},{"id":"http://arxiv.org/abs/2410.17333v1","updated":"2024-10-22T18:08:25Z","published":"2024-10-22T18:08:25Z","title":"Are Large Language Models Ready for Travel Planning?","summary":"  While large language models (LLMs) show promise in hospitality and tourism,\ntheir ability to provide unbiased service across demographic groups remains\nunclear. This paper explores gender and ethnic biases when LLMs are utilized as\ntravel planning assistants. To investigate this issue, we apply machine\nlearning techniques to analyze travel suggestions generated from three\nopen-source LLMs. Our findings reveal that the performance of race and gender\nclassifiers substantially exceeds random chance, indicating differences in how\nLLMs engage with varied subgroups. Specifically, outputs align with cultural\nexpectations tied to certain races and genders. To minimize the effect of these\nstereotypes, we used a stop-word classification strategy, which decreased\nidentifiable differences, with no disrespectful terms found. However,\nhallucinations related to African American and gender minority groups were\nnoted. In conclusion, while LLMs can generate travel plans seemingly free from\nbias, it remains essential to verify the accuracy and appropriateness of their\nrecommendations.\n","authors":["Ruiping Ren","Xing Yao","Shu Cole","Haining Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17309v1","updated":"2024-10-22T18:00:00Z","published":"2024-10-22T18:00:00Z","title":"Literature Meets Data: A Synergistic Approach to Hypothesis Generation","summary":"  AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.\n","authors":["Haokun Liu","Yangqiaoyu Zhou","Mingxuan Li","Chenfei Yuan","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17309v1.pdf","comment":"30 pages, 7 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation"},{"id":"http://arxiv.org/abs/2311.18567v2","updated":"2024-10-22T17:21:21Z","published":"2023-11-30T13:58:13Z","title":"The Causal Influence of Grammatical Gender on Distributional Semantics","summary":"  How much meaning influences gender assignment across languages is an active\narea of research in linguistics and cognitive science. We can view current\napproaches as aiming to determine where gender assignment falls on a spectrum,\nfrom being fully arbitrarily determined to being largely semantically\ndetermined. For the latter case, there is a formulation of the neo-Whorfian\nhypothesis, which claims that even inanimate noun gender influences how people\nconceive of and talk about objects (using the choice of adjective used to\nmodify inanimate nouns as a proxy for meaning). We offer a novel, causal\ngraphical model that jointly represents the interactions between a noun's\ngrammatical gender, its meaning, and adjective choice. In accordance with past\nresults, we find a significant relationship between the gender of nouns and the\nadjectives that modify them. However, when we control for the meaning of the\nnoun, the relationship between grammatical gender and adjective choice is near\nzero and insignificant.\n","authors":["Karolina Stańczak","Kevin Du","Adina Williams","Isabelle Augenstein","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2311.18567v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.16261v2","updated":"2024-10-22T08:09:52Z","published":"2024-10-21T17:58:20Z","title":"Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5%\n  Parameters and 90% Performance","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\nperformance in vision-language tasks across a broad spectrum of domains.\nHowever, the large model scale and associated high computational costs pose\nsignificant challenges for training and deploying MLLMs on consumer-grade GPUs\nor edge devices, thereby hindering their widespread application. In this work,\nwe introduce Mini-InternVL, a series of MLLMs with parameters ranging from 1B\nto 4B, which achieves 90% of the performance with only 5% of the parameters.\nThis significant improvement in efficiency and effectiveness makes our models\nmore accessible and applicable in various real-world scenarios. To further\npromote the adoption of our models, we develop a unified adaptation framework\nfor Mini-InternVL, which enables our models to transfer and outperform\nspecialized models in downstream tasks, including autonomous driving, medical\nimages, and remote sensing. We believe that our study can provide valuable\ninsights and resources to advance the development of efficient and effective\nMLLMs. Code is available at https://github.com/OpenGVLab/InternVL.\n","authors":["Zhangwei Gao","Zhe Chen","Erfei Cui","Yiming Ren","Weiyun Wang","Jinguo Zhu","Hao Tian","Shenglong Ye","Junjun He","Xizhou Zhu","Lewei Lu","Tong Lu","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16261v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2410.16152v2","updated":"2024-10-22T03:37:37Z","published":"2024-10-21T16:19:34Z","title":"Warped Diffusion: Solving Video Inverse Problems with Image Diffusion\n  Models","summary":"  Using image models naively for solving inverse video problems often suffers\nfrom flickering, texture-sticking, and temporal inconsistency in generated\nvideos. To tackle these problems, in this paper, we view frames as continuous\nfunctions in the 2D space, and videos as a sequence of continuous warping\ntransformations between different frames. This perspective allows us to train\nfunction space diffusion models only on images and utilize them to solve\ntemporally correlated inverse problems. The function space diffusion models\nneed to be equivariant with respect to the underlying spatial transformations.\nTo ensure temporal consistency, we introduce a simple post-hoc test-time\nguidance towards (self)-equivariant solutions. Our method allows us to deploy\nstate-of-the-art latent diffusion models such as Stable Diffusion XL to solve\nvideo inverse problems. We demonstrate the effectiveness of our method for\nvideo inpainting and $8\\times$ video super-resolution, outperforming existing\ntechniques based on noise transformations. We provide generated video results:\nhttps://giannisdaras.github.io/warped_diffusion.github.io/.\n","authors":["Giannis Daras","Weili Nie","Karsten Kreis","Alex Dimakis","Morteza Mardani","Nikola Borislavov Kovachki","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2410.16152v2.pdf","comment":"Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15980v2","updated":"2024-10-22T06:35:13Z","published":"2024-10-21T13:06:21Z","title":"Granularity Matters in Long-Tail Learning","summary":"  Balancing training on long-tail data distributions remains a long-standing\nchallenge in deep learning. While methods such as re-weighting and re-sampling\nhelp alleviate the imbalance issue, limited sample diversity continues to\nhinder models from learning robust and generalizable feature representations,\nparticularly for tail classes. In contrast to existing methods, we offer a\nnovel perspective on long-tail learning, inspired by an observation: datasets\nwith finer granularity tend to be less affected by data imbalance. In this\npaper, we investigate this phenomenon through both quantitative and qualitative\nstudies, showing that increased granularity enhances the generalization of\nlearned features in tail categories. Motivated by these findings, we propose a\nmethod to increase dataset granularity through category extrapolation.\nSpecifically, we introduce open-set auxiliary classes that are visually similar\nto existing ones, aiming to enhance representation learning for both head and\ntail classes. This forms the core contribution and insight of our approach. To\nautomate the curation of auxiliary data, we leverage large language models\n(LLMs) as knowledge bases to search for auxiliary categories and retrieve\nrelevant images through web crawling. To prevent the overwhelming presence of\nauxiliary classes from disrupting training, we introduce a neighbor-silencing\nloss that encourages the model to focus on class discrimination within the\ntarget dataset. During inference, the classifier weights for auxiliary\ncategories are masked out, leaving only the target class weights for use.\nExtensive experiments and ablation studies on three standard long-tail\nbenchmarks demonstrate the effectiveness of our approach, notably outperforming\nstrong baseline methods that use the same amount of data. The code will be made\npublicly available.\n","authors":["Shizhen Zhao","Xin Wen","Jiahui Liu","Chuofan Ma","Chunfeng Yuan","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2410.15980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15957v2","updated":"2024-10-22T06:26:45Z","published":"2024-10-21T12:36:27Z","title":"CamI2V: Camera-Controlled Image-to-Video Diffusion Model","summary":"  Recently, camera pose, as a user-friendly and physics-related condition, has\nbeen introduced into text-to-video diffusion model for camera control. However,\nexisting methods simply inject camera conditions through a side input. These\napproaches neglect the inherent physical knowledge of camera pose, resulting in\nimprecise camera control, inconsistencies, and also poor interpretability. In\nthis paper, we emphasize the necessity of integrating explicit physical\nconstraints into model design. Epipolar attention is proposed for modeling all\ncross-frame relationships from a novel perspective of noised condition. This\nensures that features are aggregated from corresponding epipolar lines in all\nnoised frames, overcoming the limitations of current attention mechanisms in\ntracking displaced features across frames, especially when features move\nsignificantly with the camera and become obscured by noise. Additionally, we\nintroduce register tokens to handle cases without intersections between frames,\ncommonly caused by rapid camera movements, dynamic objects, or occlusions. To\nsupport image-to-video, we propose the multiple guidance scale to allow for\nprecise control for image, text, and camera, respectively. Furthermore, we\nestablish a more robust and reproducible evaluation pipeline to solve the\ninaccuracy and instability of existing camera control measurement. We achieve a\n25.5% improvement in camera controllability on RealEstate10K while maintaining\nstrong generalization to out-of-domain images. Only 24GB and 12GB are required\nfor training and inference, respectively. We plan to release checkpoints, along\nwith training and evaluation codes. Dynamic videos are best viewed at\nhttps://zgctroy.github.io/CamI2V.\n","authors":["Guangcong Zheng","Teng Li","Rui Jiang","Yehao Lu","Tao Wu","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2410.15957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15778v2","updated":"2024-10-22T05:01:28Z","published":"2024-10-21T08:42:30Z","title":"Reducing Hallucinations in Vision-Language Models via Latent Space\n  Steering","summary":"  Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.\n","authors":["Sheng Liu","Haotian Ye","Lei Xing","James Zou"],"pdf_url":"https://arxiv.org/pdf/2410.15778v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.13621v4","updated":"2024-10-22T01:47:06Z","published":"2024-10-17T14:55:09Z","title":"EP-SAM: Weakly Supervised Histopathology Segmentation via Enhanced\n  Prompt with Segment Anything","summary":"  This work proposes a novel approach beyond supervised learning for effective\npathological image analysis, addressing the challenge of limited robust labeled\ndata. Pathological diagnosis of diseases like cancer has conventionally relied\non the evaluation of morphological features by physicians and pathologists.\nHowever, recent advancements in compute-aided diagnosis (CAD) systems are\ngaining significant attention as diagnostic support tools. Although the\nadvancement of deep learning has improved CAD significantly, segmentation\nmodels typically require large pixel-level annotated dataset, and such labeling\nis expensive. Existing studies not based on supervised approaches still\nstruggle with limited generalization, and no practical approach has emerged\nyet. To address this issue, we present a weakly supervised semantic\nsegmentation (WSSS) model by combining class activation map and Segment\nAnything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt\nthe SAM-a foundation model that is pretrained on large datasets and operates in\nzero-shot configurations using only coarse prompts. The proposed approach\ntransfer enhanced Attention Dropout Layer's knowledge to SAM, thereby\ngenerating pseudo-labels. To demonstrate the superiority of the proposed\nmethod, experimental studies are conducted on histopathological breast cancer\ndatasets. The proposed method outperformed other WSSS methods across three\ndatasets, demonstrating its efficiency by achieving this with only 12GB of GPU\nmemory during training. Our code is available at :\nhttps://github.com/QI-NemoSong/EP-SAM\n","authors":["Joonhyeon Song","Seohwan Yun","Seongho Yoon","Joohyeok Kim","Sangmin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.13621v4.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.15636v2","updated":"2024-10-22T07:10:45Z","published":"2024-10-21T04:47:01Z","title":"LucidFusion: Generating 3D Gaussians with Arbitrary Unposed Images","summary":"  Recent large reconstruction models have made notable progress in generating\nhigh-quality 3D objects from single images. However, these methods often\nstruggle with controllability, as they lack information from multiple views,\nleading to incomplete or inconsistent 3D reconstructions. To address this\nlimitation, we introduce LucidFusion, a flexible end-to-end feed-forward\nframework that leverages the Relative Coordinate Map (RCM). Unlike traditional\nmethods linking images to 3D world thorough pose, LucidFusion utilizes RCM to\nalign geometric features coherently across different views, making it highly\nadaptable for 3D generation from arbitrary, unposed images. Furthermore,\nLucidFusion seamlessly integrates with the original single-image-to-3D\npipeline, producing detailed 3D Gaussians at a resolution of $512 \\times 512$,\nmaking it well-suited for a wide range of applications.\n","authors":["Hao He","Yixun Liang","Luozhou Wang","Yuanhao Cai","Xinli Xu","Hao-Xiang Guo","Xiang Wen","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15636v2.pdf","comment":"17 pages, 12 figures, [project\n  page](https://heye0507.github.io/LucidFusion_page/)"},{"id":"http://arxiv.org/abs/2410.15629v2","updated":"2024-10-22T12:02:29Z","published":"2024-10-21T04:25:43Z","title":"Fully Explicit Dynamic Gaussian Splatting","summary":"  3D Gaussian Splatting has shown fast and high-quality rendering results in\nstatic scenes by leveraging dense 3D prior and explicit representations.\nUnfortunately, the benefits of the prior and representation do not involve\nnovel view synthesis for dynamic motions. Ironically, this is because the main\nbarrier is the reliance on them, which requires increasing training and\nrendering times to account for dynamic motions. In this paper, we design a\nExplicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate\nstatic and dynamic Gaussians during training, and to explicitly sample\npositions and rotations of the dynamic Gaussians at sparse timestamps. The\nsampled positions and rotations are then interpolated to represent both\nspatially and temporally continuous motions of objects in dynamic scenes as\nwell as reducing computational cost. Additionally, we introduce a progressive\ntraining scheme and a point-backtracking technique that improves Ex4DGS's\nconvergence. We initially train Ex4DGS using short timestamps and progressively\nextend timestamps, which makes it work well with a few point clouds. The\npoint-backtracking is used to quantify the cumulative error of each Gaussian\nover time, enabling the detection and removal of erroneous Gaussians in dynamic\nscenes. Comprehensive experiments on various scenes demonstrate the\nstate-of-the-art rendering quality from our method, achieving fast rendering of\n62 fps on a single 2080Ti GPU.\n","authors":["Junoh Lee","Chang-Yeon Won","Hyunjun Jung","Inhwan Bae","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2410.15629v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15472v2","updated":"2024-10-22T02:59:51Z","published":"2024-10-20T19:02:41Z","title":"Multi-Layer Feature Fusion with Cross-Channel Attention-Based U-Net for\n  Kidney Tumor Segmentation","summary":"  Renal tumors, especially renal cell carcinoma (RCC), show significant\nheterogeneity, posing challenges for diagnosis using radiology images such as\nMRI, echocardiograms, and CT scans. U-Net based deep learning techniques are\nemerging as a promising approach for automated medical image segmentation for\nminimally invasive diagnosis of renal tumors. However, current techniques need\nfurther improvements in accuracy to become clinically useful to radiologists.\nIn this study, we present an improved U-Net based model for end-to-end\nautomated semantic segmentation of CT scan images to identify renal tumors. The\nmodel uses residual connections across convolution layers, integrates a\nmulti-layer feature fusion (MFF) and cross-channel attention (CCA) within\nencoder blocks, and incorporates skip connections augmented with additional\ninformation derived using MFF and CCA. We evaluated our model on the KiTS19\ndataset, which contains data from 210 patients. For kidney segmentation, our\nmodel achieves a Dice Similarity Coefficient (DSC) of 0.97 and a Jaccard index\n(JI) of 0.95. For renal tumor segmentation, our model achieves a DSC of 0.96\nand a JI of 0.91. Based on a comparison of available DSC scores, our model\noutperforms the current leading models.\n","authors":["Fnu Neha","Arvind K. Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.15472v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2401.10090v5","updated":"2024-10-22T03:48:13Z","published":"2024-01-18T15:56:23Z","title":"Cross-Modality Perturbation Synergy Attack for Person Re-identification","summary":"  In recent years, there has been significant research focusing on addressing\nsecurity concerns in single-modal person re-identification (ReID) systems that\nare based on RGB images. However, the safety of cross-modality scenarios, which\nare more commonly encountered in practical applications involving images\ncaptured by infrared cameras, has not received adequate attention. The main\nchallenge in cross-modality ReID lies in effectively dealing with visual\ndifferences between different modalities. For instance, infrared images are\ntypically grayscale, unlike visible images that contain color information.\nExisting attack methods have primarily focused on the characteristics of the\nvisible image modality, overlooking the features of other modalities and the\nvariations in data distribution among different modalities. This oversight can\npotentially undermine the effectiveness of these methods in image retrieval\nacross diverse modalities. This study represents the first exploration into the\nsecurity of cross-modality ReID models and proposes a universal perturbation\nattack specifically designed for cross-modality ReID. This attack optimizes\nperturbations by leveraging gradients from diverse modality data, thereby\ndisrupting the discriminator and reinforcing the differences between\nmodalities. We conducted experiments on three widely used cross-modality\ndatasets, namely RegDB, SYSU, and LLCM. The results not only demonstrate the\neffectiveness of our method but also provide insights for future improvements\nin the robustness of cross-modality ReID systems.\n","authors":["Yunpeng Gong","Zhun Zhong","Yansong Qu","Zhiming Luo","Rongrong Ji","Min Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.10090v5.pdf","comment":"Accepted at the Thirty-eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.17251v1","updated":"2024-10-22T17:59:57Z","published":"2024-10-22T17:59:57Z","title":"Altogether: Image Captioning via Re-aligning Alt-text","summary":"  This paper focuses on creating synthetic data to improve the quality of image\ncaptions. Existing works typically have two shortcomings. First, they caption\nimages from scratch, ignoring existing alt-text metadata, and second, lack\ntransparency if the captioners' training data (e.g. GPT) is unknown. In this\npaper, we study a principled approach Altogether based on the key idea to edit\nand re-align existing alt-texts associated with the images. To generate\ntraining data, we perform human annotation where annotators start with the\nexisting alt-text and re-align it to the image content in multiple rounds,\nconsequently constructing captions with rich visual concepts. This differs from\nprior work that carries out human annotation as a one-time description task\nsolely based on images and annotator knowledge. We train a captioner on this\ndata that generalizes the process of re-aligning alt-texts at scale. Our\nresults show our Altogether approach leads to richer image captions that also\nimprove text-to-image generation and zero-shot image classification tasks.\n","authors":["Hu Xu","Po-Yao Huang","Xiaoqing Ellen Tan","Ching-Feng Yeh","Jacob Kahn","Christine Jou","Gargi Ghosh","Omer Levy","Luke Zettlemoyer","Wen-tau Yih","Shang-Wen Li","Saining Xie","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2410.17251v1.pdf","comment":"accepted by EMNLP 2024; MetaCLIPv2"},{"id":"http://arxiv.org/abs/2410.17249v1","updated":"2024-10-22T17:59:56Z","published":"2024-10-22T17:59:56Z","title":"SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes","summary":"  We present SpectroMotion, a novel approach that combines 3D Gaussian\nSplatting (3DGS) with physically-based rendering (PBR) and deformation fields\nto reconstruct dynamic specular scenes. Previous methods extending 3DGS to\nmodel dynamic scenes have struggled to accurately represent specular surfaces.\nOur method addresses this limitation by introducing a residual correction\ntechnique for accurate surface normal computation during deformation,\ncomplemented by a deformable environment map that adapts to time-varying\nlighting conditions. We implement a coarse-to-fine training strategy that\nsignificantly enhances both scene geometry and specular color prediction. We\ndemonstrate that our model outperforms prior methods for view synthesis of\nscenes containing dynamic specular objects and that it is the only existing\n3DGS method capable of synthesizing photorealistic real-world dynamic specular\nscenes, outperforming state-of-the-art methods in rendering complex, dynamic,\nand specular scenes.\n","authors":["Cheng-De Fan","Chen-Wei Chang","Yi-Ruei Liu","Jie-Ying Lee","Jiun-Long Huang","Yu-Chee Tseng","Yu-Lun Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17249v1.pdf","comment":"Project page: https://cdfan0627.github.io/spectromotion/"},{"id":"http://arxiv.org/abs/2410.17250v1","updated":"2024-10-22T17:59:56Z","published":"2024-10-22T17:59:56Z","title":"JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding\n  Benchmark for Culture-aware Evaluation","summary":"  Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.\n","authors":["Shota Onohara","Atsuyuki Miyai","Yuki Imajuku","Kazuki Egashira","Jeonghun Baek","Xiang Yue","Graham Neubig","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2410.17250v1.pdf","comment":"Project page: https://mmmu-japanese-benchmark.github.io/JMMMU/"},{"id":"http://arxiv.org/abs/2410.17247v1","updated":"2024-10-22T17:59:53Z","published":"2024-10-22T17:59:53Z","title":"PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid\n  Visual Redundancy Reduction","summary":"  In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. We hope that the insights and approach\nintroduced by PyramidDrop will inspire future research to further investigate\nthe role of image tokens in LVLMs.\n","authors":["Long Xing","Qidong Huang","Xiaoyi Dong","Jiajie Lu","Pan Zhang","Yuhang Zang","Yuhang Cao","Conghui He","Jiaqi Wang","Feng Wu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.17247v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.17243v1","updated":"2024-10-22T17:59:30Z","published":"2024-10-22T17:59:30Z","title":"Breaking the Memory Barrier: Near Infinite Batch Size Scaling for\n  Contrastive Loss","summary":"  Contrastive loss is a powerful approach for representation learning, where\nlarger batch sizes enhance performance by providing more negative samples to\nbetter distinguish between similar and dissimilar data. However, scaling batch\nsizes is constrained by the quadratic growth in GPU memory consumption,\nprimarily due to the full instantiation of the similarity matrix. To address\nthis, we propose a tile-based computation strategy that partitions the\ncontrastive loss calculation into arbitrary small blocks, avoiding full\nmaterialization of the similarity matrix. Furthermore, we introduce a\nmulti-level tiling strategy to leverage the hierarchical structure of\ndistributed systems, employing ring-based communication at the GPU level to\noptimize synchronization and fused kernels at the CUDA core level to reduce I/O\noverhead. Experimental results show that the proposed method scales batch sizes\nto unprecedented levels. For instance, it enables contrastive training of a\nCLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB\nwithout sacrificing any accuracy. Compared to SOTA memory-efficient solutions,\nit achieves a two-order-of-magnitude reduction in memory while maintaining\ncomparable speed. The code will be made publicly available.\n","authors":["Zesen Cheng","Hang Zhang","Kehan Li","Sicong Leng","Zhiqiang Hu","Fei Wu","Deli Zhao","Xin Li","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.17243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17242v1","updated":"2024-10-22T17:58:28Z","published":"2024-10-22T17:58:28Z","title":"LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias","summary":"  We propose the Large View Synthesis Model (LVSM), a novel transformer-based\napproach for scalable and generalizable novel view synthesis from sparse-view\ninputs. We introduce two architectures: (1) an encoder-decoder LVSM, which\nencodes input image tokens into a fixed number of 1D latent tokens, functioning\nas a fully learned scene representation, and decodes novel-view images from\nthem; and (2) a decoder-only LVSM, which directly maps input images to\nnovel-view outputs, completely eliminating intermediate scene representations.\nBoth models bypass the 3D inductive biases used in previous methods -- from 3D\nrepresentations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar\nprojections, plane sweeps) -- addressing novel view synthesis with a fully\ndata-driven approach. While the encoder-decoder model offers faster inference\ndue to its independent latent representation, the decoder-only LVSM achieves\nsuperior quality, scalability, and zero-shot generalization, outperforming\nprevious state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive\nevaluations across multiple datasets demonstrate that both LVSM variants\nachieve state-of-the-art novel view synthesis quality. Notably, our models\nsurpass all previous methods even with reduced computational resources (1-2\nGPUs). Please see our website for more details:\nhttps://haian-jin.github.io/projects/LVSM/ .\n","authors":["Haian Jin","Hanwen Jiang","Hao Tan","Kai Zhang","Sai Bi","Tianyuan Zhang","Fujun Luan","Noah Snavely","Zexiang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.17242v1.pdf","comment":"project page: https://haian-jin.github.io/projects/LVSM/"},{"id":"http://arxiv.org/abs/2410.17241v1","updated":"2024-10-22T17:57:12Z","published":"2024-10-22T17:57:12Z","title":"Frontiers in Intelligent Colonoscopy","summary":"  Colonoscopy is currently one of the most sensitive screening methods for\ncolorectal cancer. This study investigates the frontiers of intelligent\ncolonoscopy techniques and their prospective implications for multimodal\nmedical applications. With this goal, we begin by assessing the current\ndata-centric and model-centric landscapes through four tasks for colonoscopic\nscene perception, including classification, detection, segmentation, and\nvision-language understanding. This assessment enables us to identify\ndomain-specific challenges and reveals that multimodal research in colonoscopy\nremains open for further exploration. To embrace the coming multimodal era, we\nestablish three foundational initiatives: a large-scale multimodal instruction\ntuning dataset ColonINST, a colonoscopy-designed multimodal language model\nColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this\nrapidly evolving field, we provide a public website for the latest updates:\nhttps://github.com/ai4colonoscopy/IntelliScope.\n","authors":["Ge-Peng Ji","Jingyi Liu","Peng Xu","Nick Barnes","Fahad Shahbaz Khan","Salman Khan","Deng-Ping Fan"],"pdf_url":"https://arxiv.org/pdf/2410.17241v1.pdf","comment":"[work in progress] A comprehensive survey of intelligent colonoscopy\n  in the multimodal era"},{"id":"http://arxiv.org/abs/2410.17235v1","updated":"2024-10-22T17:54:07Z","published":"2024-10-22T17:54:07Z","title":"Automated Spinal MRI Labelling from Reports Using a Large Language Model","summary":"  We propose a general pipeline to automate the extraction of labels from\nradiology reports using large language models, which we validate on spinal MRI\nreports. The efficacy of our labelling method is measured on five distinct\nconditions: spinal cancer, stenosis, spondylolisthesis, cauda equina\ncompression and herniation. Using open-source models, our method equals or\nsurpasses GPT-4 on a held-out set of reports. Furthermore, we show that the\nextracted labels can be used to train imaging models to classify the identified\nconditions in the accompanying MR scans. All classifiers trained using\nautomated labels achieve comparable performance to models trained using scans\nmanually annotated by clinicians. Code can be found at\nhttps://github.com/robinyjpark/AutoLabelClassifier.\n","authors":["Robin Y. Park","Rhydian Windsor","Amir Jamaludin","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2410.17235v1.pdf","comment":"Accepted to Medical Image Computing and Computer Assisted\n  Intervention (MICCAI 2024, Spotlight). 11 pages plus appendix"},{"id":"http://arxiv.org/abs/2405.20090v3","updated":"2024-10-22T17:36:30Z","published":"2024-05-30T14:27:20Z","title":"Typography Leads Semantic Diversifying: Amplifying Adversarial\n  Transferability across Multimodal Large Language Models","summary":"  Recently, Multimodal Large Language Models (MLLMs) achieve remarkable\nperformance in numerous zero-shot tasks due to their outstanding cross-modal\ninteraction and comprehension abilities. However, MLLMs are found to still be\nvulnerable to human-imperceptible adversarial examples. In the exploration of\nsecurity vulnerabilities in real-world scenarios, transferability, which can\nachieve cross-model impact, is considered the greatest threat posed by\nadversarial examples. However, there is currently no systematic research on the\nthreat of cross-MLLMs adversarial transferability. Therefore, this paper as the\nfirst step to provide a comprehensive evaluation of the transferability of\nadversarial examples generated by various MLLMs. Furthermore, leveraging two\nkey factors that influence transferability performance: 1) The strength of\ninformation diversity involved in the adversarial generation process; 2)\nEditing across vision-language modality information. We propose a boosting\nmethod called Typography Augment Transferability Method (TATM) to investigate\nthe adversarial transferability performance across MLLMs further. Through\nextensive experimental validation, our TATM demonstrates exceptional\nperformance in real-world applications of \"Harmful Word Insertion\" and\n\"Important Information Protection\".\n","authors":["Hao Cheng","Erjia Xiao","Jiayan Yang","Jiahang Cao","Qiang Zhang","Le Yang","Jize Zhang","Kaidi Xu","Jindong Gu","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2405.20090v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17207v1","updated":"2024-10-22T17:27:16Z","published":"2024-10-22T17:27:16Z","title":"EPContrast: Effective Point-level Contrastive Learning for Large-scale\n  Point Cloud Understanding","summary":"  The acquisition of inductive bias through point-level contrastive learning\nholds paramount significance in point cloud pre-training. However, the square\ngrowth in computational requirements with the scale of the point cloud poses a\nsubstantial impediment to the practical deployment and execution. To address\nthis challenge, this paper proposes an Effective Point-level Contrastive\nLearning method for large-scale point cloud understanding dubbed\n\\textbf{EPContrast}, which consists of AGContrast and ChannelContrast. In\npractice, AGContrast constructs positive and negative pairs based on asymmetric\ngranularity embedding, while ChannelContrast imposes contrastive supervision\nbetween channel feature maps. EPContrast offers point-level contrastive loss\nwhile concurrently mitigating the computational resource burden. The efficacy\nof EPContrast is substantiated through comprehensive validation on S3DIS and\nScanNetV2, encompassing tasks such as semantic segmentation, instance\nsegmentation, and object detection. In addition, rich ablation experiments\ndemonstrate remarkable bias induction capabilities under label-efficient and\none-epoch training settings.\n","authors":["Zhiyi Pan","Guoqing Liu","Wei Gao","Thomas H. Li"],"pdf_url":"https://arxiv.org/pdf/2410.17207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17193v1","updated":"2024-10-22T17:13:19Z","published":"2024-10-22T17:13:19Z","title":"Emphasizing Discriminative Features for Dataset Distillation in Complex\n  Scenarios","summary":"  Dataset distillation has demonstrated strong performance on simple datasets\nlike CIFAR, MNIST, and TinyImageNet but struggles to achieve similar results in\nmore complex scenarios. In this paper, we propose EDF (emphasizes the\ndiscriminative features), a dataset distillation method that enhances key\ndiscriminative regions in synthetic images using Grad-CAM activation maps. Our\napproach is inspired by a key observation: in simple datasets, high-activation\nareas typically occupy most of the image, whereas in complex scenarios, the\nsize of these areas is much smaller. Unlike previous methods that treat all\npixels equally when synthesizing images, EDF uses Grad-CAM activation maps to\nenhance high-activation areas. From a supervision perspective, we downplay\nsupervision signals that have lower losses, as they contain common patterns.\nAdditionally, to help the DD community better explore complex scenarios, we\nbuild the Complex Dataset Distillation (Comp-DD) benchmark by meticulously\nselecting sixteen subsets, eight easy and eight hard, from ImageNet-1K. In\nparticular, EDF consistently outperforms SOTA results in complex scenarios,\nsuch as ImageNet-1K subsets. Hopefully, more researchers will be inspired and\nencouraged to improve the practicality and efficacy of DD. Our code and\nbenchmark will be made public at https://github.com/NUS-HPC-AI-Lab/EDF.\n","authors":["Kai Wang","Zekai Li","Zhi-Qi Cheng","Samir Khaki","Ahmad Sajedi","Ramakrishna Vedantam","Konstantinos N Plataniotis","Alexander Hauptmann","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.17193v1.pdf","comment":"24 pages, 13 figures"},{"id":"http://arxiv.org/abs/2408.11982v3","updated":"2024-10-22T16:58:09Z","published":"2024-08-21T20:32:45Z","title":"AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and\n  Results","summary":"  Video quality assessment (VQA) is a crucial task in the development of video\ncompression standards, as it directly impacts the viewer experience. This paper\npresents the results of the Compressed Video Quality Assessment challenge, held\nin conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV\n2024. The challenge aimed to evaluate the performance of VQA methods on a\ndiverse dataset of 459 videos, encoded with 14 codecs of various compression\nstandards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a\ncomprehensive collection of compression artifacts. To measure the methods\nperformance, we employed traditional correlation coefficients between their\npredictions and subjective scores, which were collected via large-scale\ncrowdsourced pairwise human comparisons. For training purposes, participants\nwere provided with the Compressed Video Quality Assessment Dataset (CVQAD), a\npreviously developed dataset of 1022 videos. Up to 30 participating teams\nregistered for the challenge, while we report the results of 6 teams, which\nsubmitted valid final solutions and code for reproducing the results. Moreover,\nwe calculated and present the performance of state-of-the-art VQA methods on\nthe developed dataset, providing a comprehensive benchmark for future research.\nThe dataset, results, and online leaderboard are publicly available at\nhttps://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.\n","authors":["Maksim Smirnov","Aleksandr Gushchin","Anastasia Antsiferova","Dmitry Vatolin","Radu Timofte","Ziheng Jia","Zicheng Zhang","Wei Sun","Jiaying Qian","Yuqin Cao","Yinan Sun","Yuxin Zhu","Xiongkuo Min","Guangtao Zhai","Kanjar De","Qing Luo","Ao-Xiang Zhang","Peng Zhang","Haibo Lei","Linyan Jiang","Yaqing Li","Wenhui Meng","Zhenzhong Chen","Zhengxue Cheng","Jiahao Xiao","Jun Xu","Chenlong He","Qi Zheng","Ruoxi Zhu","Min Li","Yibo Fan","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2408.11982v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17172v1","updated":"2024-10-22T16:50:34Z","published":"2024-10-22T16:50:34Z","title":"KANICE: Kolmogorov-Arnold Networks with Interactive Convolutional\n  Elements","summary":"  We introduce KANICE (Kolmogorov-Arnold Networks with Interactive\nConvolutional Elements), a novel neural architecture that combines\nConvolutional Neural Networks (CNNs) with Kolmogorov-Arnold Network (KAN)\nprinciples. KANICE integrates Interactive Convolutional Blocks (ICBs) and KAN\nlinear layers into a CNN framework. This leverages KANs' universal\napproximation capabilities and ICBs' adaptive feature learning. KANICE captures\ncomplex, non-linear data relationships while enabling dynamic,\ncontext-dependent feature extraction based on the Kolmogorov-Arnold\nrepresentation theorem. We evaluated KANICE on four datasets: MNIST,\nFashion-MNIST, EMNIST, and SVHN, comparing it against standard CNNs, CNN-KAN\nhybrids, and ICB variants. KANICE consistently outperformed baseline models,\nachieving 99.35% accuracy on MNIST and 90.05% on the SVHN dataset.\n  Furthermore, we introduce KANICE-mini, a compact variant designed for\nefficiency. A comprehensive ablation study demonstrates that KANICE-mini\nachieves comparable performance to KANICE with significantly fewer parameters.\nKANICE-mini reached 90.00% accuracy on SVHN with 2,337,828 parameters, compared\nto KANICE's 25,432,000. This study highlights the potential of KAN-based\narchitectures in balancing performance and computational efficiency in image\nclassification tasks. Our work contributes to research in adaptive neural\nnetworks, integrates mathematical theorems into deep learning architectures,\nand explores the trade-offs between model complexity and performance, advancing\ncomputer vision and pattern recognition. The source code for this paper is\npublicly accessible through our GitHub repository\n(https://github.com/m-ferdaus/kanice).\n","authors":["Md Meftahul Ferdaus","Mahdi Abdelguerfi","Elias Ioup","David Dobson","Kendall N. Niles","Ken Pathak","Steven Sloan"],"pdf_url":"https://arxiv.org/pdf/2410.17172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17149v1","updated":"2024-10-22T16:28:21Z","published":"2024-10-22T16:28:21Z","title":"Are Visual-Language Models Effective in Action Recognition? A\n  Comparative Study","summary":"  Current vision-language foundation models, such as CLIP, have recently shown\nsignificant improvement in performance across various downstream tasks.\nHowever, whether such foundation models significantly improve more complex\nfine-grained action recognition tasks is still an open question. To answer this\nquestion and better find out the future research direction on human behavior\nanalysis in-the-wild, this paper provides a large-scale study and insight on\ncurrent state-of-the-art vision foundation models by comparing their transfer\nability onto zero-shot and frame-wise action recognition tasks. Extensive\nexperiments are conducted on recent fine-grained, human-centric action\nrecognition datasets (e.g., Toyota Smarthome, Penn Action, UAV-Human, TSU,\nCharades) including action classification and segmentation.\n","authors":["Mahmoud Ali","Di Yang","François Brémond"],"pdf_url":"https://arxiv.org/pdf/2410.17149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17146v1","updated":"2024-10-22T16:26:05Z","published":"2024-10-22T16:26:05Z","title":"LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances\n  Model Merging","summary":"  Large pre-trained models exhibit impressive zero-shot performance across\ndiverse tasks, but fine-tuning often leads to catastrophic forgetting, where\nimprovements on a target domain degrade generalization on other tasks. To\naddress this challenge, we introduce LiNeS, Layer-increasing Network Scaling, a\npost-training editing technique designed to preserve pre-trained generalization\nwhile enhancing fine-tuned task performance. LiNeS scales parameter updates\nlinearly based on their layer depth within the network, maintaining shallow\nlayers close to their pre-trained values to preserve general features while\nallowing deeper layers to retain task-specific representations. We further\nextend this approach to multi-task model merging scenarios, where layer-wise\nscaling of merged parameters reduces negative task interference. LiNeS\ndemonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nIt mitigates forgetting, enhances out-of-distribution generalization,\nintegrates seamlessly with existing multi-task model merging baselines\nimproving their performance across benchmarks and model sizes, and can boost\ngeneralization when merging LLM policies aligned with different rewards via\nRLHF. Importantly, our method is simple to implement and complementary to many\nexisting techniques.\n","authors":["Ke Wang","Nikolaos Dimitriadis","Alessandro Favero","Guillermo Ortiz-Jimenez","Francois Fleuret","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2410.17146v1.pdf","comment":"The first two authors contributed equally to this work; Project\n  website: \\url{https://lines-merging.github.io/}"},{"id":"http://arxiv.org/abs/2410.17144v1","updated":"2024-10-22T16:19:55Z","published":"2024-10-22T16:19:55Z","title":"YOLO-TS: Real-Time Traffic Sign Detection with Enhanced Accuracy Using\n  Optimized Receptive Fields and Anchor-Free Fusion","summary":"  Ensuring safety in both autonomous driving and advanced driver-assistance\nsystems (ADAS) depends critically on the efficient deployment of traffic sign\nrecognition technology. While current methods show effectiveness, they often\ncompromise between speed and accuracy. To address this issue, we present a\nnovel real-time and efficient road sign detection network, YOLO-TS. This\nnetwork significantly improves performance by optimizing the receptive fields\nof multi-scale feature maps to align more closely with the size distribution of\ntraffic signs in various datasets. Moreover, our innovative feature-fusion\nstrategy, leveraging the flexibility of Anchor-Free methods, allows for\nmulti-scale object detection on a high-resolution feature map abundant in\ncontextual information, achieving remarkable enhancements in both accuracy and\nspeed. To mitigate the adverse effects of the grid pattern caused by dilated\nconvolutions on the detection of smaller objects, we have devised a unique\nmodule that not only mitigates this grid effect but also widens the receptive\nfield to encompass an extensive range of spatial contextual information, thus\nboosting the efficiency of information usage. Evaluation on challenging public\ndatasets, TT100K and CCTSDB2021, demonstrates that YOLO-TS surpasses existing\nstate-of-the-art methods in terms of both accuracy and speed. The code for our\nmethod will be available.\n","authors":["Junzhou Chen","Heqiang Huang","Ronghui Zhang","Nengchao Lyu","Yanyong Guo","Hong-Ning Dai","Hong Yan"],"pdf_url":"https://arxiv.org/pdf/2410.17144v1.pdf","comment":"13 pages, 9 figures and 7 tables"},{"id":"http://arxiv.org/abs/2409.12961v2","updated":"2024-10-22T16:17:13Z","published":"2024-09-19T17:59:51Z","title":"Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary\n  Resolution","summary":"  Visual data comes in various forms, ranging from small icons of just a few\npixels to long videos spanning hours. Existing multi-modal LLMs usually\nstandardize these diverse visual inputs to a fixed resolution for visual\nencoders and yield similar numbers of tokens for LLMs. This approach is\nnon-optimal for multimodal understanding and inefficient for processing inputs\nwith long and short visual contents. To solve the problem, we propose Oryx, a\nunified multimodal architecture for the spatial-temporal understanding of\nimages, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to\nseamlessly and efficiently process visual inputs with arbitrary spatial sizes\nand temporal lengths through two core innovations: 1) a pre-trained OryxViT\nmodel that can encode images at any resolution into LLM-friendly visual\nrepresentations; 2) a dynamic compressor module that supports 1x to 16x\ncompression on visual tokens by request. These design features enable Oryx to\naccommodate extremely long visual contexts, such as videos, with lower\nresolution and high compression while maintaining high recognition precision\nfor tasks like document understanding with native resolution and no\ncompression. Beyond the architectural improvements, enhanced data curation and\nspecialized training on long-context retrieval and spatial-aware data help Oryx\nachieve strong capabilities in image, video, and 3D multimodal understanding\nsimultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.\n","authors":["Zuyan Liu","Yuhao Dong","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2409.12961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17136v1","updated":"2024-10-22T16:08:09Z","published":"2024-10-22T16:08:09Z","title":"AlphaChimp: Tracking and Behavior Recognition of Chimpanzees","summary":"  Understanding non-human primate behavior is crucial for improving animal\nwelfare, modeling social behavior, and gaining insights into both distinctly\nhuman and shared behaviors. Despite recent advances in computer vision,\nautomated analysis of primate behavior remains challenging due to the\ncomplexity of their social interactions and the lack of specialized algorithms.\nExisting methods often struggle with the nuanced behaviors and frequent\nocclusions characteristic of primate social dynamics. This study aims to\ndevelop an effective method for automated detection, tracking, and recognition\nof chimpanzee behaviors in video footage. Here we show that our proposed\nmethod, AlphaChimp, an end-to-end approach that simultaneously detects\nchimpanzee positions and estimates behavior categories from videos,\nsignificantly outperforms existing methods in behavior recognition. AlphaChimp\nachieves approximately 10% higher tracking accuracy and a 20% improvement in\nbehavior recognition compared to state-of-the-art methods, particularly\nexcelling in the recognition of social behaviors. This superior performance\nstems from AlphaChimp's innovative architecture, which integrates temporal\nfeature fusion with a Transformer-based self-attention mechanism, enabling more\neffective capture and interpretation of complex social interactions among\nchimpanzees. Our approach bridges the gap between computer vision and\nprimatology, enhancing technical capabilities and deepening our understanding\nof primate communication and sociality. We release our code and models and hope\nthis will facilitate future research in animal social dynamics. This work\ncontributes to ethology, cognitive science, and artificial intelligence,\noffering new perspectives on social intelligence.\n","authors":["Xiaoxuan Ma","Yutang Lin","Yuan Xu","Stephan P. Kaufhold","Jack Terwilliger","Andres Meza","Yixin Zhu","Federico Rossano","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17136v1.pdf","comment":"An eXpressive extension of ChimpACT [arXiv:2310.16447], proposes\n  AlphaChimp for tracking and behavior recognition of chimpanzees. arXiv admin\n  note: substantial text overlap with arXiv:2310.16447"},{"id":"http://arxiv.org/abs/2410.14669v2","updated":"2024-10-22T16:07:22Z","published":"2024-10-18T17:58:21Z","title":"NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples","summary":"  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n","authors":["Baiqi Li","Zhiqiu Lin","Wenxuan Peng","Jean de Dieu Nyandwi","Daniel Jiang","Zixian Ma","Simran Khanuja","Ranjay Krishna","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.14669v2.pdf","comment":"Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench ; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/"},{"id":"http://arxiv.org/abs/2410.17101v1","updated":"2024-10-22T15:28:18Z","published":"2024-10-22T15:28:18Z","title":"CLAP: Concave Linear APproximation for Quadratic Graph Matching","summary":"  Solving point-wise feature correspondence in visual data is a fundamental\nproblem in computer vision. A powerful model that addresses this challenge is\nto formulate it as graph matching, which entails solving a Quadratic Assignment\nProblem (QAP) with node-wise and edge-wise constraints. However, solving such a\nQAP can be both expensive and difficult due to numerous local extreme points.\nIn this work, we introduce a novel linear model and solver designed to\naccelerate the computation of graph matching. Specifically, we employ a\npositive semi-definite matrix approximation to establish the structural\nattribute constraint.We then transform the original QAP into a linear model\nthat is concave for maximization. This model can subsequently be solved using\nthe Sinkhorn optimal transport algorithm, known for its enhanced efficiency and\nnumerical stability compared to existing approaches. Experimental results on\nthe widely used benchmark PascalVOC showcase that our algorithm achieves\nstate-of-the-art performance with significantly improved efficiency. Source\ncode: https://github.com/xmlyqing00/clap\n","authors":["Yongqing Liang","Huijun Han","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2410.17101v1.pdf","comment":"Accepted as an oral paper in International Symposium on Visual\n  Computing (ISCV2024)"},{"id":"http://arxiv.org/abs/2410.17098v1","updated":"2024-10-22T15:22:53Z","published":"2024-10-22T15:22:53Z","title":"Masked Differential Privacy","summary":"  Privacy-preserving computer vision is an important emerging problem in\nmachine learning and artificial intelligence. The prevalent methods tackling\nthis problem use differential privacy or anonymization and obfuscation\ntechniques to protect the privacy of individuals. In both cases, the utility of\nthe trained model is sacrificed heavily in this process. In this work, we\npropose an effective approach called masked differential privacy (MaskDP),\nwhich allows for controlling sensitive regions where differential privacy is\napplied, in contrast to applying DP on the entire input. Our method operates\nselectively on the data and allows for defining non-sensitive spatio-temporal\nregions without DP application or combining differential privacy with other\nprivacy techniques within data samples. Experiments on four challenging action\nrecognition datasets demonstrate that our proposed techniques result in better\nutility-privacy trade-offs compared to standard differentially private training\nin the especially demanding $\\epsilon<1$ regime.\n","authors":["David Schneider","Sina Sajadmanesh","Vikash Sehwag","Saquib Sarfraz","Rainer Stiefelhagen","Lingjuan Lyu","Vivek Sharma"],"pdf_url":"https://arxiv.org/pdf/2410.17098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17082v1","updated":"2024-10-22T15:07:07Z","published":"2024-10-22T15:07:07Z","title":"A Survey on Deep Learning-based Gaze Direction Regression: Searching for\n  the State-of-the-art","summary":"  In this paper, we present a survey of deep learning-based methods for the\nregression of gaze direction vector from head and eye images. We describe in\ndetail numerous published methods with a focus on the input data, architecture\nof the model, and loss function used to supervise the model. Additionally, we\npresent a list of datasets that can be used to train and evaluate gaze\ndirection regression methods. Furthermore, we noticed that the results reported\nin the literature are often not comparable one to another due to differences in\nthe validation or even test subsets used. To address this problem, we\nre-evaluated several methods on the commonly used in-the-wild Gaze360 dataset\nusing the same validation setup. The experimental results show that the latest\nmethods, although claiming state-of-the-art results, significantly underperform\ncompared with some older methods. Finally, we show that the temporal models\noutperform the static models under static test conditions.\n","authors":["Franko Šikić","Donik Vršnak","Sven Lončarić"],"pdf_url":"https://arxiv.org/pdf/2410.17082v1.pdf","comment":"Accepted on SPRA 2024 (Istanbul, Turkey)"},{"id":"http://arxiv.org/abs/2407.05180v2","updated":"2024-10-22T14:54:42Z","published":"2024-04-22T10:33:06Z","title":"ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in\n  Robotic Surgical Skill Assessment","summary":"  In surgical skill assessment, Objective Structured Assessments of Technical\nSkills (OSATS scores) and the Global Rating Scale (GRS) are established tools\nfor evaluating the performance of surgeons during training. These metrics,\ncoupled with feedback on their performance, enable surgeons to improve and\nachieve standards of practice. Recent studies on the open-source dataset\nJIGSAW, which contains both GRS and OSATS labels, have focused on regressing\nGRS scores from kinematic signals, video data, or a combination of both. In\nthis paper, we argue that regressing the GRS score, a unitless value, by itself\nis too restrictive, and variations throughout the surgical trial do not hold\nsignificant clinical meaning. To address this gap, we developed a recurrent\ntransformer model that outputs the surgeon's performance throughout their\ntraining session by relating the model's hidden states to five OSATS scores\nderived from kinematic signals. These scores are averaged and aggregated to\nproduce a GRS prediction, enabling assessment of the model's performance\nagainst the state-of-the-art (SOTA). We report Spearman's Correlation\nCoefficient (SCC), demonstrating that our model outperforms SOTA models for all\ntasks, except for Suturing under the leave-one-subject-out (LOSO) scheme (SCC\n0.68-0.89), while achieving comparable performance for suturing and across\ntasks under the leave-one-user-out (LOUO) scheme (SCC 0.45-0.68) and beating\nSOTA for Needle Passing (0.69). We argue that relating final OSATS scores to\nshort instances throughout a surgeon's procedure is more clinically meaningful\nthan a single GRS score. This approach also allows us to translate quantitative\npredictions into qualitative feedback, which is crucial for any automated\nsurgical skill assessment pipeline. A senior surgeon validated our model's\nbehaviour and agreed with the semi-supervised predictions 77 \\% (p = 0.006) of\nthe time.\n","authors":["Julien Quarez","Matthew Elliot","Oscar Maccormac","Marc Modat","Sebastien Ourselin","Jonathan Shapey","Alejandro Granados"],"pdf_url":"https://arxiv.org/pdf/2407.05180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17066v1","updated":"2024-10-22T14:46:20Z","published":"2024-10-22T14:46:20Z","title":"Neuronal Competition Groups with Supervised STDP for Spike-Based\n  Classification","summary":"  Spike Timing-Dependent Plasticity (STDP) is a promising substitute to\nbackpropagation for local training of Spiking Neural Networks (SNNs) on\nneuromorphic hardware. STDP allows SNNs to address classification tasks by\ncombining unsupervised STDP for feature extraction and supervised STDP for\nclassification. Unsupervised STDP is usually employed with Winner-Takes-All\n(WTA) competition to learn distinct patterns. However, WTA for supervised STDP\nclassification faces unbalanced competition challenges. In this paper, we\npropose a method to effectively implement WTA competition in a spiking\nclassification layer employing first-spike coding and supervised STDP training.\nWe introduce the Neuronal Competition Group (NCG), an architecture that\nimproves classification capabilities by promoting the learning of various\npatterns per class. An NCG is a group of neurons mapped to a specific class,\nimplementing intra-class WTA and a novel competition regulation mechanism based\non two-compartment thresholds. We incorporate our proposed architecture into\nspiking classification layers trained with state-of-the-art supervised STDP\nrules. On top of two different unsupervised feature extractors, we obtain\nsignificant accuracy improvements on image recognition datasets such as\nCIFAR-10 and CIFAR-100. We show that our competition regulation mechanism is\ncrucial for ensuring balanced competition and improved class separation.\n","authors":["Gaspard Goupy","Pierre Tirilly","Ioan Marius Bilasco"],"pdf_url":"https://arxiv.org/pdf/2410.17066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17064v1","updated":"2024-10-22T14:44:47Z","published":"2024-10-22T14:44:47Z","title":"Multi Kernel Estimation based Object Segmentation","summary":"  This paper presents a novel approach for multi-kernel estimation by enhancing\nthe KernelGAN algorithm, which traditionally estimates a single kernel for the\nentire image. We introduce Multi-KernelGAN, which extends KernelGAN's\ncapabilities by estimating two distinct kernels based on object segmentation\nmasks. Our approach is validated through three distinct methods: texture-based\npatch Fast Fourier Transform (FFT) calculation, detail-based segmentation, and\ndeep learning-based object segmentation using YOLOv8 and the Segment Anything\nModel (SAM). Among these methods, the combination of YOLO and SAM yields the\nbest results for kernel estimation. Experimental results demonstrate that our\nmulti-kernel estimation technique outperforms conventional single-kernel\nmethods in super-resolution tasks.\n","authors":["Haim Goldfisher","Asaf Yekutiel"],"pdf_url":"https://arxiv.org/pdf/2410.17064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13675v4","updated":"2024-10-22T14:28:56Z","published":"2024-05-22T14:16:30Z","title":"Context and Geometry Aware Voxel Transformer for Semantic Scene\n  Completion","summary":"  Vision-based Semantic Scene Completion (SSC) has gained much attention due to\nits widespread applications in various 3D perception tasks. Existing\nsparse-to-dense approaches typically employ shared context-independent queries\nacross various input images, which fails to capture distinctions among them as\nthe focal regions of different inputs vary and may result in undirected feature\naggregation of cross-attention. Additionally, the absence of depth information\nmay lead to points projected onto the image plane sharing the same 2D position\nor similar sampling points in the feature map, resulting in depth ambiguity. In\nthis paper, we present a novel context and geometry aware voxel transformer. It\nutilizes a context aware query generator to initialize context-dependent\nqueries tailored to individual input images, effectively capturing their unique\ncharacteristics and aggregating information within the region of interest.\nFurthermore, it extend deformable cross-attention from 2D to 3D pixel space,\nenabling the differentiation of points with similar image coordinates based on\ntheir depth coordinates. Building upon this module, we introduce a neural\nnetwork named CGFormer to achieve semantic scene completion. Simultaneously,\nCGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost\nthe semantic and geometric representation abilities of the transformed 3D\nvolume from both local and global perspectives. Experimental results\ndemonstrate that CGFormer achieves state-of-the-art performance on the\nSemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and\n20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer\neven outperforms approaches employing temporal images as inputs or much larger\nimage backbone networks.\n","authors":["Zhu Yu","Runmin Zhang","Jiacheng Ying","Junchen Yu","Xiaohai Hu","Lun Luo","Si-Yuan Cao","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2405.13675v4.pdf","comment":"NIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.14980v2","updated":"2024-10-22T14:27:32Z","published":"2024-10-19T05:10:07Z","title":"DCDepth: Progressive Monocular Depth Estimation in Discrete Cosine\n  Domain","summary":"  In this paper, we introduce DCDepth, a novel framework for the long-standing\nmonocular depth estimation task. Moving beyond conventional pixel-wise depth\nestimation in the spatial domain, our approach estimates the frequency\ncoefficients of depth patches after transforming them into the discrete cosine\ndomain. This unique formulation allows for the modeling of local depth\ncorrelations within each patch. Crucially, the frequency transformation\nsegregates the depth information into various frequency components, with\nlow-frequency components encapsulating the core scene structure and\nhigh-frequency components detailing the finer aspects. This decomposition forms\nthe basis of our progressive strategy, which begins with the prediction of\nlow-frequency components to establish a global scene context, followed by\nsuccessive refinement of local details through the prediction of\nhigher-frequency components. We conduct comprehensive experiments on\nNYU-Depth-V2, TOFDC, and KITTI datasets, and demonstrate the state-of-the-art\nperformance of DCDepth. Code is available at https://github.com/w2kun/DCDepth.\n","authors":["Kun Wang","Zhiqiang Yan","Junkai Fan","Wanlu Zhu","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14980v2.pdf","comment":"Accepted by NeurIPS-2024"},{"id":"http://arxiv.org/abs/2405.14677v2","updated":"2024-10-22T14:21:19Z","published":"2024-05-23T15:12:15Z","title":"RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance","summary":"  Customizing diffusion models to generate identity-preserving images from\nuser-provided reference images is an intriguing new problem. The prevalent\napproaches typically require training on extensive domain-specific images to\nachieve identity preservation, which lacks flexibility across different use\ncases. To address this issue, we exploit classifier guidance, a training-free\ntechnique that steers diffusion models using an existing classifier, for\npersonalized image generation. Our study shows that based on a recent rectified\nflow framework, the major limitation of vanilla classifier guidance in\nrequiring a special classifier can be resolved with a simple fixed-point\nsolution, allowing flexible personalization with off-the-shelf image\ndiscriminators. Moreover, its solving procedure proves to be stable when\nanchored to a reference flow trajectory, with a convergence guarantee. The\nderived method is implemented on rectified flow with different off-the-shelf\nimage discriminators, delivering advantageous personalization results for human\nfaces, live subjects, and certain objects. Code is available at\nhttps://github.com/feifeiobama/RectifID.\n","authors":["Zhicheng Sun","Zhenhao Yang","Yang Jin","Haozhe Chi","Kun Xu","Kun Xu","Liwei Chen","Hao Jiang","Yang Song","Kun Gai","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2405.14677v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.16320v2","updated":"2024-10-22T14:09:10Z","published":"2024-09-21T03:45:05Z","title":"Developing a Thailand solar irradiance map using Himawari-8 satellite\n  imageries and deep learning models","summary":"  This paper presents an online platform that shows Thailand's solar irradiance\nmap every 30 minutes. It is available at https://www.cusolarforecast.com. The\nmethodology for estimating global horizontal irradiance (GHI) across Thailand\nrelies on cloud index extracted from Himawari-8 satellite imagery, Ineichen\nclear-sky model with locally-tuned Linke turbidity, and machine learning\nmodels. The methods take clear-sky irradiance, cloud index, re-analyzed GHI and\ntemperature data from the MERRA-2 database, and date-time as inputs for GHI\nestimation models, including LightGBM, LSTM, Informer, and Transformer. These\nare benchmarked with the estimate from a commercial service X by evaluating\n15-minute ground GHI data from 53 ground stations over 1.5 years from\n2022-2023. The results show that the four models have competitive performances\nand outperform the service X. The best model is LightGBM, with an MAE of 78.58\nW/sqm and RMSE of 118.97 W/sqm. Obtaining re-analyzed MERRA-2 data for Thailand\nis not economically feasible for deployment. When removing these features, the\nInformer model has a winning performance of 78.67 W/sqm in MAE. The obtained\nperformance aligns with existing literature by taking the climate zone and time\ngranularity of data into consideration. As the map shows an estimate of GHI\nover 93,000 grids with a frequent update, the paper also describes a\ncomputational framework for displaying the entire map. It tests the runtime\nperformance of deep learning models in the GHI estimation process.\n","authors":["Suwichaya Suwanwimolkul","Natanon Tongamrak","Nuttamon Thungka","Naebboon Hoonchareon","Jitkomut Songsiri"],"pdf_url":"https://arxiv.org/pdf/2409.16320v2.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.07389v2","updated":"2024-10-22T14:07:54Z","published":"2024-03-12T07:57:33Z","title":"Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from\n  Duplex to Monoplex IHC Images","summary":"  Generative models enable the translation from a source image domain where\nreadily trained models are available to a target domain unseen during training.\nWhile Cycle Generative Adversarial Networks (GANs) are well established, the\nassociated cycle consistency constrain relies on that an invertible mapping\nexists between the two domains. This is, however, not the case for the\ntranslation between images stained with chromogenic monoplex and duplex\nimmunohistochemistry (IHC) assays. Focusing on the translation from the latter\nto the first, we propose - through the introduction of a novel training design,\nan alternative constrain leveraging a set of immunofluorescence (IF) images as\nan auxiliary unpaired image domain. Quantitative and qualitative results on a\ndownstream segmentation task show the benefit of the proposed method in\ncomparison to baseline approaches.\n","authors":["Nicolas Brieu","Nicolas Triltsch","Philipp Wortmann","Dominik Winter","Shashank Saran","Marlon Rebelatto","Günter Schmidt"],"pdf_url":"https://arxiv.org/pdf/2403.07389v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2410.17020v1","updated":"2024-10-22T13:44:10Z","published":"2024-10-22T13:44:10Z","title":"LFME: A Simple Framework for Learning from Multiple Experts in Domain\n  Generalization","summary":"  Domain generalization (DG) methods aim to maintain good performance in an\nunseen target domain by using training data from multiple source domains. While\nsuccess on certain occasions are observed, enhancing the baseline across most\nscenarios remains challenging. This work introduces a simple yet effective\nframework, dubbed learning from multiple experts (LFME), that aims to make the\ntarget model an expert in all source domains to improve DG. Specifically,\nbesides learning the target model used in inference, LFME will also train\nmultiple experts specialized in different domains, whose output probabilities\nprovide professional guidance by simply regularizing the logit of the target\nmodel. Delving deep into the framework, we reveal that the introduced logit\nregularization term implicitly provides effects of enabling the target model to\nharness more information, and mining hard samples from the experts during\ntraining. Extensive experiments on benchmarks from different DG tasks\ndemonstrate that LFME is consistently beneficial to the baseline and can\nachieve comparable performance to existing arts. Code is available\nat~\\url{https://github.com/liangchen527/LFME}.\n","authors":["Liang Chen","Yong Zhang","Yibing Song","Zhiqiang Shen","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17020v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17017v1","updated":"2024-10-22T13:37:55Z","published":"2024-10-22T13:37:55Z","title":"SPVSoAP3D: A Second-order Average Pooling Approach to enhance 3D Place\n  Recognition in Horticultural Environments","summary":"  3D LiDAR-based place recognition has been extensively researched in urban\nenvironments, yet it remains underexplored in agricultural settings. Unlike\nurban contexts, horticultural environments, characterized by their permeability\nto laser beams, result in sparse and overlapping LiDAR scans with suboptimal\ngeometries. This phenomenon leads to intra- and inter-row descriptor ambiguity.\nIn this work, we address this challenge by introducing SPVSoAP3D, a novel\nmodeling approach that combines a voxel-based feature extraction network with\nan aggregation technique based on a second-order average pooling operator,\ncomplemented by a descriptor enhancement stage. Furthermore, we augment the\nexisting HORTO-3DLM dataset by introducing two new sequences derived from\nhorticultural environments. We evaluate the performance of SPVSoAP3D against\nstate-of-the-art (SOTA) models, including OverlapTransformer, PointNetVLAD, and\nLOGG3D-Net, utilizing a cross-validation protocol on both the newly introduced\nsequences and the existing HORTO-3DLM dataset. The findings indicate that the\naverage operator is more suitable for horticultural environments compared to\nthe max operator and other first-order pooling techniques. Additionally, the\nresults highlight the improvements brought by the descriptor enhancement stage.\n","authors":["T. Barros","C. Premebida","S. Aravecchia","C. Pradalier","U. J. Nunes"],"pdf_url":"https://arxiv.org/pdf/2410.17017v1.pdf","comment":"This work has been accepted to IROS 2024"},{"id":"http://arxiv.org/abs/2205.07708v3","updated":"2024-10-22T13:34:45Z","published":"2022-05-16T14:21:30Z","title":"Exploring Diversity-based Active Learning for 3D Object Detection in\n  Autonomous Driving","summary":"  3D object detection has recently received much attention due to its great\npotential in autonomous vehicle (AV). The success of deep learning based object\ndetectors relies on the availability of large-scale annotated datasets, which\nis time-consuming and expensive to compile, especially for 3D bounding box\nannotation. In this work, we investigate diversity-based active learning (AL)\nas a potential solution to alleviate the annotation burden. Given limited\nannotation budget, only the most informative frames and objects are\nautomatically selected for human to annotate. Technically, we take the\nadvantage of the multimodal information provided in an AV dataset, and propose\na novel acquisition function that enforces spatial and temporal diversity in\nthe selected samples. We benchmark the proposed method against other AL\nstrategies under realistic annotation cost measurement, where the realistic\ncosts for annotating a frame and a 3D bounding box are both taken into\nconsideration. We demonstrate the effectiveness of the proposed method on the\nnuScenes dataset and show that it outperforms existing AL strategies\nsignificantly. Code is available at\nhttps://github.com/Linkon87/Exploring-Diversity-based-Active-Learning-for-3D-Object-Detection-in-Autonomous-Driving\n","authors":["Jinpeng Lin","Zhihao Liang","Shengheng Deng","Lile Cai","Tao Jiang","Tianrui Li","Kui Jia","Xun Xu"],"pdf_url":"https://arxiv.org/pdf/2205.07708v3.pdf","comment":"IEEE Transactions on Intelligent Transportation Systems. Code is\n  available at\n  https://github.com/Linkon87/Exploring-Diversity-based-Active-Learning-for-3D-Object-Detection-in-Autonomous-Driving"},{"id":"http://arxiv.org/abs/2406.12142v2","updated":"2024-10-22T13:32:34Z","published":"2024-06-17T23:08:46Z","title":"Slicing Through Bias: Explaining Performance Gaps in Medical Image\n  Analysis using Slice Discovery Methods","summary":"  Machine learning models have achieved high overall accuracy in medical image\nanalysis. However, performance disparities on specific patient groups pose\nchallenges to their clinical utility, safety, and fairness. This can affect\nknown patient groups - such as those based on sex, age, or disease subtype - as\nwell as previously unknown and unlabeled groups. Furthermore, the root cause of\nsuch observed performance disparities is often challenging to uncover,\nhindering mitigation efforts. In this paper, to address these issues, we\nleverage Slice Discovery Methods (SDMs) to identify interpretable\nunderperforming subsets of data and formulate hypotheses regarding the cause of\nobserved performance disparities. We introduce a novel SDM and apply it in a\ncase study on the classification of pneumothorax and atelectasis from chest\nx-rays. Our study demonstrates the effectiveness of SDMs in hypothesis\nformulation and yields an explanation of previously observed but unexplained\nperformance disparities between male and female patients in widely used chest\nX-ray datasets and models. Our findings indicate shortcut learning in both\nclassification tasks, through the presence of chest drains and ECG wires,\nrespectively. Sex-based differences in the prevalence of these shortcut\nfeatures appear to cause the observed classification performance gap,\nrepresenting a previously underappreciated interaction between shortcut\nlearning and model fairness analyses.\n","authors":["Vincent Olesen","Nina Weng","Aasa Feragen","Eike Petersen"],"pdf_url":"https://arxiv.org/pdf/2406.12142v2.pdf","comment":"MICCAI 2024 Workshop on Fairness of AI in Medical Imaging"},{"id":"http://arxiv.org/abs/2410.17001v1","updated":"2024-10-22T13:23:05Z","published":"2024-10-22T13:23:05Z","title":"Joint Point Cloud Upsampling and Cleaning with Octree-based CNNs","summary":"  Recovering dense and uniformly distributed point clouds from sparse or noisy\ndata remains a significant challenge. Recently, great progress has been made on\nthese tasks, but usually at the cost of increasingly intricate modules or\ncomplicated network architectures, leading to long inference time and huge\nresource consumption. Instead, we embrace simplicity and present a simple yet\nefficient method for jointly upsampling and cleaning point clouds. Our method\nleverages an off-the-shelf octree-based 3D U-Net (OUNet) with minor\nmodifications, enabling the upsampling and cleaning tasks within a single\nnetwork. Our network directly processes each input point cloud as a whole\ninstead of processing each point cloud patch as in previous works, which\nsignificantly eases the implementation and brings at least 47 times faster\ninference. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performances under huge efficiency advantages on a series of\nbenchmarks. We expect our method to serve simple baselines and inspire\nresearchers to rethink the method design on point cloud upsampling and\ncleaning.\n","authors":["Jihe Li","Bo Pang","Peng-Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17001v1.pdf","comment":"Accepted by Computational Visual Media"},{"id":"http://arxiv.org/abs/2410.16999v1","updated":"2024-10-22T13:21:36Z","published":"2024-10-22T13:21:36Z","title":"AGSENet: A Robust Road Ponding Detection Method for Proactive Traffic\n  Safety","summary":"  Road ponding, a prevalent traffic hazard, poses a serious threat to road\nsafety by causing vehicles to lose control and leading to accidents ranging\nfrom minor fender benders to severe collisions. Existing technologies struggle\nto accurately identify road ponding due to complex road textures and variable\nponding coloration influenced by reflection characteristics. To address this\nchallenge, we propose a novel approach called Self-Attention-based Global\nSaliency-Enhanced Network (AGSENet) for proactive road ponding detection and\ntraffic safety improvement. AGSENet incorporates saliency detection techniques\nthrough the Channel Saliency Information Focus (CSIF) and Spatial Saliency\nInformation Enhancement (SSIE) modules. The CSIF module, integrated into the\nencoder, employs self-attention to highlight similar features by fusing spatial\nand channel information. The SSIE module, embedded in the decoder, refines edge\nfeatures and reduces noise by leveraging correlations across different feature\nlevels. To ensure accurate and reliable evaluation, we corrected significant\nmislabeling and missing annotations in the Puddle-1000 dataset. Additionally,\nwe constructed the Foggy-Puddle and Night-Puddle datasets for road ponding\ndetection in low-light and foggy conditions, respectively. Experimental results\ndemonstrate that AGSENet outperforms existing methods, achieving IoU\nimprovements of 2.03\\%, 0.62\\%, and 1.06\\% on the Puddle-1000, Foggy-Puddle,\nand Night-Puddle datasets, respectively, setting a new state-of-the-art in this\nfield. Finally, we verified the algorithm's reliability on edge computing\ndevices. This work provides a valuable reference for proactive warning research\nin road traffic safety.\n","authors":["Ronghui Zhang","Shangyu Yang","Dakang Lyu","Zihan Wang","Junzhou Chen","Yilong Ren","Bolin Gao","Zhihan Lv"],"pdf_url":"https://arxiv.org/pdf/2410.16999v1.pdf","comment":"21 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.16995v1","updated":"2024-10-22T13:17:20Z","published":"2024-10-22T13:17:20Z","title":"E-3DGS: Gaussian Splatting with Exposure and Motion Events","summary":"  Estimating Neural Radiance Fields (NeRFs) from images captured under optimal\nconditions has been extensively explored in the vision community. However,\nrobotic applications often face challenges such as motion blur, insufficient\nillumination, and high computational overhead, which adversely affect\ndownstream tasks like navigation, inspection, and scene visualization. To\naddress these challenges, we propose E-3DGS, a novel event-based approach that\npartitions events into motion (from camera or object movement) and exposure\n(from camera exposure), using the former to handle fast-motion scenes and using\nthe latter to reconstruct grayscale images for high-quality training and\noptimization of event-based 3D Gaussian Splatting (3DGS). We introduce a novel\nintegration of 3DGS with exposure events for high-quality reconstruction of\nexplicit scene representations. Our versatile framework can operate on motion\nevents alone for 3D reconstruction, enhance quality using exposure events, or\nadopt a hybrid mode that balances quality and effectiveness by optimizing with\ninitial exposure events followed by high-speed motion events. We also introduce\nEME-3D, a real-world 3D dataset with exposure events, motion events, camera\ncalibration parameters, and sparse point clouds. Our method is faster and\ndelivers better reconstruction quality than event-based NeRF while being more\ncost-effective than NeRF methods that combine event and RGB data by using a\nsingle event sensor. By combining motion and exposure events, E-3DGS sets a new\nbenchmark for event-based 3D reconstruction with robust performance in\nchallenging conditions and lower hardware demands. The source code and dataset\nwill be available at https://github.com/MasterHow/E-3DGS.\n","authors":["Xiaoting Yin","Hao Shi","Yuhan Bao","Zhenshan Bing","Yiyi Liao","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16995v1.pdf","comment":"The source code and dataset will be available at\n  https://github.com/MasterHow/E-3DGS"},{"id":"http://arxiv.org/abs/2404.06050v2","updated":"2024-10-22T13:15:20Z","published":"2024-04-09T06:27:35Z","title":"Incremental Joint Learning of Depth, Pose and Implicit Scene\n  Representation on Monocular Camera in Large-scale Scenes","summary":"  Dense scene reconstruction for photo-realistic view synthesis has various\napplications, such as VR/AR, autonomous vehicles. However, most existing\nmethods have difficulties in large-scale scenes due to three core challenges:\n\\textit{(a) inaccurate depth input.} Accurate depth input is impossible to get\nin real-world large-scale scenes. \\textit{(b) inaccurate pose estimation.} Most\nexisting approaches rely on accurate pre-estimated camera poses. \\textit{(c)\ninsufficient scene representation capability.} A single global radiance field\nlacks the capacity to effectively scale to large-scale scenes. To this end, we\npropose an incremental joint learning framework, which can achieve accurate\ndepth, pose estimation, and large-scale scene reconstruction. A vision\ntransformer-based network is adopted as the backbone to enhance performance in\nscale information estimation. For pose estimation, a feature-metric bundle\nadjustment (FBA) method is designed for accurate and robust camera tracking in\nlarge-scale scenes. In terms of implicit scene representation, we propose an\nincremental scene representation method to construct the entire large-scale\nscene as multiple local radiance fields to enhance the scalability of 3D scene\nrepresentation. Extended experiments have been conducted to demonstrate the\neffectiveness and accuracy of our method in depth estimation, pose estimation,\nand large-scale scene reconstruction.\n","authors":["Tianchen Deng","Nailin Wang","Chongdi Wang","Shenghai Yuan","Jingchuan Wang","Danwei Wang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2404.06050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09553v3","updated":"2024-10-22T13:04:29Z","published":"2024-06-28T08:21:49Z","title":"DPEC: Dual-Path Error Compensation Method for Enhanced Low-Light Image\n  Clarity","summary":"  For the task of low-light image enhancement, deep learning-based algorithms\nhave demonstrated superiority and effectiveness compared to traditional\nmethods. Existing deep learning algorithms are proposed mainly based on the\nRetinex theory but overlook the noise and color distortion present in the\ninput, which frequently results in significant noise amplification and local\ncolor distortion in the final results. To address this, we propose a Dual-Path\nError Compensation method (DPEC), which aims to improve image quality in\nlow-light conditions. DPEC performs precise pixel-level error estimation, which\naccurately captures subtle pixels differences, and independent denoising, which\neffectively removes unnecessary noise. This method restores image brightness\nwhile preserving local texture details and avoiding noise amplification.\nFurthermore, to compensate for the traditional CNN's limited ability to capture\nlong-range semantic information and considering both computational speed and\nresource efficiency, we integrated the VMamba architecture into the backbone of\nDPEC. In addition, we introduced the HIS-Retinex loss to constrain the training\nof DPEC, ensuring that the overall brightness distribution of the images more\nclosely aligns with real-world conditions. Comprehensive quantitative and\nqualitative experimental results demonstrate that our algorithm significantly\noutperforms state-of-the-art methods across six benchmark tests.\n","authors":["Shuang Wang","Qianwen Lu","Yihe Nie","Qingchuan Tao","Yanmei Yu"],"pdf_url":"https://arxiv.org/pdf/2407.09553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16978v1","updated":"2024-10-22T12:56:58Z","published":"2024-10-22T12:56:58Z","title":"Multi-Layer Gaussian Splatting for Immersive Anatomy Visualization","summary":"  In medical image visualization, path tracing of volumetric medical data like\nCT scans produces lifelike three-dimensional visualizations. Immersive VR\ndisplays can further enhance the understanding of complex anatomies. Going\nbeyond the diagnostic quality of traditional 2D slices, they enable interactive\n3D evaluation of anatomies, supporting medical education and planning.\nRendering high-quality visualizations in real-time, however, is computationally\nintensive and impractical for compute-constrained devices like mobile headsets.\n  We propose a novel approach utilizing GS to create an efficient but static\nintermediate representation of CT scans. We introduce a layered GS\nrepresentation, incrementally including different anatomical structures while\nminimizing overlap and extending the GS training to remove inactive Gaussians.\nWe further compress the created model with clustering across layers.\n  Our approach achieves interactive frame rates while preserving anatomical\nstructures, with quality adjustable to the target hardware. Compared to\nstandard GS, our representation retains some of the explorative qualities\ninitially enabled by immersive path tracing. Selective activation and clipping\nof layers are possible at rendering time, adding a degree of interactivity to\notherwise static GS models. This could enable scenarios where high\ncomputational demands would otherwise prohibit using path-traced medical\nvolumes.\n","authors":["Constantin Kleinbeck","Hannah Schieber","Klaus Engel","Ralf Gutjahr","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2410.16978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16592v2","updated":"2024-10-22T12:46:00Z","published":"2024-06-24T12:33:21Z","title":"Toward Fairer Face Recognition Datasets","summary":"  Face recognition and verification are two computer vision tasks whose\nperformance has progressed with the introduction of deep representations.\nHowever, ethical, legal, and technical challenges due to the sensitive\ncharacter of face data and biases in real training datasets hinder their\ndevelopment. Generative AI addresses privacy by creating fictitious identities,\nbut fairness problems persist. We promote fairness by introducing a demographic\nattributes balancing mechanism in generated training datasets. We experiment\nwith an existing real dataset, three generated training datasets, and the\nbalanced versions of a diffusion-based dataset. We propose a comprehensive\nevaluation that considers accuracy and fairness equally and includes a rigorous\nregression-based statistical analysis of attributes. The analysis shows that\nbalancing reduces demographic unfairness. Also, a performance gap persists\ndespite generation becoming more accurate with time. The proposed balancing\nmethod and comprehensive verification evaluation promote fairer and transparent\nface recognition and verification.\n","authors":["Alexandre Fournier-Mongieux","Michael Soumm","Adrian Popescu","Bertrand Luvison","Hervé Le Borgne"],"pdf_url":"https://arxiv.org/pdf/2406.16592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13717v5","updated":"2024-10-22T12:42:06Z","published":"2023-11-22T22:21:26Z","title":"Feature Extraction for Generative Medical Imaging Evaluation: New\n  Evidence Against an Evolving Trend","summary":"  Fr\\'echet Inception Distance (FID) is a widely used metric for assessing\nsynthetic image quality. It relies on an ImageNet-based feature extractor,\nmaking its applicability to medical imaging unclear. A recent trend is to adapt\nFID to medical imaging through feature extractors trained on medical images.\nOur study challenges this practice by demonstrating that ImageNet-based\nextractors are more consistent and aligned with human judgment than their\nRadImageNet counterparts. We evaluated sixteen StyleGAN2 networks across four\nmedical imaging modalities and four data augmentation techniques with Fr\\'echet\ndistances (FDs) computed using eleven ImageNet or RadImageNet-trained feature\nextractors. Comparison with human judgment via visual Turing tests revealed\nthat ImageNet-based extractors produced rankings consistent with human\njudgment, with the FD derived from the ImageNet-trained SwAV extractor\nsignificantly correlating with expert evaluations. In contrast,\nRadImageNet-based rankings were volatile and inconsistent with human judgment.\nOur findings challenge prevailing assumptions, providing novel evidence that\nmedical image-trained feature extractors do not inherently improve FDs and can\neven compromise their reliability. Our code is available at\nhttps://github.com/mckellwoodland/fid-med-eval.\n","authors":["McKell Woodland","Austin Castelo","Mais Al Taie","Jessica Albuquerque Marques Silva","Mohamed Eltaher","Frank Mohn","Alexander Shieh","Suprateek Kundu","Joshua P. Yung","Ankit B. Patel","Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2311.13717v5.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in LNCS vol. 15012, and is available online at\n  https://doi.org/10.1007/978-3-031-72390-2_9"},{"id":"http://arxiv.org/abs/2410.16958v1","updated":"2024-10-22T12:38:39Z","published":"2024-10-22T12:38:39Z","title":"Leaky ReLUs That Differ in Forward and Backward Pass Facilitate\n  Activation Maximization in Deep Neural Networks","summary":"  Activation maximization (AM) strives to generate optimal input stimuli,\nrevealing features that trigger high responses in trained deep neural networks.\nAM is an important method of explainable AI. We demonstrate that AM fails to\nproduce optimal input stimuli for simple functions containing ReLUs or Leaky\nReLUs, casting doubt on the practical usefulness of AM and the visual\ninterpretation of the generated images. This paper proposes a solution based on\nusing Leaky ReLUs with a high negative slope in the backward pass while keeping\nthe original, usually zero, slope in the forward pass. The approach\nsignificantly increases the maxima found by AM. The resulting ProxyGrad\nalgorithm implements a novel optimization technique for neural networks that\nemploys a secondary network as a proxy for gradient computation. This proxy\nnetwork is designed to have a simpler loss landscape with fewer local maxima\nthan the original network. Our chosen proxy network is an identical copy of the\noriginal network, including its weights, with distinct negative slopes in the\nLeaky ReLUs. Moreover, we show that ProxyGrad can be used to train the weights\nof Convolutional Neural Networks for classification such that, on some of the\ntested benchmarks, they outperform traditional networks.\n","authors":["Christoph Linse","Erhardt Barth","Thomas Martinetz"],"pdf_url":"https://arxiv.org/pdf/2410.16958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16955v1","updated":"2024-10-22T12:36:03Z","published":"2024-10-22T12:36:03Z","title":"PGCS: Physical Law embedded Generative Cloud Synthesis in Remote Sensing\n  Images","summary":"  Data quantity and quality are both critical for information extraction and\nanalyzation in remote sensing. However, the current remote sensing datasets\noften fail to meet these two requirements, for which cloud is a primary factor\ndegrading the data quantity and quality. This limitation affects the precision\nof results in remote sensing application, particularly those derived from\ndata-driven techniques. In this paper, a physical law embedded generative cloud\nsynthesis method (PGCS) is proposed to generate diverse realistic cloud images\nto enhance real data and promote the development of algorithms for subsequent\ntasks, such as cloud correction, cloud detection, and data augmentation for\nclassification, recognition, and segmentation. The PGCS method involves two key\nphases: spatial synthesis and spectral synthesis. In the spatial synthesis\nphase, a style-based generative adversarial network is utilized to simulate the\nspatial characteristics, generating an infinite number of single-channel\nclouds. In the spectral synthesis phase, the atmospheric scattering law is\nembedded through a local statistics and global fitting method, converting the\nsingle-channel clouds into multi-spectral clouds. The experimental results\ndemonstrate that PGCS achieves a high accuracy in both phases and performs\nbetter than three other existing cloud synthesis methods. Two cloud correction\nmethods are developed from PGCS and exhibits a superior performance compared to\nstate-of-the-art methods in the cloud correction task. Furthermore, the\napplication of PGCS with data from various sensors was investigated and\nsuccessfully extended. Code will be provided at\nhttps://github.com/Liying-Xu/PGCS.\n","authors":["Liying Xu","Huifang Li","Huanfeng Shen","Mingyang Lei","Tao Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.16955v1.pdf","comment":"20 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.16953v1","updated":"2024-10-22T12:33:38Z","published":"2024-10-22T12:33:38Z","title":"Towards Real Zero-Shot Camouflaged Object Segmentation without\n  Camouflaged Annotations","summary":"  Camouflaged Object Segmentation (COS) faces significant challenges due to the\nscarcity of annotated data, where meticulous pixel-level annotation is both\nlabor-intensive and costly, primarily due to the intricate object-background\nboundaries. Addressing the core question, \"Can COS be effectively achieved in a\nzero-shot manner without manual annotations for any camouflaged object?\" we\naffirmatively respond and introduce a robust zero-shot COS framework. This\nframework leverages the inherent local pattern bias of COS and employs a broad\nsemantic feature space derived from salient object segmentation (SOS) for\nefficient zero-shot transfer. We incorporate an Masked Image Modeling (MIM)\nbased image encoder optimized for Parameter-Efficient Fine-Tuning (PEFT), a\nMultimodal Large Language Model (M-LLM), and a Multi-scale Fine-grained\nAlignment (MFA) mechanism. The MIM pre-trained image encoder focuses on\ncapturing essential low-level features, while the M-LLM generates caption\nembeddings processed alongside these visual cues. These embeddings are\nprecisely aligned using MFA, enabling our framework to accurately interpret and\nnavigate complex semantic contexts. To optimize operational efficiency, we\nintroduce a learnable codebook that represents the M-LLM during inference,\nsignificantly reducing computational overhead. Our framework demonstrates its\nversatility and efficacy through rigorous experimentation, achieving\nstate-of-the-art performance in zero-shot COS with $F_{\\beta}^w$ scores of\n72.9\\% on CAMO and 71.7\\% on COD10K. By removing the M-LLM during inference, we\nachieve an inference speed comparable to that of traditional end-to-end models,\nreaching 18.1 FPS. Code: https://github.com/R-LEI360725/ZSCOS-CaMF\n","authors":["Cheng Lei","Jie Fan","Xinran Li","Tianzhu Xiang","Ao Li","Ce Zhu","Le Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10353v2","updated":"2024-10-22T12:31:51Z","published":"2024-09-16T15:04:14Z","title":"Taming Diffusion Models for Image Restoration: A Review","summary":"  Diffusion models have achieved remarkable progress in generative modelling,\nparticularly in enhancing image quality to conform to human preferences.\nRecently, these models have also been applied to low-level computer vision for\nphoto-realistic image restoration (IR) in tasks such as image denoising,\ndeblurring, dehazing, etc. In this review paper, we introduce key constructions\nin diffusion models and survey contemporary techniques that make use of\ndiffusion models in solving general IR tasks. Furthermore, we point out the\nmain challenges and limitations of existing diffusion-based IR frameworks and\nprovide potential directions for future work.\n","authors":["Ziwei Luo","Fredrik K. Gustafsson","Zheng Zhao","Jens Sjölund","Thomas B. Schön"],"pdf_url":"https://arxiv.org/pdf/2409.10353v2.pdf","comment":"Review paper; any comments and suggestions are most welcome!"},{"id":"http://arxiv.org/abs/2407.21497v3","updated":"2024-10-22T12:23:20Z","published":"2024-07-31T10:11:57Z","title":"Mitral Regurgitation Recognition based on Unsupervised\n  Out-of-Distribution Detection with Residual Diffusion Amplification","summary":"  Mitral regurgitation (MR) is a serious heart valve disease. Early and\naccurate diagnosis of MR via ultrasound video is critical for timely clinical\ndecision-making and surgical intervention. However, manual MR diagnosis heavily\nrelies on the operator's experience, which may cause misdiagnosis and\ninter-observer variability. Since MR data is limited and has large intra-class\nvariability, we propose an unsupervised out-of-distribution (OOD) detection\nmethod to identify MR rather than building a deep classifier. To our knowledge,\nwe are the first to explore OOD in MR ultrasound videos. Our method consists of\na feature extractor, a feature reconstruction model, and a residual\naccumulation amplification algorithm. The feature extractor obtains features\nfrom the video clips and feeds them into the feature reconstruction model to\nrestore the original features. The residual accumulation amplification\nalgorithm then iteratively performs noise feature reconstruction, amplifying\nthe reconstructed error of OOD features. This algorithm is straightforward yet\nefficient and can seamlessly integrate as a plug-and-play component in\nreconstruction-based OOD detection methods. We validated the proposed method on\na large ultrasound dataset containing 893 non-MR and 267 MR videos.\nExperimental results show that our OOD detection method can effectively\nidentify MR samples.\n","authors":["Zhe Liu","Xiliang Zhu","Tong Han","Yuhao Huang","Jian Wang","Lian Liu","Fang Wang","Dong Ni","Zhongshan Gou","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2407.21497v3.pdf","comment":"Accepted by MICCAI MLMI 2024, 11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.16947v1","updated":"2024-10-22T12:21:39Z","published":"2024-10-22T12:21:39Z","title":"ISImed: A Framework for Self-Supervised Learning using Intrinsic Spatial\n  Information in Medical Images","summary":"  This paper demonstrates that spatial information can be used to learn\ninterpretable representations in medical images using Self-Supervised Learning\n(SSL). Our proposed method, ISImed, is based on the observation that medical\nimages exhibit a much lower variability among different images compared to\nclassic data vision benchmarks. By leveraging this resemblance of human body\nstructures across multiple images, we establish a self-supervised objective\nthat creates a latent representation capable of capturing its location in the\nphysical realm. More specifically, our method involves sampling image crops and\ncreating a distance matrix that compares the learned representation vectors of\nall possible combinations of these crops to the true distance between them. The\nintuition is, that the learned latent space is a positional encoding for a\ngiven image crop. We hypothesize, that by learning these positional encodings,\ncomprehensive image representations have to be generated. To test this\nhypothesis and evaluate our method, we compare our learned representation with\ntwo state-of-the-art SSL benchmarking methods on two publicly available medical\nimaging datasets. We show that our method can efficiently learn representations\nthat capture the underlying structure of the data and can be used to transfer\nto a downstream classification task.\n","authors":["Nabil Jabareen","Dongsheng Yuan","Sören Lukassen"],"pdf_url":"https://arxiv.org/pdf/2410.16947v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.16945v1","updated":"2024-10-22T12:20:15Z","published":"2024-10-22T12:20:15Z","title":"IdenBAT: Disentangled Representation Learning for Identity-Preserved\n  Brain Age Transformation","summary":"  Brain age transformation aims to convert reference brain images into\nsynthesized images that accurately reflect the age-specific features of a\ntarget age group. The primary objective of this task is to modify only the\nage-related attributes of the reference image while preserving all other\nage-irrelevant attributes. However, achieving this goal poses substantial\nchallenges due to the inherent entanglement of various image attributes within\nfeatures extracted from a backbone encoder, resulting in simultaneous\nalterations during the image generation. To address this challenge, we propose\na novel architecture that employs disentangled representation learning for\nidentity-preserved brain age transformation called IdenBAT. This approach\nfacilitates the decomposition of image features, ensuring the preservation of\nindividual traits while selectively transforming age-related characteristics to\nmatch those of the target age group. Through comprehensive experiments\nconducted on both 2D and full-size 3D brain datasets, our method adeptly\nconverts input images to target age while retaining individual characteristics\naccurately. Furthermore, our approach demonstrates superiority over existing\nstate-of-the-art regarding performance fidelity.\n","authors":["Junyeong Maeng","Kwanseok Oh","Wonsik Jung","Heung-Il Suk"],"pdf_url":"https://arxiv.org/pdf/2410.16945v1.pdf","comment":"16 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.16942v1","updated":"2024-10-22T12:18:24Z","published":"2024-10-22T12:18:24Z","title":"DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization","summary":"  Diffusion models have achieved remarkable progress in the field of image\ngeneration due to their outstanding capabilities. However, these models require\nsubstantial computing resources because of the multi-step denoising process\nduring inference. While traditional pruning methods have been employed to\noptimize these models, the retraining process necessitates large-scale training\ndatasets and extensive computational costs to maintain generalization ability,\nmaking it neither convenient nor efficient. Recent studies attempt to utilize\nthe similarity of features across adjacent denoising stages to reduce\ncomputational costs through simple and static strategies. However, these\nstrategies cannot fully harness the potential of the similar feature patterns\nacross adjacent timesteps. In this work, we propose a novel pruning method that\nderives an efficient diffusion model via a more intelligent and differentiable\npruner. At the core of our approach is casting the model pruning process into a\nSubNet search process. Specifically, we first introduce a SuperNet based on\nstandard diffusion via adding some backup connections built upon the similar\nfeatures. We then construct a plugin pruner network and design optimization\nlosses to identify redundant computation. Finally, our method can identify an\noptimal SubNet through few-step gradient optimization and a simple\npost-processing procedure. We conduct extensive experiments on various\ndiffusion models including Stable Diffusion series and DiTs. Our DiP-GO\napproach achieves 4.4 x speedup for SD-1.5 without any loss of accuracy,\nsignificantly outperforming the previous state-of-the-art methods.\n","authors":["Haowei Zhu","Dehua Tang","Ji Liu","Mingjie Lu","Jintu Zheng","Jinzhang Peng","Dong Li","Yu Wang","Fan Jiang","Lu Tian","Spandan Tiwari","Ashish Sirasao","Jun-Hai Yong","Bin Wang","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2410.16942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16939v1","updated":"2024-10-22T12:13:47Z","published":"2024-10-22T12:13:47Z","title":"LIMIS: Towards Language-based Interactive Medical Image Segmentation","summary":"  Within this work, we introduce LIMIS: The first purely language-based\ninteractive medical image segmentation model. We achieve this by adapting\nGrounded SAM to the medical domain and designing a language-based model\ninteraction strategy that allows radiologists to incorporate their knowledge\ninto the segmentation process. LIMIS produces high-quality initial segmentation\nmasks by leveraging medical foundation models and allows users to adapt\nsegmentation masks using only language, opening up interactive segmentation to\nscenarios where physicians require using their hands for other tasks. We\nevaluate LIMIS on three publicly available medical datasets in terms of\nperformance and usability with experts from the medical domain confirming its\nhigh-quality segmentation masks and its interactive usability.\n","authors":["Lena Heinemann","Alexander Jaus","Zdravko Marinov","Moon Kim","Maria Francesca Spadea","Jens Kleesiek","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2410.16939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15330v2","updated":"2024-10-22T12:01:45Z","published":"2024-05-24T08:12:41Z","title":"Towards Understanding the Working Mechanism of Text-to-Image Diffusion\n  Model","summary":"  Recently, the strong latent Diffusion Probabilistic Model (DPM) has been\napplied to high-quality Text-to-Image (T2I) generation (e.g., Stable\nDiffusion), by injecting the encoded target text prompt into the gradually\ndenoised diffusion image generator. Despite the success of DPM in practice, the\nmechanism behind it remains to be explored. To fill this blank, we begin by\nexamining the intermediate statuses during the gradual denoising generation\nprocess in DPM. The empirical observations indicate, the shape of image is\nreconstructed after the first few denoising steps, and then the image is filled\nwith details (e.g., texture). The phenomenon is because the low-frequency\nsignal (shape relevant) of the noisy image is not corrupted until the final\nstage in the forward process (initial stage of generation) of adding noise in\nDPM. Inspired by the observations, we proceed to explore the influence of each\ntoken in the text prompt during the two stages. After a series of experiments\nof T2I generations conditioned on a set of text prompts. We conclude that in\nthe earlier generation stage, the image is mostly decided by the special token\n[\\texttt{EOS}] in the text prompt, and the information in the text prompt is\nalready conveyed in this stage. After that, the diffusion model completes the\ndetails of generated images by information from themselves. Finally, we propose\nto apply this observation to accelerate the process of T2I generation by\nproperly removing text guidance, which finally accelerates the sampling up to\n25\\%+.\n","authors":["Mingyang Yi","Aoxue Li","Yi Xin","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2405.15330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16910v1","updated":"2024-10-22T11:35:36Z","published":"2024-10-22T11:35:36Z","title":"Hierarchical Clustering for Conditional Diffusion in Image Generation","summary":"  Finding clusters of data points with similar characteristics and generating\nnew cluster-specific samples can significantly enhance our understanding of\ncomplex data distributions. While clustering has been widely explored using\nVariational Autoencoders, these models often lack generation quality in\nreal-world datasets. This paper addresses this gap by introducing\nTreeDiffusion, a deep generative model that conditions Diffusion Models on\nhierarchical clusters to obtain high-quality, cluster-specific generations. The\nproposed pipeline consists of two steps: a VAE-based clustering model that\nlearns the hierarchical structure of the data, and a conditional diffusion\nmodel that generates realistic images for each cluster. We propose this\ntwo-stage process to ensure that the generated samples remain representative of\ntheir respective clusters and enhance image fidelity to the level of diffusion\nmodels. A key strength of our method is its ability to create images for each\ncluster, providing better visualization of the learned representations by the\nclustering model, as demonstrated through qualitative results. This method\neffectively addresses the generative limitations of VAE-based approaches while\npreserving their clustering performance. Empirically, we demonstrate that\nconditioning diffusion models on hierarchical clusters significantly enhances\ngenerative performance, thereby advancing the state of generative clustering\nmodels.\n","authors":["Jorge da Silva Goncalves","Laura Manduchi","Moritz Vandenhirtz","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2410.16910v1.pdf","comment":"25 pages, submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.16908v1","updated":"2024-10-22T11:28:39Z","published":"2024-10-22T11:28:39Z","title":"Mitigating Vanishing Activations in Deep CapsNets Using Channel Pruning","summary":"  Capsule Networks outperform Convolutional Neural Networks in learning the\npart-whole relationships with viewpoint invariance, and the credit goes to\ntheir multidimensional capsules. It was assumed that increasing the number of\ncapsule layers in the capsule networks would enhance the model performance.\nHowever, recent studies found that Capsule Networks lack scalability due to\nvanishing activations in the capsules of deeper layers. This paper thoroughly\ninvestigates the vanishing activation problem in deep Capsule Networks. To\nanalyze this issue and understand how increasing capsule dimensions can\nfacilitate deeper networks, various Capsule Network models are constructed and\nevaluated with different numbers of capsules, capsule dimensions, and\nintermediate layers for this paper. Unlike traditional model pruning, which\nreduces the number of model parameters and expedites model training, this study\nuses pruning to mitigate the vanishing activations in the deeper capsule\nlayers. In addition, the backbone network and capsule layers are pruned with\ndifferent pruning ratios to reduce the number of inactive capsules and achieve\nbetter model accuracy than the unpruned models.\n","authors":["Siddharth Sahu","Abdulrahman Altahhan"],"pdf_url":"https://arxiv.org/pdf/2410.16908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16898v1","updated":"2024-10-22T11:03:06Z","published":"2024-10-22T11:03:06Z","title":"MBD: Multi b-value Denoising of Diffusion Magnetic Resonance Images","summary":"  We propose a novel approach to denoising diffusion magnetic resonance images\n(dMRI) using convolutional neural networks, that exploits the benefits of data\nacquired at multiple b-values to offset the need for many redundant\nobservations. Denoising is especially relevant in dMRI since noise can have a\ndeleterious impact on both quantification accuracy and image preprocessing. The\nmost successful methods proposed to date, like Marchenko-Pastur Principal\nComponent Analysis (MPPCA) denoising, are tailored to diffusion-weighting\nrepeated for many encoding directions. They exploit high redundancy of the\ndataset that oversamples the diffusion-encoding direction space, since many\ndirections have collinear components.\n  However, there are many dMRI techniques that do not entail a large number of\nencoding directions or repetitions, and are therefore less suited to this\napproach. For example, clinical dMRI exams may include as few as three encoding\ndirections, with low or negligible data redundancy across directions. Moreover,\npromising new dMRI approaches, like spherical b-tensor encoding (STE), benefit\nfrom high b-values while sensitizing the signal to diffusion along all\ndirections in just a single shot.\n  We introduce a convolutional neural network approach that we call\nmulti-b-value-based denoising (MBD). MBD exploits the similarity in\ndiffusion-weighted images (DWI) across different b-values but along the same\ndiffusion encoding direction. It allows denoising of diffusion images with high\nnoise variance while avoiding blurring, and using just a small number input\nimages.\n","authors":["Jakub Jurek","Andrzej Materka","Kamil Ludwisiak","Agata Majos","Filip Szczepankiewicz"],"pdf_url":"https://arxiv.org/pdf/2410.16898v1.pdf","comment":"this is a biomedical engineering work using machine learning to\n  enhance medical images"},{"id":"http://arxiv.org/abs/2410.16897v1","updated":"2024-10-22T11:02:32Z","published":"2024-10-22T11:02:32Z","title":"Enhancing Generalization in Convolutional Neural Networks through\n  Regularization with Edge and Line Features","summary":"  This paper proposes a novel regularization approach to bias Convolutional\nNeural Networks (CNNs) toward utilizing edge and line features in their hidden\nlayers. Rather than learning arbitrary kernels, we constrain the convolution\nlayers to edge and line detection kernels. This intentional bias regularizes\nthe models, improving generalization performance, especially on small datasets.\nAs a result, test accuracies improve by margins of 5-11 percentage points\nacross four challenging fine-grained classification datasets with limited\ntraining data and an identical number of trainable parameters. Instead of\ntraditional convolutional layers, we use Pre-defined Filter Modules, which\nconvolve input data using a fixed set of 3x3 pre-defined edge and line filters.\nA subsequent ReLU erases information that did not trigger any positive\nresponse. Next, a 1x1 convolutional layer generates linear combinations.\nNotably, the pre-defined filters are a fixed component of the architecture,\nremaining unchanged during the training phase. Our findings reveal that the\nnumber of dimensions spanned by the set of pre-defined filters has a low impact\non recognition performance. However, the size of the set of filters matters,\nwith nine or more filters providing optimal results.\n","authors":["Christoph Linse","Beatrice Brückner","Thomas Martinetz"],"pdf_url":"https://arxiv.org/pdf/2410.16897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16892v1","updated":"2024-10-22T10:55:59Z","published":"2024-10-22T10:55:59Z","title":"VistaDream: Sampling multiview consistent images for single-view scene\n  reconstruction","summary":"  In this paper, we propose VistaDream a novel framework to reconstruct a 3D\nscene from a single-view image. Recent diffusion models enable generating\nhigh-quality novel-view images from a single-view input image. Most existing\nmethods only concentrate on building the consistency between the input image\nand the generated images while losing the consistency between the generated\nimages. VistaDream addresses this problem by a two-stage pipeline. In the first\nstage, VistaDream begins with building a global coarse 3D scaffold by zooming\nout a little step with inpainted boundaries and an estimated depth map. Then,\non this global scaffold, we use iterative diffusion-based RGB-D inpainting to\ngenerate novel-view images to inpaint the holes of the scaffold. In the second\nstage, we further enhance the consistency between the generated novel-view\nimages by a novel training-free Multiview Consistency Sampling (MCS) that\nintroduces multi-view consistency constraints in the reverse sampling process\nof diffusion models. Experimental results demonstrate that without training or\nfine-tuning existing diffusion models, VistaDream achieves consistent and\nhigh-quality novel view synthesis using just single-view images and outperforms\nbaseline methods by a large margin. The code, videos, and interactive demos are\navailable at https://vistadream-project-page.github.io/.\n","authors":["Haiping Wang","Yuan Liu","Ziwei Liu","Wenping Wang","Zhen Dong","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16892v1.pdf","comment":"Project Page: https://vistadream-project-page.github.io/"},{"id":"http://arxiv.org/abs/2410.16884v1","updated":"2024-10-22T10:42:08Z","published":"2024-10-22T10:42:08Z","title":"Network Inversion for Training-Like Data Reconstruction","summary":"  Machine Learning models are often trained on proprietary and private data\nthat cannot be shared, though the trained models themselves are distributed\nopenly assuming that sharing model weights is privacy preserving, as training\ndata is not expected to be inferred from the model weights. In this paper, we\npresent Training-Like Data Reconstruction (TLDR), a network inversion-based\napproach to reconstruct training-like data from trained models. To begin with,\nwe introduce a comprehensive network inversion technique that learns the input\nspace corresponding to different classes in the classifier using a single\nconditioned generator. While inversion may typically return random and\narbitrary input images for a given output label, we modify the inversion\nprocess to incentivize the generator to reconstruct training-like data by\nexploiting key properties of the classifier with respect to the training data\nalong with some prior knowledge about the images. To validate our approach, we\nconduct empirical evaluations on multiple standard vision classification\ndatasets, thereby highlighting the potential privacy risks involved in sharing\nmachine learning models.\n","authors":["Pirzada Suhail","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2410.16884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16868v1","updated":"2024-10-22T10:12:57Z","published":"2024-10-22T10:12:57Z","title":"Rethinking generalization of classifiers in separable classes scenarios\n  and over-parameterized regimes","summary":"  We investigate the learning dynamics of classifiers in scenarios where\nclasses are separable or classifiers are over-parameterized. In both cases,\nEmpirical Risk Minimization (ERM) results in zero training error. However,\nthere are many global minima with a training error of zero, some of which\ngeneralize well and some of which do not. We show that in separable classes\nscenarios the proportion of \"bad\" global minima diminishes exponentially with\nthe number of training data n. Our analysis provides bounds and learning curves\ndependent solely on the density distribution of the true error for the given\nclassifier function set, irrespective of the set's size or complexity (e.g.,\nnumber of parameters). This observation may shed light on the unexpectedly good\ngeneralization of over-parameterized Neural Networks. For the\nover-parameterized scenario, we propose a model for the density distribution of\nthe true error, yielding learning curves that align with experiments on MNIST\nand CIFAR-10.\n","authors":["Julius Martinetz","Christoph Linse","Thomas Martinetz"],"pdf_url":"https://arxiv.org/pdf/2410.16868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05375v6","updated":"2024-10-22T09:52:42Z","published":"2023-10-09T03:11:08Z","title":"IPDreamer: Appearance-Controllable 3D Object Generation with Complex\n  Image Prompts","summary":"  Recent advances in 3D generation have been remarkable, with methods such as\nDreamFusion leveraging large-scale text-to-image diffusion-based models to\nguide 3D object generation. These methods enable the synthesis of detailed and\nphotorealistic textured objects. However, the appearance of 3D objects produced\nby such text-to-3D models is often unpredictable, and it is hard for\nsingle-image-to-3D methods to deal with images lacking a clear subject,\ncomplicating the generation of appearance-controllable 3D objects from complex\nimages. To address these challenges, we present IPDreamer, a novel method that\ncaptures intricate appearance features from complex $\\textbf{I}$mage\n$\\textbf{P}$rompts and aligns the synthesized 3D object with these extracted\nfeatures, enabling high-fidelity, appearance-controllable 3D object generation.\nOur experiments demonstrate that IPDreamer consistently generates high-quality\n3D objects that align with both the textual and complex image prompts,\nhighlighting its promising capability in appearance-controlled, complex 3D\nobject generation. Our code is available at\nhttps://github.com/zengbohan0217/IPDreamer.\n","authors":["Bohan Zeng","Shanglin Li","Yutang Feng","Ling Yang","Hong Li","Sicheng Gao","Jiaming Liu","Conghui He","Wentao Zhang","Jianzhuang Liu","Baochang Zhang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2310.05375v6.pdf","comment":"20 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.16857v1","updated":"2024-10-22T09:46:09Z","published":"2024-10-22T09:46:09Z","title":"Nash Meets Wertheimer: Using Good Continuation in Jigsaw Puzzles","summary":"  Jigsaw puzzle solving is a challenging task for computer vision since it\nrequires high-level spatial and semantic reasoning. To solve the problem,\nexisting approaches invariably use color and/or shape information but in many\nreal-world scenarios, such as in archaeological fresco reconstruction, this\nkind of clues is often unreliable due to severe physical and pictorial\ndeterioration of the individual fragments. This makes state-of-the-art\napproaches entirely unusable in practice. On the other hand, in such cases,\nsimple geometrical patterns such as lines or curves offer a powerful yet\nunexplored clue. In an attempt to fill in this gap, in this paper we introduce\na new challenging version of the puzzle solving problem in which one\ndeliberately ignores conventional color and shape features and relies solely on\nthe presence of linear geometrical patterns. The reconstruction process is then\nonly driven by one of the most fundamental principles of Gestalt perceptual\norganization, namely Wertheimer's {\\em law of good continuation}. In order to\ntackle this problem, we formulate the puzzle solving problem as the problem of\nfinding a Nash equilibrium of a (noncooperative) multiplayer game and use\nclassical multi-population replicator dynamics to solve it. The proposed\napproach is general and allows us to deal with pieces of arbitrary shape, size\nand orientation. We evaluate our approach on both synthetic and real-world data\nand compare it with state-of-the-art algorithms. The results show the intrinsic\ncomplexity of our purely line-based puzzle problem as well as the relative\neffectiveness of our game-theoretic formulation.\n","authors":["Marina Khoroshiltseva","Luca Palmieri","Sinem Aslan","Sebastiano Vascon","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2410.16857v1.pdf","comment":"to be published in ACCV2024"},{"id":"http://arxiv.org/abs/2402.02500v3","updated":"2024-10-22T09:42:39Z","published":"2024-02-04T14:18:45Z","title":"Point Cloud Matters: Rethinking the Impact of Different Observation\n  Spaces on Robot Learning","summary":"  In robot learning, the observation space is crucial due to the distinct\ncharacteristics of different modalities, which can potentially become a\nbottleneck alongside policy design. In this study, we explore the influence of\nvarious observation spaces on robot learning, focusing on three predominant\nmodalities: RGB, RGB-D, and point cloud. We introduce OBSBench, a benchmark\ncomprising two simulators and 125 tasks, along with standardized pipelines for\nvarious encoders and policy baselines. Extensive experiments on diverse\ncontact-rich manipulation tasks reveal a notable trend: point cloud-based\nmethods, even those with the simplest designs, frequently outperform their RGB\nand RGB-D counterparts. This trend persists in both scenarios: training from\nscratch and utilizing pre-training. Furthermore, our findings demonstrate that\npoint cloud observations often yield better policy performance and\nsignificantly stronger generalization capabilities across various geometric and\nvisual conditions. These outcomes suggest that the 3D point cloud is a valuable\nobservation modality for intricate robotic tasks. We also suggest that\nincorporating both appearance and coordinate information can enhance the\nperformance of point cloud methods. We hope our work provides valuable insights\nand guidance for designing more generalizable and robust robotic models. Codes\nare available at https://github.com/HaoyiZhu/PointCloudMatters.\n","authors":["Haoyi Zhu","Yating Wang","Di Huang","Weicai Ye","Wanli Ouyang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2402.02500v3.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.16853v1","updated":"2024-10-22T09:37:29Z","published":"2024-10-22T09:37:29Z","title":"Bridging the Modality Gap: Dimension Information Alignment and Sparse\n  Spatial Constraint for Image-Text Matching","summary":"  Many contrastive learning based models have achieved advanced performance in\nimage-text matching tasks. The key of these models lies in analyzing the\ncorrelation between image-text pairs, which involves cross-modal interaction of\nembeddings in corresponding dimensions. However, the embeddings of different\nmodalities are from different models or modules, and there is a significant\nmodality gap. Directly interacting such embeddings lacks rationality and may\ncapture inaccurate correlation. Therefore, we propose a novel method called\nDIAS to bridge the modality gap from two aspects: (1) We align the information\nrepresentation of embeddings from different modalities in corresponding\ndimension to ensure the correlation calculation is based on interactions of\nsimilar information. (2) The spatial constraints of inter- and intra-modalities\nunmatched pairs are introduced to ensure the effectiveness of semantic\nalignment of the model. Besides, a sparse correlation algorithm is proposed to\nselect strong correlated spatial relationships, enabling the model to learn\nmore significant features and avoid being misled by weak correlation. Extensive\nexperiments demonstrate the superiority of DIAS, achieving 4.3\\%-10.2\\% rSum\nimprovements on Flickr30k and MSCOCO benchmarks.\n","authors":["Xiang Ma","Xuemei Li","Lexin Fang","Caiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09534v2","updated":"2024-10-22T09:27:26Z","published":"2023-09-18T07:26:58Z","title":"Selective Volume Mixup for Video Action Recognition","summary":"  The recent advances in Convolutional Neural Networks (CNNs) and Vision\nTransformers have convincingly demonstrated high learning capability for video\naction recognition on large datasets. Nevertheless, deep models often suffer\nfrom the overfitting effect on small-scale datasets with a limited number of\ntraining videos. A common solution is to exploit the existing image\naugmentation strategies for each frame individually including Mixup, Cutmix,\nand RandAugment, which are not particularly optimized for video data. In this\npaper, we propose a novel video augmentation strategy named Selective Volume\nMixup (SV-Mix) to improve the generalization ability of deep models with\nlimited training videos. SV-Mix devises a learnable selective module to choose\nthe most informative volumes from two videos and mixes the volumes up to\nachieve a new training video. Technically, we propose two new modules, i.e., a\nspatial selective module to select the local patches for each spatial position,\nand a temporal selective module to mix the entire frames for each timestamp and\nmaintain the spatial pattern. At each time, we randomly choose one of the two\nmodules to expand the diversity of training samples. The selective modules are\njointly optimized with the video action recognition framework to find the\noptimal augmentation strategy. We empirically demonstrate the merits of the\nSV-Mix augmentation on a wide range of video action recognition benchmarks and\nconsistently boot the performances of both CNN-based and transformer-based\nmodels.\n","authors":["Yi Tan","Zhaofan Qiu","Yanbin Hao","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2309.09534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16840v1","updated":"2024-10-22T09:20:03Z","published":"2024-10-22T09:20:03Z","title":"MPDS: A Movie Posters Dataset for Image Generation with Diffusion Model","summary":"  Movie posters are vital for captivating audiences, conveying themes, and\ndriving market competition in the film industry. While traditional designs are\nlaborious, intelligent generation technology offers efficiency gains and design\nenhancements. Despite exciting progress in image generation, current models\noften fall short in producing satisfactory poster results. The primary issue\nlies in the absence of specialized poster datasets for targeted model training.\nIn this work, we propose a Movie Posters DataSet (MPDS), tailored for\ntext-to-image generation models to revolutionize poster production. As\ndedicated to posters, MPDS stands out as the first image-text pair dataset to\nour knowledge, composing of 373k+ image-text pairs and 8k+ actor images\n(covering 4k+ actors). Detailed poster descriptions, such as movie titles,\ngenres, casts, and synopses, are meticulously organized and standardized based\non public movie synopsis, also named movie-synopsis prompt. To bolster poster\ndescriptions as well as reduce differences from movie synopsis, further, we\nleverage a large-scale vision-language model to automatically produce\nvision-perceptive prompts for each poster, then perform manual rectification\nand integration with movie-synopsis prompt. In addition, we introduce a prompt\nof poster captions to exhibit text elements in posters like actor names and\nmovie titles. For movie poster generation, we develop a multi-condition\ndiffusion framework that takes poster prompt, poster caption, and actor image\n(for personalization) as inputs, yielding excellent results through the\nlearning of a diffusion model. Experiments demonstrate the valuable role of our\nproposed MPDS dataset in advancing personalized movie poster generation. MPDS\nis available at https://anonymous.4open.science/r/MPDS-373k-BD3B.\n","authors":["Meng Xu","Tong Zhang","Fuyun Wang","Yi Lei","Xin Liu","Zhen Cui"],"pdf_url":"https://arxiv.org/pdf/2410.16840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15574v3","updated":"2024-10-22T09:05:30Z","published":"2024-05-24T14:04:03Z","title":"Meteor: Mamba-based Traversal of Rationale for Large Language and Vision\n  Models","summary":"  The rapid development of large language and vision models (LLVMs) has been\ndriven by advances in visual instruction tuning. Recently, open-source LLVMs\nhave curated high-quality visual instruction tuning datasets and utilized\nadditional vision encoders or multiple computer vision models in order to\nnarrow the performance gap with powerful closed-source LLVMs. These\nadvancements are attributed to multifaceted information required for diverse\ncapabilities, including fundamental image understanding, real-world knowledge\nabout common-sense and non-object concepts (e.g., charts, diagrams, symbols,\nsigns, and math problems), and step-by-step procedures for solving complex\nquestions. Drawing from the multifaceted information, we present a new\nefficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages\nmultifaceted rationale to enhance understanding and answering capabilities. To\nembed lengthy rationales containing abundant information, we employ the Mamba\narchitecture, capable of processing sequential data with linear time\ncomplexity. We introduce a new concept of traversal of rationale that\nfacilitates efficient embedding of rationale. Subsequently, the backbone\nmultimodal language model (MLM) is trained to generate answers with the aid of\nrationale. Through these steps, Meteor achieves significant improvements in\nvision language performances across multiple evaluation benchmarks requiring\ndiverse capabilities, without scaling up the model size or employing additional\nvision encoders and computer vision models.\n","authors":["Byung-Kwan Lee","Chae Won Kim","Beomchan Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2405.15574v3.pdf","comment":"Code is available in https://github.com/ByungKwanLee/Meteor"},{"id":"http://arxiv.org/abs/2410.16824v1","updated":"2024-10-22T08:57:17Z","published":"2024-10-22T08:57:17Z","title":"PerspectiveNet: Multi-View Perception for Dynamic Scene Understanding","summary":"  Generating detailed descriptions from multiple cameras and viewpoints is\nchallenging due to the complex and inconsistent nature of visual data. In this\npaper, we introduce PerspectiveNet, a lightweight yet efficient model for\ngenerating long descriptions across multiple camera views. Our approach\nutilizes a vision encoder, a compact connector module to convert visual\nfeatures into a fixed-size tensor, and large language models (LLMs) to harness\nthe strong natural language generation capabilities of LLMs. The connector\nmodule is designed with three main goals: mapping visual features onto LLM\nembeddings, emphasizing key information needed for description generation, and\nproducing a fixed-size feature matrix. Additionally, we augment our solution\nwith a secondary task, the correct frame sequence detection, enabling the model\nto search for the correct sequence of frames to generate descriptions. Finally,\nwe integrate the connector module, the secondary task, the LLM, and a visual\nfeature extraction model into a single architecture, which is trained for the\nTraffic Safety Description and Analysis task. This task requires generating\ndetailed, fine-grained descriptions of events from multiple cameras and\nviewpoints. The resulting model is lightweight, ensuring efficient training and\ninference, while remaining highly effective.\n","authors":["Vinh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.16824v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2405.20674v2","updated":"2024-10-22T08:50:16Z","published":"2024-05-31T08:18:39Z","title":"4Diffusion: Multi-view Video Diffusion Model for 4D Generation","summary":"  Current 4D generation methods have achieved noteworthy efficacy with the aid\nof advanced diffusion generative models. However, these methods lack multi-view\nspatial-temporal modeling and encounter challenges in integrating diverse prior\nknowledge from multiple diffusion models, resulting in inconsistent temporal\nappearance and flickers. In this paper, we propose a novel 4D generation\npipeline, namely 4Diffusion, aimed at generating spatial-temporally consistent\n4D content from a monocular video. We first design a unified diffusion model\ntailored for multi-view video generation by incorporating a learnable motion\nmodule into a frozen 3D-aware diffusion model to capture multi-view\nspatial-temporal correlations. After training on a curated dataset, our\ndiffusion model acquires reasonable temporal consistency and inherently\npreserves the generalizability and spatial consistency of the 3D-aware\ndiffusion model. Subsequently, we propose 4D-aware Score Distillation Sampling\nloss, which is based on our multi-view video diffusion model, to optimize 4D\nrepresentation parameterized by dynamic NeRF. This aims to eliminate\ndiscrepancies arising from multiple diffusion models, allowing for generating\nspatial-temporally consistent 4D content. Moreover, we devise an anchor loss to\nenhance the appearance details and facilitate the learning of dynamic NeRF.\nExtensive qualitative and quantitative experiments demonstrate that our method\nachieves superior performance compared to previous methods.\n","authors":["Haiyu Zhang","Xinyuan Chen","Yaohui Wang","Xihui Liu","Yunhong Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2405.20674v2.pdf","comment":"NeurIPS 2024. Project Page: https://aejion.github.io/4diffusion/"},{"id":"http://arxiv.org/abs/2410.16820v1","updated":"2024-10-22T08:48:41Z","published":"2024-10-22T08:48:41Z","title":"AttriPrompter: Auto-Prompting with Attribute Semantics for Zero-shot\n  Nuclei Detection via Visual-Language Pre-trained Models","summary":"  Large-scale visual-language pre-trained models (VLPMs) have demonstrated\nexceptional performance in downstream object detection through text prompts for\nnatural scenes. However, their application to zero-shot nuclei detection on\nhistopathology images remains relatively unexplored, mainly due to the\nsignificant gap between the characteristics of medical images and the\nweb-originated text-image pairs used for pre-training. This paper aims to\ninvestigate the potential of the object-level VLPM, Grounded Language-Image\nPre-training (GLIP), for zero-shot nuclei detection. Specifically, we propose\nan innovative auto-prompting pipeline, named AttriPrompter, comprising\nattribute generation, attribute augmentation, and relevance sorting, to avoid\nsubjective manual prompt design. AttriPrompter utilizes VLPMs' text-to-image\nalignment to create semantically rich text prompts, which are then fed into\nGLIP for initial zero-shot nuclei detection. Additionally, we propose a\nself-trained knowledge distillation framework, where GLIP serves as the teacher\nwith its initial predictions used as pseudo labels, to address the challenges\nposed by high nuclei density, including missed detections, false positives, and\noverlapping instances. Our method exhibits remarkable performance in label-free\nnuclei detection, outperforming all existing unsupervised methods and\ndemonstrating excellent generality. Notably, this work highlights the\nastonishing potential of VLPMs pre-trained on natural image-text pairs for\ndownstream tasks in the medical field as well. Code will be released at\nhttps://github.com/wuyongjianCODE/AttriPrompter.\n","authors":["Yongjian Wu","Yang Zhou","Jiya Saiyin","Bingzheng Wei","Maode Lai","Jianzhong Shou","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2410.16820v1.pdf","comment":"This article has been accepted for publication in a future issue of\n  IEEE Transactions on Medical Imaging (TMI), but has not been fully edited.\n  Content may change prior to final publication. Citation information: DOI:\n  https://doi.org/10.1109/TMI.2024.3473745 . Code:\n  https://github.com/wuyongjianCODE/AttriPrompter"},{"id":"http://arxiv.org/abs/2407.11965v2","updated":"2024-10-22T08:31:44Z","published":"2024-07-16T17:59:29Z","title":"UrbanWorld: An Urban World Model for 3D City Generation","summary":"  Cities, as the essential environment of human life, encompass diverse\nphysical elements such as buildings, roads and vegetation, which continuously\ninteract with dynamic entities like people and vehicles. Crafting realistic,\ninteractive 3D urban environments is essential for nurturing AGI systems and\nconstructing AI agents capable of perceiving, decision-making, and acting like\nhumans in real-world environments. However, creating high-fidelity 3D urban\nenvironments usually entails extensive manual labor from designers, involving\nintricate detailing and representation of complex urban elements. Therefore,\naccomplishing this automatically remains a longstanding challenge. Toward this\nproblem, we propose UrbanWorld, the first generative urban world model that can\nautomatically create a customized, realistic and interactive 3D urban world\nwith flexible control conditions. UrbanWorld incorporates four key stages in\nthe generation pipeline: flexible 3D layout generation from OSM data or urban\nlayout with semantic and height maps, urban scene design with Urban MLLM,\ncontrollable urban asset rendering via progressive 3D diffusion, and\nMLLM-assisted scene refinement. We conduct extensive quantitative analysis on\nfive visual metrics, demonstrating that UrbanWorld achieves SOTA generation\nrealism. Next, we provide qualitative results about the controllable generation\ncapabilities of UrbanWorld using both textual and image-based prompts. Lastly,\nwe verify the interactive nature of these environments by showcasing the agent\nperception and navigation within the created environments. We contribute\nUrbanWorld as an open-source tool available at\nhttps://github.com/Urban-World/UrbanWorld.\n","authors":["Yu Shang","Yuming Lin","Yu Zheng","Hangyu Fan","Jingtao Ding","Jie Feng","Jiansheng Chen","Li Tian","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2407.11965v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2406.05723v2","updated":"2024-10-22T08:28:13Z","published":"2024-06-09T10:30:25Z","title":"Binarized Diffusion Model for Image Super-Resolution","summary":"  Advanced diffusion models (DMs) perform impressively in image\nsuper-resolution (SR), but the high memory and computational costs hinder their\ndeployment. Binarization, an ultra-compression algorithm, offers the potential\nfor effectively accelerating DMs. Nonetheless, due to the model structure and\nthe multi-step iterative attribute of DMs, existing binarization methods result\nin significant performance degradation. In this paper, we introduce a novel\nbinarized diffusion model, BI-DiffSR, for image SR. First, for the model\nstructure, we design a UNet architecture optimized for binarization. We propose\nthe consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up)\nto maintain dimension consistent and facilitate the full-precision information\ntransfer. Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to\nenhance feature fusion in skip connection. Second, for the activation\ndifference across timestep, we design the timestep-aware redistribution (TaR)\nand activation function (TaA). The TaR and TaA dynamically adjust the\ndistribution of activations based on different timesteps, improving the\nflexibility and representation alability of the binarized module. Comprehensive\nexperiments demonstrate that our BI-DiffSR outperforms existing binarization\nmethods. Code is released at: https://github.com/zhengchen1999/BI-DiffSR.\n","authors":["Zheng Chen","Haotong Qin","Yong Guo","Xiongfei Su","Xin Yuan","Linghe Kong","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.05723v2.pdf","comment":"Accepted to NeurIPS 2024. Code is available at\n  https://github.com/zhengchen1999/BI-DiffSR"},{"id":"http://arxiv.org/abs/2410.16802v1","updated":"2024-10-22T08:27:43Z","published":"2024-10-22T08:27:43Z","title":"Evaluating the Effectiveness of Attack-Agnostic Features for Morphing\n  Attack Detection","summary":"  Morphing attacks have diversified significantly over the past years, with new\nmethods based on generative adversarial networks (GANs) and diffusion models\nposing substantial threats to face recognition systems. Recent research has\ndemonstrated the effectiveness of features extracted from large vision models\npretrained on bonafide data only (attack-agnostic features) for detecting deep\ngenerative images. Building on this, we investigate the potential of these\nimage representations for morphing attack detection (MAD). We develop\nsupervised detectors by training a simple binary linear SVM on the extracted\nfeatures and one-class detectors by modeling the distribution of bonafide\nfeatures with a Gaussian Mixture Model (GMM). Our method is evaluated across a\ncomprehensive set of attacks and various scenarios, including generalization to\nunseen attacks, different source datasets, and print-scan data. Our results\nindicate that attack-agnostic features can effectively detect morphing attacks,\noutperforming traditional supervised and one-class detectors from the\nliterature in most scenarios. Additionally, we provide insights into the\nstrengths and limitations of each considered representation and discuss\npotential future research directions to further enhance the robustness and\ngeneralizability of our approach.\n","authors":["Laurent Colbois","Sébastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2410.16802v1.pdf","comment":"Published in the 2024 IEEE International Joint Conference on\n  Biometrics (IJCB)"},{"id":"http://arxiv.org/abs/2410.16794v1","updated":"2024-10-22T08:17:20Z","published":"2024-10-22T08:17:20Z","title":"One-Step Diffusion Distillation through Score Implicit Matching","summary":"  Despite their strong performances on many generative tasks, diffusion models\nrequire a large number of sampling steps in order to generate realistic\nsamples. This has motivated the community to develop effective methods to\ndistill pre-trained diffusion models into more efficient models, but these\nmethods still typically require few-step inference or perform substantially\nworse than the underlying model. In this paper, we present Score Implicit\nMatching (SIM) a new approach to distilling pre-trained diffusion models into\nsingle-step generator models, while maintaining almost the same sample\ngeneration ability as the original model as well as being data-free with no\nneed of training samples for distillation. The method rests upon the fact that,\nalthough the traditional score-based loss is intractable to minimize for\ngenerator models, under certain conditions we can efficiently compute the\ngradients for a wide class of score-based divergences between a diffusion model\nand a generator. SIM shows strong empirical performances for one-step\ngenerators: on the CIFAR10 dataset, it achieves an FID of 2.06 for\nunconditional generation and 1.96 for class-conditional generation. Moreover,\nby applying SIM to a leading transformer-based diffusion model, we distill a\nsingle-step generator for text-to-image (T2I) generation that attains an\naesthetic score of 6.42 with no performance decline over the original\nmulti-step counterpart, clearly outperforming the other one-step generators\nincluding SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85. We\nwill release this industry-ready one-step transformer-based T2I generator along\nwith this paper.\n","authors":["Weijian Luo","Zemin Huang","Zhengyang Geng","J. Zico Kolter","Guo-jun Qi"],"pdf_url":"https://arxiv.org/pdf/2410.16794v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.00432v2","updated":"2024-10-22T07:59:51Z","published":"2024-06-01T13:10:43Z","title":"Localize, Understand, Collaborate: Semantic-Aware Dragging via Intention\n  Reasoner","summary":"  Flexible and accurate drag-based editing is a challenging task that has\nrecently garnered significant attention. Current methods typically model this\nproblem as automatically learning \"how to drag\" through point dragging and\noften produce one deterministic estimation, which presents two key limitations:\n1) Overlooking the inherently ill-posed nature of drag-based editing, where\nmultiple results may correspond to a given input, as illustrated in Fig.1; 2)\nIgnoring the constraint of image quality, which may lead to unexpected\ndistortion. To alleviate this, we propose LucidDrag, which shifts the focus\nfrom \"how to drag\" to \"what-then-how\" paradigm. LucidDrag comprises an\nintention reasoner and a collaborative guidance sampling mechanism. The former\ninfers several optimal editing strategies, identifying what content and what\nsemantic direction to be edited. Based on the former, the latter addresses \"how\nto drag\" by collaboratively integrating existing editing guidance with the\nnewly proposed semantic guidance and quality guidance. Specifically, semantic\nguidance is derived by establishing a semantic editing direction based on\nreasoned intentions, while quality guidance is achieved through classifier\nguidance using an image fidelity discriminator. Both qualitative and\nquantitative comparisons demonstrate the superiority of LucidDrag over previous\nmethods.\n","authors":["Xing Cui","Peipei Li","Zekun Li","Xuannan Liu","Yueying Zou","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2406.00432v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.12214v3","updated":"2024-10-22T07:58:01Z","published":"2024-06-18T02:28:02Z","title":"Is Your HD Map Constructor Reliable under Sensor Corruptions?","summary":"  Driving systems often rely on high-definition (HD) maps for precise\nenvironmental information, which is crucial for planning and navigation. While\ncurrent HD map constructors perform well under ideal conditions, their\nresilience to real-world challenges, \\eg, adverse weather and sensor failures,\nis not well understood, raising safety concerns. This work introduces MapBench,\nthe first comprehensive benchmark designed to evaluate the robustness of HD map\nconstruction methods against various sensor corruptions. Our benchmark\nencompasses a total of 29 types of corruptions that occur from cameras and\nLiDAR sensors. Extensive evaluations across 31 HD map constructors reveal\nsignificant performance degradation of existing methods under adverse weather\nconditions and sensor failures, underscoring critical safety concerns. We\nidentify effective strategies for enhancing robustness, including innovative\napproaches that leverage multi-modal fusion, advanced data augmentation, and\narchitectural techniques. These insights provide a pathway for developing more\nreliable HD map construction methods, which are essential for the advancement\nof autonomous driving technology. The benchmark toolkit and affiliated code and\nmodel checkpoints have been made publicly accessible.\n","authors":["Xiaoshuai Hao","Mengchuan Wei","Yifan Yang","Haimei Zhao","Hui Zhang","Yi Zhou","Qiang Wang","Weiming Li","Lingdong Kong","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.12214v3.pdf","comment":"NeurIPS 2024; 40 pages, 17 figures, 23 tables; Code at\n  https://mapbench.github.io/"},{"id":"http://arxiv.org/abs/2211.12702v2","updated":"2024-10-22T07:57:55Z","published":"2022-11-23T04:48:49Z","title":"Evaluating Feature Attribution Methods for Electrocardiogram","summary":"  The performance of cardiac arrhythmia detection with electrocardiograms(ECGs)\nhas been considerably improved since the introduction of deep learning models.\nIn practice, the high performance alone is not sufficient and a proper\nexplanation is also required. Recently, researchers have started adopting\nfeature attribution methods to address this requirement, but it has been\nunclear which of the methods are appropriate for ECG. In this work, we identify\nand customize three evaluation metrics for feature attribution methods based on\nthe characteristics of ECG: localization score, pointing game, and degradation\nscore. Using the three evaluation metrics, we evaluate and analyze eleven\nwidely-used feature attribution methods. We find that some of the feature\nattribution methods are much more adequate for explaining ECG, where Grad-CAM\noutperforms the second-best method by a large margin.\n","authors":["Jangwon Suh","Jimyeong Kim","Euna Jung","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2211.12702v2.pdf","comment":"This is preliminary research related to\n  https://www.sciencedirect.com/science/article/pii/S0010482524011739 . Code is\n  available at https://github.com/SNU-DRL/Attribution-ECG"},{"id":"http://arxiv.org/abs/2408.10787v3","updated":"2024-10-22T07:51:43Z","published":"2024-08-20T12:27:53Z","title":"A Lightweight Modular Framework for Low-Cost Open-Vocabulary Object\n  Detection Training","summary":"  Object detection is a fundamental challenge in computer vision, centered on\nrecognizing objects within images, with diverse applications in areas like\nimage analysis, robotics, and autonomous vehicles. Although existing methods\nhave achieved great success, they are often constrained by a fixed vocabulary\nof objects. To overcome this limitation, approaches like MDETR have redefined\nobject detection by incorporating region-level vision-language pre-training,\nenabling open-vocabulary object detectors. However, these methods are\ncomputationally heavy due to the simultaneous training of large models for both\nvision and language representations. To address this, we introduce a\nlightweight framework that significantly reduces the number of parameters while\npreserving, or even improving, performance. Our solution is applied to MDETR,\nresulting in the development of Lightweight MDETR (LightMDETR), an optimized\nversion of MDETR designed to enhance computational efficiency without\nsacrificing accuracy. The core of our approach involves freezing the MDETR\nbackbone and training only the Universal Projection module (UP), which bridges\nvision and language representations. A learnable modality token parameter\nallows the UP to seamlessly switch between modalities. Evaluations on tasks\nlike phrase grounding, referring expression comprehension, and segmentation\nshow that LightMDETR not only reduces computational costs but also outperforms\nseveral state-of-the-art methods in terms of accuracy.\n","authors":["Bilal Faye","Binta Sow","Hanane Azzag","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2408.10787v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16770v1","updated":"2024-10-22T07:40:20Z","published":"2024-10-22T07:40:20Z","title":"The Scene Language: Representing Scenes with Programs, Words, and\n  Embeddings","summary":"  We introduce the Scene Language, a visual scene representation that concisely\nand precisely describes the structure, semantics, and identity of visual\nscenes. It represents a scene with three key components: a program that\nspecifies the hierarchical and relational structure of entities in the scene,\nwords in natural language that summarize the semantic class of each entity, and\nembeddings that capture the visual identity of each entity. This representation\ncan be inferred from pre-trained language models via a training-free inference\ntechnique, given text or image inputs. The resulting scene can be rendered into\nimages using traditional, neural, or hybrid graphics renderers. Together, this\nforms a robust, automated system for high-quality 3D and 4D scene generation.\nCompared with existing representations like scene graphs, our proposed Scene\nLanguage generates complex scenes with higher fidelity, while explicitly\nmodeling the scene structures to enable precise control and editing.\n","authors":["Yunzhi Zhang","Zizhang Li","Matt Zhou","Shangzhe Wu","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2410.16770v1.pdf","comment":"Project page:\n  https://ai.stanford.edu/~yzzhang/projects/scene-language/"},{"id":"http://arxiv.org/abs/2004.04454v2","updated":"2024-10-22T07:40:17Z","published":"2020-04-09T09:52:49Z","title":"TensorProjection Layer: A Tensor-Based Dimension Reduction Method in\n  Deep Neural Networks","summary":"  In this paper, we propose a dimension reduction method specifically designed\nfor tensor-structured feature data in deep neural networks. The method is\nimplemented as a hidden layer, called the TensorProjection layer, which\ntransforms input tensors into output tensors with reduced dimensions through\nmode-wise projections. The projection directions are treated as model\nparameters of the layer and are optimized during model training. Our method can\nserve as an alternative to pooling layers for summarizing image data, or to\nconvolutional layers as a technique for reducing the number of channels. We\nconduct experiments on tasks such as medical image classification and\nsegmentation, integrating the TensorProjection layer into commonly used\nbaseline architectures to evaluate its effectiveness. Numerical experiments\nindicate that the proposed method can outperform traditional downsampling\nmethods, such as pooling layers, in our tasks, suggesting it as a promising\nalternative for feature summarization.\n","authors":["Toshinari Morimoto","Su-Yun Huang"],"pdf_url":"https://arxiv.org/pdf/2004.04454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16769v1","updated":"2024-10-22T07:37:47Z","published":"2024-10-22T07:37:47Z","title":"DSORT-MCU: Detecting Small Objects in Real-Time on Microcontroller Units","summary":"  Advances in lightweight neural networks have revolutionized computer vision\nin a broad range of IoT applications, encompassing remote monitoring and\nprocess automation. However, the detection of small objects, which is crucial\nfor many of these applications, remains an underexplored area in current\ncomputer vision research, particularly for low-power embedded devices that host\nresource-constrained processors. To address said gap, this paper proposes an\nadaptive tiling method for lightweight and energy-efficient object detection\nnetworks, including YOLO-based models and the popular FOMO network. The\nproposed tiling enables object detection on low-power MCUs with no compromise\non accuracy compared to large-scale detection models. The benefit of the\nproposed method is demonstrated by applying it to FOMO and TinyissimoYOLO\nnetworks on a novel RISC-V-based MCU with built-in ML accelerators. Extensive\nexperimental results show that the proposed tiling method boosts the F1-score\nby up to 225% for both FOMO and TinyissimoYOLO networks while reducing the\naverage object count error by up to 76% with FOMO and up to 89% for\nTinyissimoYOLO. Furthermore, the findings of this work indicate that using a\nsoft F1 loss over the popular binary cross-entropy loss can serve as an\nimplicit non-maximum suppression for the FOMO network. To evaluate the\nreal-world performance, the networks are deployed on the RISC-V based GAP9\nmicrocontroller from GreenWaves Technologies, showcasing the proposed method's\nability to strike a balance between detection performance ($58% - 95%$ F1\nscore), low latency (0.6 ms/Inference - 16.2 ms/Inference}), and energy\nefficiency (31 uJ/Inference} - 1.27 mJ/Inference) while performing multiple\npredictions using high-resolution images on a MCU.\n","authors":["Liam Boyle","Julian Moosmann","Nicolas Baumann","Seonyeong Heo","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2410.16769v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.07163"},{"id":"http://arxiv.org/abs/2306.16759v3","updated":"2024-10-22T07:29:07Z","published":"2023-06-29T07:55:43Z","title":"Boosting the Generalization Ability for Hyperspectral Image\n  Classification using Spectral-spatial Axial Aggregation Transformer","summary":"  In the hyperspectral image classification (HSIC) task, the most commonly used\nmodel validation paradigm is partitioning the training-test dataset through\npixel-wise random sampling. By training on a small amount of data, the deep\nlearning model can achieve almost perfect accuracy. However, in our\nexperiments, we found that the high accuracy was reached because the training\nand test datasets share a lot of information. On non-overlapping dataset\npartitions, well-performing models suffer significant performance degradation.\nTo this end, we propose a spectral-spatial axial aggregation transformer model,\nnamely SaaFormer, that preserves generalization across dataset partitions.\nSaaFormer applies a multi-level spectral extraction structure to segment the\nspectrum into multiple spectrum clips, such that the wavelength continuity of\nthe spectrum across the channel are preserved. For each spectrum clip, the\naxial aggregation attention mechanism, which integrates spatial features along\nmultiple spectral axes is applied to mine the spectral characteristic. The\nmulti-level spectral extraction and the axial aggregation attention emphasize\nspectral characteristic to improve the model generalization. The experimental\nresults on five publicly available datasets demonstrate that our model exhibits\ncomparable performance on the random partition, while significantly\noutperforming other methods on non-overlapping partitions. Moreover, SaaFormer\nshows excellent performance on background classification.\n","authors":["Enzhe Zhao","Zhichang Guo","Shengzhu Shi","Yao Li","Jia Li","Dazhi Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.16759v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2107.02988 by other authors"},{"id":"http://arxiv.org/abs/2410.04183v2","updated":"2024-10-22T07:24:05Z","published":"2024-10-05T14:57:52Z","title":"Unsupervised Assessment of Landscape Shifts Based on Persistent Entropy\n  and Topological Preservation","summary":"  In Continual Learning (CL) contexts, concept drift typically refers to the\nanalysis of changes in data distribution. A drift in the input data can have\nnegative consequences on a learning predictor and the system's stability. The\nmajority of concept drift methods emphasize the analysis of statistical changes\nin non-stationary data over time. In this context, we consider another\nperspective, where the concept drift also integrates substantial changes in the\ntopological characteristics of the data stream. In this article, we introduce a\nnovel framework for monitoring changes in multi-dimensional data streams. We\nexplore variations in the topological structures of the data, presenting\nanother angle on the standard concept drift. Our developed approach is based on\npersistent entropy and topology-preserving projections in a continual learning\nscenario. The framework operates in both unsupervised and supervised\nenvironments. To show the utility of the proposed framework, we analyze the\nmodel across three scenarios using data streams generated with MNIST samples.\nThe obtained results reveal the potential of applying topological data analysis\nfor shift detection and encourage further research in this area.\n","authors":["Sebastian Basterrech"],"pdf_url":"https://arxiv.org/pdf/2410.04183v2.pdf","comment":"KDD'2024. Workshop on Drift Detection and Landscape Shifts"},{"id":"http://arxiv.org/abs/2401.06960v2","updated":"2024-10-22T07:17:47Z","published":"2024-01-13T03:17:57Z","title":"Transformer for Object Re-Identification: A Survey","summary":"  Object Re-identification (Re-ID) aims to identify specific objects across\ndifferent times and scenes, which is a widely researched task in computer\nvision. For a prolonged period, this field has been predominantly driven by\ndeep learning technology based on convolutional neural networks. In recent\nyears, the emergence of Vision Transformers has spurred a growing number of\nstudies delving deeper into Transformer-based Re-ID, continuously breaking\nperformance records and witnessing significant progress in the Re-ID field.\nOffering a powerful, flexible, and unified solution, Transformers cater to a\nwide array of Re-ID tasks with unparalleled efficacy. This paper provides a\ncomprehensive review and in-depth analysis of the Transformer-based Re-ID. In\ncategorizing existing works into Image/Video-Based Re-ID, Re-ID with limited\ndata/annotations, Cross-Modal Re-ID, and Special Re-ID Scenarios, we thoroughly\nelucidate the advantages demonstrated by the Transformer in addressing a\nmultitude of challenges across these domains. Considering the trending\nunsupervised Re-ID, we propose a new Transformer baseline, UntransReID,\nachieving state-of-the-art performance on both single/cross modal tasks. For\nthe under-explored animal Re-ID, we devise a standardized experimental\nbenchmark and conduct extensive experiments to explore the applicability of\nTransformer for this task and facilitate future research. Finally, we discuss\nsome important yet under-investigated open issues in the large foundation model\nera, we believe it will serve as a new handbook for researchers in this field.\nA periodically updated website will be available at\nhttps://github.com/mangye16/ReID-Survey.\n","authors":["Mang Ye","Shuoyi Chen","Chenyue Li","Wei-Shi Zheng","David Crandall","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2401.06960v2.pdf","comment":"Accepted by International Journal of Computer Vision (IJCV) in\n  October 2024"},{"id":"http://arxiv.org/abs/2410.16746v1","updated":"2024-10-22T07:00:43Z","published":"2024-10-22T07:00:43Z","title":"SpikMamba: When SNN meets Mamba in Event-based Human Action Recognition","summary":"  Human action recognition (HAR) plays a key role in various applications such\nas video analysis, surveillance, autonomous driving, robotics, and healthcare.\nMost HAR algorithms are developed from RGB images, which capture detailed\nvisual information. However, these algorithms raise concerns in\nprivacy-sensitive environments due to the recording of identifiable features.\nEvent cameras offer a promising solution by capturing scene brightness changes\nsparsely at the pixel level, without capturing full images. Moreover, event\ncameras have high dynamic ranges that can effectively handle scenarios with\ncomplex lighting conditions, such as low light or high contrast environments.\nHowever, using event cameras introduces challenges in modeling the spatially\nsparse and high temporal resolution event data for HAR. To address these\nissues, we propose the SpikMamba framework, which combines the energy\nefficiency of spiking neural networks and the long sequence modeling capability\nof Mamba to efficiently capture global features from spatially sparse and high\na temporal resolution event data. Additionally, to improve the locality of\nmodeling, a spiking window-based linear attention mechanism is used. Extensive\nexperiments show that SpikMamba achieves remarkable recognition performance,\nsurpassing the previous state-of-the-art by 1.45%, 7.22%, 0.15%, and 3.92% on\nthe PAF, HARDVS, DVS128, and E-FAction datasets, respectively. The code is\navailable at https://github.com/Typistchen/SpikMamba.\n","authors":["Jiaqi Chen","Yan Yang","Shizhuo Deng","Da Teng","Liyuan Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16746v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.16744v1","updated":"2024-10-22T06:58:37Z","published":"2024-10-22T06:58:37Z","title":"Time-Resolved MNIST Dataset for Single-Photon Recognition","summary":"  Time-resolved single photon imaging is a promising imaging modality\ncharacterized by the unique capability of timestamping the arrivals of single\nphotons. Single-Photon Avalanche Diodes (SPADs) are the leading technology for\nimplementing modern time-resolved pixels, suitable for passive imaging with\nasynchronous readout. However, they are currently limited to small sized\narrays, thus there is a lack of datasets for passive time-resolved SPAD\nimaging, which in turn hinders research on this peculiar imaging data. In this\npaper we describe a realistic simulation process for SPAD imaging, which takes\ninto account both the stochastic nature of photon arrivals and all the noise\nsources involved in the acquisition process of time-resolved SPAD arrays. We\nhave implemented this simulator in a software prototype able to generate\narbitrary-sized time-resolved SPAD arrays operating in passive mode. Starting\nfrom a reference image, our simulator generates a realistic stream of\ntimestamped photon detections. We use our simulator to generate a time-resolved\nversion of MNIST, which we make publicly available. Our dataset has the purpose\nof encouraging novel research directions in time-resolved SPAD imaging, as well\nas investigating the performance of CNN classifiers in extremely low-light\nconditions.\n","authors":["Aleksi Suonsivu","Lauri Salmela","Edoardo Peretti","Leevi Uosukainen","Radu Ciprian Bilcu","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2410.16744v1.pdf","comment":"12 pages, 4 figures. Accepted for Workshop on Synthetic Data for\n  Computer Vision at ECCV 2024"},{"id":"http://arxiv.org/abs/2212.04681v3","updated":"2024-10-22T06:56:00Z","published":"2022-12-09T06:06:47Z","title":"Dynamic Test-Time Augmentation via Differentiable Functions","summary":"  Distribution shifts, which often occur in the real world, degrade the\naccuracy of deep learning systems, and thus improving robustness to\ndistribution shifts is essential for practical applications. To improve\nrobustness, we study an image enhancement method that generates\nrecognition-friendly images without retraining the recognition model. We\npropose a novel image enhancement method, DynTTA, which is based on\ndifferentiable data augmentation techniques and generates a blended image from\nmany augmented images to improve the recognition accuracy under distribution\nshifts. In addition to standard data augmentations, DynTTA also incorporates\ndeep neural network-based image transformation, further improving the\nrobustness. Because DynTTA is composed of differentiable functions, it can be\ndirectly trained with the classification loss of the recognition model. In\nexperiments with widely used image recognition datasets using various\nclassification models, DynTTA improves the robustness with almost no reduction\nin classification accuracy for clean images, thus outperforming the existing\nmethods. Furthermore, the results show that robustness is significantly\nimproved by estimating the training-time augmentations for distribution-shifted\ndatasets using DynTTA and retraining the recognition model with the estimated\naugmentations. DynTTA is a promising approach for applications that require\nboth clean accuracy and robustness. Our code is available at\n\\url{https://github.com/s-enmt/DynTTA}.\n","authors":["Shohei Enomoto","Monikka Roslianna Busto","Takeharu Eda"],"pdf_url":"https://arxiv.org/pdf/2212.04681v3.pdf","comment":"IEEE Access"},{"id":"http://arxiv.org/abs/2407.11664v2","updated":"2024-10-22T06:41:38Z","published":"2024-07-16T12:36:26Z","title":"Mask-guided cross-image attention for zero-shot in-silico\n  histopathologic image generation with a diffusion model","summary":"  Creating in-silico data with generative AI promises a cost-effective\nalternative to staining, imaging, and annotating whole slide images in\ncomputational pathology. Diffusion models are the state-of-the-art solution for\ngenerating in-silico images, offering unparalleled fidelity and realism. Using\nappearance transfer diffusion models allows for zero-shot image generation,\nfacilitating fast application and making model training unnecessary. However\ncurrent appearance transfer diffusion models are designed for natural images,\nwhere the main task is to transfer the foreground object from an origin to a\ntarget domain, while the background is of insignificant importance. In\ncomputational pathology, specifically in oncology, it is however not\nstraightforward to define which objects in an image should be classified as\nforeground and background, as all objects in an image may be of critical\nimportance for the detailed understanding the tumor micro-environment. We\ncontribute to the applicability of appearance transfer diffusion models to\nimmunohistochemistry-stained images by modifying the appearance transfer\nguidance to alternate between class-specific AdaIN feature statistics matchings\nusing existing segmentation masks. The performance of the proposed method is\ndemonstrated on the downstream task of supervised epithelium segmentation,\nshowing that the number of manual annotations required for model training can\nbe reduced by 75%, outperforming the baseline approach. Additionally, we\nconsulted with a certified pathologist to investigate future improvements. We\nanticipate this work to inspire the application of zero-shot diffusion models\nin computational pathology, providing an efficient method to generate in-silico\nimages with unmatched fidelity and realism, which prove meaningful for\ndownstream tasks, such as training existing deep learning models or finetuning\nfoundation models.\n","authors":["Dominik Winter","Nicolas Triltsch","Marco Rosati","Anatoliy Shumilov","Ziya Kokaragac","Yuri Popov","Thomas Padel","Laura Sebastian Monasor","Ross Hill","Markus Schick","Nicolas Brieu"],"pdf_url":"https://arxiv.org/pdf/2407.11664v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2410.16732v1","updated":"2024-10-22T06:30:37Z","published":"2024-10-22T06:30:37Z","title":"Polyp-E: Benchmarking the Robustness of Deep Segmentation Models via\n  Polyp Editing","summary":"  Automatic polyp segmentation is helpful to assist clinical diagnosis and\ntreatment. In daily clinical practice, clinicians exhibit robustness in\nidentifying polyps with both location and size variations. It is uncertain if\ndeep segmentation models can achieve comparable robustness in automated\ncolonoscopic analysis. To benchmark the model robustness, we focus on\nevaluating the robustness of segmentation models on the polyps with various\nattributes (e.g. location and size) and healthy samples. Based on the Latent\nDiffusion Model, we perform attribute editing on real polyps and build a new\ndataset named Polyp-E. Our synthetic dataset boasts exceptional realism, to the\nextent that clinical experts find it challenging to discern them from real\ndata. We evaluate several existing polyp segmentation models on the proposed\nbenchmark. The results reveal most of the models are highly sensitive to\nattribute variations. As a novel data augmentation technique, the proposed\nediting pipeline can improve both in-distribution and out-of-distribution\ngeneralization ability. The code and datasets will be released.\n","authors":["Runpu Wei","Zijin Yin","Kongming Liang","Min Min","Chengwei Pan","Gang Yu","Haonan Huang","Yan Liu","Zhanyu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.16732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16719v1","updated":"2024-10-22T05:59:29Z","published":"2024-10-22T05:59:29Z","title":"Progressive Compositionality In Text-to-Image Generative Models","summary":"  Despite the impressive text-to-image (T2I) synthesis capabilities of\ndiffusion models, they often struggle to understand compositional relationships\nbetween objects and attributes, especially in complex settings. Existing\nsolutions have tackled these challenges by optimizing the cross-attention\nmechanism or learning from the caption pairs with minimal semantic changes.\nHowever, can we generate high-quality complex contrastive images that diffusion\nmodels can directly discriminate based on visual representations? In this work,\nwe leverage large-language models (LLMs) to compose realistic, complex\nscenarios and harness Visual-Question Answering (VQA) systems alongside\ndiffusion models to automatically curate a contrastive dataset, ConPair,\nconsisting of 15k pairs of high-quality contrastive images. These pairs feature\nminimal visual discrepancies and cover a wide range of attribute categories,\nespecially complex and natural scenarios. To learn effectively from these error\ncases, i.e., hard negative images, we propose EvoGen, a new multi-stage\ncurriculum for contrastive learning of diffusion models. Through extensive\nexperiments across a wide range of compositional scenarios, we showcase the\neffectiveness of our proposed framework on compositional T2I benchmarks.\n","authors":["Xu Han","Linghao Jin","Xiaofeng Liu","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2410.16719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16711v1","updated":"2024-10-22T05:37:51Z","published":"2024-10-22T05:37:51Z","title":"Development of CNN Architectures using Transfer Learning Methods for\n  Medical Image Classification","summary":"  The application of deep learning-based architecture has seen a tremendous\nrise in recent years. For example, medical image classification using deep\nlearning achieved breakthrough results. Convolutional Neural Networks (CNNs)\nare implemented predominantly in medical image classification and segmentation.\nOn the other hand, transfer learning has emerged as a prominent supporting tool\nfor enhancing the efficiency and accuracy of deep learning models. This paper\ninvestigates the development of CNN architectures using transfer learning\ntechniques in the field of medical image classification using a timeline\nmapping model for key image classification challenges. Our findings help make\nan informed decision while selecting the optimum and state-of-the-art CNN\narchitectures.\n","authors":["Ganga Prasad Basyal","David Zeng","Bhaskar Pm Rimal"],"pdf_url":"https://arxiv.org/pdf/2410.16711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16707v1","updated":"2024-10-22T05:22:49Z","published":"2024-10-22T05:22:49Z","title":"DI-MaskDINO: A Joint Object Detection and Instance Segmentation Model","summary":"  This paper is motivated by an interesting phenomenon: the performance of\nobject detection lags behind that of instance segmentation (i.e., performance\nimbalance) when investigating the intermediate results from the beginning\ntransformer decoder layer of MaskDINO (i.e., the SOTA model for joint detection\nand segmentation). This phenomenon inspires us to think about a question: will\nthe performance imbalance at the beginning layer of transformer decoder\nconstrain the upper bound of the final performance? With this question in mind,\nwe further conduct qualitative and quantitative pre-experiments, which validate\nthe negative impact of detection-segmentation imbalance issue on the model\nperformance. To address this issue, this paper proposes DI-MaskDINO model, the\ncore idea of which is to improve the final performance by alleviating the\ndetection-segmentation imbalance. DI-MaskDINO is implemented by configuring our\nproposed De-Imbalance (DI) module and Balance-Aware Tokens Optimization (BATO)\nmodule to MaskDINO. DI is responsible for generating balance-aware query, and\nBATO uses the balance-aware query to guide the optimization of the initial\nfeature tokens. The balance-aware query and optimized feature tokens are\nrespectively taken as the Query and Key&Value of transformer decoder to perform\njoint object detection and instance segmentation. DI-MaskDINO outperforms\nexisting joint object detection and instance segmentation models on COCO and\nBDD100K benchmarks, achieving +1.2 $AP^{box}$ and +0.9 $AP^{mask}$ improvements\ncompared to SOTA joint detection and segmentation model MaskDINO. In addition,\nDI-MaskDINO also obtains +1.0 $AP^{box}$ improvement compared to SOTA object\ndetection model DINO and +3.0 $AP^{mask}$ improvement compared to SOTA\nsegmentation model Mask2Former.\n","authors":["Zhixiong Nan","Xianghong Li","Tao Xiang","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2410.16707v1.pdf","comment":"16 pages, 3 figures, Conference on Neural Information Processing\n  Systems"},{"id":"http://arxiv.org/abs/2410.16695v1","updated":"2024-10-22T04:57:28Z","published":"2024-10-22T04:57:28Z","title":"MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark","summary":"  Phytoplankton are a crucial component of aquatic ecosystems, and effective\nmonitoring of them can provide valuable insights into ocean environments and\necosystem changes. Traditional phytoplankton monitoring methods are often\ncomplex and lack timely analysis. Therefore, deep learning algorithms offer a\npromising approach for automated phytoplankton monitoring. However, the lack of\nlarge-scale, high-quality training samples has become a major bottleneck in\nadvancing phytoplankton tracking. In this paper, we propose a challenging\nbenchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse\nbackground information and variations in motion during observation. The dataset\nincludes 27 species of phytoplankton and zooplankton, 14 different backgrounds\nto simulate diverse and complex underwater environments, and a total of 140\nvideos. To enable accurate real-time observation of phytoplankton, we introduce\na multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion\nTracker(DSFT), which addresses issues such as focus shifts during tracking and\nthe loss of small target information when computing frame-to-frame similarity.\nSpecifically, we introduce an additional feature extractor to predict the\nresiduals of the standard feature extractor's output, and compute multi-scale\nframe-to-frame similarity based on features from different layers of the\nextractor. Extensive experiments on the MPT have demonstrated the validity of\nthe dataset and the superiority of DSFT in tracking phytoplankton, providing an\neffective solution for phytoplankton monitoring.\n","authors":["Yang Yu","Yuezun Li","Xin Sun","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2410.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05338v6","updated":"2024-10-22T04:22:16Z","published":"2024-06-08T03:44:25Z","title":"MotionClone: Training-Free Motion Cloning for Controllable Video\n  Generation","summary":"  Motion-based controllable video generation offers the potential for creating\ncaptivating visual content. Existing methods typically necessitate model\ntraining to encode particular motion cues or incorporate fine-tuning to inject\ncertain motion patterns, resulting in limited flexibility and generalization.\nIn this work, we propose MotionClone, a training-free framework that enables\nmotion cloning from reference videos to versatile motion-controlled video\ngeneration, including text-to-video and image-to-video. Based on the\nobservation that the dominant components in temporal-attention maps drive\nmotion synthesis, while the rest mainly capture noisy or very subtle motions,\nMotionClone utilizes sparse temporal attention weights as motion\nrepresentations for motion guidance, facilitating diverse motion transfer\nacross varying scenarios. Meanwhile, MotionClone allows for the direct\nextraction of motion representation through a single denoising step, bypassing\nthe cumbersome inversion processes and thus promoting both efficiency and\nflexibility. Extensive experiments demonstrate that MotionClone exhibits\nproficiency in both global camera motion and local object motion, with notable\nsuperiority in terms of motion fidelity, textual alignment, and temporal\nconsistency.\n","authors":["Pengyang Ling","Jiazi Bu","Pan Zhang","Xiaoyi Dong","Yuhang Zang","Tong Wu","Huaian Chen","Jiaqi Wang","Yi Jin"],"pdf_url":"https://arxiv.org/pdf/2406.05338v6.pdf","comment":"18 pages, 14 figures,\n  https://bujiazi.github.io/motionclone.github.io/"},{"id":"http://arxiv.org/abs/2410.16671v1","updated":"2024-10-22T04:03:36Z","published":"2024-10-22T04:03:36Z","title":"NucleiMix: Realistic Data Augmentation for Nuclei Instance Segmentation","summary":"  Nuclei instance segmentation is an essential task in pathology image\nanalysis, serving as the foundation for many downstream applications. The\nrelease of several public datasets has significantly advanced research in this\narea, yet many existing methods struggle with data imbalance issues. To address\nthis challenge, this study introduces a data augmentation method, called\nNucleiMix, which is designed to balance the distribution of nuclei types by\nincreasing the number of rare-type nuclei within datasets. NucleiMix operates\nin two phases. In the first phase, it identifies candidate locations similar to\nthe surroundings of rare-type nuclei and inserts rare-type nuclei into the\ncandidate locations. In the second phase, it employs a progressive inpainting\nstrategy using a pre-trained diffusion model to seamlessly integrate rare-type\nnuclei into their new environments in replacement of major-type nuclei or\nbackground locations. We systematically evaluate the effectiveness of NucleiMix\non three public datasets using two popular nuclei instance segmentation models.\nThe results demonstrate the superior ability of NucleiMix to synthesize\nrealistic rare-type nuclei and to enhance the quality of nuclei segmentation\nand classification in an accurate and robust manner.\n","authors":["Jiamu Wang","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2410.16671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04833v4","updated":"2024-10-22T03:41:08Z","published":"2024-07-05T19:38:10Z","title":"3D Adaptive Structural Convolution Network for Domain-Invariant Point\n  Cloud Recognition","summary":"  Adapting deep learning networks for point cloud data recognition in\nself-driving vehicles faces challenges due to the variability in datasets and\nsensor technologies, emphasizing the need for adaptive techniques to maintain\naccuracy across different conditions. In this paper, we introduce the 3D\nAdaptive Structural Convolution Network (3D-ASCN), a cutting-edge framework for\n3D point cloud recognition. It combines 3D convolution kernels, a structural\ntree structure, and adaptive neighborhood sampling for effective geometric\nfeature extraction. This method obtains domain-invariant features and\ndemonstrates robust, adaptable performance on a variety of point cloud\ndatasets, ensuring compatibility across diverse sensor configurations without\nthe need for parameter adjustments. This highlights its potential to\nsignificantly enhance the reliability and efficiency of self-driving vehicle\ntechnology.\n","authors":["Younggun Kim","Beomsik Cho","Seonghoon Ryoo","Soomok Lee"],"pdf_url":"https://arxiv.org/pdf/2407.04833v4.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.00664v2","updated":"2024-10-22T03:34:43Z","published":"2024-06-30T11:22:36Z","title":"SCMIL: Sparse Context-aware Multiple Instance Learning for Predicting\n  Cancer Survival Probability Distribution in Whole Slide Images","summary":"  Cancer survival prediction is a challenging task that involves analyzing of\nthe tumor microenvironment within Whole Slide Image (WSI). Previous methods\ncannot effectively capture the intricate interaction features among instances\nwithin the local area of WSI. Moreover, existing methods for cancer survival\nprediction based on WSI often fail to provide better clinically meaningful\npredictions. To overcome these challenges, we propose a Sparse Context-aware\nMultiple Instance Learning (SCMIL) framework for predicting cancer survival\nprobability distributions. SCMIL innovatively segments patches into various\nclusters based on their morphological features and spatial location\ninformation, subsequently leveraging sparse self-attention to discern the\nrelationships between these patches with a context-aware perspective.\nConsidering many patches are irrelevant to the task, we introduce a learnable\npatch filtering module called SoftFilter, which ensures that only interactions\nbetween task-relevant patches are considered. To enhance the clinical relevance\nof our prediction, we propose a register-based mixture density network to\nforecast the survival probability distribution for individual patients. We\nevaluate SCMIL on two public WSI datasets from the The Cancer Genome Atlas\n(TCGA) specifically focusing on lung adenocarcinom (LUAD) and kidney renal\nclear cell carcinoma (KIRC). Our experimental results indicate that SCMIL\noutperforms current state-of-the-art methods for survival prediction, offering\nmore clinically meaningful and interpretable outcomes. Our code is accessible\nat https://github.com/yang-ze-kang/SCMIL.\n","authors":["Zekang Yang","Hong Liu","Xiangdong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.00664v2.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2410.16662v1","updated":"2024-10-22T03:28:41Z","published":"2024-10-22T03:28:41Z","title":"Visual Question Answering in Ophthalmology: A Progressive and Practical\n  Perspective","summary":"  Accurate diagnosis of ophthalmic diseases relies heavily on the\ninterpretation of multimodal ophthalmic images, a process often time-consuming\nand expertise-dependent. Visual Question Answering (VQA) presents a potential\ninterdisciplinary solution by merging computer vision and natural language\nprocessing to comprehend and respond to queries about medical images. This\nreview article explores the recent advancements and future prospects of VQA in\nophthalmology from both theoretical and practical perspectives, aiming to\nprovide eye care professionals with a deeper understanding and tools for\nleveraging the underlying models. Additionally, we discuss the promising trend\nof large language models (LLM) in enhancing various components of the VQA\nframework to adapt to multimodal ophthalmic tasks. Despite the promising\noutlook, ophthalmic VQA still faces several challenges, including the scarcity\nof annotated multimodal image datasets, the necessity of comprehensive and\nunified evaluation methods, and the obstacles to achieving effective real-world\napplications. This article highlights these challenges and clarifies future\ndirections for advancing ophthalmic VQA with LLMs. The development of LLM-based\nophthalmic VQA systems calls for collaborative efforts between medical\nprofessionals and AI experts to overcome existing obstacles and advance the\ndiagnosis and care of eye diseases.\n","authors":["Xiaolan Chen","Ruoyu Chen","Pusheng Xu","Weiyi Zhang","Xianwen Shang","Mingguang He","Danli Shi"],"pdf_url":"https://arxiv.org/pdf/2410.16662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16657v1","updated":"2024-10-22T03:02:29Z","published":"2024-10-22T03:02:29Z","title":"Dual-Model Defense: Safeguarding Diffusion Models from Membership\n  Inference Attacks through Disjoint Data Splitting","summary":"  Diffusion models have demonstrated remarkable capabilities in image\nsynthesis, but their recently proven vulnerability to Membership Inference\nAttacks (MIAs) poses a critical privacy concern. This paper introduces two\nnovel and efficient approaches (DualMD and DistillMD) to protect diffusion\nmodels against MIAs while maintaining high utility. Both methods are based on\ntraining two separate diffusion models on disjoint subsets of the original\ndataset. DualMD then employs a private inference pipeline that utilizes both\nmodels. This strategy significantly reduces the risk of black-box MIAs by\nlimiting the information any single model contains about individual training\nsamples. The dual models can also generate \"soft targets\" to train a private\nstudent model in DistillMD, enhancing privacy guarantees against all types of\nMIAs. Extensive evaluations of DualMD and DistillMD against state-of-the-art\nMIAs across various datasets in white-box and black-box settings demonstrate\ntheir effectiveness in substantially reducing MIA success rates while\npreserving competitive image generation performance. Notably, our experiments\nreveal that DistillMD not only defends against MIAs but also mitigates model\nmemorization, indicating that both vulnerabilities stem from overfitting and\ncan be addressed simultaneously with our unified approach.\n","authors":["Bao Q. Tran","Viet Nguyen","Anh Tran","Toan Tran"],"pdf_url":"https://arxiv.org/pdf/2410.16657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16646v1","updated":"2024-10-22T02:45:46Z","published":"2024-10-22T02:45:46Z","title":"TopoDiffusionNet: A Topology-aware Diffusion Model","summary":"  Diffusion models excel at creating visually impressive images but often\nstruggle to generate images with a specified topology. The Betti number, which\nrepresents the number of structures in an image, is a fundamental measure in\ntopology. Yet, diffusion models fail to satisfy even this basic constraint.\nThis limitation restricts their utility in applications requiring exact\ncontrol, like robotics and environmental modeling. To address this, we propose\nTopoDiffusionNet (TDN), a novel approach that enforces diffusion models to\nmaintain the desired topology. We leverage tools from topological data\nanalysis, particularly persistent homology, to extract the topological\nstructures within an image. We then design a topology-based objective function\nto guide the denoising process, preserving intended structures while\nsuppressing noisy ones. Our experiments across four datasets demonstrate\nsignificant improvements in topological accuracy. TDN is the first to integrate\ntopology with diffusion models, opening new avenues of research in this area.\n","authors":["Saumya Gupta","Dimitris Samaras","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16646v1.pdf","comment":"20 pages, 11 figures, 7 tables"},{"id":"http://arxiv.org/abs/2309.06118v6","updated":"2024-10-22T02:42:30Z","published":"2023-09-12T10:33:19Z","title":"CHITNet: A Complementary to Harmonious Information Transfer Network for\n  Infrared and Visible Image Fusion","summary":"  Current infrared and visible image fusion (IVIF) methods go to great lengths\nto excavate complementary features and design complex fusion strategies, which\nis extremely challenging. To this end, we rethink the IVIF outside the box,\nproposing a complementary to harmonious information transfer network (CHITNet).\nIt reasonably transfers complementary information into harmonious one, which\nintegrates both the shared and complementary features from two modalities.\nSpecifically, to skillfully sidestep aggregating complementary information in\nIVIF, we design a mutual information transfer (MIT) module to mutually\nrepresent features from two modalities, roughly transferring complementary\ninformation into harmonious one. Then, a harmonious information acquisition\nsupervised by source image (HIASSI) module is devised to further ensure the\ncomplementary to harmonious information transfer after MIT. Meanwhile, we also\npropose a structure information preservation (SIP) module to guarantee that the\nedge structure information of the source images can be transferred to the\nfusion results. Moreover, a mutual promotion training paradigm with interaction\nloss is adopted to facilitate better collaboration among MIT, HIASSI and SIP.\nIn this way, the proposed method is able to generate fused images with higher\nqualities. Extensive experimental results demonstrate the superiority of\nCHITNet over state-of-the-art algorithms in terms of visual quality and\nquantitative evaluations.\n","authors":["Keying Du","Huafeng Li","Yafei Zhang","Zhengtao Yu"],"pdf_url":"https://arxiv.org/pdf/2309.06118v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16642v1","updated":"2024-10-22T02:41:37Z","published":"2024-10-22T02:41:37Z","title":"Fire and Smoke Detection with Burning Intensity Representation","summary":"  An effective Fire and Smoke Detection (FSD) and analysis system is of\nparamount importance due to the destructive potential of fire disasters.\nHowever, many existing FSD methods directly employ generic object detection\ntechniques without considering the transparency of fire and smoke, which leads\nto imprecise localization and reduces detection performance. To address this\nissue, a new Attentive Fire and Smoke Detection Model (a-FSDM) is proposed.\nThis model not only retains the robust feature extraction and fusion\ncapabilities of conventional detection algorithms but also redesigns the\ndetection head specifically for transparent targets in FSD, termed the\nAttentive Transparency Detection Head (ATDH). In addition, Burning Intensity\n(BI) is introduced as a pivotal feature for fire-related downstream risk\nassessments in traditional FSD methodologies. Extensive experiments on multiple\nFSD datasets showcase the effectiveness and versatility of the proposed FSD\nmodel. The project is available at\n\\href{https://xiaoyihan6.github.io/FSD/}{https://xiaoyihan6.github.io/FSD/}.\n","authors":["Xiaoyi Han","Yanfei Wu","Nan Pu","Zunlei Feng","Qifei Zhang","Yijun Bei","Lechao Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.16642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02216v2","updated":"2024-10-22T02:33:25Z","published":"2023-06-03T23:53:57Z","title":"Forgettable Federated Linear Learning with Certified Data Unlearning","summary":"  The advent of Federated Learning (FL) has revolutionized the way distributed\nsystems handle collaborative model training while preserving user privacy.\nRecently, Federated Unlearning (FU) has emerged to address demands for the\n\"right to be forgotten\"\" and unlearning of the impact of poisoned clients\nwithout requiring retraining in FL. Most FU algorithms require the cooperation\nof retained or target clients (clients to be unlearned), introducing additional\ncommunication overhead and potential security risks. In addition, some FU\nmethods need to store historical models to execute the unlearning process.\nThese challenges hinder the efficiency and memory constraints of the current FU\nmethods. Moreover, due to the complexity of nonlinear models and their training\nstrategies, most existing FU methods for deep neural networks (DNN) lack\ntheoretical certification. In this work, we introduce a novel FL training and\nunlearning strategy in DNN, termed Forgettable Federated Linear Learning\n(F^2L^2). F^2L^2 considers a common practice of using pre-trained models to\napproximate DNN linearly, allowing them to achieve similar performance as the\noriginal networks via Federated Linear Training (FLT). We then present\nFedRemoval, a certified, efficient, and secure unlearning strategy that enables\nthe server to unlearn a target client without requiring client communication or\nadding additional storage. We have conducted extensive empirical validation on\nsmall- to large-scale datasets, using both convolutional neural networks and\nmodern foundation models. These experiments demonstrate the effectiveness of\nF^2L^2 in balancing model accuracy with the successful unlearning of target\nclients. F^2L^2 represents a promising pipeline for efficient and trustworthy\nFU. The code is available here.\n","authors":["Ruinan Jin","Minghui Chen","Qiong Zhang","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2306.02216v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16631v1","updated":"2024-10-22T02:19:23Z","published":"2024-10-22T02:19:23Z","title":"Benchmarking Multi-Scene Fire and Smoke Detection","summary":"  The current irregularities in existing public Fire and Smoke Detection (FSD)\ndatasets have become a bottleneck in the advancement of FSD technology. Upon\nin-depth analysis, we identify the core issue as the lack of standardized\ndataset construction, uniform evaluation systems, and clear performance\nbenchmarks. To address this issue and drive innovation in FSD technology, we\nsystematically gather diverse resources from public sources to create a more\ncomprehensive and refined FSD benchmark. Additionally, recognizing the\ninadequate coverage of existing dataset scenes, we strategically expand scenes,\nrelabel, and standardize existing public FSD datasets to ensure accuracy and\nconsistency. We aim to establish a standardized, realistic, unified, and\nefficient FSD research platform that mirrors real-life scenes closely. Through\nour efforts, we aim to provide robust support for the breakthrough and\ndevelopment of FSD technology. The project is available at\n\\href{https://xiaoyihan6.github.io/FSD/}{https://xiaoyihan6.github.io/FSD/}.\n","authors":["Xiaoyi Han","Nan Pu","Zunlei Feng","Yijun Bei","Qifei Zhang","Lechao Cheng","Liang Xue"],"pdf_url":"https://arxiv.org/pdf/2410.16631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16624v1","updated":"2024-10-22T02:16:02Z","published":"2024-10-22T02:16:02Z","title":"EVC-MF: End-to-end Video Captioning Network with Multi-scale Features","summary":"  Conventional approaches for video captioning leverage a variety of\noffline-extracted features to generate captions. Despite the availability of\nvarious offline-feature-extractors that offer diverse information from\ndifferent perspectives, they have several limitations due to fixed parameters.\nConcretely, these extractors are solely pre-trained on image/video\ncomprehension tasks, making them less adaptable to video caption datasets.\nAdditionally, most of these extractors only capture features prior to the\nclassifier of the pre-training task, ignoring a significant amount of valuable\nshallow information. Furthermore, employing multiple offline-features may\nintroduce redundant information. To address these issues, we propose an\nend-to-end encoder-decoder-based network (EVC-MF) for video captioning, which\nefficiently utilizes multi-scale visual and textual features to generate video\ndescriptions. Specifically, EVC-MF consists of three modules. Firstly, instead\nof relying on multiple feature extractors, we directly feed video frames into a\ntransformer-based network to obtain multi-scale visual features and update\nfeature extractor parameters. Secondly, we fuse the multi-scale features and\ninput them into a masked encoder to reduce redundancy and encourage learning\nuseful features. Finally, we utilize an enhanced transformer-based decoder,\nwhich can efficiently leverage shallow textual information, to generate video\ndescriptions. To evaluate our proposed model, we conduct extensive experiments\non benchmark datasets. The results demonstrate that EVC-MF yields competitive\nperformance compared with the state-of-theart methods.\n","authors":["Tian-Zi Niu","Zhen-Duo Chen","Xin Luo","Xin-Shun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.16624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09564v2","updated":"2024-10-22T02:14:24Z","published":"2024-06-13T20:12:46Z","title":"Towards Domain Adaptive Neural Contextual Bandits","summary":"  Contextual bandit algorithms are essential for solving real-world decision\nmaking problems. In practice, collecting a contextual bandit's feedback from\ndifferent domains may involve different costs. For example, measuring drug\nreaction from mice (as a source domain) and humans (as a target domain).\nUnfortunately, adapting a contextual bandit algorithm from a source domain to a\ntarget domain with distribution shift still remains a major challenge and\nlargely unexplored. In this paper, we introduce the first general domain\nadaptation method for contextual bandits. Our approach learns a bandit model\nfor the target domain by collecting feedback from the source domain. Our\ntheoretical analysis shows that our algorithm maintains a sub-linear regret\nbound even adapting across domains. Empirical results show that our approach\noutperforms the state-of-the-art contextual bandit algorithms on real-world\ndatasets.\n","authors":["Ziyan Wang","Xiaoming Huo","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.09564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02622v2","updated":"2024-10-22T01:41:21Z","published":"2023-11-05T11:27:03Z","title":"Hierarchical Simplicity Bias of Neural Networks","summary":"  Neural networks often exhibit simplicity bias, favoring simpler features over\nmore complex ones, even when both are equally predictive. We introduce a novel\nmethod called imbalanced label coupling to explore and extend this simplicity\nbias across multiple hierarchical levels. Our approach demonstrates that\ntrained networks sequentially consider features of increasing complexity based\non their correlation with labels in the training set, regardless of their\nactual predictive power. For example, in CIFAR-10, simple spurious features can\ncause misclassifications where most cats are predicted as dogs and most trucks\nas automobiles. We empirically show that last-layer retraining with target data\ndistribution \\citep{kirichenko2022last} is insufficient to fully recover core\nfeatures when spurious features perfectly correlate with target labels in our\nsynthetic datasets. Our findings deepen the understanding of the implicit\nbiases inherent in neural networks.\n","authors":["Zhehang Du"],"pdf_url":"https://arxiv.org/pdf/2311.02622v2.pdf","comment":"20 pages, 21 figures, revised version, accepted at OPT2024: 16th\n  Annual Workshop on Optimization for Machine Learning"},{"id":"http://arxiv.org/abs/2410.16602v1","updated":"2024-10-22T01:08:21Z","published":"2024-10-22T01:08:21Z","title":"Foundation Models for Remote Sensing and Earth Observation: A Survey","summary":"  Remote Sensing (RS) is a crucial technology for observing, monitoring, and\ninterpreting our planet, with broad applications across geoscience, economics,\nhumanitarian fields, etc. While artificial intelligence (AI), particularly deep\nlearning, has achieved significant advances in RS, unique challenges persist in\ndeveloping more intelligent RS systems, including the complexity of Earth's\nenvironments, diverse sensor modalities, distinctive feature patterns, varying\nspatial and spectral resolutions, and temporal dynamics. Meanwhile, recent\nbreakthroughs in large Foundation Models (FMs) have expanded AI's potential\nacross many domains due to their exceptional generalizability and zero-shot\ntransfer capabilities. However, their success has largely been confined to\nnatural data like images and video, with degraded performance and even failures\nfor RS data of various non-optical modalities. This has inspired growing\ninterest in developing Remote Sensing Foundation Models (RSFMs) to address the\ncomplex demands of Earth Observation (EO) tasks, spanning the surface,\natmosphere, and oceans. This survey systematically reviews the emerging field\nof RSFMs. It begins with an outline of their motivation and background,\nfollowed by an introduction of their foundational concepts. It then categorizes\nand reviews existing RSFM studies including their datasets and technical\ncontributions across Visual Foundation Models (VFMs), Visual-Language Models\n(VLMs), Large Language Models (LLMs), and beyond. In addition, we benchmark\nthese models against publicly available datasets, discuss existing challenges,\nand propose future research directions in this rapidly evolving field.\n","authors":["Aoran Xiao","Weihao Xuan","Junjue Wang","Jiaxing Huang","Dacheng Tao","Shijian Lu","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2410.16602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11194v2","updated":"2024-10-22T00:42:31Z","published":"2024-08-20T21:02:54Z","title":"Compress Guidance in Conditional Diffusion Sampling","summary":"  We found that enforcing guidance throughout the sampling process is often\ncounterproductive due to the model-fitting issue, where samples are 'tuned' to\nmatch the classifier's parameters rather than generalizing the expected\ncondition. This work identifies and quantifies the problem, demonstrating that\nreducing or excluding guidance at numerous timesteps can mitigate this issue.\nBy distributing a small amount of guidance over a large number of sampling\ntimesteps, we observe a significant improvement in image quality and diversity\nwhile also reducing the required guidance timesteps by nearly 40%. This\napproach addresses a major challenge in applying guidance effectively to\ngenerative tasks. Consequently, our proposed method, termed Compress Guidance,\nallows for the exclusion of a substantial number of guidance timesteps while\nstill surpassing baseline models in image quality. We validate our approach\nthrough benchmarks on label-conditional and text-to-image generative tasks\nacross various datasets and models.\n","authors":["Anh-Dung Dinh","Daochang Liu","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2408.11194v2.pdf","comment":"10 pages, 5 figures, Computer Vision and Machine Learning"},{"id":"http://arxiv.org/abs/2410.16239v2","updated":"2024-10-22T22:22:14Z","published":"2024-10-21T17:42:41Z","title":"MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays,\n  ECGs, and Diagnostic Report","summary":"  In this paper, we introduce a novel Multi-Modal Contrastive Pre-training\nFramework that synergistically combines X-rays, electrocardiograms (ECGs), and\nradiology/cardiology reports. Our approach leverages transformers to encode\nthese diverse modalities into a unified representation space, aiming to enhance\ndiagnostic accuracy and facilitate comprehensive patient assessments. We\nutilize LoRA-Peft to significantly reduce trainable parameters in the LLM and\nincorporate recent linear attention dropping strategy in the Vision\nTransformer(ViT) for smoother attention. Furthermore, we provide novel\nmultimodal attention explanations and retrieval for our model. To the best of\nour knowledge, we are the first to propose an integrated model that combines\nX-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing\ncontrastive loss, MoRE effectively aligns modality-specific features into a\ncoherent embedding, which supports various downstream tasks such as zero-shot\nclassification and multimodal retrieval. Employing our proposed methodology, we\nachieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and\nPtbXl downstream datasets, surpassing existing multimodal approaches. Our\nproposed framework shows significant improvements in capturing intricate\ninter-modal relationships and its robustness in medical diagnosis that\nestablishes a framework for future research in multimodal learning in the\nhealthcare sector.\n","authors":["Samrajya Thapa","Koushik Howlader","Subhankar Bhattacharjee","Wei le"],"pdf_url":"https://arxiv.org/pdf/2410.16239v2.pdf","comment":"10 pages, 5 figures, 9 tables. Supplementary detail in Appendix. Code\n  made available in Github for reproducibility"},{"id":"http://arxiv.org/abs/2410.15947v2","updated":"2024-10-22T17:58:06Z","published":"2024-10-21T12:26:53Z","title":"AI-Driven Approaches for Glaucoma Detection -- A Comprehensive Review","summary":"  The diagnosis of glaucoma plays a critical role in the management and\ntreatment of this vision-threatening disease. Glaucoma is a group of eye\ndiseases that cause blindness by damaging the optic nerve at the back of the\neye. Often called \"silent thief of sight\", it exhibits no symptoms during the\nearly stages. Therefore, early detection is crucial to prevent vision loss.\nWith the rise of Artificial Intelligence (AI), particularly Deep Learning (DL)\ntechniques, Computer-Aided Diagnosis (CADx) systems have emerged as promising\ntools to assist clinicians in accurately diagnosing glaucoma early. This paper\naims to provide a comprehensive overview of AI techniques utilized in CADx\nsystems for glaucoma diagnosis. Through a detailed analysis of current\nliterature, we identify key gaps and challenges in these systems, emphasizing\nthe need for improved safety, reliability, interpretability, and\nexplainability. By identifying research gaps, we aim to advance the field of\nCADx systems especially for the early diagnosis of glaucoma, in order to\nprevent any potential loss of vision.\n","authors":["Yuki Hagiwara","Octavia-Andreea Ciora","Maureen Monnet","Gino Lancho","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2410.15947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11402v2","updated":"2024-10-22T23:13:34Z","published":"2024-09-17T17:59:06Z","title":"NVLM: Open Frontier-Class Multimodal LLMs","summary":"  We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B\nand will open-source the training code for the community soon.\n","authors":["Wenliang Dai","Nayeon Lee","Boxin Wang","Zhuolin Yang","Zihan Liu","Jon Barker","Tuomas Rintamaki","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2409.11402v2.pdf","comment":"Fixed the typos. For more information, please visit our project page\n  at: https://research.nvidia.com/labs/adlr/NVLM-1"},{"id":"http://arxiv.org/abs/2410.17434v1","updated":"2024-10-22T21:21:37Z","published":"2024-10-22T21:21:37Z","title":"LongVU: Spatiotemporal Adaptive Compression for Long Video-Language\n  Understanding","summary":"  Multimodal Large Language Models (MLLMs) have shown promising progress in\nunderstanding and analyzing video content. However, processing long videos\nremains a significant challenge constrained by LLM's context size. To address\nthis limitation, we propose LongVU, a spatiotemporal adaptive compression\nmechanism thats reduces the number of video tokens while preserving visual\ndetails of long videos. Our idea is based on leveraging cross-modal query and\ninter-frame dependencies to adaptively reduce temporal and spatial redundancy\nin videos. Specifically, we leverage DINOv2 features to remove redundant frames\nthat exhibit high similarity. Then we utilize text-guided cross-modal query for\nselective frame feature reduction. Further, we perform spatial token reduction\nacross frames based on their temporal dependencies. Our adaptive compression\nstrategy effectively processes a large number of frames with little visual\ninformation loss within given context length. Our LongVU consistently surpass\nexisting methods across a variety of video understanding benchmarks, especially\non hour-long video understanding tasks such as VideoMME and MLVU. Given a\nlight-weight LLM, our LongVU also scales effectively into a smaller size with\nstate-of-the-art video understanding performance.\n","authors":["Xiaoqian Shen","Yunyang Xiong","Changsheng Zhao","Lemeng Wu","Jun Chen","Chenchen Zhu","Zechun Liu","Fanyi Xiao","Balakrishnan Varadarajan","Florian Bordes","Zhuang Liu","Hu Xu","Hyunwoo J. Kim","Bilge Soran","Raghuraman Krishnamoorthi","Mohamed Elhoseiny","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2410.17434v1.pdf","comment":"Project page: https://vision-cair.github.io/LongVU"},{"id":"http://arxiv.org/abs/2410.17427v1","updated":"2024-10-22T20:56:04Z","published":"2024-10-22T20:56:04Z","title":"SigCLR: Sigmoid Contrastive Learning of Visual Representations","summary":"  We propose SigCLR: Sigmoid Contrastive Learning of Visual Representations.\nSigCLR utilizes the logistic loss that only operates on pairs and does not\nrequire a global view as in the cross-entropy loss used in SimCLR. We show that\nlogistic loss shows competitive performance on CIFAR-10, CIFAR-100, and Tiny-IN\ncompared to other established SSL objectives. Our findings verify the\nimportance of learnable bias as in the case of SigLUP, however, it requires a\nfixed temperature as in the SimCLR to excel. Overall, SigCLR is a promising\nreplacement for the SimCLR which is ubiquitous and has shown tremendous success\nin various domains.\n","authors":["Ömer Veysel Çağatan"],"pdf_url":"https://arxiv.org/pdf/2410.17427v1.pdf","comment":"Neurips 2024 SSL Workshop"},{"id":"http://arxiv.org/abs/2312.02521v3","updated":"2024-10-22T20:52:38Z","published":"2023-12-05T06:04:16Z","title":"RetriBooru: Leakage-Free Retrieval of Conditions from Reference Images\n  for Subject-Driven Generation","summary":"  Diffusion-based methods have demonstrated remarkable capabilities in\ngenerating a diverse array of high-quality images, sparking interests for\nstyled avatars, virtual try-on, and more. Previous methods use the same\nreference image as the target. An overlooked aspect is the leakage of the\ntarget's spatial information, style, etc. from the reference, harming the\ngenerated diversity and causing shortcuts. However, this approach continues as\nwidely available datasets usually consist of single images not grouped by\nidentities, and it is expensive to recollect large-scale same-identity data.\nMoreover, existing metrics adopt decoupled evaluation on text alignment and\nidentity preservation, which fail at distinguishing between balanced outputs\nand those that over-fit to one aspect. In this paper, we propose a multi-level,\nsame-identity dataset RetriBooru, which groups anime characters by both face\nand cloth identities. RetriBooru enables adopting reference images of the same\ncharacter and outfits as the target, while keeping flexible gestures and\nactions. We benchmark previous methods on our dataset, and demonstrate the\neffectiveness of training with a reference image different from target (but\nsame identity). We introduce a new concept composition task, where the\nconditioning encoder learns to retrieve different concepts from several\nreference images, and modify a baseline network RetriNet for the new task.\nFinally, we introduce a novel class of metrics named Similarity Weighted\nDiversity (SWD), to measure the overlooked diversity and better evaluate the\nalignment between similarity and diversity.\n","authors":["Haoran Tang","Jieren Deng","Zhihong Pan","Hao Tian","Pratik Chaudhari","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.02521v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17422v1","updated":"2024-10-22T20:51:45Z","published":"2024-10-22T20:51:45Z","title":"AG-SLAM: Active Gaussian Splatting SLAM","summary":"  We present AG-SLAM, the first active SLAM system utilizing 3D Gaussian\nSplatting (3DGS) for online scene reconstruction. In recent years, radiance\nfield scene representations, including 3DGS have been widely used in SLAM and\nexploration, but actively planning trajectories for robotic exploration is\nstill unvisited. In particular, many exploration methods assume precise\nlocalization and thus do not mitigate the significant risk of constructing a\ntrajectory, which is difficult for a SLAM system to operate on. This can cause\ncamera tracking failure and lead to failures in real-world robotic\napplications. Our method leverages Fisher Information to balance the dual\nobjectives of maximizing the information gain for the environment while\nminimizing the cost of localization errors. Experiments conducted on the Gibson\nand Habitat-Matterport 3D datasets demonstrate state-of-the-art results of the\nproposed method.\n","authors":["Wen Jiang","Boshu Lei","Katrina Ashton","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2410.17422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01984v5","updated":"2024-10-22T20:39:52Z","published":"2024-01-03T21:24:44Z","title":"AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed\n  and Low Tolerance","summary":"  Recent advances in visual anomaly detection research have seen AUROC and\nAUPRO scores on public benchmark datasets such as MVTec and VisA converge\ntowards perfect recall, giving the impression that these benchmarks are\nnear-solved. However, high AUROC and AUPRO scores do not always reflect\nqualitative performance, which limits the validity of these metrics in\nreal-world applications. We argue that the artificial ceiling imposed by the\nlack of an adequate evaluation metric restrains progression of the field, and\nit is crucial that we revisit the evaluation metrics used to rate our\nalgorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric\nthat addresses the shortcomings of AUROC and AUPRO. PIMO retains the\nrecall-based nature of the existing metrics but introduces two distinctions:\nthe assignment of curves (and respective area under the curve) is per-image,\nand its X-axis relies solely on normal images. Measuring recall per image\nsimplifies instance score indexing and is more robust to noisy annotations. As\nwe show, it also accelerates computation and enables the usage of statistical\ntests to compare models. By imposing low tolerance for false positives on\nnormal images, PIMO provides an enhanced model validation procedure and\nhighlights performance variations across datasets. Our experiments demonstrate\nthat PIMO offers practical advantages and nuanced performance insights that\nredefine anomaly detection benchmarks -- notably challenging the perception\nthat MVTec AD and VisA datasets have been solved by contemporary models.\nAvailable on GitHub: https://github.com/jpcbertoldo/aupimo.\n","authors":["Joao P. C. Bertoldo","Dick Ameln","Ashwin Vaidya","Samet Akçay"],"pdf_url":"https://arxiv.org/pdf/2401.01984v5.pdf","comment":"Accepted to BMVC 2024. Official implementation:\n  https://github.com/jpcbertoldo/aupimo. Integrated in anomalib\n  https://github.com/openvinotoolkit/anomalib. This research was conducted\n  during Google Summer of Code 2023 (GSoC 2023) with the anomalib team from\n  Intel's OpenVINO Toolkit"},{"id":"http://arxiv.org/abs/2410.17409v1","updated":"2024-10-22T20:33:10Z","published":"2024-10-22T20:33:10Z","title":"Geometric Graph Neural Network Modeling of Human Interactions in Crowded\n  Environments","summary":"  Modeling human trajectories in crowded environments is challenging due to the\ncomplex nature of pedestrian behavior and interactions. This paper proposes a\ngeometric graph neural network (GNN) architecture that integrates domain\nknowledge from psychological studies to model pedestrian interactions and\npredict future trajectories. Unlike prior studies using complete graphs, we\ndefine interaction neighborhoods using pedestrians' field of view, motion\ndirection, and distance-based kernel functions to construct graph\nrepresentations of crowds. Evaluations across multiple datasets demonstrate\nimproved prediction accuracy through reduced average and final displacement\nerror metrics. Our findings underscore the importance of integrating domain\nknowledge with data-driven approaches for effective modeling of human\ninteractions in crowds.\n","authors":["Sara Honarvar","Yancy Diaz-Mercado"],"pdf_url":"https://arxiv.org/pdf/2410.17409v1.pdf","comment":"\\c{opyright} 2024 the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND"},{"id":"http://arxiv.org/abs/2403.09240v2","updated":"2024-10-22T20:26:09Z","published":"2024-03-14T10:03:58Z","title":"XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via\n  Controllable Diffusion Model","summary":"  Large-scale generative models have demonstrated impressive capabilities in\nproducing visually compelling images, with increasing applications in medical\nimaging. However, they continue to grapple with hallucination challenges and\nthe generation of anatomically inaccurate outputs. These limitations are mainly\ndue to the reliance on textual inputs and lack of spatial control over the\ngenerated images, hindering the potential usefulness of such models in\nreal-life settings. In this work, we present XReal, a novel controllable\ndiffusion model for generating realistic chest X-ray images through precise\nanatomy and pathology location control. Our lightweight method comprises an\nAnatomy Controller and a Pathology Controller to introduce spatial control over\nanatomy and pathology in a pre-trained Text-to-Image Diffusion Model,\nrespectively, without fine-tuning the model. XReal outperforms state-of-the-art\nX-ray diffusion models in quantitative metrics and radiologists' ratings,\nshowing significant gains in anatomy and pathology realism. Our model holds\npromise for advancing generative models in medical imaging, offering greater\nprecision and adaptability while inviting further exploration in this evolving\nfield. The code and pre-trained model weights are publicly available at\nhttps://github.com/BioMedIA-MBZUAI/XReal.\n","authors":["Anees Ur Rehman Hashmi","Ibrahim Almakky","Mohammad Areeb Qazi","Santosh Sanjeev","Vijay Ram Papineni","Jagalpathy Jagdish","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.09240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17396v1","updated":"2024-10-22T20:02:38Z","published":"2024-10-22T20:02:38Z","title":"Efficient Feature Extraction Using Light-Weight CNN Attention-Based Deep\n  Learning Architectures for Ultrasound Fetal Plane Classification","summary":"  Ultrasound fetal imaging is beneficial to support prenatal development\nbecause it is affordable and non-intrusive. Nevertheless, fetal plane\nclassification (FPC) remains challenging and time-consuming for obstetricians\nsince it depends on nuanced clinical aspects, which increases the difficulty in\nidentifying relevant features of the fetal anatomy. Thus, to assist with its\naccurate feature extraction, a lightweight artificial intelligence architecture\nleveraging convolutional neural networks and attention mechanisms is proposed\nto classify the largest benchmark ultrasound dataset. The approach fine-tunes\nfrom lightweight EfficientNet feature extraction backbones pre-trained on the\nImageNet1k. to classify key fetal planes such as the brain, femur, thorax,\ncervix, and abdomen. Our methodology incorporates the attention mechanism to\nrefine features and 3-layer perceptrons for classification, achieving superior\nperformance with the highest Top-1 accuracy of 96.25%, Top-2 accuracy of 99.80%\nand F1-Score of 0.9576. Importantly, the model has 40x fewer trainable\nparameters than existing benchmark ensemble or transformer pipelines,\nfacilitating easy deployment on edge devices to help clinical practitioners\nwith real-time FPC. The findings are also interpreted using GradCAM to carry\nout clinical correlation to aid doctors with diagnostics and improve treatment\nplans for expectant mothers.\n","authors":["Arrun Sivasubramanian","Divya Sasidharan","Sowmya V","Vinayakumar Ravi"],"pdf_url":"https://arxiv.org/pdf/2410.17396v1.pdf","comment":"Submitted to Computers in Biology and Medicine journal"},{"id":"http://arxiv.org/abs/2403.00946v2","updated":"2024-10-22T20:01:45Z","published":"2024-03-01T19:50:22Z","title":"Fine-tuning with Very Large Dropout","summary":"  It is impossible today to pretend that the practice of machine learning is\ncompatible with the idea that training and testing data follow the same\ndistribution. Several authors have recently used ensemble techniques to show\nhow scenarios involving multiple data distributions are best served by\nrepresentations that are both richer than those obtained by regularizing for\nthe best in-distribution performance, and richer than those obtained under the\ninfluence of the implicit sparsity bias of common stochastic gradient\nprocedures.\n  This contribution investigates the use of very high dropout rates instead of\nensembles to obtain such rich representations. Although training a deep network\nfrom scratch using such dropout rates is virtually impossible, fine-tuning a\nlarge pre-trained model under such conditions is not only possible but also\nachieves out-of-distribution performances that exceed those of both ensembles\nand weight averaging methods such as model soups. This result has practical\nsignificance because the importance of the fine-tuning scenario has\nconsiderably grown in recent years. This result also provides interesting\ninsights on the nature of rich representations and on the intrinsically linear\nnature of fine-tuning a large network using a comparatively small dataset.\n","authors":["Jianyu Zhang","Léon Bottou"],"pdf_url":"https://arxiv.org/pdf/2403.00946v2.pdf","comment":"Fine-tuning with very large dropout outperforms weight-averaging and\n  ensemble on ResNet and large vision transformer"},{"id":"http://arxiv.org/abs/2410.17393v1","updated":"2024-10-22T20:01:00Z","published":"2024-10-22T20:01:00Z","title":"Denoise-I2W: Mapping Images to Denoising Words for Accurate Zero-Shot\n  Composed Image Retrieval","summary":"  Zero-Shot Composed Image Retrieval (ZS-CIR) supports diverse tasks with a\nbroad range of visual content manipulation intentions that can be related to\ndomain, scene, object, and attribute. A key challenge for ZS-CIR is to\naccurately map image representation to a pseudo-word token that captures the\nmanipulation intention relevant image information for generalized CIR. However,\nexisting methods between the retrieval and pre-training stages lead to\nsignificant redundancy in the pseudo-word tokens. In this paper, we propose a\nnovel denoising image-to-word mapping approach, named Denoise-I2W, for mapping\nimages into denoising pseudo-word tokens that, without intention-irrelevant\nvisual information, enhance accurate ZS-CIR. Specifically, a pseudo triplet\nconstruction module first automatically constructs pseudo triples\n(\\textit{i.e.,} a pseudo-reference image, a pseudo-manipulation text, and a\ntarget image) for pre-training the denoising mapping network. Then, a\npseudo-composed mapping module maps the pseudo-reference image to a pseudo-word\ntoken and combines it with the pseudo-manipulation text with manipulation\nintention. This combination aligns with the target image, facilitating\ndenoising intention-irrelevant visual information for mapping. Our proposed\nDenoise-I2W is a model-agnostic and annotation-free approach. It demonstrates\nstrong generalization capabilities across three state-of-the-art ZS-CIR models\non four benchmark datasets. By integrating Denoise-I2W with existing best\nmodels, we obtain consistent and significant performance boosts ranging from\n1.45\\% to 4.17\\% over the best methods without increasing inference costs. and\nachieve new state-of-the-art results on ZS-CIR. Our code is available at\n\\url{https://github.com/Pter61/denoise-i2w-tmm}.\n","authors":["Yuanmin Tang","Jing Yu","Keke Gai","Jiamin Zhuang","Gaopeng Gou","Gang Xiong","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17393v1.pdf","comment":"This work was submitted to IJCAI 2024, with a score of weak accept\n  and borderline accept"},{"id":"http://arxiv.org/abs/2410.17385v1","updated":"2024-10-22T19:39:15Z","published":"2024-10-22T19:39:15Z","title":"Do Vision-Language Models Represent Space and How? Evaluating Spatial\n  Frame of Reference Under Ambiguities","summary":"  Spatial expressions in situated communication can be ambiguous, as their\nmeanings vary depending on the frames of reference (FoR) adopted by speakers\nand listeners. While spatial language understanding and reasoning by\nvision-language models (VLMs) have gained increasing attention, potential\nambiguities in these models are still under-explored. To address this issue, we\npresent the COnsistent Multilingual Frame Of Reference Test (COMFORT), an\nevaluation protocol to systematically assess the spatial reasoning capabilities\nof VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing\nsome alignment with English conventions in resolving ambiguities, our\nexperiments reveal significant shortcomings of VLMs: notably, the models (1)\nexhibit poor robustness and consistency, (2) lack the flexibility to\naccommodate multiple FoRs, and (3) fail to adhere to language-specific or\nculture-specific conventions in cross-lingual tests, as English tends to\ndominate other languages. With a growing effort to align vision-language models\nwith human cognitive intuitions, we call for more attention to the ambiguous\nnature and cross-cultural diversity of spatial reasoning.\n","authors":["Zheyuan Zhang","Fengyuan Hu","Jayjun Lee","Freda Shi","Parisa Kordjamshidi","Joyce Chai","Ziqiao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.17385v1.pdf","comment":"Accepted to Pluralistic Alignment @ NeurIPS 2024 | Project page:\n  https://spatial-comfort.github.io/"},{"id":"http://arxiv.org/abs/2410.17377v1","updated":"2024-10-22T19:26:05Z","published":"2024-10-22T19:26:05Z","title":"PtychoFormer: A Transformer-based Model for Ptychographic Phase\n  Retrieval","summary":"  Ptychography is a computational method of microscopy that recovers\nhigh-resolution transmission images of samples from a series of diffraction\npatterns. While conventional phase retrieval algorithms can iteratively recover\nthe images, they require oversampled diffraction patterns, incur significant\ncomputational costs, and struggle to recover the absolute phase of the sample's\ntransmission function. Deep learning algorithms for ptychography are a\npromising approach to resolving the limitations of iterative algorithms. We\npresent PtychoFormer, a hierarchical transformer-based model for data-driven\nsingle-shot ptychographic phase retrieval. PtychoFormer processes subsets of\ndiffraction patterns, generating local inferences that are seamlessly stitched\ntogether to produce a high-quality reconstruction. Our model exhibits tolerance\nto sparsely scanned diffraction patterns and achieves up to 3600 times faster\nimaging speed than the extended ptychographic iterative engine (ePIE). We also\npropose the extended-PtychoFormer (ePF), a hybrid approach that combines the\nbenefits of PtychoFormer with the ePIE. ePF minimizes global phase shifts and\nsignificantly enhances reconstruction quality, achieving state-of-the-art phase\nretrieval in ptychography.\n","authors":["Ryuma Nakahata","Shehtab Zaman","Mingyuan Zhang","Fake Lu","Kenneth Chiu"],"pdf_url":"https://arxiv.org/pdf/2410.17377v1.pdf","comment":"20 pages, 12 figures"},{"id":"http://arxiv.org/abs/2307.15250v4","updated":"2024-10-22T19:09:54Z","published":"2023-07-28T01:20:12Z","title":"D2S: Representing sparse descriptors and 3D coordinates for camera\n  relocalization","summary":"  State-of-the-art visual localization methods mostly rely on complex\nprocedures to match local descriptors and 3D point clouds. However, these\nprocedures can incur significant costs in terms of inference, storage, and\nupdates over time. In this study, we propose a direct learning-based approach\nthat utilizes a simple network named D2S to represent complex local descriptors\nand their scene coordinates. Our method is characterized by its simplicity and\ncost-effectiveness. It solely leverages a single RGB image for localization\nduring the testing phase and only requires a lightweight model to encode a\ncomplex sparse scene. The proposed D2S employs a combination of a simple loss\nfunction and graph attention to selectively focus on robust descriptors while\ndisregarding areas such as clouds, trees, and several dynamic objects. This\nselective attention enables D2S to effectively perform a binary-semantic\nclassification for sparse descriptors. Additionally, we propose a simple\noutdoor dataset to evaluate the capabilities of visual localization methods in\nscene-specific generalization and self-updating from unlabeled observations.\nOur approach outperforms the previous regression-based methods in both indoor\nand outdoor environments. It demonstrates the ability to generalize beyond\ntraining data, including scenarios involving transitions from day to night and\nadapting to domain shifts. The source code, trained models, dataset, and demo\nvideos are available at the following link: https://thpjp.github.io/d2s.\n","authors":["Bach-Thuan Bui","Huy-Hoang Bui","Dinh-Tuan Tran","Joo-Ho Lee"],"pdf_url":"https://arxiv.org/pdf/2307.15250v4.pdf","comment":"Accepted to IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2407.00633v2","updated":"2024-10-22T18:52:19Z","published":"2024-06-30T09:15:21Z","title":"DEAR: Disentangled Environment and Agent Representations for\n  Reinforcement Learning without Reconstruction","summary":"  Reinforcement Learning (RL) algorithms can learn robotic control tasks from\nvisual observations, but they often require a large amount of data, especially\nwhen the visual scene is complex and unstructured. In this paper, we explore\nhow the agent's knowledge of its shape can improve the sample efficiency of\nvisual RL methods. We propose a novel method, Disentangled Environment and\nAgent Representations (DEAR), that uses the segmentation mask of the agent as\nsupervision to learn disentangled representations of the environment and the\nagent through feature separation constraints. Unlike previous approaches, DEAR\ndoes not require reconstruction of visual observations. These representations\nare then used as an auxiliary loss to the RL objective, encouraging the agent\nto focus on the relevant features of the environment. We evaluate DEAR on two\nchallenging benchmarks: Distracting DeepMind control suite and Franka Kitchen\nmanipulation tasks. Our findings demonstrate that DEAR surpasses\nstate-of-the-art methods in sample efficiency, achieving comparable or superior\nperformance with reduced parameters. Our results indicate that integrating\nagent knowledge into visual RL methods has the potential to enhance their\nlearning efficiency and robustness.\n","authors":["Ameya Pore","Riccardo Muradore","Diego Dall'Alba"],"pdf_url":"https://arxiv.org/pdf/2407.00633v2.pdf","comment":"6 pages, 7 figures, 2 tables. Accepted at 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2305.17828v2","updated":"2024-10-22T18:51:50Z","published":"2023-05-28T23:42:35Z","title":"Counter-Hypothetical Particle Filters for Single Object Pose Tracking","summary":"  Particle filtering is a common technique for six degrees of freedom (6D) pose\nestimation due to its ability to tractably represent belief over object pose.\nHowever, the particle filter is prone to particle deprivation due to the\nhigh-dimensional nature of 6D pose. When particle deprivation occurs, it can\ncause mode collapse of the underlying belief distribution during importance\nsampling. If the region surrounding the true state suffers from mode collapse,\nrecovering its belief is challenging since the area is no longer represented in\nthe probability mass formed by the particles. Previous methods mitigate this\nproblem by randomizing and resetting particles in the belief distribution, but\ndetermining the frequency of reinvigoration has relied on hand-tuning abstract\nheuristics. In this paper, we estimate the necessary reinvigoration rate at\neach time step by introducing a Counter-Hypothetical likelihood function, which\nis used alongside the standard likelihood. Inspired by the notions of\nplausibility and implausibility from Evidential Reasoning, the addition of our\nCounter-Hypothetical likelihood function assigns a level of doubt to each\nparticle. The competing cumulative values of confidence and doubt across the\nparticle set are used to estimate the level of failure within the filter, in\norder to determine the portion of particles to be reinvigorated. We demonstrate\nthe effectiveness of our method on the rigid body object 6D pose tracking task.\n","authors":["Elizabeth A. Olson","Jana Pavlasek","Jasmine A. Berry","Odest Chadwicke Jenkins"],"pdf_url":"https://arxiv.org/pdf/2305.17828v2.pdf","comment":"International Conference on Robotics and Automation (ICRA) 2023"},{"id":"http://arxiv.org/abs/2410.17357v1","updated":"2024-10-22T18:50:20Z","published":"2024-10-22T18:50:20Z","title":"Image-aware Evaluation of Generated Medical Reports","summary":"  The paper proposes a novel evaluation metric for automatic medical report\ngeneration from X-ray images, VLScore. It aims to overcome the limitations of\nexisting evaluation methods, which either focus solely on textual similarities,\nignoring clinical aspects, or concentrate only on a single clinical aspect, the\npathology, neglecting all other factors. The key idea of our metric is to\nmeasure the similarity between radiology reports while considering the\ncorresponding image. We demonstrate the benefit of our metric through\nevaluation on a dataset where radiologists marked errors in pairs of reports,\nshowing notable alignment with radiologists' judgments. In addition, we provide\na new dataset for evaluating metrics. This dataset includes well-designed\nperturbations that distinguish between significant modifications (e.g., removal\nof a diagnosis) and insignificant ones. It highlights the weaknesses in current\nevaluation metrics and provides a clear framework for analysis.\n","authors":["Gefen Dawidowicz","Elad Hirsch","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2410.17357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15392v2","updated":"2024-10-22T18:22:20Z","published":"2024-10-20T13:44:24Z","title":"EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting","summary":"  Scene reconstruction from casually captured videos has wide applications in\nreal-world scenarios. With recent advancements in differentiable rendering\ntechniques, several methods have attempted to simultaneously optimize scene\nrepresentations (NeRF or 3DGS) and camera poses. Despite recent progress,\nexisting methods relying on traditional camera input tend to fail in high-speed\n(or equivalently low-frame-rate) scenarios. Event cameras, inspired by\nbiological vision, record pixel-wise intensity changes asynchronously with high\ntemporal resolution, providing valuable scene and motion information in blind\ninter-frame intervals. In this paper, we introduce the event camera to aid\nscene construction from a casually captured video for the first time, and\npropose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly\nintegrates the advantages of event cameras into 3DGS through three key\ncomponents. First, we leverage the Event Generation Model (EGM) to fuse events\nand frames, supervising the rendered views observed by the event stream.\nSecond, we adopt the Contrast Maximization (CMax) framework in a piece-wise\nmanner to extract motion information by maximizing the contrast of the Image of\nWarped Events (IWE), thereby calibrating the estimated poses. Besides, based on\nthe Linear Event Generation Model (LEGM), the brightness information encoded in\nthe IWE is also utilized to constrain the 3DGS in the gradient domain. Third,\nto mitigate the absence of color information of events, we introduce\nphotometric bundle adjustment (PBA) to ensure view consistency across events\nand frames. We evaluate our method on the public Tanks and Temples benchmark\nand a newly collected real-world dataset, RealEv-DAVIS. Our project page is\nhttps://lbh666.github.io/ef-3dgs/.\n","authors":["Bohao Liao","Wei Zhai","Zengyu Wan","Tianzhu Zhang","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2410.15392v2.pdf","comment":"Project Page: https://lbh666.github.io/ef-3dgs/"},{"id":"http://arxiv.org/abs/2410.17331v1","updated":"2024-10-22T18:04:00Z","published":"2024-10-22T18:04:00Z","title":"Offline Evaluation of Set-Based Text-to-Image Generation","summary":"  Text-to-Image (TTI) systems often support people during ideation, the early\nstages of a creative process when exposure to a broad set of relevant images\ncan help explore the design space. Since ideation is an important subclass of\nTTI tasks, understanding how to quantitatively evaluate TTI systems according\nto how well they support ideation is crucial to promoting research and\ndevelopment for these users. However, existing evaluation metrics for TTI\nremain focused on distributional similarity metrics like Fr\\'echet Inception\nDistance (FID). We take an alternative approach and, based on established\nmethods from ranking evaluation, develop TTI evaluation metrics with explicit\nmodels of how users browse and interact with sets of spatially arranged\ngenerated images. Our proposed offline evaluation metrics for TTI not only\ncapture how relevant generated images are with respect to the user's ideation\nneed but also take into consideration the diversity and arrangement of the set\nof generated images. We analyze our proposed family of TTI metrics using human\nstudies on image grids generated by three different TTI systems based on\nsubsets of the widely used benchmarks such as MS-COCO captions and Localized\nNarratives as well as prompts used in naturalistic settings. Our results\ndemonstrate that grounding metrics in how people use systems is an important\nand understudied area of benchmark design.\n","authors":["Negar Arabzadeh","Fernando Diaz","Junfeng He"],"pdf_url":"https://arxiv.org/pdf/2410.17331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01486v4","updated":"2024-10-22T17:51:22Z","published":"2023-05-02T15:10:01Z","title":"ARBEx: Attentive Feature Extraction with Reliability Balancing for\n  Robust Facial Expression Learning","summary":"  In this paper, we introduce a framework ARBEx, a novel attentive feature\nextraction framework driven by Vision Transformer with reliability balancing to\ncope against poor class distributions, bias, and uncertainty in the facial\nexpression learning (FEL) task. We reinforce several data pre-processing and\nrefinement methods along with a window-based cross-attention ViT to squeeze the\nbest of the data. We also employ learnable anchor points in the embedding space\nwith label distributions and multi-head self-attention mechanism to optimize\nperformance against weak predictions with reliability balancing, which is a\nstrategy that leverages anchor points, attention scores, and confidence values\nto enhance the resilience of label predictions. To ensure correct label\nclassification and improve the models' discriminative power, we introduce\nanchor loss, which encourages large margins between anchor points.\nAdditionally, the multi-head self-attention mechanism, which is also trainable,\nplays an integral role in identifying accurate labels. This approach provides\ncritical elements for improving the reliability of predictions and has a\nsubstantial positive effect on final prediction capabilities. Our adaptive\nmodel can be integrated with any deep neural network to forestall challenges in\nvarious recognition tasks. Our strategy outperforms current state-of-the-art\nmethodologies, according to extensive experiments conducted in a variety of\ncontexts.\n","authors":["Azmine Toushik Wasi","Karlo Šerbetar","Raima Islam","Taki Hasan Rafi","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2305.01486v4.pdf","comment":"Extended version is accepted in ACCV 2024 as GReFEL\n  (arXiv:2410.15927)"},{"id":"http://arxiv.org/abs/2407.00557v2","updated":"2024-10-22T15:50:00Z","published":"2024-06-30T01:31:54Z","title":"Explaining Chest X-ray Pathology Models using Textual Concepts","summary":"  Deep learning models have revolutionized medical imaging and diagnostics, yet\ntheir opaque nature poses challenges for clinical adoption and trust. Amongst\napproaches to improve model interpretability, concept-based explanations aim to\nprovide concise and human-understandable explanations of any arbitrary\nclassifier. However, such methods usually require a large amount of manually\ncollected data with concept annotation, which is often scarce in the medical\ndomain. In this paper, we propose Conceptual Counterfactual Explanations for\nChest X-ray (CoCoX), which leverages the joint embedding space of an existing\nvision-language model (VLM) to explain black-box classifier outcomes without\nthe need for annotated datasets. Specifically, we utilize textual concepts\nderived from chest radiography reports and a pre-trained chest\nradiography-based VLM to explain three common cardiothoracic pathologies. We\ndemonstrate that the explanations generated by our method are semantically\nmeaningful and faithful to underlying pathologies.\n","authors":["Vijay Sadashivaiah","Pingkun Yan","James A. Hendler"],"pdf_url":"https://arxiv.org/pdf/2407.00557v2.pdf","comment":"Accepted at NeurIPS'24 workshop on Advancements In Medical Foundation\n  Models: Explainability, Robustness, Security, and Beyond (AIM-FM)"}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2408.08464v4","updated":"2024-10-22T02:01:16Z","published":"2024-08-16T00:18:23Z","title":"$\\textit{MMJ-Bench}$: A Comprehensive Study on Jailbreak Attacks and\n  Defenses for Multimodal Large Language Models","summary":"  As deep learning advances, Large Language Models (LLMs) and their multimodal\ncounterparts, Multimodal Large Language Models (MLLMs), have shown exceptional\nperformance in many real-world tasks. However, MLLMs face significant security\nchallenges, such as jailbreak attacks, where attackers attempt to bypass the\nmodel's safety alignment to elicit harmful responses. The threat of jailbreak\nattacks on MLLMs arises from both the inherent vulnerabilities of LLMs and the\nmultiple information channels that MLLMs process. While various attacks and\ndefenses have been proposed, there is a notable gap in unified and\ncomprehensive evaluations, as each method is evaluated on different dataset and\nmetrics, making it impossible to compare the effectiveness of each method. To\naddress this gap, we introduce \\textit{MMJ-Bench}, a unified pipeline for\nevaluating jailbreak attacks and defense techniques for MLLMs. Through\nextensive experiments, we assess the effectiveness of various attack methods\nagainst SoTA MLLMs and evaluate the impact of defense mechanisms on both\ndefense effectiveness and model utility for normal tasks. Our comprehensive\nevaluation contribute to the field by offering a unified and systematic\nevaluation framework and the first public-available benchmark for MLLM\njailbreak research. We also demonstrate several insightful findings that\nhighlights directions for future studies.\n","authors":["Fenghua Weng","Yue Xu","Chengyan Fu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08464v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15028v2","updated":"2024-10-22T14:55:54Z","published":"2024-10-19T07:59:10Z","title":"A Novel Reinforcement Learning Model for Post-Incident Malware\n  Investigations","summary":"  This Research proposes a Novel Reinforcement Learning (RL) model to optimise\nmalware forensics investigation during cyber incident response. It aims to\nimprove forensic investigation efficiency by reducing false negatives and\nadapting current practices to evolving malware signatures. The proposed RL\nframework leverages techniques such as Q-learning and the Markov Decision\nProcess (MDP) to train the system to identify malware patterns in live memory\ndumps, thereby automating forensic tasks. The RL model is based on a detailed\nmalware workflow diagram that guides the analysis of malware artefacts using\nstatic and behavioural techniques as well as machine learning algorithms.\nFurthermore, it seeks to address challenges in the UK justice system by\nensuring the accuracy of forensic evidence. We conduct testing and evaluation\nin controlled environments, using datasets created with Windows operating\nsystems to simulate malware infections. The experimental results demonstrate\nthat RL improves malware detection rates compared to conventional methods, with\nthe RL model's performance varying depending on the complexity and learning\nrate of the environment. The study concludes that while RL offers promising\npotential for automating malware forensics, its efficacy across diverse malware\ntypes requires ongoing refinement of reward systems and feature extraction\nmethods.\n","authors":["Dipo Dunsin","Mohamed Chahine Ghanem","Karim Ouazzane","Vassil Vassilev"],"pdf_url":"https://arxiv.org/pdf/2410.15028v2.pdf","comment":"8 pages. arXiv admin note: substantial text overlap with\n  arXiv:2408.01999"},{"id":"http://arxiv.org/abs/2410.14923v2","updated":"2024-10-22T00:53:48Z","published":"2024-10-19T01:00:57Z","title":"Imprompter: Tricking LLM Agents into Improper Tool Use","summary":"  Large Language Model (LLM) Agents are an emerging computing paradigm that\nblends generative machine learning with tools such as code interpreters, web\nbrowsing, email, and more generally, external resources. These agent-based\nsystems represent an emerging shift in personal computing. We contribute to the\nsecurity foundations of agent-based systems and surface a new class of\nautomatically computed obfuscated adversarial prompt attacks that violate the\nconfidentiality and integrity of user resources connected to an LLM agent. We\nshow how prompt optimization techniques can find such prompts automatically\ngiven the weights of a model. We demonstrate that such attacks transfer to\nproduction-level agents. For example, we show an information exfiltration\nattack on Mistral's LeChat agent that analyzes a user's conversation, picks out\npersonally identifiable information, and formats it into a valid markdown\ncommand that results in leaking that data to the attacker's server. This attack\nshows a nearly 80% success rate in an end-to-end evaluation. We conduct a range\nof experiments to characterize the efficacy of these attacks and find that they\nreliably work on emerging agent-based systems like Mistral's LeChat, ChatGLM,\nand Meta's Llama. These attacks are multimodal, and we show variants in the\ntext-only and image domains.\n","authors":["Xiaohan Fu","Shuheng Li","Zihan Wang","Yihao Liu","Rajesh K. Gupta","Taylor Berg-Kirkpatrick","Earlence Fernandes"],"pdf_url":"https://arxiv.org/pdf/2410.14923v2.pdf","comment":"website: https://imprompter.ai code:\n  https://github.com/Reapor-Yurnero/imprompter v2 changelog: add new results to\n  Table 3, correct several typos"},{"id":"http://arxiv.org/abs/2410.14262v2","updated":"2024-10-22T10:12:00Z","published":"2024-10-18T08:18:18Z","title":"Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation","summary":"  This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.\n","authors":["Ted Kwartler","Matthew Berman","Alan Aqrawi"],"pdf_url":"https://arxiv.org/pdf/2410.14262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17204v1","updated":"2024-10-22T17:21:28Z","published":"2024-10-22T17:21:28Z","title":"Vulnerability anti-patterns in Solidity: Increasing smart contracts\n  security by reducing false alarms","summary":"  Turing completeness has made Ethereum smart contracts attractive to\nblockchain developers and attackers alike. To increase code security, many\ntools can now spot most known vulnerabilities$-$at the cost of production\nefficiency. Recent studies show false-positive ratios over 99% in\nstate-of-the-art technologies: this makes them impractical for use in industry\nand have raised questions on the direction of academic research. In this work\nwe show how integrating and extending current analyses is not only feasible,\nbut also a next logical step in smart-contract security. We propose\nlight-weight static checks on the morphology and dynamics of Solidity code,\nstemming from a developer-centric notion of vulnerability, that we use to\nverify the output of other tools, flag potential false alarms, and suggest\nverifications. Besides technical details we implemented an open-source\nprototype. For three top-10 vulnerabilities it flags 324 warnings of other\ntools as false-positives, in 60 verified de-duplicated smart contracts selected\nfrom the blockchain by the presence of true (and false) vulnerabilities. This\namounts to a 92%- to 100%-reduction in the number of false-positives for these\nvulnerabilities.\n","authors":["Tommaso Oss","Carlos E. Budde"],"pdf_url":"https://arxiv.org/pdf/2410.17204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17184v1","updated":"2024-10-22T17:02:43Z","published":"2024-10-22T17:02:43Z","title":"Technical Report: Toward Applying Quantum Computing to Network\n  Verification","summary":"  Network verification (NWV), broadly defined as the verification of properties\nof distributed protocols used in network systems, cannot be efficiently solved\non classical hardware via brute force. Prior work has developed a variety of\nmethods that scale by observing a structure in the search space and then\nevaluating classes within the search space instead of individual instances.\nHowever, even these classification mechanisms have their limitations. In this\npaper, we consider a radically different approach: applying quantum computing\nto more efficiently solve NWV problems. We provide an overview of how to map\nvariants of NWV problems into unstructured search problems that can be solved\nvia quantum computing with quadratic speedup, making the approach feasible in\ntheory to problems that are double in size (of the input). Emerging quantum\nsystems cannot yet tackle problems of practical interest, but rapid advances in\nhardware and algorithm development make now a great time to start thinking\nabout their application. With this in mind, we explore the limits of scale of\nthe problem for which quantum computing can solve NWV problems as unstructured\nsearch.\n","authors":["Kahlil Dozier","Justin Beltran","Kylie Berg","Hugo Matousek","Loqman Salamatian","Ethan Katz-Bassett","Dan Rubenstein"],"pdf_url":"https://arxiv.org/pdf/2410.17184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17175v1","updated":"2024-10-22T16:51:36Z","published":"2024-10-22T16:51:36Z","title":"Remote Timing Attacks on Efficient Language Model Inference","summary":"  Scaling up language models has significantly increased their capabilities.\nBut larger models are slower models, and so there is now an extensive body of\nwork (e.g., speculative sampling or parallel decoding) that improves the\n(average case) efficiency of language model generation. But these techniques\nintroduce data-dependent timing characteristics. We show it is possible to\nexploit these timing differences to mount a timing attack. By monitoring the\n(encrypted) network traffic between a victim user and a remote language model,\nwe can learn information about the content of messages by noting when responses\nare faster or slower. With complete black-box access, on open source systems we\nshow how it is possible to learn the topic of a user's conversation (e.g.,\nmedical advice vs. coding assistance) with 90%+ precision, and on production\nsystems like OpenAI's ChatGPT and Anthropic's Claude we can distinguish between\nspecific messages or infer the user's language. We further show that an active\nadversary can leverage a boosting attack to recover PII placed in messages\n(e.g., phone numbers or credit card numbers) for open source systems. We\nconclude with potential defenses and directions for future work.\n","authors":["Nicholas Carlini","Milad Nasr"],"pdf_url":"https://arxiv.org/pdf/2410.17175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11338v3","updated":"2024-10-22T16:39:19Z","published":"2023-06-20T07:14:37Z","title":"FDINet: Protecting against DNN Model Extraction via Feature Distortion\n  Index","summary":"  Machine Learning as a Service (MLaaS) platforms have gained popularity due to\ntheir accessibility, cost-efficiency, scalability, and rapid development\ncapabilities. However, recent research has highlighted the vulnerability of\ncloud-based models in MLaaS to model extraction attacks. In this paper, we\nintroduce FDINET, a novel defense mechanism that leverages the feature\ndistribution of deep neural network (DNN) models. Concretely, by analyzing the\nfeature distribution from the adversary's queries, we reveal that the feature\ndistribution of these queries deviates from that of the model's training set.\nBased on this key observation, we propose Feature Distortion Index (FDI), a\nmetric designed to quantitatively measure the feature distribution deviation of\nreceived queries. The proposed FDINET utilizes FDI to train a binary detector\nand exploits FDI similarity to identify colluding adversaries from distributed\nextraction attacks. We conduct extensive experiments to evaluate FDINET against\nsix state-of-the-art extraction attacks on four benchmark datasets and four\npopular model architectures. Empirical results demonstrate the following\nfindings FDINET proves to be highly effective in detecting model extraction,\nachieving a 100% detection accuracy on DFME and DaST. FDINET is highly\nefficient, using just 50 queries to raise an extraction alarm with an average\nconfidence of 96.08% for GTSRB. FDINET exhibits the capability to identify\ncolluding adversaries with an accuracy exceeding 91%. Additionally, it\ndemonstrates the ability to detect two types of adaptive attacks.\n","authors":["Hongwei Yao","Zheng Li","Haiqin Weng","Feng Xue","Zhan Qin","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2306.11338v3.pdf","comment":"Accepted to IEEE Transactions on Dependable and Secure Computing"},{"id":"http://arxiv.org/abs/2410.17141v1","updated":"2024-10-22T16:18:41Z","published":"2024-10-22T16:18:41Z","title":"Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements","summary":"  Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models.\n","authors":["Isamu Isozaki","Manil Shrestha","Rick Console","Edward Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17141v1.pdf","comment":"Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures"},{"id":"http://arxiv.org/abs/2410.17127v1","updated":"2024-10-22T16:00:26Z","published":"2024-10-22T16:00:26Z","title":"PAPILLON: PrivAcy Preservation from Internet-based and Local Language\n  MOdel ENsembles","summary":"  Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code will be available at https://github.com/siyan-sylvia-li/PAPILLON.\n","authors":["Li Siyan","Vethavikashini Chithrra Raghuram","Omar Khattab","Julia Hirschberg","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.17127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17116v1","updated":"2024-10-22T15:48:00Z","published":"2024-10-22T15:48:00Z","title":"Security and RAS in the Computing Continuum","summary":"  Security and RAS are two non-functional requirements under focus for current\nsystems developed for the computing continuum. Due to the increased number of\ninterconnected computer systems across the continuum, security becomes\nespecially pervasive at all levels, from the smallest edge device to the\nhigh-performance cloud at the other end. Similarly, RAS (Reliability,\nAvailability, and Serviceability) ensures the robustness of a system towards\nhardware defects. Namely, making them reliable, with high availability and\ndesign for easy service. In this paper and as a result of the Vitamin-V EU\nproject, the authors detail the comprehensive approach to malware and hardware\nattack detection; as well as, the RAS features envisioned for future systems\nacross the computing continuum.\n","authors":["Martí Alonso","David Andreu","Ramon Canal","Stefano Di Carlo","Odysseas Chatzopoulos","Cristiano Chenet","Juanjo Costa","Andreu Girones","Dimitris Gizopoulos","George Papadimitriou","Enric Morancho","Beatriz Otero","Alessandro Savino"],"pdf_url":"https://arxiv.org/pdf/2410.17116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17106v1","updated":"2024-10-22T15:30:24Z","published":"2024-10-22T15:30:24Z","title":"Feature Homomorphism -- A Cryptographic Scheme For Data Verification\n  Under Ciphertext-Only Conditions","summary":"  Privacy computing involves the extensive exchange and processing of encrypted\ndata. For the parties involved in these interactions, how to determine the\nconsistency of exchanged data without accessing the original data, ensuring\ntamper resistance, non-repudiation, quality traceability, indexing, and\nretrieval during the use of encrypted data, which is a key topic of achieving\n\"Data Availability versus Visibility\". This paper proposes a new type of\nhomomorphism: Feature Homomorphism, and based on this feature, introduces a\ncryptographic scheme for data verification under ciphertext-only conditions.\nThe proposed scheme involves designing a group of algorithms that meet the\nrequirements outlined in this paper, including encryption/decryption algorithms\nand Feature Homomorphic Algorithm. This group of algorithms not only allows for\nthe encryption and decryption of data but also ensures that the plaintext and\nits corresponding ciphertext, encrypted using the specified encryption\nalgorithm, satisfy the following property: the eigenvalue of the plaintext\nobtained using the Feature Homomorphic Algorithm is equal to the eigenvalue of\nthe ciphertext obtained using the same algorithm. With this group of\nalgorithms, it is possible to verify data consistency directly by comparing the\neigenvalues of the plaintext and ciphertext without accessing the original data\n(i.e., under ciphertext-only conditions). This can be used for tamper\nresistance, non-repudiation, and quality traceability. Additionally, the\neigenvalue can serve as a ciphertext index, enabling searchable encryption.\nThis scheme completes a piece of the puzzle in homomorphic encryption.\n  Keywords: Privacy Computing, Data Consistency, Searchable Encryption,\nZero-Knowledge Proof, Feature Homomorphism\n","authors":["Huang Neng"],"pdf_url":"https://arxiv.org/pdf/2410.17106v1.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17095v1","updated":"2024-10-22T15:21:00Z","published":"2024-10-22T15:21:00Z","title":"Inferentially-Private Private Information","summary":"  Information disclosure can compromise privacy when revealed information is\ncorrelated with private information. We consider the notion of inferential\nprivacy, which measures privacy leakage by bounding the inferential power a\nBayesian adversary can gain by observing a released signal. Our goal is to\ndevise an inferentially-private private information structure that maximizes\nthe informativeness of the released signal, following the Blackwell ordering\nprinciple, while adhering to inferential privacy constraints. To achieve this,\nwe devise an efficient release mechanism that achieves the\ninferentially-private Blackwell optimal private information structure for the\nsetting where the private information is binary. Additionally, we propose a\nprogramming approach to compute the optimal structure for general cases given\nthe utility function. The design of our mechanisms builds on our geometric\ncharacterization of the Blackwell-optimal disclosure mechanisms under privacy\nconstraints, which may be of independent interest.\n","authors":["Shuaiqi Wang","Shuran Zheng","Zinan Lin","Giulia Fanti","Zhiwei Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18816v3","updated":"2024-10-22T15:12:37Z","published":"2024-04-29T15:52:45Z","title":"AppPoet: Large Language Model based Android malware detection via\n  multi-view prompt engineering","summary":"  Due to the vast array of Android applications, their multifarious functions\nand intricate behavioral semantics, attackers can adopt various tactics to\nconceal their genuine attack intentions within legitimate functions. However,\nnumerous learning-based methods suffer from a limitation in mining behavioral\nsemantic information, thus impeding the accuracy and efficiency of Android\nmalware detection. Besides, the majority of existing learning-based methods are\nweakly interpretive and fail to furnish researchers with effective and readable\ndetection reports. Inspired by the success of the Large Language Models (LLMs)\nin natural language understanding, we propose AppPoet, a LLM-assisted\nmulti-view system for Android malware detection. Firstly, AppPoet employs a\nstatic method to comprehensively collect application features and formulate\nvarious observation views. Then, using our carefully crafted multi-view prompt\ntemplates, it guides the LLM to generate function descriptions and behavioral\nsummaries for each view, enabling deep semantic analysis of the views. Finally,\nwe collaboratively fuse the multi-view information to efficiently and\naccurately detect malware through a deep neural network (DNN) classifier and\nthen generate the human-readable diagnostic reports. Experimental results\ndemonstrate that our method achieves a detection accuracy of 97.15% and an F1\nscore of 97.21%, which is superior to the baseline methods. Furthermore, the\ncase study evaluates the effectiveness of our generated diagnostic reports.\n","authors":["Wenxiang Zhao","Juntao Wu","Zhaoyi Meng"],"pdf_url":"https://arxiv.org/pdf/2404.18816v3.pdf","comment":"Accepted by Expert Systems With Applications"},{"id":"http://arxiv.org/abs/2410.17052v1","updated":"2024-10-22T14:31:53Z","published":"2024-10-22T14:31:53Z","title":"On the Vulnerability of Text Sanitization","summary":"  Text sanitization, which employs differential privacy to replace sensitive\ntokens with new ones, represents a significant technique for privacy\nprotection. Typically, its performance in preserving privacy is evaluated by\nmeasuring the attack success rate (ASR) of reconstruction attacks, where\nattackers attempt to recover the original tokens from the sanitized ones.\nHowever, current reconstruction attacks on text sanitization are developed\nempirically, making it challenging to accurately assess the effectiveness of\nsanitization. In this paper, we aim to provide a more accurate evaluation of\nsanitization effectiveness. Inspired by the works of Palamidessi et al., we\nimplement theoretically optimal reconstruction attacks targeting text\nsanitization. We derive their bounds on ASR as benchmarks for evaluating\nsanitization performance. For real-world applications, we propose two practical\nreconstruction attacks based on these theoretical findings. Our experimental\nresults underscore the necessity of reassessing these overlooked risks.\nNotably, one of our attacks achieves a 46.4% improvement in ASR over the\nstate-of-the-art baseline, with a privacy budget of epsilon=4.0 on the SST-2\ndataset. Our code is available at:\nhttps://github.com/mengtong0110/On-the-Vulnerability-of-Text-Sanitization.\n","authors":["Meng Tong","Kejiang Chen","Xiaojian Yuang","Jiayang Liu","Weiming Zhang","Nenghai Yu","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17000v1","updated":"2024-10-22T13:22:08Z","published":"2024-10-22T13:22:08Z","title":"Beyond Yao's Millionaires: Secure Multi-Party Computation of\n  Non-Polynomial Functions","summary":"  In this paper, we present an unconditionally secure $N$-party comparison\nscheme based on Shamir secret sharing, utilizing the binary representation of\nprivate inputs to determine the $\\max$ without disclosing any private inputs or\nintermediate results. Specifically, each party holds a private number and aims\nto ascertain the greatest number among the $N$ available private numbers\nwithout revealing its input, assuming that there are at most $T < \\frac{N}{2}$\nhonest-but-curious parties. The proposed scheme demonstrates a lower\ncomputational complexity compared to existing schemes that can only compare two\nsecret numbers at a time. To the best of our knowledge, our scheme is the only\ninformation-theoretically secure method for comparing $N$ private numbers\nwithout revealing either the private inputs or any intermediate results. We\ndemonstrate that by modifying the proposed scheme, we can compute other\nwell-known non-polynomial functions of the inputs, including the minimum,\nmedian, and rank. Additionally, in the proposed scheme, before the final reveal\nphase, each party possesses a share of the result, enabling the nodes to\ncompute any polynomial function of the comparison result. We also explore\nvarious applications of the proposed comparison scheme, including federated\nlearning.\n","authors":["Seyed Reza Hoseini Najarkolaei","Mohammad Mahdi Mojahedian","Mohammad Reza Aref"],"pdf_url":"https://arxiv.org/pdf/2410.17000v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.16975v1","updated":"2024-10-22T12:55:02Z","published":"2024-10-22T12:55:02Z","title":"Publishing Neural Networks in Drug Discovery Might Compromise Training\n  Data Privacy","summary":"  This study investigates the risks of exposing confidential chemical\nstructures when machine learning models trained on these structures are made\npublicly available. We use membership inference attacks, a common method to\nassess privacy that is largely unexplored in the context of drug discovery, to\nexamine neural networks for molecular property prediction in a black-box\nsetting. Our results reveal significant privacy risks across all evaluated\ndatasets and neural network architectures. Combining multiple attacks increases\nthese risks. Molecules from minority classes, often the most valuable in drug\ndiscovery, are particularly vulnerable. We also found that representing\nmolecules as graphs and using message-passing neural networks may mitigate\nthese risks. We provide a framework to assess privacy risks of classification\nmodels and molecular representations. Our findings highlight the need for\ncareful consideration when sharing neural networks trained on proprietary\nchemical structures, informing organisations and researchers about the\ntrade-offs between data confidentiality and model openness.\n","authors":["Fabian P. Krüger","Johan Östman","Lewis Mervin","Igor V. Tetko","Ola Engkvist"],"pdf_url":"https://arxiv.org/pdf/2410.16975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16965v1","updated":"2024-10-22T12:47:10Z","published":"2024-10-22T12:47:10Z","title":"Downtime Required for Bitcoin Quantum-Safety","summary":"  Quantum devices capable of breaking the public-key cryptosystems that Bitcoin\nrelies on to secure its transactions are expected with reasonable probability\nwithin a decade. Quantum attacks would put at risk the entire Bitcoin network,\nwhich has an estimated value of around 500 billion USD. To prevent this threat,\na proactive approach is critical. The only known way to prevent any such attack\nis to upgrade the currently used public-key cryptosystems, namely ECDSA, with\nso-called post-quantum cryptosystems which have no known vulnerabilities to\nquantum attacks. In this paper, we analyse the technical cost of such an\nupgrade. We calculate a non-tight lower bound on the cumulative downtime\nrequired for the above transition to be 1827.96 hours, or 76.16 days. We also\ndemonstrate that the transition needs to be fully completed before the\navailability of ECDSA-256 breaking quantum devices, in order to ensure\nBitcoin's ongoing security. The conclusion is that the Bitcoin upgrade to\nquantum-safe protocols needs to be started as soon as possible in order to\nguarantee its ongoing operations.\n","authors":["Jamie J. Pont","Joseph J. Kearney","Jack Moyler","Carlos A. Perez-Delgado"],"pdf_url":"https://arxiv.org/pdf/2410.16965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16950v1","updated":"2024-10-22T12:24:41Z","published":"2024-10-22T12:24:41Z","title":"Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In","summary":"  Following the advancement of large language models (LLMs), the development of\nLLM-based autonomous agents has become increasingly prevalent. As a result, the\nneed to understand the security vulnerabilities of these agents has become a\ncritical task. We examine how ReAct agents can be exploited using a\nstraightforward yet effective method we refer to as the foot-in-the-door\nattack. Our experiments show that indirect prompt injection attacks, prompted\nby harmless and unrelated requests (such as basic calculations) can\nsignificantly increase the likelihood of the agent performing subsequent\nmalicious actions. Our results show that once a ReAct agents thought includes a\nspecific tool or action, the likelihood of executing this tool in the\nsubsequent steps increases significantly, as the agent seldom re-evaluates its\nactions. Consequently, even random, harmless requests can establish a\nfoot-in-the-door, allowing an attacker to embed malicious instructions into the\nagents thought process, making it more susceptible to harmful directives. To\nmitigate this vulnerability, we propose implementing a simple reflection\nmechanism that prompts the agent to reassess the safety of its actions during\nexecution, which can help reduce the success of such attacks.\n","authors":["Itay Nakash","George Kour","Guy Uziel","Ateret Anaby-Tavor"],"pdf_url":"https://arxiv.org/pdf/2410.16950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18736v3","updated":"2024-10-22T09:49:44Z","published":"2024-09-27T13:27:29Z","title":"Adversarial Challenges in Network Intrusion Detection Systems: Research\n  Insights and Future Prospects","summary":"  Machine learning has brought significant advances in cybersecurity,\nparticularly in the development of Intrusion Detection Systems (IDS). These\nimprovements are mainly attributed to the ability of machine learning\nalgorithms to identify complex relationships between features and effectively\ngeneralize to unseen data. Deep neural networks, in particular, contributed to\nthis progress by enabling the analysis of large amounts of training data,\nsignificantly enhancing detection performance. However, machine learning models\nremain vulnerable to adversarial attacks, where carefully crafted input data\ncan mislead the model into making incorrect predictions. While adversarial\nthreats in unstructured data, such as images and text, have been extensively\nstudied, their impact on structured data like network traffic is less explored.\nThis survey aims to address this gap by providing a comprehensive review of\nmachine learning-based Network Intrusion Detection Systems (NIDS) and\nthoroughly analyzing their susceptibility to adversarial attacks. We critically\nexamine existing research in NIDS, highlighting key trends, strengths, and\nlimitations, while identifying areas that require further exploration.\nAdditionally, we discuss emerging challenges in the field and offer insights\nfor the development of more robust and resilient NIDS. In summary, this paper\nenhances the understanding of adversarial attacks and defenses in NIDS and\nguide future research in improving the robustness of machine learning models in\ncybersecurity applications.\n","authors":["Sabrine Ennaji","Fabio De Gaspari","Dorjan Hitaj","Alicia Kbidi","Luigi V. Mancini"],"pdf_url":"https://arxiv.org/pdf/2409.18736v3.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2404.03348v2","updated":"2024-10-22T09:31:49Z","published":"2024-04-04T10:28:55Z","title":"Knowledge Distillation-Based Model Extraction Attack using GAN-based\n  Private Counterfactual Explanations","summary":"  In recent years, there has been a notable increase in the deployment of\nmachine learning (ML) models as services (MLaaS) across diverse production\nsoftware applications. In parallel, explainable AI (XAI) continues to evolve,\naddressing the necessity for transparency and trustworthiness in ML models. XAI\ntechniques aim to enhance the transparency of ML models by providing insights,\nin terms of model's explanations, into their decision-making process.\nSimultaneously, some MLaaS platforms now offer explanations alongside the ML\nprediction outputs. This setup has elevated concerns regarding vulnerabilities\nin MLaaS, particularly in relation to privacy leakage attacks such as model\nextraction attacks (MEA). This is due to the fact that explanations can unveil\ninsights about the inner workings of the model which could be exploited by\nmalicious users. In this work, we focus on investigating how model\nexplanations, particularly counterfactual explanations (CFs), can be exploited\nfor performing MEA within the MLaaS platform. We also delve into assessing the\neffectiveness of incorporating differential privacy (DP) as a mitigation\nstrategy. To this end, we first propose a novel approach for MEA based on\nKnowledge Distillation (KD) to enhance the efficiency of extracting a\nsubstitute model of a target model exploiting CFs, without any knowledge about\nthe training data distribution by the attacker. Then, we advise an approach for\ntraining CF generators incorporating DP to generate private CFs. We conduct\nthorough experimental evaluations on real-world datasets and demonstrate that\nour proposed KD-based MEA can yield a high-fidelity substitute model with a\nreduced number of queries with respect to baseline approaches. Furthermore, our\nfindings reveal that including a privacy layer can allow mitigating the MEA.\nHowever, on the account of the quality of CFs, impacts the performance of the\nexplanations.\n","authors":["Fatima Ezzeddine","Omran Ayoub","Silvia Giordano"],"pdf_url":"https://arxiv.org/pdf/2404.03348v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2410.11673v2","updated":"2024-10-22T09:10:00Z","published":"2024-10-15T15:06:13Z","title":"Generative Image Steganography Based on Point Cloud","summary":"  In deep steganography, the model size is usually related to the underlying\nmesh resolution, and a separate neural network needs to be trained as a message\nextractor. In this paper, we propose a generative image steganography based on\npoint cloud representation, which represents image data as a point cloud,\nlearns the distribution of the point cloud data, and represents it in the form\nof a continuous function. This method breaks through the limitation of the\nimage resolution, and can generate images with arbitrary resolution according\nto the actual need, and omits the need for explicit data for image\nsteganography. At the same time, using a fixed point cloud extractor transfers\nthe training of the network to the point cloud data, which saves the training\ntime and avoids the risk of exposing the steganography behavior caused by the\ntransmission of the message extractor. Experiments prove that the\nsteganographic images generated by the scheme have very high image quality and\nthe accuracy of message extraction reaches more than 99%.\n","authors":["Zhong Yangjie","Liu Jia","Liu Meiqi","Ke Yan","Zhang Minqing"],"pdf_url":"https://arxiv.org/pdf/2410.11673v2.pdf","comment":"11pages,13figures"},{"id":"http://arxiv.org/abs/2410.16805v1","updated":"2024-10-22T08:32:17Z","published":"2024-10-22T08:32:17Z","title":"Test-time Adversarial Defense with Opposite Adversarial Path and High\n  Attack Time Cost","summary":"  Deep learning models are known to be vulnerable to adversarial attacks by\ninjecting sophisticated designed perturbations to input data. Training-time\ndefenses still exhibit a significant performance gap between natural accuracy\nand robust accuracy. In this paper, we investigate a new test-time adversarial\ndefense method via diffusion-based recovery along opposite adversarial paths\n(OAPs). We present a purifier that can be plugged into a pre-trained model to\nresist adversarial attacks. Different from prior arts, the key idea is\nexcessive denoising or purification by integrating the opposite adversarial\ndirection with reverse diffusion to push the input image further toward the\nopposite adversarial direction. For the first time, we also exemplify the\npitfall of conducting AutoAttack (Rand) for diffusion-based defense methods.\nThrough the lens of time complexity, we examine the trade-off between the\neffectiveness of adaptive attack and its computation complexity against our\ndefense. Experimental evaluation along with time cost analysis verifies the\neffectiveness of the proposed method.\n","authors":["Cheng-Han Yeh","Kuanchun Yu","Chun-Shien Lu"],"pdf_url":"https://arxiv.org/pdf/2410.16805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16720v1","updated":"2024-10-22T06:00:04Z","published":"2024-10-22T06:00:04Z","title":"NodeOP: Optimizing Node Management for Decentralized Networks","summary":"  We present NodeOP, a novel framework designed to optimize the management of\nGeneral Node Operators in decentralized networks. By integrating Agent-Based\nModeling (ABM) with a Tendermint Byzantine Fault Tolerance (BFT)-based\nconsensus mechanism, NodeOP addresses key challenges in task allocation,\nconsensus formation, and system stability. Through rigorous mathematical\nmodeling and formal optimization, NodeOP ensures stable equilibrium in node\ntask distribution. We validate the framework via convergence analysis and\nperformance metrics such as transaction throughput, system latency, and fault\ntolerance. We further demonstrate NodeOP's practical utility through two use\ncases: decentralized sequencer management in Layer 2 networks and off-chain\npayment validation. These examples underscore how NodeOP enhances validation\nefficiency and unlocks new revenue opportunities in large-scale decentralized\nenvironments. Our results position NodeOP as a scalable and flexible solution,\nsignificantly improving operational efficiency and economic sustainability in\ndecentralized systems.\n","authors":["Angela Tsang","Jiankai Sun","Boo Xie","Azeem Khan","Ender Lu","Fletcher Fan","Maggie Wu","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2410.16720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16705v1","updated":"2024-10-22T05:20:21Z","published":"2024-10-22T05:20:21Z","title":"Privacy-hardened and hallucination-resistant synthetic data generation\n  with logic-solvers","summary":"  Machine-generated data is a valuable resource for training Artificial\nIntelligence algorithms, evaluating rare workflows, and sharing data under\nstricter data legislations. The challenge is to generate data that is accurate\nand private. Current statistical and deep learning methods struggle with large\ndata volumes, are prone to hallucinating scenarios incompatible with reality,\nand seldom quantify privacy meaningfully. Here we introduce Genomator, a logic\nsolving approach (SAT solving), which efficiently produces private and\nrealistic representations of the original data. We demonstrate the method on\ngenomic data, which arguably is the most complex and private information.\nSynthetic genomes hold great potential for balancing underrepresented\npopulations in medical research and advancing global data exchange. We\nbenchmark Genomator against state-of-the-art methodologies (Markov generation,\nRestricted Boltzmann Machine, Generative Adversarial Network and Conditional\nRestricted Boltzmann Machines), demonstrating an 84-93% accuracy improvement\nand 95-98% higher privacy. Genomator is also 1000-1600 times more efficient,\nmaking it the only tested method that scales to whole genomes. We show the\nuniversal trade-off between privacy and accuracy, and use Genomator's tuning\ncapability to cater to all applications along the spectrum, from provable\nprivate representations of sensitive cohorts, to datasets with\nindistinguishable pharmacogenomic profiles. Demonstrating the production-scale\ngeneration of tuneable synthetic data can increase trust and pave the way into\nthe clinic.\n","authors":["Mark A. Burgess","Brendan Hosking","Roc Reguant","Anubhav Kaphle","Mitchell J. O'Brien","Letitia M. F. Sng","Yatish Jain","Denis C. Bauer"],"pdf_url":"https://arxiv.org/pdf/2410.16705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03065v2","updated":"2024-10-22T05:11:09Z","published":"2023-07-06T15:32:50Z","title":"Quantum Complexity for Discrete Logarithms and Related Problems","summary":"  This paper studies the quantum computational complexity of the discrete\nlogarithm (DL) and related group-theoretic problems in the context of generic\nalgorithms -- that is, algorithms that do not exploit any properties of the\ngroup encoding.\n  We establish a generic model of quantum computation for group-theoretic\nproblems, which we call the quantum generic group model. Shor's algorithm for\nthe DL problem and related algorithms can be described in this model. We show\nthe quantum complexity lower bounds and almost matching algorithms of the DL\nand related problems in this model. More precisely, we prove the following\nresults for a cyclic group $G$ of prime order.\n  - Any generic quantum DL algorithm must make $\\Omega(\\log |G|)$ depth of\ngroup operations. This shows that Shor's algorithm is asymptotically optimal\namong the generic quantum algorithms, even considering parallel algorithms.\n  - We observe that variations of Shor's algorithm can take advantage of\nclassical computations to reduce the number of quantum group operations. We\nintroduce a model for generic hybrid quantum-classical algorithms and show that\nthese algorithms are almost optimal in this model. Any generic hybrid algorithm\nfor the DL problem with a total number of group operations $Q$ must make\n$\\Omega(\\log |G|/\\log Q)$ quantum group operations of depth $\\Omega(\\log\\log\n|G| - \\log\\log Q)$.\n  - When the quantum memory can only store $t$ group elements and use quantum\nrandom access memory of $r$ group elements, any generic hybrid algorithm must\nmake either $\\Omega(\\sqrt{|G|})$ group operations in total or $\\Omega(\\log\n|G|/\\log (tr))$ quantum group operations.\n  As a side contribution, we show a multiple DL problem admits a better\nalgorithm than solving each instance one by one, refuting a strong form of the\nquantum annoying property suggested in the context of password-authenticated\nkey exchange protocol.\n","authors":["Minki Hhan","Takashi Yamakawa","Aaram Yun"],"pdf_url":"https://arxiv.org/pdf/2307.03065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08660v2","updated":"2024-10-22T04:22:45Z","published":"2024-10-11T09:39:11Z","title":"RePD: Defending Jailbreak Attack through a Retrieval-based Prompt\n  Decomposition Process","summary":"  In this study, we introduce RePD, an innovative attack Retrieval-based Prompt\nDecomposition framework designed to mitigate the risk of jailbreak attacks on\nlarge language models (LLMs). Despite rigorous pretraining and finetuning\nfocused on ethical alignment, LLMs are still susceptible to jailbreak exploits.\nRePD operates on a one-shot learning model, wherein it accesses a database of\npre-collected jailbreak prompt templates to identify and decompose harmful\ninquiries embedded within user prompts. This process involves integrating the\ndecomposition of the jailbreak prompt into the user's original query into a\none-shot learning example to effectively teach the LLM to discern and separate\nmalicious components. Consequently, the LLM is equipped to first neutralize any\npotentially harmful elements before addressing the user's prompt in a manner\nthat aligns with its ethical guidelines. RePD is versatile and compatible with\na variety of open-source LLMs acting as agents. Through comprehensive\nexperimentation with both harmful and benign prompts, we have demonstrated the\nefficacy of our proposed RePD in enhancing the resilience of LLMs against\njailbreak attacks, without compromising their performance in responding to\ntypical user requests.\n","authors":["Peiran Wang","Xiaogeng Liu","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.08660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16648v1","updated":"2024-10-22T02:48:19Z","published":"2024-10-22T02:48:19Z","title":"BETA: Automated Black-box Exploration for Timing Attacks in Processors","summary":"  Modern processor advancements have introduced security risks, particularly in\nthe form of microarchitectural timing attacks. High-profile attacks such as\nMeltdown and Spectre have revealed critical flaws, compromising the entire\nsystem's security. Recent black-box automated methods have demonstrated their\nadvantages in identifying these vulnerabilities on various commercial\nprocessors. However, they often focus on specific attack types or incorporate\nnumerous ineffective test cases, which severely limits the detection scope and\nefficiency.\n  In this paper, we present BETA, a novel black-box framework that harnesses\nfuzzing to efficiently uncover multifaceted timing vulnerabilities in\nprocessors. Our framework employs a two-pronged approach, enhancing both\nmutation space and exploration efficiency: 1) we introduce an innovative fuzzer\nthat precisely constrains mutation direction for diverse instruction\ncombinations, including opcode, data, address, and execution level; 2) we\ndevelop a coverage feedback mechanism based on our instruction classification\nto discard potentially trivial or redundant test cases. This mechanism\nsignificantly expands coverage across a broader spectrum of instruction types.\nWe evaluate the performance and effectiveness of BETA on four processors from\nIntel and AMD, each featuring distinct microarchitectures. BETA has\nsuccessfully detected all x86 processor vulnerabilities previously identified\nby recent black-box methods, as well as 8 previously undiscovered timing\nvulnerabilities. BETA outperforms the existing state-of-the-art black-box\nmethods, achieving at least 3x faster detection speed.\n","authors":["Congcong Chen","Jinhua Cui","Jiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16648v1.pdf","comment":"This manuscript was first submitted to the ACM International\n  Conference on Architectural Support for Programming Languages and Operating\n  Systems on October 18, 2024 (Fall Cycle)"},{"id":"http://arxiv.org/abs/2406.18262v3","updated":"2024-10-22T02:27:55Z","published":"2024-06-26T11:20:15Z","title":"GlucOS: Security, correctness, and simplicity for automated insulin\n  delivery","summary":"  We present GlucOS, a novel system for trustworthy automated insulin delivery.\nFundamentally, this paper is about a system we designed, implemented, and\ndeployed on real humans and the lessons learned from our experiences. GlucOS\ncombines algorithmic security, driver security, and end-to-end verification to\nprotect against malicious ML models, vulnerable pump drivers, and drastic\nchanges in human physiology. We use formal methods to prove correctness of\ncritical components and incorporate humans as part of our defensive strategy.\nOur evaluation includes both a real-world deployment with seven individuals and\nresults from simulation to show that our techniques generalize. Our results\nshow that GlucOS maintains safety and improves glucose control even under\nattack conditions. This work demonstrates the potential for secure,\npersonalized, automated healthcare systems. Our source code is open source.\n","authors":["Hari Venugopalan","Shreyas Madhav Ambattur Vijayanand","Caleb Stanford","Stephanie Crossen","Samuel T. King"],"pdf_url":"https://arxiv.org/pdf/2406.18262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16618v1","updated":"2024-10-22T02:06:38Z","published":"2024-10-22T02:06:38Z","title":"SoK: Dataset Copyright Auditing in Machine Learning Systems","summary":"  As the implementation of machine learning (ML) systems becomes more\nwidespread, especially with the introduction of larger ML models, we perceive a\nspring demand for massive data. However, it inevitably causes infringement and\nmisuse problems with the data, such as using unauthorized online artworks or\nface images to train ML models. To address this problem, many efforts have been\nmade to audit the copyright of the model training dataset. However, existing\nsolutions vary in auditing assumptions and capabilities, making it difficult to\ncompare their strengths and weaknesses. In addition, robustness evaluations\nusually consider only part of the ML pipeline and hardly reflect the\nperformance of algorithms in real-world ML applications. Thus, it is essential\nto take a practical deployment perspective on the current dataset copyright\nauditing tools, examining their effectiveness and limitations. Concretely, we\ncategorize dataset copyright auditing research into two prominent strands:\nintrusive methods and non-intrusive methods, depending on whether they require\nmodifications to the original dataset. Then, we break down the intrusive\nmethods into different watermark injection options and examine the\nnon-intrusive methods using various fingerprints. To summarize our results, we\noffer detailed reference tables, highlight key points, and pinpoint unresolved\nissues in the current literature. By combining the pipeline in ML systems and\nanalyzing previous studies, we highlight several future directions to make\nauditing tools more suitable for real-world copyright protection requirements.\n","authors":["Linkang Du","Xuanru Zhou","Min Chen","Chusong Zhang","Zhou Su","Peng Cheng","Jiming Chen","Zhikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16618v1.pdf","comment":"To appear in the IEEE Symposium on Security and Privacy 2025, San\n  Francisco, CA, USA"},{"id":"http://arxiv.org/abs/2410.16612v1","updated":"2024-10-22T01:50:07Z","published":"2024-10-22T01:50:07Z","title":"OMLog: Online Log Anomaly Detection for Evolving System with\n  Meta-learning","summary":"  Log anomaly detection (LAD) is essential to ensure safe and stable operation\nof software systems. Although current LAD methods exhibit significant potential\nin addressing challenges posed by unstable log events and temporal sequence\npatterns, their limitations in detection efficiency and generalization ability\npresent a formidable challenge when dealing with evolving systems. To construct\na real-time and reliable online log anomaly detection model, we propose OMLog,\na semi-supervised online meta-learning method, to effectively tackle the\ndistribution shift issue caused by changes in log event types and frequencies.\nSpecifically, we introduce a maximum mean discrepancy-based distribution shift\ndetection method to identify distribution changes in unseen log sequences.\nDepending on the identified distribution gap, the method can automatically\ntrigger online fine-grained detection or offline fast inference. Furthermore,\nwe design an online learning mechanism based on meta-learning, which can\neffectively learn the highly repetitive patterns of log sequences in the\nfeature space, thereby enhancing the generalization ability of the model to\nevolving data. Extensive experiments conducted on two publicly available log\ndatasets, HDFS and BGL, validate the effectiveness of the OMLog approach. When\ntrained using only normal log sequences, the proposed approach achieves the\nF1-Score of 93.7\\% and 64.9\\%, respectively, surpassing the performance of the\nstate-of-the-art (SOTA) LAD methods and demonstrating superior detection\nefficiency.\n","authors":["Jiyu Tian","Mingchu Li","Zumin Wang","Liming Chen","Jing Qin","Runfa Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16612v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2307.00143v2","updated":"2024-10-22T01:39:34Z","published":"2023-06-30T21:27:54Z","title":"FP-Rowhammer: DRAM-Based Device Fingerprinting","summary":"  Device fingerprinting leverages attributes that capture heterogeneity in\nhardware and software configurations to extract unique and stable fingerprints.\nFingerprinting countermeasures attempt to either present a uniform fingerprint\nacross different devices through normalization or present different\nfingerprints for the same device each time through obfuscation. We present\nFP-Rowhammer, a Rowhammer-based device fingerprinting approach that can build\nunique and stable fingerprints even across devices with normalized or\nobfuscated hardware and software configurations. To this end, FP-Rowhammer\nleverages the DRAM manufacturing process variation that gives rise to unique\ndistributions of Rowhammer-induced bit flips across different DRAM modules. Our\nevaluation on a test bed of 98 DRAM modules shows that FP-Rowhammer achieves\n99.91% fingerprinting accuracy. FP-Rowhammer's fingerprints are also stable,\nwith no degradation in fingerprinting accuracy over a period of ten days. We\nalso demonstrate that FP-Rowhammer is efficient, taking less than five seconds\nto extract a fingerprint. FP-Rowhammer is the first Rowhammer fingerprinting\napproach that is able to extract unique and stable fingerprints efficiently and\nat scale.\n","authors":["Hari Venugopalan","Kaustav Goswami","Zainul Abi Din","Jason Lowe-Power","Samuel T. King","Zubair Shafiq"],"pdf_url":"https://arxiv.org/pdf/2307.00143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16595v1","updated":"2024-10-22T00:41:47Z","published":"2024-10-22T00:41:47Z","title":"(Quantum) Indifferentiability and Pre-Computation","summary":"  Indifferentiability is a popular cryptographic paradigm for analyzing the\nsecurity of ideal objects -- both in a classical as well as in a quantum world.\nIt is typically stated in the form of a composable and simulation-based\ndefinition, and captures what it means for a construction (e.g., a\ncryptographic hash function) to be ``as good as'' an ideal object (e.g., a\nrandom oracle). Despite its strength, indifferentiability is not known to offer\nsecurity against pre-processing attacks in which the adversary gains access to\n(classical or quantum) advice that is relevant to the particular construction.\nIn this work, we show that indifferentiability is (generically) insufficient\nfor capturing pre-computation. To accommodate this shortcoming, we propose a\nstrengthening of indifferentiability which is not only composable but also\ntakes arbitrary pre-computation into account. As an application, we show that\nthe one-round sponge is indifferentiable (with pre-computation) from a random\noracle. This yields the first (and tight) classical/quantum space-time\ntrade-off for one-round sponge inversion.\n","authors":["Joseph Carolan","Alexander Poremba","Mark Zhandry"],"pdf_url":"https://arxiv.org/pdf/2410.16595v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2410.17468v1","updated":"2024-10-22T22:50:17Z","published":"2024-10-22T22:50:17Z","title":"Formal Privacy Guarantees with Invariant Statistics","summary":"  Motivated by the 2020 US Census products, this paper extends differential\nprivacy (DP) to address the joint release of DP outputs and nonprivate\nstatistics, referred to as invariant. Our framework, Semi-DP, redefines\nadjacency by focusing on datasets that conform to the given invariant, ensuring\nindistinguishability between adjacent datasets within invariant-conforming\ndatasets. We further develop customized mechanisms that satisfy Semi-DP,\nincluding the Gaussian mechanism and the optimal $K$-norm mechanism for\nrank-deficient sensitivity spaces. Our framework is applied to contingency\ntable analysis which is relevant to the 2020 US Census, illustrating how\nSemi-DP enables the release of private outputs given the one-way margins as the\ninvariant. Additionally, we provide a privacy analysis of the 2020 US Decennial\nCensus using the Semi-DP framework, revealing that the effective privacy\nguarantees are weaker than advertised.\n","authors":["Young Hyun Cho","Jordan Awan"],"pdf_url":"https://arxiv.org/pdf/2410.17468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17459v1","updated":"2024-10-22T22:31:03Z","published":"2024-10-22T22:31:03Z","title":"Data Obfuscation through Latent Space Projection (LSP) for\n  Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and\n  Finance Fraud Detection","summary":"  As AI systems increasingly integrate into critical societal sectors, the\ndemand for robust privacy-preserving methods has escalated. This paper\nintroduces Data Obfuscation through Latent Space Projection (LSP), a novel\ntechnique aimed at enhancing AI governance and ensuring Responsible AI\ncompliance. LSP uses machine learning to project sensitive data into a latent\nspace, effectively obfuscating it while preserving essential features for model\ntraining and inference. Unlike traditional privacy methods like differential\nprivacy or homomorphic encryption, LSP transforms data into an abstract,\nlower-dimensional form, achieving a delicate balance between data utility and\nprivacy. Leveraging autoencoders and adversarial training, LSP separates\nsensitive from non-sensitive information, allowing for precise control over\nprivacy-utility trade-offs. We validate LSP's effectiveness through experiments\non benchmark datasets and two real-world case studies: healthcare cancer\ndiagnosis and financial fraud analysis. Our results show LSP achieves high\nperformance (98.7% accuracy in image classification) while providing strong\nprivacy (97.3% protection against sensitive attribute inference), outperforming\ntraditional anonymization and privacy-preserving methods. The paper also\nexamines LSP's alignment with global AI governance frameworks, such as GDPR,\nCCPA, and HIPAA, highlighting its contribution to fairness, transparency, and\naccountability. By embedding privacy within the machine learning pipeline, LSP\noffers a promising approach to developing AI systems that respect privacy while\ndelivering valuable insights. We conclude by discussing future research\ndirections, including theoretical privacy guarantees, integration with\nfederated learning, and enhancing latent space interpretability, positioning\nLSP as a critical tool for ethical AI advancement.\n","authors":["Mahesh Vaijainthymala Krishnamoorthy"],"pdf_url":"https://arxiv.org/pdf/2410.17459v1.pdf","comment":"19 pages, 6 figures, submitted to Conference ICADCML2025"},{"id":"http://arxiv.org/abs/2410.17442v1","updated":"2024-10-22T21:42:59Z","published":"2024-10-22T21:42:59Z","title":"Detecting Adversarial Examples","summary":"  Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial\nexamples. While numerous successful adversarial attacks have been proposed,\ndefenses against these attacks remain relatively understudied. Existing defense\napproaches either focus on negating the effects of perturbations caused by the\nattacks to restore the DNNs' original predictions or use a secondary model to\ndetect adversarial examples. However, these methods often become ineffective\ndue to the continuous advancements in attack techniques. We propose a novel\nuniversal and lightweight method to detect adversarial examples by analyzing\nthe layer outputs of DNNs. Through theoretical justification and extensive\nexperiments, we demonstrate that our detection method is highly effective,\ncompatible with any DNN architecture, and applicable across different domains,\nsuch as image, video, and audio.\n","authors":["Furkan Mumcu","Yasin Yilmaz"],"pdf_url":"https://arxiv.org/pdf/2410.17442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17340v2","updated":"2024-10-22T21:28:16Z","published":"2024-08-30T15:08:51Z","title":"On Computational Indistinguishability and Logical Relations","summary":"  A $\\lambda$-calculus is introduced in which all programs can be evaluated in\nprobabilistic polynomial time and in which there is sufficient structure to\nrepresent sequential cryptographic constructions and adversaries for them, even\nwhen the latter are oracle-based. A notion of observational equivalence\ncapturing computational indistinguishability and a class of approximate logical\nrelations are then presented, showing that the latter represent a sound proof\ntechnique for the former. The work concludes with the presentation of an\nexample of a security proof in which the encryption scheme induced by a\npseudorandom function is proven secure against active adversaries in a purely\nequational style.\n","authors":["Ugo Dal Lago","Zeinab Galal","Giulia Giusti"],"pdf_url":"https://arxiv.org/pdf/2408.17340v2.pdf","comment":"APLAS 2024 conference"},{"id":"http://arxiv.org/abs/2410.17431v1","updated":"2024-10-22T21:08:28Z","published":"2024-10-22T21:08:28Z","title":"Meta Stackelberg Game: Robust Federated Learning against Adaptive and\n  Mixed Poisoning Attacks","summary":"  Federated learning (FL) is susceptible to a range of security threats.\nAlthough various defense mechanisms have been proposed, they are typically\nnon-adaptive and tailored to specific types of attacks, leaving them\ninsufficient in the face of multiple uncertain, unknown, and adaptive attacks\nemploying diverse strategies. This work formulates adversarial federated\nlearning under a mixture of various attacks as a Bayesian Stackelberg Markov\ngame, based on which we propose the meta-Stackelberg defense composed of\npre-training and online adaptation. {The gist is to simulate strong attack\nbehavior using reinforcement learning (RL-based attacks) in pre-training and\nthen design meta-RL-based defense to combat diverse and adaptive attacks.} We\ndevelop an efficient meta-learning approach to solve the game, leading to a\nrobust and adaptive FL defense. Theoretically, our meta-learning algorithm,\nmeta-Stackelberg learning, provably converges to the first-order\n$\\varepsilon$-meta-equilibrium point in $O(\\varepsilon^{-2})$ gradient\niterations with $O(\\varepsilon^{-4})$ samples per iteration. Experiments show\nthat our meta-Stackelberg framework performs superbly against strong model\npoisoning and backdoor attacks of uncertain and unknown types.\n","authors":["Tao Li","Henger Li","Yunian Pan","Tianyi Xu","Zizhan Zheng","Quanyan Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.17431v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2410.17406v1","updated":"2024-10-22T20:28:57Z","published":"2024-10-22T20:28:57Z","title":"ProveRAG: Provenance-Driven Vulnerability Analysis with Automated\n  Retrieval-Augmented LLMs","summary":"  In cybersecurity, security analysts face the challenge of mitigating newly\ndiscovered vulnerabilities in real-time, with over 300,000 Common\nVulnerabilities and Exposures (CVEs) identified since 1999. The sheer volume of\nknown vulnerabilities complicates the detection of patterns for unknown\nthreats. While LLMs can assist, they often hallucinate and lack alignment with\nrecent threats. Over 25,000 vulnerabilities have been identified so far in\n2024, which are introduced after popular LLMs' (e.g., GPT-4) training data\ncutoff. This raises a major challenge of leveraging LLMs in cybersecurity,\nwhere accuracy and up-to-date information are paramount. In this work, we aim\nto improve the adaptation of LLMs in vulnerability analysis by mimicking how\nanalysts perform such tasks. We propose ProveRAG, an LLM-powered system\ndesigned to assist in rapidly analyzing CVEs with automated retrieval\naugmentation of web data while self-evaluating its responses with verifiable\nevidence. ProveRAG incorporates a self-critique mechanism to help alleviate\nomission and hallucination common in the output of LLMs applied in\ncybersecurity applications. The system cross-references data from verifiable\nsources (NVD and CWE), giving analysts confidence in the actionable insights\nprovided. Our results indicate that ProveRAG excels in delivering verifiable\nevidence to the user with over 99% and 97% accuracy in exploitation and\nmitigation strategies, respectively. This system outperforms direct prompting\nand chunking retrieval in vulnerability analysis by overcoming temporal and\ncontext-window limitations. ProveRAG guides analysts to secure their systems\nmore effectively while documenting the process for future audits.\n","authors":["Reza Fayyazi","Stella Hoyos Trueba","Michael Zuzak","Shanchieh Jay Yang"],"pdf_url":"https://arxiv.org/pdf/2410.17406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17401v1","updated":"2024-10-22T20:18:26Z","published":"2024-10-22T20:18:26Z","title":"AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents","summary":"  Vision Language Models (VLMs) have revolutionized the creation of generalist\nweb agents, empowering them to autonomously complete diverse tasks on\nreal-world websites, thereby boosting human efficiency and productivity.\nHowever, despite their remarkable capabilities, the safety and security of\nthese agents against malicious attacks remain critically underexplored, raising\nsignificant concerns about their safe deployment. To uncover and exploit such\nvulnerabilities in web agents, we provide AdvWeb, a novel black-box attack\nframework designed against web agents. AdvWeb trains an adversarial prompter\nmodel that generates and injects adversarial prompts into web pages, misleading\nweb agents into executing targeted adversarial actions such as inappropriate\nstock purchases or incorrect bank transactions, actions that could lead to\nsevere real-world consequences. With only black-box access to the web agent, we\ntrain and optimize the adversarial prompter model using DPO, leveraging both\nsuccessful and failed attack strings against the target agent. Unlike prior\napproaches, our adversarial string injection maintains stealth and control: (1)\nthe appearance of the website remains unchanged before and after the attack,\nmaking it nearly impossible for users to detect tampering, and (2) attackers\ncan modify specific substrings within the generated adversarial string to\nseamlessly change the attack objective (e.g., purchasing stocks from a\ndifferent company), enhancing attack flexibility and efficiency. We conduct\nextensive evaluations, demonstrating that AdvWeb achieves high success rates in\nattacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings\nexpose critical vulnerabilities in current LLM/VLM-based agents, emphasizing\nthe urgent need for developing more reliable web agents and effective defenses.\nOur code and data are available at https://ai-secure.github.io/AdvWeb/ .\n","authors":["Chejian Xu","Mintong Kang","Jiawei Zhang","Zeyi Liao","Lingbo Mo","Mengqi Yuan","Huan Sun","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.17401v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2405.11724v2","updated":"2024-10-22T19:07:08Z","published":"2024-05-20T01:57:34Z","title":"Token-wise Influential Training Data Retrieval for Large Language Models","summary":"  Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.\n","authors":["Huawei Lin","Jikai Long","Zhaozhuo Xu","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.11724v2.pdf","comment":"Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution"},{"id":"http://arxiv.org/abs/2402.13459v2","updated":"2024-10-22T19:01:35Z","published":"2024-02-21T01:30:03Z","title":"Learning to Poison Large Language Models During Instruction Tuning","summary":"  The advent of Large Language Models (LLMs) has marked significant\nachievements in language processing and reasoning capabilities. Despite their\nadvancements, LLMs face vulnerabilities to data poisoning attacks, where\nadversaries insert backdoor triggers into training data to manipulate outputs\nfor malicious purposes. This work further identifies additional security risks\nin LLMs by designing a new data poisoning attack tailored to exploit the\ninstruction tuning process. We propose a novel gradient-guided backdoor trigger\nlearning (GBTL) algorithm to identify adversarial triggers efficiently,\nensuring an evasion of detection by conventional defenses while maintaining\ncontent integrity. Through experimental validation across various tasks,\nincluding sentiment analysis, domain generation, and question answering, our\npoisoning strategy demonstrates a high success rate in compromising various\nLLMs' outputs. We further propose two defense strategies against data poisoning\nattacks, including in-context learning (ICL) and continuous learning (CL),\nwhich effectively rectify the behavior of LLMs and significantly reduce the\ndecline in performance. Our work highlights the significant security risks\npresent during the instruction tuning of LLMs and emphasizes the necessity of\nsafeguarding LLMs against data poisoning attacks.\n","authors":["Yao Qiang","Xiangyu Zhou","Saleh Zare Zade","Mohammad Amin Roshani","Prashant Khanduri","Douglas Zytko","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.13459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17361v1","updated":"2024-10-22T18:54:12Z","published":"2024-10-22T18:54:12Z","title":"Characterizing Robocalls with Multiple Vantage Points","summary":"  Telephone spam has been among the highest network security concerns for users\nfor many years. In response, industry and government have deployed new\ntechnologies and regulations to curb the problem, and academic and industry\nresearchers have provided methods and measurements to characterize robocalls.\nHave these efforts borne fruit? Are the research characterizations reliable,\nand have the prevention and deterrence mechanisms succeeded?\n  In this paper, we address these questions through analysis of data from\nseveral independently-operated vantage points, ranging from industry and\nacademic voice honeypots to public enforcement and consumer complaints, some\nwith over 5 years of historic data. We first describe how we address the\nnon-trivial methodological challenges of comparing disparate data sources,\nincluding comparing audio and transcripts from about 3 million voice calls. We\nalso detail the substantial coherency of these diverse perspectives, which\ndramatically strengthens the evidence for the conclusions we draw about\nrobocall characterization and mitigation while highlighting advantages of each\napproach. Among our many findings, we find that unsolicited calls are in slow\ndecline, though complaints and call volumes remain high. We also find that\nrobocallers have managed to adapt to STIR/SHAKEN, a mandatory call\nauthentication scheme. In total, our findings highlight the most promising\ndirections for future efforts to characterize and stop telephone spam.\n","authors":["Sathvik Prasad","Aleksandr Nahapetyan","Bradley Reaves"],"pdf_url":"https://arxiv.org/pdf/2410.17361v1.pdf","comment":"Accepted for publication at the 46th IEEE Symposium on Security and\n  Privacy, 2025"},{"id":"http://arxiv.org/abs/2410.17351v1","updated":"2024-10-22T18:35:05Z","published":"2024-10-22T18:35:05Z","title":"Hierarchical Multi-agent Reinforcement Learning for Cyber Network\n  Defense","summary":"  Recent advances in multi-agent reinforcement learning (MARL) have created\nopportunities to solve complex real-world tasks. Cybersecurity is a notable\napplication area, where defending networks against sophisticated adversaries\nremains a challenging task typically performed by teams of security operators.\nIn this work, we explore novel MARL strategies for building autonomous cyber\nnetwork defenses that address challenges such as large policy spaces, partial\nobservability, and stealthy, deceptive adversarial strategies. To facilitate\nefficient and generalized learning, we propose a hierarchical Proximal Policy\nOptimization (PPO) architecture that decomposes the cyber defense task into\nspecific sub-tasks like network investigation and host recovery. Our approach\ninvolves training sub-policies for each sub-task using PPO enhanced with domain\nexpertise. These sub-policies are then leveraged by a master defense policy\nthat coordinates their selection to solve complex network defense tasks.\nFurthermore, the sub-policies can be fine-tuned and transferred with minimal\ncost to defend against shifts in adversarial behavior or changes in network\nsettings. We conduct extensive experiments using CybORG Cage 4, the\nstate-of-the-art MARL environment for cyber defense. Comparisons with multiple\nbaselines across different adversaries show that our hierarchical learning\napproach achieves top performance in terms of convergence speed, episodic\nreturn, and several interpretable metrics relevant to cybersecurity, including\nthe fraction of clean machines on the network, precision, and false positives\non recoveries.\n","authors":["Aditya Vikram Singh","Ethan Rathbun","Emma Graham","Lisa Oakley","Simona Boboila","Alina Oprea","Peter Chin"],"pdf_url":"https://arxiv.org/pdf/2410.17351v1.pdf","comment":"9 pages, 7 figures, AAMAS preprint"}],"Networking and Internet Architecture":[{"id":"http://arxiv.org/abs/2410.15357v2","updated":"2024-10-22T02:10:57Z","published":"2024-10-20T10:53:42Z","title":"Wireless Link Quality Estimation Using LSTM Model","summary":"  In recent years, various services have been provided through high-speed and\nhigh-capacity wireless networks on mobile communication devices, necessitating\nstable communication regardless of indoor or outdoor environments. To achieve\nstable communication, it is essential to implement proactive measures, such as\nswitching to an alternative path and ensuring data buffering before the\ncommunication quality becomes unstable. The technology of Wireless Link Quality\nEstimation (WLQE), which predicts the communication quality of wireless\nnetworks in advance, plays a crucial role in this context. In this paper, we\npropose a novel WLQE model for estimating the communication quality of wireless\nnetworks by leveraging sequential information. Our proposed method is based on\nLong Short-Term Memory (LSTM), enabling highly accurate estimation by\nconsidering the sequential information of link quality. We conducted a\ncomparative evaluation with the conventional model, stacked autoencoder-based\nlink quality estimator (LQE-SAE), using a dataset recorded in real-world\nenvironmental conditions. Our LSTM-based LQE model demonstrates its\nsuperiority, achieving a 4.0% higher accuracy and a 4.6% higher macro-F1 score\nthan the LQE-SAE model in the evaluation.\n","authors":["Yuki Kanto","Kohei Watabe"],"pdf_url":"https://arxiv.org/pdf/2410.15357v2.pdf","comment":"This paper was submitted to IEEE Network Operations and Management\n  Symposium"},{"id":"http://arxiv.org/abs/2409.18736v3","updated":"2024-10-22T09:49:44Z","published":"2024-09-27T13:27:29Z","title":"Adversarial Challenges in Network Intrusion Detection Systems: Research\n  Insights and Future Prospects","summary":"  Machine learning has brought significant advances in cybersecurity,\nparticularly in the development of Intrusion Detection Systems (IDS). These\nimprovements are mainly attributed to the ability of machine learning\nalgorithms to identify complex relationships between features and effectively\ngeneralize to unseen data. Deep neural networks, in particular, contributed to\nthis progress by enabling the analysis of large amounts of training data,\nsignificantly enhancing detection performance. However, machine learning models\nremain vulnerable to adversarial attacks, where carefully crafted input data\ncan mislead the model into making incorrect predictions. While adversarial\nthreats in unstructured data, such as images and text, have been extensively\nstudied, their impact on structured data like network traffic is less explored.\nThis survey aims to address this gap by providing a comprehensive review of\nmachine learning-based Network Intrusion Detection Systems (NIDS) and\nthoroughly analyzing their susceptibility to adversarial attacks. We critically\nexamine existing research in NIDS, highlighting key trends, strengths, and\nlimitations, while identifying areas that require further exploration.\nAdditionally, we discuss emerging challenges in the field and offer insights\nfor the development of more robust and resilient NIDS. In summary, this paper\nenhances the understanding of adversarial attacks and defenses in NIDS and\nguide future research in improving the robustness of machine learning models in\ncybersecurity applications.\n","authors":["Sabrine Ennaji","Fabio De Gaspari","Dorjan Hitaj","Alicia Kbidi","Luigi V. Mancini"],"pdf_url":"https://arxiv.org/pdf/2409.18736v3.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2410.17184v1","updated":"2024-10-22T17:02:43Z","published":"2024-10-22T17:02:43Z","title":"Technical Report: Toward Applying Quantum Computing to Network\n  Verification","summary":"  Network verification (NWV), broadly defined as the verification of properties\nof distributed protocols used in network systems, cannot be efficiently solved\non classical hardware via brute force. Prior work has developed a variety of\nmethods that scale by observing a structure in the search space and then\nevaluating classes within the search space instead of individual instances.\nHowever, even these classification mechanisms have their limitations. In this\npaper, we consider a radically different approach: applying quantum computing\nto more efficiently solve NWV problems. We provide an overview of how to map\nvariants of NWV problems into unstructured search problems that can be solved\nvia quantum computing with quadratic speedup, making the approach feasible in\ntheory to problems that are double in size (of the input). Emerging quantum\nsystems cannot yet tackle problems of practical interest, but rapid advances in\nhardware and algorithm development make now a great time to start thinking\nabout their application. With this in mind, we explore the limits of scale of\nthe problem for which quantum computing can solve NWV problems as unstructured\nsearch.\n","authors":["Kahlil Dozier","Justin Beltran","Kylie Berg","Hugo Matousek","Loqman Salamatian","Ethan Katz-Bassett","Dan Rubenstein"],"pdf_url":"https://arxiv.org/pdf/2410.17184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17078v1","updated":"2024-10-22T14:56:50Z","published":"2024-10-22T14:56:50Z","title":"FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters","summary":"  The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce.\n","authors":["Hasibul Jamil","Abdul Alim","Laurent Schares","Pavlos Maniotis","Liran Schour","Ali Sydney","Abdullah Kayi","Tevfik Kosar","Bengi Karacali"],"pdf_url":"https://arxiv.org/pdf/2410.17078v1.pdf","comment":"Submitted for peer reviewing in IEEE ICC 2025"},{"id":"http://arxiv.org/abs/2410.17043v1","updated":"2024-10-22T14:19:29Z","published":"2024-10-22T14:19:29Z","title":"Optimizing Mixture-of-Experts Inference Time Combining Model Deployment\n  and Communication Scheduling","summary":"  As machine learning models scale in size and complexity, their computational\nrequirements become a significant barrier. Mixture-of-Experts (MoE) models\nalleviate this issue by selectively activating relevant experts. Despite this,\nMoE models are hindered by high communication overhead from all-to-all\noperations, low GPU utilization due to the synchronous communication\nconstraint, and complications from heterogeneous GPU environments.\n  This paper presents Aurora, which optimizes both model deployment and\nall-to-all communication scheduling to address these challenges in MoE\ninference. Aurora achieves minimal communication times by strategically\nordering token transmissions in all-to-all communications. It improves GPU\nutilization by colocating experts from different models on the same device,\navoiding the limitations of synchronous all-to-all communication. We analyze\nAurora's optimization strategies theoretically across four common GPU cluster\nsettings: exclusive vs. colocated models on GPUs, and homogeneous vs.\nheterogeneous GPUs. Aurora provides optimal solutions for three cases, and for\nthe remaining NP-hard scenario, it offers a polynomial-time sub-optimal\nsolution with only a 1.07x degradation from the optimal.\n  Aurora is the first approach to minimize MoE inference time via optimal model\ndeployment and communication scheduling across various scenarios. Evaluations\ndemonstrate that Aurora significantly accelerates inference, achieving speedups\nof up to 2.38x in homogeneous clusters and 3.54x in heterogeneous environments.\nMoreover, Aurora enhances GPU utilization by up to 1.5x compared to existing\nmethods.\n","authors":["Jialong Li","Shreyansh Tripathi","Lakshay Rastogi","Yiming Lei","Rui Pan","Yiting Xia"],"pdf_url":"https://arxiv.org/pdf/2410.17043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06997v3","updated":"2024-10-22T14:09:57Z","published":"2024-04-10T13:24:27Z","title":"Agent-driven Generative Semantic Communication with Cross-Modality and\n  Prediction","summary":"  In the era of 6G, with compelling visions of intelligent transportation\nsystems and digital twins, remote surveillance is poised to become a ubiquitous\npractice. Substantial data volume and frequent updates present challenges in\nwireless networks. To address these challenges, we propose a novel agent-driven\ngenerative semantic communication (A-GSC) framework based on reinforcement\nlearning. In contrast to the existing research on semantic communication\n(SemCom), which mainly focuses on either semantic extraction or semantic\nsampling, we seamlessly integrate both by jointly considering the intrinsic\nattributes of source information and the contextual information regarding the\ntask. Notably, the introduction of generative artificial intelligence (GAI)\nenables the independent design of semantic encoders and decoders. In this work,\nwe develop an agent-assisted semantic encoder with cross-modality capability,\nwhich can track the semantic changes, channel condition, to perform adaptive\nsemantic extraction and sampling. Accordingly, we design a semantic decoder\nwith both predictive and generative capabilities, consisting of two tailored\nmodules. Moreover, the effectiveness of the designed models has been verified\nusing the UA-DETRAC dataset, demonstrating the performance gains of the overall\nA-GSC framework in both energy saving and reconstruction accuracy.\n","authors":["Wanting Yang","Zehui Xiong","Yanli Yuan","Wenchao Jiang","Tony Q. S. Quek","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2404.06997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17012v1","updated":"2024-10-22T13:33:34Z","published":"2024-10-22T13:33:34Z","title":"Nanosecond Precision Time Synchronization for Optical Data Center\n  Networks","summary":"  Optical data center networks (DCNs) are renovating the infrastructure design\nfor the cloud in the post Moore's law era. The fact that optical DCNs rely on\noptical circuits of microsecond-scale durations makes nanosecond-precision time\nsynchronization essential for the correct functioning of routing on the network\nfabric. However, current studies on optical DCNs neglect the fundamental need\nfor accurate time synchronization. In this paper, we bridge the gap by\ndeveloping Nanosecond Optical Synchronization (NOS), the first\nnanosecond-precision synchronization solution for optical DCNs general to\nvarious optical hardware. NOS builds clock propagation trees on top of the\ndynamically reconfigured circuits in optical DCNs, allowing switches to seek\nbetter sync parents throughout time. It predicts drifts in the tree-building\nprocess, which enables minimization of sync errors. We also tailor today's sync\nprotocols to the needs of optical DCNs, including reducing the number of sync\nmessages to fit into short circuit durations and correcting timestamp errors\nfor higher sync accuracy. Our implementation on programmable switches shows\n28ns sync accuracy in a 192-ToR setting.\n","authors":["Yiming Lei","Jialong Li","Zhengqing Liu","Raj Joshi","Yiting Xia"],"pdf_url":"https://arxiv.org/pdf/2410.17012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16965v1","updated":"2024-10-22T12:47:10Z","published":"2024-10-22T12:47:10Z","title":"Downtime Required for Bitcoin Quantum-Safety","summary":"  Quantum devices capable of breaking the public-key cryptosystems that Bitcoin\nrelies on to secure its transactions are expected with reasonable probability\nwithin a decade. Quantum attacks would put at risk the entire Bitcoin network,\nwhich has an estimated value of around 500 billion USD. To prevent this threat,\na proactive approach is critical. The only known way to prevent any such attack\nis to upgrade the currently used public-key cryptosystems, namely ECDSA, with\nso-called post-quantum cryptosystems which have no known vulnerabilities to\nquantum attacks. In this paper, we analyse the technical cost of such an\nupgrade. We calculate a non-tight lower bound on the cumulative downtime\nrequired for the above transition to be 1827.96 hours, or 76.16 days. We also\ndemonstrate that the transition needs to be fully completed before the\navailability of ECDSA-256 breaking quantum devices, in order to ensure\nBitcoin's ongoing security. The conclusion is that the Bitcoin upgrade to\nquantum-safe protocols needs to be started as soon as possible in order to\nguarantee its ongoing operations.\n","authors":["Jamie J. Pont","Joseph J. Kearney","Jack Moyler","Carlos A. Perez-Delgado"],"pdf_url":"https://arxiv.org/pdf/2410.16965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16846v1","updated":"2024-10-22T09:34:22Z","published":"2024-10-22T09:34:22Z","title":"Safe Load Balancing in Software-Defined-Networking","summary":"  High performance, reliability and safety are crucial properties of any\nSoftware-Defined-Networking (SDN) system. Although the use of Deep\nReinforcement Learning (DRL) algorithms has been widely studied to improve\nperformance, their practical applications are still limited as they fail to\nensure safe operations in exploration and decision-making. To fill this gap, we\nexplore the design of a Control Barrier Function (CBF) on top of Deep\nReinforcement Learning (DRL) algorithms for load-balancing. We show that our\nDRL-CBF approach is capable of meeting safety requirements during training and\ntesting while achieving near-optimal performance in testing. We provide results\nusing two simulators: a flow-based simulator, which is used for\nproof-of-concept and benchmarking, and a packet-based simulator that implements\nreal protocols and scheduling. Thanks to the flow-based simulator, we compared\nthe performance against the optimal policy, solving a Non Linear Programming\n(NLP) problem with the SCIP solver. Furthermore, we showed that pre-trained\nmodels in the flow-based simulator, which is faster, can be transferred to the\npacket simulator, which is slower but more accurate, with some fine-tuning.\nOverall, the results suggest that near-optimal Quality-of-Service (QoS)\nperformance in terms of end-to-end delay can be achieved while safety\nrequirements related to link capacity constraints are guaranteed. In the\npacket-based simulator, we also show that our DRL-CBF algorithms outperform\nnon-RL baseline algorithms. When the models are fine-tuned over a few episodes,\nwe achieved smoother QoS and safety in training, and similar performance in\ntesting compared to the case where models have been trained from scratch.\n","authors":["Lam Dinh","Pham Tran Anh Quang","Jérémie Leguay"],"pdf_url":"https://arxiv.org/pdf/2410.16846v1.pdf","comment":"Accepted to Computer Communications 2024. arXiv admin note: text\n  overlap with arXiv:2401.05525"},{"id":"http://arxiv.org/abs/2407.04594v2","updated":"2024-10-22T06:47:21Z","published":"2024-07-05T15:41:50Z","title":"Experiences with Sub-Arctic Sensor Network Deployment and Feasibility of\n  Geothermal Energy Harvesting","summary":"  This paper discusses the experiences gained from designing, deploying and\nmaintaining low-power wireless sensor networks in three geothermally active\nremote locations in Iceland. The purpose of deploying the network was to\ncollect soil temperature data and investigate the impact of global warming on\n(sub)Arctic climate and subsequent carbon release. Functional networks from\nthree sites with no direct access to power and the internet have been providing\nresearchers with insight into the warming impacts since 2021. The network\nemploys low-power wireless sensor nodes equipped with DASH7 communication\nprotocol, providing real-time data and remote access to sensors and instruments\ndeployed in the field. In addition to discussing the architecture and\ndeployment of the network, we conduct a primary analysis using models and\nmethods to demonstrate the feasibility of harvesting energy from the\ntemperature gradient between geothermally active soil and air.\n","authors":["Priyesh Pappinisseri Puluckul","Maarten Weyn"],"pdf_url":"https://arxiv.org/pdf/2407.04594v2.pdf","comment":"A replacement article has been submitted and is available online\n  [arXiv:2405.02986]"},{"id":"http://arxiv.org/abs/2410.16723v1","updated":"2024-10-22T06:12:04Z","published":"2024-10-22T06:12:04Z","title":"Resource-Efficient Sensor Fusion via System-Wide Dynamic Gated Neural\n  Networks","summary":"  Mobile systems will have to support multiple AI-based applications, each\nleveraging heterogeneous data sources through DNN architectures collaboratively\nexecuted within the network. To minimize the cost of the AI inference task\nsubject to requirements on latency, quality, and - crucially - reliability of\nthe inference process, it is vital to optimize (i) the set of sensors/data\nsources and (ii) the DNN architecture, (iii) the network nodes executing\nsections of the DNN, and (iv) the resources to use. To this end, we leverage\ndynamic gated neural networks with branches, and propose a novel algorithmic\nstrategy called Quantile-constrained Inference (QIC), based upon\nquantile-Constrained policy optimization. QIC makes joint, high-quality, swift\ndecisions on all the above aspects of the system, with the aim to minimize\ninference energy cost. We remark that this is the first contribution connecting\ngated dynamic DNNs with infrastructure-level decision making. We evaluate QIC\nusing a dynamic gated DNN with stems and branches for optimal sensor fusion and\ninference, trained on the RADIATE dataset offering Radar, LiDAR, and Camera\ndata, and real-world wireless measurements. Our results confirm that QIC\nmatches the optimum and outperforms its alternatives by over 80%.\n","authors":["Chetna Singhal","Yashuo Wu","Francesco Malandrino","Sharon Ladron de Guevara Contreras","Marco Levorato","Carla Fabiana Chiasserini"],"pdf_url":"https://arxiv.org/pdf/2410.16723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01617v2","updated":"2024-10-22T04:09:15Z","published":"2024-04-02T03:43:55Z","title":"Designing Network Algorithms via Large Language Models","summary":"  We introduce NADA, the first framework to autonomously design network\nalgorithms by leveraging the generative capabilities of large language models\n(LLMs). Starting with an existing algorithm implementation, NADA enables LLMs\nto create a wide variety of alternative designs in the form of code blocks. It\nthen efficiently identifies the top-performing designs through a series of\nfiltering techniques, minimizing the need for full-scale evaluations and\nsignificantly reducing computational costs. Using adaptive bitrate (ABR)\nstreaming as a case study, we demonstrate that NADA produces novel ABR\nalgorithms -- previously unknown to human developers -- that consistently\noutperform the original algorithm in diverse network environments, including\nbroadband, satellite, 4G, and 5G.\n","authors":["Zhiyuan He","Aashish Gottipati","Lili Qiu","Xufang Luo","Kenuo Xu","Yuqing Yang","Francis Y. Yan"],"pdf_url":"https://arxiv.org/pdf/2404.01617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04174v5","updated":"2024-10-22T22:10:24Z","published":"2024-07-04T22:22:16Z","title":"Gemini: Integrating Full-fledged Sensing upon Millimeter Wave\n  Communications","summary":"  Integrating millimeter wave (mmWave)technology in both communication and\nsensing is promising as it enables the reuse of existing spectrum and\ninfrastructure without draining resources. Most existing systems piggyback\nsensing onto conventional communication modes without fully exploiting the\npotential of integrated sensing and communication (ISAC) in mmWave radios (not\nfull-fledged). In this paper, we design and implement a full-fledged mmWave\nISAC system Gemini; it delivers raw channel states to serve a broad category of\nsensing applications. We first propose the mmWave self-interference\ncancellation approach to extract the weak reflected signals for near-field\nsensing purposes. Then, we develop a joint optimization scheduling framework\nthat can be utilized in accurate radar sensing while maximizing the\ncommunication throughput. Finally, we design a united fusion sensing algorithm\nto offer a better sensing performance via combining monostatic and bistatic\nmodes. We evaluate our system in extensive experiments to demonstrate Gemini's\ncapability of simultaneously operating sensing and communication, enabling\nmmWave ISAC to perform better than the commercial off-the-shelf mmWave radar\nfor 5G cellular networks.\n","authors":["Yilong Li","Zhe Chen","Jun Luo","Suman Banerjee"],"pdf_url":"https://arxiv.org/pdf/2407.04174v5.pdf","comment":"12 pages. arXiv admin note: substantial text overlap with\n  arXiv:2310.05507"},{"id":"http://arxiv.org/abs/2410.17361v1","updated":"2024-10-22T18:54:12Z","published":"2024-10-22T18:54:12Z","title":"Characterizing Robocalls with Multiple Vantage Points","summary":"  Telephone spam has been among the highest network security concerns for users\nfor many years. In response, industry and government have deployed new\ntechnologies and regulations to curb the problem, and academic and industry\nresearchers have provided methods and measurements to characterize robocalls.\nHave these efforts borne fruit? Are the research characterizations reliable,\nand have the prevention and deterrence mechanisms succeeded?\n  In this paper, we address these questions through analysis of data from\nseveral independently-operated vantage points, ranging from industry and\nacademic voice honeypots to public enforcement and consumer complaints, some\nwith over 5 years of historic data. We first describe how we address the\nnon-trivial methodological challenges of comparing disparate data sources,\nincluding comparing audio and transcripts from about 3 million voice calls. We\nalso detail the substantial coherency of these diverse perspectives, which\ndramatically strengthens the evidence for the conclusions we draw about\nrobocall characterization and mitigation while highlighting advantages of each\napproach. Among our many findings, we find that unsolicited calls are in slow\ndecline, though complaints and call volumes remain high. We also find that\nrobocallers have managed to adapt to STIR/SHAKEN, a mandatory call\nauthentication scheme. In total, our findings highlight the most promising\ndirections for future efforts to characterize and stop telephone spam.\n","authors":["Sathvik Prasad","Aleksandr Nahapetyan","Bradley Reaves"],"pdf_url":"https://arxiv.org/pdf/2410.17361v1.pdf","comment":"Accepted for publication at the 46th IEEE Symposium on Security and\n  Privacy, 2025"}],"Neural and Evolutionary Computing":[{"id":"http://arxiv.org/abs/2410.11488v2","updated":"2024-10-22T11:18:29Z","published":"2024-10-15T10:46:03Z","title":"Advancing Training Efficiency of Deep Spiking Neural Networks through\n  Rate-based Backpropagation","summary":"  Recent insights have revealed that rate-coding is a primary form of\ninformation representation captured by surrogate-gradient-based Backpropagation\nThrough Time (BPTT) in training deep Spiking Neural Networks (SNNs). Motivated\nby these findings, we propose rate-based backpropagation, a training strategy\nspecifically designed to exploit rate-based representations to reduce the\ncomplexity of BPTT. Our method minimizes reliance on detailed temporal\nderivatives by focusing on averaged dynamics, streamlining the computational\ngraph to reduce memory and computational demands of SNNs training. We\nsubstantiate the rationality of the gradient approximation between BPTT and the\nproposed method through both theoretical analysis and empirical observations.\nComprehensive experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS\nvalidate that our method achieves comparable performance to BPTT counterparts,\nand surpasses state-of-the-art efficient training techniques. By leveraging the\ninherent benefits of rate-coding, this work sets the stage for more scalable\nand efficient SNNs training within resource-constrained environments. Our code\nis available at https://github.com/Tab-ct/rate-based-backpropagation.\n","authors":["Chengting Yu","Lei Liu","Gaoang Wang","Erping Li","Aili Wang"],"pdf_url":"https://arxiv.org/pdf/2410.11488v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.16656v2","updated":"2024-10-22T09:30:36Z","published":"2024-04-25T14:48:29Z","title":"A Self-Organizing Clustering System for Unsupervised Distribution Shift\n  Detection","summary":"  Modeling non-stationary data is a challenging problem in the field of\ncontinual learning, and data distribution shifts may result in negative\nconsequences on the performance of a machine learning model. Classic learning\ntools are often vulnerable to perturbations of the input covariates, and are\nsensitive to outliers and noise, and some tools are based on rigid algebraic\nassumptions. Distribution shifts are frequently occurring due to changes in raw\nmaterials for production, seasonality, a different user base, or even\nadversarial attacks. Therefore, there is a need for more effective distribution\nshift detection techniques. In this work, we propose a continual learning\nframework for monitoring and detecting distribution changes. We explore the\nproblem in a latent space generated by a bio-inspired self-organizing\nclustering and statistical aspects of the latent space. In particular, we\ninvestigate the projections made by two topology-preserving maps: the\nSelf-Organizing Map and the Scale Invariant Map. Our method can be applied in\nboth a supervised and an unsupervised context. We construct the assessment of\nchanges in the data distribution as a comparison of Gaussian signals, making\nthe proposed method fast and robust. We compare it to other unsupervised\ntechniques, specifically Principal Component Analysis (PCA) and Kernel-PCA. Our\ncomparison involves conducting experiments using sequences of images (based on\nMNIST and injected shifts with adversarial samples), chemical sensor\nmeasurements, and the environmental variable related to ozone levels. The\nempirical study reveals the potential of the proposed approach.\n","authors":["Sebastián Basterrech","Line Clemmensen","Gerardo Rubino"],"pdf_url":"https://arxiv.org/pdf/2404.16656v2.pdf","comment":"Revised version of the accepted manuscript to IJCNN'2024. Main\n  corrections were in Section 2.2 and Section 3.3. In Section 2.2 was corrected\n  expression (3), and in Section 3.3 in the definition of the elements of the\n  matrix $D$ it was a typo where $\\phi(x)$ was written instead of $x$"},{"id":"http://arxiv.org/abs/2410.16734v1","updated":"2024-10-22T06:39:19Z","published":"2024-10-22T06:39:19Z","title":"High-Order Associative Learning Based on Memristive Circuits for\n  Efficient Learning","summary":"  Memristive associative learning has gained significant attention for its\nability to mimic fundamental biological learning mechanisms while maintaining\nsystem simplicity. In this work, we introduce a high-order memristive\nassociative learning framework with a biologically realistic structure. By\nutilizing memristors as synaptic modules and their state information to bridge\ndifferent orders of associative learning, our design effectively establishes\nassociations between multiple stimuli and replicates the transient nature of\nhigh-order associative learning. In Pavlov's classical conditioning\nexperiments, our design achieves a 230% improvement in learning efficiency\ncompared to previous works, with memristor power consumption in the synaptic\nmodules remaining below 11 {\\mu}W. In large-scale image recognition tasks, we\nutilize a 20*20 memristor array to represent images, enabling the system to\nrecognize and label test images with semantic information at 100% accuracy.\nThis scalability across different tasks highlights the framework's potential\nfor a wide range of applications, offering enhanced learning efficiency for\ncurrent memristor-based neuromorphic systems.\n","authors":["Shengbo Wang","Xuemeng Li","Jialin Ding","Weihao Ma","Ying Wang","Luigi Occhipinti","Arokia Nathan","Shuo Gao"],"pdf_url":"https://arxiv.org/pdf/2410.16734v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.16677v1","updated":"2024-10-22T04:20:51Z","published":"2024-10-22T04:20:51Z","title":"The Neuromorphic Analog Electronic Nose","summary":"  Rapid detection of gas concentration is important in different domains like\ngas leakage monitoring, pollution control, and so on, for the prevention of\nhealth hazards. Out of different types of gas sensors, Metal oxide (MOx)\nsensors are extensively used in such applications because of their portability,\nlow cost, and high sensitivity for specific gases. However, how to effectively\nsample the MOx data for the real-time detection of gas and its concentration\nlevel remains an open question. Here we introduce a simple analog front-end for\none MOx sensor that encodes the gas concentration in the time difference\nbetween pulses of two separate pathways. This front-end design is inspired by\nthe spiking output of a mammalian olfactory bulb. We show that for a gas pulse\ninjected in a constant airflow, the time difference between pulses decreases\nwith increasing gas concentration, similar to the spike time difference between\nthe two principal output neurons in the olfactory bulb. The circuit design is\nfurther extended to a MOx sensor array and this sensor array front-end was\ntested in the same environment for gas identification and concentration\nestimation. Encoding of gas stimulus features in analog spikes at the sensor\nlevel itself may result in data and power-efficient real-time gas sensing\nsystems in the future that can ultimately be used in uncontrolled and turbulent\nenvironments for longer periods without data explosion.\n","authors":["Shavika Rastogi","Nik Dennler","Michael Schmuker","André van Schaik"],"pdf_url":"https://arxiv.org/pdf/2410.16677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16613v1","updated":"2024-10-22T01:55:02Z","published":"2024-10-22T01:55:02Z","title":"Real-time Sub-milliwatt Epilepsy Detection Implemented on a Spiking\n  Neural Network Edge Inference Processor","summary":"  Analyzing electroencephalogram (EEG) signals to detect the epileptic seizure\nstatus of a subject presents a challenge to existing technologies aimed at\nproviding timely and efficient diagnosis. In this study, we aimed to detect\ninterictal and ictal periods of epileptic seizures using a spiking neural\nnetwork (SNN). Our proposed approach provides an online and real-time\npreliminary diagnosis of epileptic seizures and helps to detect possible\npathological conditions.To validate our approach, we conducted experiments\nusing multiple datasets. We utilized a trained SNN to identify the presence of\nepileptic seizures and compared our results with those of related studies. The\nSNN model was deployed on Xylo, a digital SNN neuromorphic processor designed\nto process temporal signals. Xylo efficiently simulates spiking leaky\nintegrate-and-fire neurons with exponential input synapses. Xylo has much lower\nenergy requirments than traditional approaches to signal processing, making it\nan ideal platform for developing low-power seizure detection systems.Our\nproposed method has a high test accuracy of 93.3% and 92.9% when classifying\nictal and interictal periods. At the same time, the application has an average\npower consumption of 87.4 uW(IO power) + 287.9 uW(computational power) when\ndeployed to Xylo. Our method demonstrates excellent low-latency performance\nwhen tested on multiple datasets. Our work provides a new solution for seizure\ndetection, and it is expected to be widely used in portable and wearable\ndevices in the future.\n","authors":["Ruixin Lia","Guoxu Zhaoa","Dylan Richard Muir","Yuya Ling","Karla Burelo","Mina Khoei","Dong Wang","Yannan Xing","Ning Qiao"],"pdf_url":"https://arxiv.org/pdf/2410.16613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17441v1","updated":"2024-10-22T21:42:07Z","published":"2024-10-22T21:42:07Z","title":"On the Sampling Sparsity of Neuromorphic Analog-to-Spike Conversion\n  based on Leaky Integrate-and-Fire","summary":"  In contrast to the traditional principle of periodic sensing neuromorphic\nengineering pursues a paradigm shift towards bio-inspired event-based sensing,\nwhere events are primarily triggered by a change in the perceived stimulus. We\nshow in a rigorous mathematical way that information encoding by means of\nThreshold-Based Representation based on either Leaky Integrate-and-Fire (LIF)\nor Send-on-Delta (SOD) is linked to an analog-to-spike conversion that\nguarantees maximum sparsity while satisfying an approximation condition based\non the Alexiewicz norm.\n","authors":["Bernhard A. Moser","Michael Lunglmayr"],"pdf_url":"https://arxiv.org/pdf/2410.17441v1.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2410.15473v2","updated":"2024-10-22T16:57:37Z","published":"2024-10-20T19:11:24Z","title":"A Bayesian Framework for Clustered Federated Learning","summary":"  One of the main challenges of federated learning (FL) is handling\nnon-independent and identically distributed (non-IID) client data, which may\noccur in practice due to unbalanced datasets and use of different data sources\nacross clients. Knowledge sharing and model personalization are key strategies\nfor addressing this issue. Clustered federated learning is a class of FL\nmethods that groups clients that observe similarly distributed data into\nclusters, such that every client is typically associated with one data\ndistribution and participates in training a model for that distribution along\ntheir cluster peers. In this paper, we present a unified Bayesian framework for\nclustered FL which associates clients to clusters. Then we propose several\npractical algorithms to handle the, otherwise growing, data associations in a\nway that trades off performance and computational complexity. This work\nprovides insights on client-cluster associations and enables client knowledge\nsharing in new ways. The proposed framework circumvents the need for unique\nclient-cluster associations, which is seen to increase the performance of the\nresulting models in a variety of experiments.\n","authors":["Peng Wu","Tales Imbiriba","Pau Closas"],"pdf_url":"https://arxiv.org/pdf/2410.15473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17226v1","updated":"2024-10-22T17:48:36Z","published":"2024-10-22T17:48:36Z","title":"Parallel Cluster-BFS and Applications to Shortest Paths","summary":"  Breadth-first Search (BFS) is one of the most important graph processing\nsubroutines, especially to compute the unweighted distance. Many applications\nmay require running BFS from multiple sources. Sequentially, when running BFS\non a cluster of nearby vertices, a known optimization is to use\nbit-parallelism. Given a subset of vertices with size $k$ and the distance\nbetween any pair of them is no more than $d$, BFS can be applied to all of them\nin a total work of $O(dk/w+1)$, where $w$ is the length of a word in bits. We\nwill refer to this approach as cluster-BFS (C-BFS). Such an approach has been\nstudied and shown effective both in theory and in practice in the sequential\nsetting. However, it remains unknown how this can be combined with thread-level\nparallelism for C-BFS.\n  In this paper, we focus on designing efficient parallel C-BFS based on BFS to\nanswer unweighted distance queries. Our solution combines the strengths of\nbit-level parallelism and thread-level parallelism, and achieves significant\nspeedup over the plain sequential solution. We also apply our algorithm to\nreal-world applications. In particular, we identified another application\n(landmark-labeling for the approximate distance oracle) that can take advantage\nof parallel C-BFS. Under the same memory budget, our new solution improves\naccuracy and/or time on all the 18 tested graphs.\n","authors":["Letong Wang","Guy Blelloch","Yan Gu","Yihan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.17226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17116v1","updated":"2024-10-22T15:48:00Z","published":"2024-10-22T15:48:00Z","title":"Security and RAS in the Computing Continuum","summary":"  Security and RAS are two non-functional requirements under focus for current\nsystems developed for the computing continuum. Due to the increased number of\ninterconnected computer systems across the continuum, security becomes\nespecially pervasive at all levels, from the smallest edge device to the\nhigh-performance cloud at the other end. Similarly, RAS (Reliability,\nAvailability, and Serviceability) ensures the robustness of a system towards\nhardware defects. Namely, making them reliable, with high availability and\ndesign for easy service. In this paper and as a result of the Vitamin-V EU\nproject, the authors detail the comprehensive approach to malware and hardware\nattack detection; as well as, the RAS features envisioned for future systems\nacross the computing continuum.\n","authors":["Martí Alonso","David Andreu","Ramon Canal","Stefano Di Carlo","Odysseas Chatzopoulos","Cristiano Chenet","Juanjo Costa","Andreu Girones","Dimitris Gizopoulos","George Papadimitriou","Enric Morancho","Beatriz Otero","Alessandro Savino"],"pdf_url":"https://arxiv.org/pdf/2410.17116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17078v1","updated":"2024-10-22T14:56:50Z","published":"2024-10-22T14:56:50Z","title":"FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters","summary":"  The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce.\n","authors":["Hasibul Jamil","Abdul Alim","Laurent Schares","Pavlos Maniotis","Liran Schour","Ali Sydney","Abdullah Kayi","Tevfik Kosar","Bengi Karacali"],"pdf_url":"https://arxiv.org/pdf/2410.17078v1.pdf","comment":"Submitted for peer reviewing in IEEE ICC 2025"},{"id":"http://arxiv.org/abs/2410.16954v1","updated":"2024-10-22T12:33:45Z","published":"2024-10-22T12:33:45Z","title":"LoRA-C: Parameter-Efficient Fine-Tuning of Robust CNN for IoT Devices","summary":"  Efficient fine-tuning of pre-trained convolutional neural network (CNN)\nmodels using local data is essential for providing high-quality services to\nusers using ubiquitous and resource-limited Internet of Things (IoT) devices.\nLow-Rank Adaptation (LoRA) fine-tuning has attracted widespread attention from\nindustry and academia because it is simple, efficient, and does not incur any\nadditional reasoning burden. However, most of the existing advanced methods use\nLoRA to fine-tune Transformer, and there are few studies on using LoRA to\nfine-tune CNN. The CNN model is widely deployed on IoT devices for application\ndue to its advantages in comprehensive resource occupancy and performance.\nMoreover, IoT devices are widely deployed outdoors and usually process data\naffected by the environment (such as fog, snow, rain, etc.). The goal of this\npaper is to use LoRA technology to efficiently improve the robustness of the\nCNN model. To this end, this paper first proposes a strong, robust CNN\nfine-tuning method for IoT devices, LoRA-C, which performs low-rank\ndecomposition in convolutional layers rather than kernel units to reduce the\nnumber of fine-tuning parameters. Then, this paper analyzes two different rank\nsettings in detail and observes that the best performance is usually achieved\nwhen ${\\alpha}/{r}$ is a constant in either standard data or corrupted data.\nThis discovery provides experience for the widespread application of LoRA-C.\nFinally, this paper conducts many experiments based on pre-trained models.\nExperimental results on CIFAR-10, CIFAR-100, CIFAR-10-C, and Icons50 datasets\nshow that the proposed LoRA-Cs outperforms standard ResNets. Specifically, on\nthe CIFAR-10-C dataset, the accuracy of LoRA-C-ResNet-101 achieves 83.44%\naccuracy, surpassing the standard ResNet-101 result by +9.5%.\n","authors":["Chuntao Ding","Xu Cao","Jianhang Xie","Linlin Fan","Shangguang Wang","Zhichao Lu"],"pdf_url":"https://arxiv.org/pdf/2410.16954v1.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.11415v4","updated":"2024-10-22T09:11:15Z","published":"2024-01-21T06:51:08Z","title":"A Fast Parallel Approach for Neighborhood-based Link Prediction by\n  Disregarding Large Hubs","summary":"  Link prediction can help rectify inaccuracies in various graph algorithms,\nstemming from unaccounted-for or overlooked links within networks. However,\nmany existing works use a baseline approach, which incurs unnecessary\ncomputational costs due to its high time complexity. Further, many studies\nfocus on smaller graphs, which can lead to misleading conclusions. Here, we\nstudy the prediction of links using neighborhood-based similarity measures on\nlarge graphs. In particular, we improve upon the baseline approach (IBase), and\npropose a heuristic approach that additionally disregards large hubs (DLH),\nbased on the idea that high-degree nodes contribute little similarity among\ntheir neighbors. On a server equipped with dual 16-core Intel Xeon Gold 6226R\nprocessors, DLH is on average 1019x faster than IBase, especially on web graphs\nand social networks, while maintaining similar prediction accuracy. Notably,\nDLH achieves a link prediction rate of 38.1M edges/s and improves performance\nby 1.6x for every doubling of threads.\n","authors":["Subhajit Sahu"],"pdf_url":"https://arxiv.org/pdf/2401.11415v4.pdf","comment":"17 pages, 12 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.07476v3","updated":"2024-10-22T07:33:36Z","published":"2024-07-10T09:02:35Z","title":"A Transverse-Read-assisted Fast Valid-Bits Collection in Stochastic\n  Computing MACs for Energy-Efficient in-RTM DNNs","summary":"  It looks very attractive to coordinate racetrack-memory (RM) and\nstochastic-computing (SC) jointly to build an ultra-low power\nneuron-architecture.However, the above combination has always been questioned\nin a fatal weakness that the heavy valid-bits collection of RM-MTJ, a.k.a.\naccumulative parallel counters (APCs), cannot physically match the requirement\nfor energy-efficient in-memory DNNs.Fortunately, a recently developed\nTransverse-Read (TR) provides a lightweight collection of valid-bits by\ndetecting domain-wall resistance between a couple of MTJs on a single\nnanowire.In this work, we first propose a neuron-architecture that utilizes\nparallel TRs to build an ultra-fast valid-bits collection for SC, in which, a\nvector multiplication is successfully degraded as swift TRs.To solve huge\nstorage for full stochastic sequences caused by the limited TR banks, a hybrid\ncoding, pseudo-fractal compression, is designed to generate stochastic\nsequences by segments.To overcome the misalignment by the parallel\nearly-termination, an asynchronous schedule of TR is further designed to\nregularize the vectorization, in which, the valid-bits from different lanes are\nmerged in multiple RM-stacks for vector-level valid-bits collection.However, an\ninherent defect of TR, i.e., neighbor parts cannot be accessed simultaneously,\ncould limit the throughput of the parallel vector multiplication, therefore, an\ninterleaving data placement is used for full utilization of memory bus among\ndifferent vectors.The results show that the SC-MAC assisted with TR achieves\n$2.88\\times-4.40\\times $speedup compared to CORUSCANT, at the same time, energy\nconsumption is reduced by $1.26\\times-1.42\\times$.\n","authors":["Jihe Wang","Zhiying Zhang","Xingwu Dong","Danghui Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16724v1","updated":"2024-10-22T06:16:00Z","published":"2024-10-22T06:16:00Z","title":"Efficient Scheduling of Vehicular Tasks on Edge Systems with Green\n  Energy and Battery Storage","summary":"  The autonomous vehicle industry is rapidly expanding, requiring significant\ncomputational resources for tasks like perception and decision-making.\nVehicular edge computing has emerged to meet this need, utilizing roadside\ncomputational units (roadside edge servers) to support autonomous vehicles.\nAligning with the trend of green cloud computing, these roadside edge servers\noften get energy from solar power. Additionally, each roadside computational\nunit is equipped with a battery for storing solar power, ensuring continuous\ncomputational operation during periods of low solar energy availability.\n  In our research, we address the scheduling of computational tasks generated\nby autonomous vehicles to roadside units with power consumption proportional to\nthe cube of the computational load of the server. Each computational task is\nassociated with a revenue, dependent on its computational needs and deadline.\nOur objective is to maximize the total revenue of the system of roadside\ncomputational units.\n  We propose an offline heuristics approach based on predicted solar energy and\nincoming task patterns for different time slots. Additionally, we present\nheuristics for real-time adaptation to varying solar energy and task patterns\nfrom predicted values for different time slots. Our comparative analysis shows\nthat our methods outperform state-of-the-art approaches upto 40\\% for real-life\ndatasets.\n","authors":["Suvarthi Sarkar","binash Kumar Ray","Aryabartta Sahu"],"pdf_url":"https://arxiv.org/pdf/2410.16724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17057v3","updated":"2024-10-22T21:56:06Z","published":"2024-04-25T21:53:35Z","title":"Portable, Massively Parallel Implementation of a Material Point Method\n  for Compressible Flows","summary":"  The recent evolution of software and hardware technologies is leading to a\nrenewed computational interest in Particle-In-Cell (PIC) methods such as the\nMaterial Point Method (MPM). Indeed, provided some critical aspects are\nproperly handled, PIC methods can be cast in formulations suitable for the\nrequirements of data locality and fine-grained parallelism of modern hardware\naccelerators such as Graphics Processing Units (GPUs). Such a rapid and\ncontinuous technological development increases also the importance of generic\nand portable implementations. While the capabilities of MPM on a wide range\ncontinuum mechanics problem have been already well assessed, the use of the\nmethod in compressible fluid dynamics has received less attention. In this\npaper we present a portable, highly parallel, GPU based MPM solver for\ncompressible gas dynamics. The implementation aims to reach a good compromise\nbetween portability and efficiency in order to provide a first assessment of\nthe potential of this approach in solving strongly compressible gas flow\nproblems, also taking into account solid obstacles. The numerical model\nconsidered constitutes a first step towards the development of a monolithic MPM\nsolver for Fluid-Structure Interaction (FSI) problems at all Mach numbers up to\nthe supersonic regime.\n","authors":["Paolo Joseph Baioni","Tommaso Benacchio","Luigi Capone","Carlo de Falco"],"pdf_url":"https://arxiv.org/pdf/2404.17057v3.pdf","comment":"54 pages, 21 figures"},{"id":"http://arxiv.org/abs/2401.10369v3","updated":"2024-10-22T21:48:14Z","published":"2024-01-18T20:27:24Z","title":"Autobahn: Seamless high speed BFT","summary":"  Today's practical, high performance Byzantine Fault Tolerant (BFT) consensus\nprotocols operate in the partial synchrony model. However, existing protocols\nare inefficient when deployments are indeed partially synchronous. They deliver\neither low latency during fault-free, synchronous periods (good intervals) or\nrobust recovery from events that interrupt progress (blips). At one end,\ntraditional, view-based BFT protocols optimize for latency during good\nintervals, but, when blips occur, can suffer from performance degradation\n(hangovers) that can last beyond the return of a good interval. At the other\nend, modern DAG-based BFT protocols recover more gracefully from blips, but\nexhibit lackluster latency during good intervals. To close the gap, this work\npresents Autobahn, a novel high-throughput BFT protocol that offers both low\nlatency and seamless recovery from blips. By combining a highly parallel\nasynchronous data dissemination layer with a low-latency, partially synchronous\nconsensus mechanism, Autobahn (i) avoids the hangovers incurred by traditional\nBFT protocols and (ii) matches the throughput of state of the art DAG-based BFT\nprotocols while cutting their latency in half, matching the latency of\ntraditional BFT protocols.\n","authors":["Neil Giridharan","Florian Suri-Payer","Ittai Abraham","Lorenzo Alvisi","Natacha Crooks"],"pdf_url":"https://arxiv.org/pdf/2401.10369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17435v1","updated":"2024-10-22T21:22:05Z","published":"2024-10-22T21:22:05Z","title":"AI-focused HPC Data Centers Can Provide More Power Grid Flexibility and\n  at Lower Cost","summary":"  The recent growth of Artificial Intelligence (AI), particularly large\nlanguage models, requires energy-demanding high-performance computing (HPC)\ndata centers, which poses a significant burden on power system capacity.\nScheduling data center computing jobs to manage power demand can alleviate\nnetwork stress with minimal infrastructure investment and contribute to fast\ntime-scale power system balancing. This study, for the first time,\ncomprehensively analyzes the capability and cost of grid flexibility provision\nby GPU-heavy AI-focused HPC data centers, along with a comparison with\nCPU-heavy general-purpose HPC data centers traditionally used for scientific\ncomputing. Using real-world data from 7 AI-focused HPC data centers, 7\ngeneral-purpose HPC data centers, and 3 cloud platforms, we find that\nAI-focused HPC data centers can offer greater flexibility at 50% lower cost for\na range of power system services. By comparing the cost to flexibility market\nprices, we illustrate the financial profitability of flexibility provision for\nAI-focused HPC data centers.\n","authors":["Yihong Zhou","Angel Paredes","Chaimaa Essayeh","Thomas Morstyn"],"pdf_url":"https://arxiv.org/pdf/2410.17435v1.pdf","comment":"22 pages (including supplementary materials and references), under\n  review for Joule"},{"id":"http://arxiv.org/abs/2407.15716v3","updated":"2024-10-22T20:52:09Z","published":"2024-07-22T15:22:07Z","title":"CrashEventLLM: Predicting System Crashes with Large Language Models","summary":"  As the dependence on computer systems expands across various domains,\nfocusing on personal, industrial, and large-scale applications, there arises a\ncompelling need to enhance their reliability to sustain business operations\nseamlessly and ensure optimal user satisfaction. System logs generated by these\ndevices serve as valuable repositories of historical trends and past failures.\nThe use of machine learning techniques for failure prediction has become\ncommonplace, enabling the extraction of insights from past data to anticipate\nfuture behavior patterns. Recently, large language models have demonstrated\nremarkable capabilities in tasks including summarization, reasoning, and event\nprediction. Therefore, in this paper, we endeavor to investigate the potential\nof large language models in predicting system failures, leveraging insights\nlearned from past failure behavior to inform reasoning and decision-making\nprocesses effectively. Our approach involves leveraging data from the Intel\nComputing Improvement Program (ICIP) system crash logs to identify significant\nevents and develop CrashEventLLM. This model, built upon a large language model\nframework, serves as our foundation for crash event prediction. Specifically,\nour model utilizes historical data to forecast future crash events, informed by\nexpert annotations. Additionally, it goes beyond mere prediction, offering\ninsights into potential causes for each crash event. This work provides the\npreliminary insights into prompt-based large language models for the log-based\nevent prediction task.\n","authors":["Priyanka Mudgal","Bijan Arbab","Swaathi Sampath Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.15716v3.pdf","comment":"Accepted in ICITCOM'24. Copyrights will be with IEEE"},{"id":"http://arxiv.org/abs/2410.17375v1","updated":"2024-10-22T19:15:35Z","published":"2024-10-22T19:15:35Z","title":"AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM\n  Acceleration","summary":"  Large language models typically generate tokens autoregressively, using each\ntoken as input for the next. Recent work on Speculative Decoding has sought to\naccelerate this process by employing a smaller, faster draft model to more\nquickly generate candidate tokens. These candidates are then verified in\nparallel by the larger (original) verify model, resulting in overall speedup\ncompared to using the larger model by itself in an autoregressive fashion. In\nthis work, we introduce AMUSD (Asynchronous Multi-device Speculative Decoding),\na system that further accelerates generation by decoupling the draft and verify\nphases into a continuous, asynchronous approach. Unlike conventional\nspeculative decoding, where only one model (draft or verify) performs token\ngeneration at a time, AMUSD enables both models to perform predictions\nindependently on separate devices (e.g., GPUs). We evaluate our approach over\nmultiple datasets and show that AMUSD achieves an average 29% improvement over\nspeculative decoding and up to 1.96$\\times$ speedup over conventional\nautoregressive decoding, while achieving identical output quality. Our system\nis open-source and available at https://github.com/BradMcDanel/AMUSD/.\n","authors":["Bradley McDanel"],"pdf_url":"https://arxiv.org/pdf/2410.17375v1.pdf","comment":"4 pages, 5 figures, 1 table, 1 algorithm"}],"Hardware Architecture":[{"id":"http://arxiv.org/abs/2410.15473v2","updated":"2024-10-22T16:57:37Z","published":"2024-10-20T19:11:24Z","title":"A Bayesian Framework for Clustered Federated Learning","summary":"  One of the main challenges of federated learning (FL) is handling\nnon-independent and identically distributed (non-IID) client data, which may\noccur in practice due to unbalanced datasets and use of different data sources\nacross clients. Knowledge sharing and model personalization are key strategies\nfor addressing this issue. Clustered federated learning is a class of FL\nmethods that groups clients that observe similarly distributed data into\nclusters, such that every client is typically associated with one data\ndistribution and participates in training a model for that distribution along\ntheir cluster peers. In this paper, we present a unified Bayesian framework for\nclustered FL which associates clients to clusters. Then we propose several\npractical algorithms to handle the, otherwise growing, data associations in a\nway that trades off performance and computational complexity. This work\nprovides insights on client-cluster associations and enables client knowledge\nsharing in new ways. The proposed framework circumvents the need for unique\nclient-cluster associations, which is seen to increase the performance of the\nresulting models in a variety of experiments.\n","authors":["Peng Wu","Tales Imbiriba","Pau Closas"],"pdf_url":"https://arxiv.org/pdf/2410.15473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17226v1","updated":"2024-10-22T17:48:36Z","published":"2024-10-22T17:48:36Z","title":"Parallel Cluster-BFS and Applications to Shortest Paths","summary":"  Breadth-first Search (BFS) is one of the most important graph processing\nsubroutines, especially to compute the unweighted distance. Many applications\nmay require running BFS from multiple sources. Sequentially, when running BFS\non a cluster of nearby vertices, a known optimization is to use\nbit-parallelism. Given a subset of vertices with size $k$ and the distance\nbetween any pair of them is no more than $d$, BFS can be applied to all of them\nin a total work of $O(dk/w+1)$, where $w$ is the length of a word in bits. We\nwill refer to this approach as cluster-BFS (C-BFS). Such an approach has been\nstudied and shown effective both in theory and in practice in the sequential\nsetting. However, it remains unknown how this can be combined with thread-level\nparallelism for C-BFS.\n  In this paper, we focus on designing efficient parallel C-BFS based on BFS to\nanswer unweighted distance queries. Our solution combines the strengths of\nbit-level parallelism and thread-level parallelism, and achieves significant\nspeedup over the plain sequential solution. We also apply our algorithm to\nreal-world applications. In particular, we identified another application\n(landmark-labeling for the approximate distance oracle) that can take advantage\nof parallel C-BFS. Under the same memory budget, our new solution improves\naccuracy and/or time on all the 18 tested graphs.\n","authors":["Letong Wang","Guy Blelloch","Yan Gu","Yihan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.17226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17116v1","updated":"2024-10-22T15:48:00Z","published":"2024-10-22T15:48:00Z","title":"Security and RAS in the Computing Continuum","summary":"  Security and RAS are two non-functional requirements under focus for current\nsystems developed for the computing continuum. Due to the increased number of\ninterconnected computer systems across the continuum, security becomes\nespecially pervasive at all levels, from the smallest edge device to the\nhigh-performance cloud at the other end. Similarly, RAS (Reliability,\nAvailability, and Serviceability) ensures the robustness of a system towards\nhardware defects. Namely, making them reliable, with high availability and\ndesign for easy service. In this paper and as a result of the Vitamin-V EU\nproject, the authors detail the comprehensive approach to malware and hardware\nattack detection; as well as, the RAS features envisioned for future systems\nacross the computing continuum.\n","authors":["Martí Alonso","David Andreu","Ramon Canal","Stefano Di Carlo","Odysseas Chatzopoulos","Cristiano Chenet","Juanjo Costa","Andreu Girones","Dimitris Gizopoulos","George Papadimitriou","Enric Morancho","Beatriz Otero","Alessandro Savino"],"pdf_url":"https://arxiv.org/pdf/2410.17116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17078v1","updated":"2024-10-22T14:56:50Z","published":"2024-10-22T14:56:50Z","title":"FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters","summary":"  The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce.\n","authors":["Hasibul Jamil","Abdul Alim","Laurent Schares","Pavlos Maniotis","Liran Schour","Ali Sydney","Abdullah Kayi","Tevfik Kosar","Bengi Karacali"],"pdf_url":"https://arxiv.org/pdf/2410.17078v1.pdf","comment":"Submitted for peer reviewing in IEEE ICC 2025"},{"id":"http://arxiv.org/abs/2410.16954v1","updated":"2024-10-22T12:33:45Z","published":"2024-10-22T12:33:45Z","title":"LoRA-C: Parameter-Efficient Fine-Tuning of Robust CNN for IoT Devices","summary":"  Efficient fine-tuning of pre-trained convolutional neural network (CNN)\nmodels using local data is essential for providing high-quality services to\nusers using ubiquitous and resource-limited Internet of Things (IoT) devices.\nLow-Rank Adaptation (LoRA) fine-tuning has attracted widespread attention from\nindustry and academia because it is simple, efficient, and does not incur any\nadditional reasoning burden. However, most of the existing advanced methods use\nLoRA to fine-tune Transformer, and there are few studies on using LoRA to\nfine-tune CNN. The CNN model is widely deployed on IoT devices for application\ndue to its advantages in comprehensive resource occupancy and performance.\nMoreover, IoT devices are widely deployed outdoors and usually process data\naffected by the environment (such as fog, snow, rain, etc.). The goal of this\npaper is to use LoRA technology to efficiently improve the robustness of the\nCNN model. To this end, this paper first proposes a strong, robust CNN\nfine-tuning method for IoT devices, LoRA-C, which performs low-rank\ndecomposition in convolutional layers rather than kernel units to reduce the\nnumber of fine-tuning parameters. Then, this paper analyzes two different rank\nsettings in detail and observes that the best performance is usually achieved\nwhen ${\\alpha}/{r}$ is a constant in either standard data or corrupted data.\nThis discovery provides experience for the widespread application of LoRA-C.\nFinally, this paper conducts many experiments based on pre-trained models.\nExperimental results on CIFAR-10, CIFAR-100, CIFAR-10-C, and Icons50 datasets\nshow that the proposed LoRA-Cs outperforms standard ResNets. Specifically, on\nthe CIFAR-10-C dataset, the accuracy of LoRA-C-ResNet-101 achieves 83.44%\naccuracy, surpassing the standard ResNet-101 result by +9.5%.\n","authors":["Chuntao Ding","Xu Cao","Jianhang Xie","Linlin Fan","Shangguang Wang","Zhichao Lu"],"pdf_url":"https://arxiv.org/pdf/2410.16954v1.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.11415v4","updated":"2024-10-22T09:11:15Z","published":"2024-01-21T06:51:08Z","title":"A Fast Parallel Approach for Neighborhood-based Link Prediction by\n  Disregarding Large Hubs","summary":"  Link prediction can help rectify inaccuracies in various graph algorithms,\nstemming from unaccounted-for or overlooked links within networks. However,\nmany existing works use a baseline approach, which incurs unnecessary\ncomputational costs due to its high time complexity. Further, many studies\nfocus on smaller graphs, which can lead to misleading conclusions. Here, we\nstudy the prediction of links using neighborhood-based similarity measures on\nlarge graphs. In particular, we improve upon the baseline approach (IBase), and\npropose a heuristic approach that additionally disregards large hubs (DLH),\nbased on the idea that high-degree nodes contribute little similarity among\ntheir neighbors. On a server equipped with dual 16-core Intel Xeon Gold 6226R\nprocessors, DLH is on average 1019x faster than IBase, especially on web graphs\nand social networks, while maintaining similar prediction accuracy. Notably,\nDLH achieves a link prediction rate of 38.1M edges/s and improves performance\nby 1.6x for every doubling of threads.\n","authors":["Subhajit Sahu"],"pdf_url":"https://arxiv.org/pdf/2401.11415v4.pdf","comment":"17 pages, 12 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.07476v3","updated":"2024-10-22T07:33:36Z","published":"2024-07-10T09:02:35Z","title":"A Transverse-Read-assisted Fast Valid-Bits Collection in Stochastic\n  Computing MACs for Energy-Efficient in-RTM DNNs","summary":"  It looks very attractive to coordinate racetrack-memory (RM) and\nstochastic-computing (SC) jointly to build an ultra-low power\nneuron-architecture.However, the above combination has always been questioned\nin a fatal weakness that the heavy valid-bits collection of RM-MTJ, a.k.a.\naccumulative parallel counters (APCs), cannot physically match the requirement\nfor energy-efficient in-memory DNNs.Fortunately, a recently developed\nTransverse-Read (TR) provides a lightweight collection of valid-bits by\ndetecting domain-wall resistance between a couple of MTJs on a single\nnanowire.In this work, we first propose a neuron-architecture that utilizes\nparallel TRs to build an ultra-fast valid-bits collection for SC, in which, a\nvector multiplication is successfully degraded as swift TRs.To solve huge\nstorage for full stochastic sequences caused by the limited TR banks, a hybrid\ncoding, pseudo-fractal compression, is designed to generate stochastic\nsequences by segments.To overcome the misalignment by the parallel\nearly-termination, an asynchronous schedule of TR is further designed to\nregularize the vectorization, in which, the valid-bits from different lanes are\nmerged in multiple RM-stacks for vector-level valid-bits collection.However, an\ninherent defect of TR, i.e., neighbor parts cannot be accessed simultaneously,\ncould limit the throughput of the parallel vector multiplication, therefore, an\ninterleaving data placement is used for full utilization of memory bus among\ndifferent vectors.The results show that the SC-MAC assisted with TR achieves\n$2.88\\times-4.40\\times $speedup compared to CORUSCANT, at the same time, energy\nconsumption is reduced by $1.26\\times-1.42\\times$.\n","authors":["Jihe Wang","Zhiying Zhang","Xingwu Dong","Danghui Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16724v1","updated":"2024-10-22T06:16:00Z","published":"2024-10-22T06:16:00Z","title":"Efficient Scheduling of Vehicular Tasks on Edge Systems with Green\n  Energy and Battery Storage","summary":"  The autonomous vehicle industry is rapidly expanding, requiring significant\ncomputational resources for tasks like perception and decision-making.\nVehicular edge computing has emerged to meet this need, utilizing roadside\ncomputational units (roadside edge servers) to support autonomous vehicles.\nAligning with the trend of green cloud computing, these roadside edge servers\noften get energy from solar power. Additionally, each roadside computational\nunit is equipped with a battery for storing solar power, ensuring continuous\ncomputational operation during periods of low solar energy availability.\n  In our research, we address the scheduling of computational tasks generated\nby autonomous vehicles to roadside units with power consumption proportional to\nthe cube of the computational load of the server. Each computational task is\nassociated with a revenue, dependent on its computational needs and deadline.\nOur objective is to maximize the total revenue of the system of roadside\ncomputational units.\n  We propose an offline heuristics approach based on predicted solar energy and\nincoming task patterns for different time slots. Additionally, we present\nheuristics for real-time adaptation to varying solar energy and task patterns\nfrom predicted values for different time slots. Our comparative analysis shows\nthat our methods outperform state-of-the-art approaches upto 40\\% for real-life\ndatasets.\n","authors":["Suvarthi Sarkar","binash Kumar Ray","Aryabartta Sahu"],"pdf_url":"https://arxiv.org/pdf/2410.16724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17057v3","updated":"2024-10-22T21:56:06Z","published":"2024-04-25T21:53:35Z","title":"Portable, Massively Parallel Implementation of a Material Point Method\n  for Compressible Flows","summary":"  The recent evolution of software and hardware technologies is leading to a\nrenewed computational interest in Particle-In-Cell (PIC) methods such as the\nMaterial Point Method (MPM). Indeed, provided some critical aspects are\nproperly handled, PIC methods can be cast in formulations suitable for the\nrequirements of data locality and fine-grained parallelism of modern hardware\naccelerators such as Graphics Processing Units (GPUs). Such a rapid and\ncontinuous technological development increases also the importance of generic\nand portable implementations. While the capabilities of MPM on a wide range\ncontinuum mechanics problem have been already well assessed, the use of the\nmethod in compressible fluid dynamics has received less attention. In this\npaper we present a portable, highly parallel, GPU based MPM solver for\ncompressible gas dynamics. The implementation aims to reach a good compromise\nbetween portability and efficiency in order to provide a first assessment of\nthe potential of this approach in solving strongly compressible gas flow\nproblems, also taking into account solid obstacles. The numerical model\nconsidered constitutes a first step towards the development of a monolithic MPM\nsolver for Fluid-Structure Interaction (FSI) problems at all Mach numbers up to\nthe supersonic regime.\n","authors":["Paolo Joseph Baioni","Tommaso Benacchio","Luigi Capone","Carlo de Falco"],"pdf_url":"https://arxiv.org/pdf/2404.17057v3.pdf","comment":"54 pages, 21 figures"},{"id":"http://arxiv.org/abs/2401.10369v3","updated":"2024-10-22T21:48:14Z","published":"2024-01-18T20:27:24Z","title":"Autobahn: Seamless high speed BFT","summary":"  Today's practical, high performance Byzantine Fault Tolerant (BFT) consensus\nprotocols operate in the partial synchrony model. However, existing protocols\nare inefficient when deployments are indeed partially synchronous. They deliver\neither low latency during fault-free, synchronous periods (good intervals) or\nrobust recovery from events that interrupt progress (blips). At one end,\ntraditional, view-based BFT protocols optimize for latency during good\nintervals, but, when blips occur, can suffer from performance degradation\n(hangovers) that can last beyond the return of a good interval. At the other\nend, modern DAG-based BFT protocols recover more gracefully from blips, but\nexhibit lackluster latency during good intervals. To close the gap, this work\npresents Autobahn, a novel high-throughput BFT protocol that offers both low\nlatency and seamless recovery from blips. By combining a highly parallel\nasynchronous data dissemination layer with a low-latency, partially synchronous\nconsensus mechanism, Autobahn (i) avoids the hangovers incurred by traditional\nBFT protocols and (ii) matches the throughput of state of the art DAG-based BFT\nprotocols while cutting their latency in half, matching the latency of\ntraditional BFT protocols.\n","authors":["Neil Giridharan","Florian Suri-Payer","Ittai Abraham","Lorenzo Alvisi","Natacha Crooks"],"pdf_url":"https://arxiv.org/pdf/2401.10369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17435v1","updated":"2024-10-22T21:22:05Z","published":"2024-10-22T21:22:05Z","title":"AI-focused HPC Data Centers Can Provide More Power Grid Flexibility and\n  at Lower Cost","summary":"  The recent growth of Artificial Intelligence (AI), particularly large\nlanguage models, requires energy-demanding high-performance computing (HPC)\ndata centers, which poses a significant burden on power system capacity.\nScheduling data center computing jobs to manage power demand can alleviate\nnetwork stress with minimal infrastructure investment and contribute to fast\ntime-scale power system balancing. This study, for the first time,\ncomprehensively analyzes the capability and cost of grid flexibility provision\nby GPU-heavy AI-focused HPC data centers, along with a comparison with\nCPU-heavy general-purpose HPC data centers traditionally used for scientific\ncomputing. Using real-world data from 7 AI-focused HPC data centers, 7\ngeneral-purpose HPC data centers, and 3 cloud platforms, we find that\nAI-focused HPC data centers can offer greater flexibility at 50% lower cost for\na range of power system services. By comparing the cost to flexibility market\nprices, we illustrate the financial profitability of flexibility provision for\nAI-focused HPC data centers.\n","authors":["Yihong Zhou","Angel Paredes","Chaimaa Essayeh","Thomas Morstyn"],"pdf_url":"https://arxiv.org/pdf/2410.17435v1.pdf","comment":"22 pages (including supplementary materials and references), under\n  review for Joule"},{"id":"http://arxiv.org/abs/2407.15716v3","updated":"2024-10-22T20:52:09Z","published":"2024-07-22T15:22:07Z","title":"CrashEventLLM: Predicting System Crashes with Large Language Models","summary":"  As the dependence on computer systems expands across various domains,\nfocusing on personal, industrial, and large-scale applications, there arises a\ncompelling need to enhance their reliability to sustain business operations\nseamlessly and ensure optimal user satisfaction. System logs generated by these\ndevices serve as valuable repositories of historical trends and past failures.\nThe use of machine learning techniques for failure prediction has become\ncommonplace, enabling the extraction of insights from past data to anticipate\nfuture behavior patterns. Recently, large language models have demonstrated\nremarkable capabilities in tasks including summarization, reasoning, and event\nprediction. Therefore, in this paper, we endeavor to investigate the potential\nof large language models in predicting system failures, leveraging insights\nlearned from past failure behavior to inform reasoning and decision-making\nprocesses effectively. Our approach involves leveraging data from the Intel\nComputing Improvement Program (ICIP) system crash logs to identify significant\nevents and develop CrashEventLLM. This model, built upon a large language model\nframework, serves as our foundation for crash event prediction. Specifically,\nour model utilizes historical data to forecast future crash events, informed by\nexpert annotations. Additionally, it goes beyond mere prediction, offering\ninsights into potential causes for each crash event. This work provides the\npreliminary insights into prompt-based large language models for the log-based\nevent prediction task.\n","authors":["Priyanka Mudgal","Bijan Arbab","Swaathi Sampath Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.15716v3.pdf","comment":"Accepted in ICITCOM'24. Copyrights will be with IEEE"},{"id":"http://arxiv.org/abs/2410.17375v1","updated":"2024-10-22T19:15:35Z","published":"2024-10-22T19:15:35Z","title":"AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM\n  Acceleration","summary":"  Large language models typically generate tokens autoregressively, using each\ntoken as input for the next. Recent work on Speculative Decoding has sought to\naccelerate this process by employing a smaller, faster draft model to more\nquickly generate candidate tokens. These candidates are then verified in\nparallel by the larger (original) verify model, resulting in overall speedup\ncompared to using the larger model by itself in an autoregressive fashion. In\nthis work, we introduce AMUSD (Asynchronous Multi-device Speculative Decoding),\na system that further accelerates generation by decoupling the draft and verify\nphases into a continuous, asynchronous approach. Unlike conventional\nspeculative decoding, where only one model (draft or verify) performs token\ngeneration at a time, AMUSD enables both models to perform predictions\nindependently on separate devices (e.g., GPUs). We evaluate our approach over\nmultiple datasets and show that AMUSD achieves an average 29% improvement over\nspeculative decoding and up to 1.96$\\times$ speedup over conventional\nautoregressive decoding, while achieving identical output quality. Our system\nis open-source and available at https://github.com/BradMcDanel/AMUSD/.\n","authors":["Bradley McDanel"],"pdf_url":"https://arxiv.org/pdf/2410.17375v1.pdf","comment":"4 pages, 5 figures, 1 table, 1 algorithm"}]},"2024-10-23T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2410.14312v2","updated":"2024-10-23T09:00:57Z","published":"2024-10-18T09:17:35Z","title":"TiMePReSt: Time and Memory Efficient Pipeline Parallel DNN Training with\n  Removed Staleness","summary":"  DNN training is time-consuming and requires efficient multi-accelerator\nparallelization, where a single training iteration is split over available\naccelerators. Current approaches often parallelize training using intra-batch\nparallelization. Combining inter-batch and intra-batch pipeline parallelism is\ncommon to further improve training throughput. In this article, we develop a\nsystem, called TiMePReSt, that combines them in a novel way which helps to\nbetter overlap computation and communication, and limits the amount of\ncommunication. The traditional pipeline-parallel training of DNNs maintains\nsimilar working principle as sequential or conventional training of DNNs by\nmaintaining consistent weight versions in forward and backward passes of a\nmini-batch. Thus, it suffers from high GPU memory footprint during training. In\nthis paper, experimental study demonstrates that compromising weight\nconsistency doesn't decrease prediction capability of a parallelly trained DNN.\nMoreover, TiMePReSt overcomes GPU memory overhead and achieves zero weight\nstaleness. State-of-the-art techniques often become costly in terms of training\ntime. In order to address this issue, TiMePReSt introduces a variant of\nintra-batch parallelism that parallelizes the forward pass of each mini-batch\nby decomposing it into smaller micro-batches. A novel synchronization method\nbetween forward and backward passes reduces training time in TiMePReSt. The\noccurrence of multiple sequence problem and its relation with version\ndifference have been observed in TiMePReSt. This paper presents a mathematical\nrelationship between the number of micro-batches and worker machines,\nhighlighting the variation in version difference. A mathematical expression has\nbeen developed to calculate version differences for various combinations of\nthese two without creating diagrams for all combinations.\n","authors":["Ankita Dutta","Nabendu Chaki","Rajat K. De"],"pdf_url":"https://arxiv.org/pdf/2410.14312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14740v2","updated":"2024-10-23T01:08:59Z","published":"2024-10-17T08:33:39Z","title":"Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching","summary":"  Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.\n","authors":["Jie Peng","Zhang Cao","Huaizhi Qu","Zhengyu Zhang","Chang Guo","Yanyong Zhang","Zhichao Cao","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14740v2.pdf","comment":"24 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.18038v1","updated":"2024-10-23T17:06:56Z","published":"2024-10-23T17:06:56Z","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM\n  Inference","summary":"  Each request in LLM inference goes through two phases: compute-bound prefill\nand memory-bandwidth-bound decode. To improve GPU utilization, recent systems\nuse hybrid batching that combines the prefill and decode phases of different\nrequests into the same batch. Hybrid batching works well for linear operations\nas it amortizes the cost of loading model weights from HBM. However, attention\ncomputation in hybrid batches remains inefficient because existing attention\nkernels are optimized for either prefill or decode.\n  In this paper, we present POD-Attention -- the first GPU kernel that\nefficiently computes attention for hybrid batches. POD-Attention aims to\nmaximize the utilization of both compute and memory bandwidth by carefully\nallocating the GPU's resources such that prefill and decode operations happen\nconcurrently on the same multiprocessor. We integrate POD-Attention in a\nstate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up\nattention computation by up to 75% (mean 28%) and increases LLM serving\nthroughput by up to 22% in offline inference. In online inference,\nPOD-Attention enables lower time-to-first-token (TTFT), time-between-tokens\n(TBT), and request execution latency versus Sarathi-Serve.\n","authors":["Aditya K Kamath","Ramya Prabhu","Jayashree Mohan","Simon Peter","Ramachandran Ramjee","Ashish Panwar"],"pdf_url":"https://arxiv.org/pdf/2410.18038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08496v2","updated":"2024-10-23T16:52:47Z","published":"2024-04-25T20:12:10Z","title":"Large Scale Multi-GPU Based Parallel Traffic Simulation for Accelerated\n  Traffic Assignment and Propagation","summary":"  Traffic propagation simulation is crucial for urban planning, enabling\ncongestion analysis, travel time estimation, and route optimization.\nTraditional micro-simulation frameworks are limited to main roads due to the\ncomplexity of urban mobility and large-scale data. We introduce the Large Scale\nMulti-GPU Parallel Computing based Regional Scale Traffic Simulation Framework\n(LPSim), a scalable tool that leverages GPU parallel computing to simulate\nextensive traffic networks with high fidelity and reduced computation time.\nLPSim performs millions of vehicle dynamics simulations simultaneously,\noutperforming CPU-based methods. It can complete simulations of 2.82 million\ntrips in 6.28 minutes using a single GPU, and 9.01 million trips in 21.16\nminutes on dual GPUs. LPSim is also tested on dual NVIDIA A100 GPUs, achieving\nsimulations about 113 times faster than traditional CPU methods. This\ndemonstrates its scalability and efficiency for large-scale applications,\nmaking LPSim a valuable resource for researchers and planners. Code:\nhttps://github.com/Xuan-1998/LPSim\n","authors":["Xuan Jiang","Raja Sengupta","James Demmel","Samuel Williams"],"pdf_url":"https://arxiv.org/pdf/2406.08496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08437v2","updated":"2024-10-23T15:44:09Z","published":"2023-10-12T16:01:46Z","title":"Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions","summary":"  Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.\n","authors":["Muhammed Golec","Guneet Kaur Walia","Mohit Kumar","Felix Cuadrado","Sukhpal Singh Gill","Steve Uhlig"],"pdf_url":"https://arxiv.org/pdf/2310.08437v2.pdf","comment":"Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024"},{"id":"http://arxiv.org/abs/2407.12819v2","updated":"2024-10-23T14:00:36Z","published":"2024-07-01T10:33:46Z","title":"I've Got 99 Problems But FLOPS Ain't One","summary":"  Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.\n","authors":["Alexandru M. Gherghescu","Vlad-Andrei Bădoiu","Alexandru Agache","Mihai-Valentin Dumitru","Iuliu Vasilescu","Radu Mantu","Costin Raiciu"],"pdf_url":"https://arxiv.org/pdf/2407.12819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17813v1","updated":"2024-10-23T12:18:09Z","published":"2024-10-23T12:18:09Z","title":"Optimal Fault-Tolerant Dispersion on Oriented Grids","summary":"  Dispersion of mobile robots over the nodes of an anonymous graph is an\nimportant problem and turns out to be a crucial subroutine for designing\nefficient algorithms for many fundamental graph problems via mobile robots. In\nthis problem, starting from an arbitrary initial distribution of $n$ robots\nacross the $n$ nodes, the goal is to achieve a final configuration where each\nnode holds at most one robot. This paper investigates the dispersion problem on\nan oriented grid, considering the possibility of robot failures (crashes) at\nany time during the algorithm's execution. We present a crash-tolerant\ndispersion algorithm that solves the dispersion problem on an anonymous\noriented grid in $O(\\sqrt{n})$ time and using $O(\\log n)$ bits of memory per\nrobot. The algorithm is optimal in terms of both time and memory per robot. We\nfurther extend this algorithm to deal with weak Byzantine robots. The weak\nByzantine fault dispersion algorithm takes optimal $O(\\sqrt{n})$ rounds but\nrequires $O(n\\log n)$ bits of memory per robot.\n","authors":["Rik Banerjee","Manish Kumar","Anisur Rahaman Molla"],"pdf_url":"https://arxiv.org/pdf/2410.17813v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.02002"},{"id":"http://arxiv.org/abs/2408.04307v2","updated":"2024-10-23T12:08:33Z","published":"2024-08-08T08:40:15Z","title":"MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts\n  Model Training","summary":"  As large language models continue to scale up, distributed training systems\nhave expanded beyond 10k nodes, intensifying the importance of fault tolerance.\nCheckpoint has emerged as the predominant fault tolerance strategy, with\nextensive studies dedicated to optimizing its efficiency. However, the advent\nof the sparse Mixture-of-Experts (MoE) model presents new challenges due to the\nsubstantial increase in model size, despite comparable computational demands to\ndense models.\n  In this work, we propose the Mixture-of-Checkpoint System (MoC-System) to\norchestrate the vast array of checkpoint shards produced in distributed\ntraining systems. MoC-System features a novel Partial Experts Checkpointing\n(PEC) mechanism, an algorithm-system co-design that strategically saves a\nselected subset of experts, effectively reducing the MoE checkpoint size to\nlevels comparable with dense models. Incorporating hybrid parallel strategies,\nMoC-System involves fully sharded checkpointing strategies to evenly distribute\nthe workload across distributed ranks. Furthermore, MoC-System introduces a\ntwo-level checkpointing management method that asynchronously handles in-memory\nsnapshots and persistence processes.\n  We build MoC-System upon the Megatron-DeepSpeed framework, achieving up to a\n98.9% reduction in overhead for each checkpointing process compared to the\noriginal method, during MoE model training with ZeRO-2 data parallelism and\nexpert parallelism. Additionally, extensive empirical analyses substantiate\nthat our methods enhance efficiency while maintaining comparable model\naccuracy, even achieving an average accuracy increase of 1.08% on downstream\ntasks.\n","authors":["Weilin Cai","Le Qin","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.04307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17709v1","updated":"2024-10-23T09:35:35Z","published":"2024-10-23T09:35:35Z","title":"Deoxys: A Causal Inference Engine for Unhealthy Node Mitigation in\n  Large-scale Cloud Infrastructure","summary":"  The presence of unhealthy nodes in cloud infrastructure signals the potential\nfailure of machines, which can significantly impact the availability and\nreliability of cloud services, resulting in negative customer experiences.\nEffectively addressing unhealthy node mitigation is therefore vital for\nsustaining cloud system performance. This paper introduces Deoxys, a causal\ninference engine tailored to recommending mitigation actions for unhealthy node\nin cloud systems to minimize virtual machine downtime and interruptions during\nunhealthy events. It employs double machine learning combined with causal\nforest to produce precise and reliable mitigation recommendations based solely\non limited observational data collected from the historical unhealthy events.\nTo enhance the causal inference model, Deoxys further incorporates a policy\nfallback mechanism based on model uncertainty and action overriding mechanisms\nto (i) improve the reliability of the system, and (ii) strike a good tradeoff\nbetween downtime reduction and resource utilization, thereby enhancing the\noverall system performance.\n  After deploying Deoxys in a large-scale cloud infrastructure at Microsoft,\nour observations demonstrate that Deoxys significantly reduces average VM\ndowntime by 53% compared to a legacy policy, while leading to 49.5% lower VM\ninterruption rate. This substantial improvement enhances the reliability and\nstability of cloud platforms, resulting in a seamless customer experience.\n","authors":["Chaoyun Zhang","Randolph Yao","Si Qin","Ze Li","Shekhar Agrawal","Binit R. Mishra","Tri Tran","Minghua Ma","Qingwei Lin","Murali Chintalapati","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17623v1","updated":"2024-10-23T07:27:26Z","published":"2024-10-23T07:27:26Z","title":"Signature-based IaaS Performance Change Detection","summary":"  We propose a novel change detection framework to identify changes in the\nlong-term performance behavior of an IaaS service. An IaaS service's long-term\nperformance behavior is represented by an IaaS performance signature. The\nproposed framework leverages time series similarity measures and a sliding\nwindow technique to detect changes in IaaS performance signatures. We introduce\na new IaaS performance noise model that enables the proposed framework to\ndistinguish between performance noise and actual changes in performance. The\nproposed framework utilizes a novel Signal-to-Noise Ratio (SNR) based approach\nto detect changes when prior knowledge about performance noise is available. A\nset of experiments is conducted using real-world datasets to demonstrate the\neffectiveness of the proposed change detection framework.\n","authors":["Sheik Mohammad Mostakim Fattah","Athman Bouguettaya"],"pdf_url":"https://arxiv.org/pdf/2410.17623v1.pdf","comment":"accepted in ACM transaction on Internet Technology in October 2024.\n  arXiv admin note: text overlap with arXiv:2007.11705"},{"id":"http://arxiv.org/abs/2410.17563v1","updated":"2024-10-23T05:10:41Z","published":"2024-10-23T05:10:41Z","title":"Efficiently Scheduling Parallel DAG Tasks on Identical Multiprocessors","summary":"  Parallel real-time embedded applications can be modelled as directed acyclic\ngraphs (DAGs) whose nodes model subtasks and whose edges model precedence\nconstraints among subtasks. Efficiently scheduling such parallel tasks can be\nchallenging in itself, particularly in hard real-time systems where it must be\nensured offline that the deadlines of the parallel applications will be met at\nrun time. In this paper, we tackle the problem of scheduling DAG tasks on\nidentical multiprocessor systems efficiently, in terms of processor\nutilisation. We propose a new algorithm that attempts to use dedicated\nprocessor clusters for high-utilisation tasks, as in federated scheduling, but\nis also capable of reclaiming the processing capacity lost to fragmentation, by\nsplitting the execution of parallel tasks over different existing clusters, in\na manner inspired by semi-partitioned C=D scheduling (originally devised for\nnon-parallel tasks). In the experiments with synthetic DAG task sets, our\nSegmented-Flattened-and-Split scheduling approach achieves a significantly\nhigher scheduling success ratio than federated scheduling.\n","authors":["Shardul Lendve","Konstantinos Bletsas","Pedro F. Souto"],"pdf_url":"https://arxiv.org/pdf/2410.17563v1.pdf","comment":"Version submitted to RTNS 2024, on 16/08/2024 (with some typos fixed)"}],"Hardware Architecture":[{"id":"http://arxiv.org/abs/2410.14312v2","updated":"2024-10-23T09:00:57Z","published":"2024-10-18T09:17:35Z","title":"TiMePReSt: Time and Memory Efficient Pipeline Parallel DNN Training with\n  Removed Staleness","summary":"  DNN training is time-consuming and requires efficient multi-accelerator\nparallelization, where a single training iteration is split over available\naccelerators. Current approaches often parallelize training using intra-batch\nparallelization. Combining inter-batch and intra-batch pipeline parallelism is\ncommon to further improve training throughput. In this article, we develop a\nsystem, called TiMePReSt, that combines them in a novel way which helps to\nbetter overlap computation and communication, and limits the amount of\ncommunication. The traditional pipeline-parallel training of DNNs maintains\nsimilar working principle as sequential or conventional training of DNNs by\nmaintaining consistent weight versions in forward and backward passes of a\nmini-batch. Thus, it suffers from high GPU memory footprint during training. In\nthis paper, experimental study demonstrates that compromising weight\nconsistency doesn't decrease prediction capability of a parallelly trained DNN.\nMoreover, TiMePReSt overcomes GPU memory overhead and achieves zero weight\nstaleness. State-of-the-art techniques often become costly in terms of training\ntime. In order to address this issue, TiMePReSt introduces a variant of\nintra-batch parallelism that parallelizes the forward pass of each mini-batch\nby decomposing it into smaller micro-batches. A novel synchronization method\nbetween forward and backward passes reduces training time in TiMePReSt. The\noccurrence of multiple sequence problem and its relation with version\ndifference have been observed in TiMePReSt. This paper presents a mathematical\nrelationship between the number of micro-batches and worker machines,\nhighlighting the variation in version difference. A mathematical expression has\nbeen developed to calculate version differences for various combinations of\nthese two without creating diagrams for all combinations.\n","authors":["Ankita Dutta","Nabendu Chaki","Rajat K. De"],"pdf_url":"https://arxiv.org/pdf/2410.14312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14740v2","updated":"2024-10-23T01:08:59Z","published":"2024-10-17T08:33:39Z","title":"Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching","summary":"  Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.\n","authors":["Jie Peng","Zhang Cao","Huaizhi Qu","Zhengyu Zhang","Chang Guo","Yanyong Zhang","Zhichao Cao","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14740v2.pdf","comment":"24 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.18038v1","updated":"2024-10-23T17:06:56Z","published":"2024-10-23T17:06:56Z","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM\n  Inference","summary":"  Each request in LLM inference goes through two phases: compute-bound prefill\nand memory-bandwidth-bound decode. To improve GPU utilization, recent systems\nuse hybrid batching that combines the prefill and decode phases of different\nrequests into the same batch. Hybrid batching works well for linear operations\nas it amortizes the cost of loading model weights from HBM. However, attention\ncomputation in hybrid batches remains inefficient because existing attention\nkernels are optimized for either prefill or decode.\n  In this paper, we present POD-Attention -- the first GPU kernel that\nefficiently computes attention for hybrid batches. POD-Attention aims to\nmaximize the utilization of both compute and memory bandwidth by carefully\nallocating the GPU's resources such that prefill and decode operations happen\nconcurrently on the same multiprocessor. We integrate POD-Attention in a\nstate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up\nattention computation by up to 75% (mean 28%) and increases LLM serving\nthroughput by up to 22% in offline inference. In online inference,\nPOD-Attention enables lower time-to-first-token (TTFT), time-between-tokens\n(TBT), and request execution latency versus Sarathi-Serve.\n","authors":["Aditya K Kamath","Ramya Prabhu","Jayashree Mohan","Simon Peter","Ramachandran Ramjee","Ashish Panwar"],"pdf_url":"https://arxiv.org/pdf/2410.18038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08496v2","updated":"2024-10-23T16:52:47Z","published":"2024-04-25T20:12:10Z","title":"Large Scale Multi-GPU Based Parallel Traffic Simulation for Accelerated\n  Traffic Assignment and Propagation","summary":"  Traffic propagation simulation is crucial for urban planning, enabling\ncongestion analysis, travel time estimation, and route optimization.\nTraditional micro-simulation frameworks are limited to main roads due to the\ncomplexity of urban mobility and large-scale data. We introduce the Large Scale\nMulti-GPU Parallel Computing based Regional Scale Traffic Simulation Framework\n(LPSim), a scalable tool that leverages GPU parallel computing to simulate\nextensive traffic networks with high fidelity and reduced computation time.\nLPSim performs millions of vehicle dynamics simulations simultaneously,\noutperforming CPU-based methods. It can complete simulations of 2.82 million\ntrips in 6.28 minutes using a single GPU, and 9.01 million trips in 21.16\nminutes on dual GPUs. LPSim is also tested on dual NVIDIA A100 GPUs, achieving\nsimulations about 113 times faster than traditional CPU methods. This\ndemonstrates its scalability and efficiency for large-scale applications,\nmaking LPSim a valuable resource for researchers and planners. Code:\nhttps://github.com/Xuan-1998/LPSim\n","authors":["Xuan Jiang","Raja Sengupta","James Demmel","Samuel Williams"],"pdf_url":"https://arxiv.org/pdf/2406.08496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08437v2","updated":"2024-10-23T15:44:09Z","published":"2023-10-12T16:01:46Z","title":"Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions","summary":"  Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.\n","authors":["Muhammed Golec","Guneet Kaur Walia","Mohit Kumar","Felix Cuadrado","Sukhpal Singh Gill","Steve Uhlig"],"pdf_url":"https://arxiv.org/pdf/2310.08437v2.pdf","comment":"Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024"},{"id":"http://arxiv.org/abs/2407.12819v2","updated":"2024-10-23T14:00:36Z","published":"2024-07-01T10:33:46Z","title":"I've Got 99 Problems But FLOPS Ain't One","summary":"  Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.\n","authors":["Alexandru M. Gherghescu","Vlad-Andrei Bădoiu","Alexandru Agache","Mihai-Valentin Dumitru","Iuliu Vasilescu","Radu Mantu","Costin Raiciu"],"pdf_url":"https://arxiv.org/pdf/2407.12819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17813v1","updated":"2024-10-23T12:18:09Z","published":"2024-10-23T12:18:09Z","title":"Optimal Fault-Tolerant Dispersion on Oriented Grids","summary":"  Dispersion of mobile robots over the nodes of an anonymous graph is an\nimportant problem and turns out to be a crucial subroutine for designing\nefficient algorithms for many fundamental graph problems via mobile robots. In\nthis problem, starting from an arbitrary initial distribution of $n$ robots\nacross the $n$ nodes, the goal is to achieve a final configuration where each\nnode holds at most one robot. This paper investigates the dispersion problem on\nan oriented grid, considering the possibility of robot failures (crashes) at\nany time during the algorithm's execution. We present a crash-tolerant\ndispersion algorithm that solves the dispersion problem on an anonymous\noriented grid in $O(\\sqrt{n})$ time and using $O(\\log n)$ bits of memory per\nrobot. The algorithm is optimal in terms of both time and memory per robot. We\nfurther extend this algorithm to deal with weak Byzantine robots. The weak\nByzantine fault dispersion algorithm takes optimal $O(\\sqrt{n})$ rounds but\nrequires $O(n\\log n)$ bits of memory per robot.\n","authors":["Rik Banerjee","Manish Kumar","Anisur Rahaman Molla"],"pdf_url":"https://arxiv.org/pdf/2410.17813v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.02002"},{"id":"http://arxiv.org/abs/2408.04307v2","updated":"2024-10-23T12:08:33Z","published":"2024-08-08T08:40:15Z","title":"MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts\n  Model Training","summary":"  As large language models continue to scale up, distributed training systems\nhave expanded beyond 10k nodes, intensifying the importance of fault tolerance.\nCheckpoint has emerged as the predominant fault tolerance strategy, with\nextensive studies dedicated to optimizing its efficiency. However, the advent\nof the sparse Mixture-of-Experts (MoE) model presents new challenges due to the\nsubstantial increase in model size, despite comparable computational demands to\ndense models.\n  In this work, we propose the Mixture-of-Checkpoint System (MoC-System) to\norchestrate the vast array of checkpoint shards produced in distributed\ntraining systems. MoC-System features a novel Partial Experts Checkpointing\n(PEC) mechanism, an algorithm-system co-design that strategically saves a\nselected subset of experts, effectively reducing the MoE checkpoint size to\nlevels comparable with dense models. Incorporating hybrid parallel strategies,\nMoC-System involves fully sharded checkpointing strategies to evenly distribute\nthe workload across distributed ranks. Furthermore, MoC-System introduces a\ntwo-level checkpointing management method that asynchronously handles in-memory\nsnapshots and persistence processes.\n  We build MoC-System upon the Megatron-DeepSpeed framework, achieving up to a\n98.9% reduction in overhead for each checkpointing process compared to the\noriginal method, during MoE model training with ZeRO-2 data parallelism and\nexpert parallelism. Additionally, extensive empirical analyses substantiate\nthat our methods enhance efficiency while maintaining comparable model\naccuracy, even achieving an average accuracy increase of 1.08% on downstream\ntasks.\n","authors":["Weilin Cai","Le Qin","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.04307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17709v1","updated":"2024-10-23T09:35:35Z","published":"2024-10-23T09:35:35Z","title":"Deoxys: A Causal Inference Engine for Unhealthy Node Mitigation in\n  Large-scale Cloud Infrastructure","summary":"  The presence of unhealthy nodes in cloud infrastructure signals the potential\nfailure of machines, which can significantly impact the availability and\nreliability of cloud services, resulting in negative customer experiences.\nEffectively addressing unhealthy node mitigation is therefore vital for\nsustaining cloud system performance. This paper introduces Deoxys, a causal\ninference engine tailored to recommending mitigation actions for unhealthy node\nin cloud systems to minimize virtual machine downtime and interruptions during\nunhealthy events. It employs double machine learning combined with causal\nforest to produce precise and reliable mitigation recommendations based solely\non limited observational data collected from the historical unhealthy events.\nTo enhance the causal inference model, Deoxys further incorporates a policy\nfallback mechanism based on model uncertainty and action overriding mechanisms\nto (i) improve the reliability of the system, and (ii) strike a good tradeoff\nbetween downtime reduction and resource utilization, thereby enhancing the\noverall system performance.\n  After deploying Deoxys in a large-scale cloud infrastructure at Microsoft,\nour observations demonstrate that Deoxys significantly reduces average VM\ndowntime by 53% compared to a legacy policy, while leading to 49.5% lower VM\ninterruption rate. This substantial improvement enhances the reliability and\nstability of cloud platforms, resulting in a seamless customer experience.\n","authors":["Chaoyun Zhang","Randolph Yao","Si Qin","Ze Li","Shekhar Agrawal","Binit R. Mishra","Tri Tran","Minghua Ma","Qingwei Lin","Murali Chintalapati","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17623v1","updated":"2024-10-23T07:27:26Z","published":"2024-10-23T07:27:26Z","title":"Signature-based IaaS Performance Change Detection","summary":"  We propose a novel change detection framework to identify changes in the\nlong-term performance behavior of an IaaS service. An IaaS service's long-term\nperformance behavior is represented by an IaaS performance signature. The\nproposed framework leverages time series similarity measures and a sliding\nwindow technique to detect changes in IaaS performance signatures. We introduce\na new IaaS performance noise model that enables the proposed framework to\ndistinguish between performance noise and actual changes in performance. The\nproposed framework utilizes a novel Signal-to-Noise Ratio (SNR) based approach\nto detect changes when prior knowledge about performance noise is available. A\nset of experiments is conducted using real-world datasets to demonstrate the\neffectiveness of the proposed change detection framework.\n","authors":["Sheik Mohammad Mostakim Fattah","Athman Bouguettaya"],"pdf_url":"https://arxiv.org/pdf/2410.17623v1.pdf","comment":"accepted in ACM transaction on Internet Technology in October 2024.\n  arXiv admin note: text overlap with arXiv:2007.11705"},{"id":"http://arxiv.org/abs/2410.17563v1","updated":"2024-10-23T05:10:41Z","published":"2024-10-23T05:10:41Z","title":"Efficiently Scheduling Parallel DAG Tasks on Identical Multiprocessors","summary":"  Parallel real-time embedded applications can be modelled as directed acyclic\ngraphs (DAGs) whose nodes model subtasks and whose edges model precedence\nconstraints among subtasks. Efficiently scheduling such parallel tasks can be\nchallenging in itself, particularly in hard real-time systems where it must be\nensured offline that the deadlines of the parallel applications will be met at\nrun time. In this paper, we tackle the problem of scheduling DAG tasks on\nidentical multiprocessor systems efficiently, in terms of processor\nutilisation. We propose a new algorithm that attempts to use dedicated\nprocessor clusters for high-utilisation tasks, as in federated scheduling, but\nis also capable of reclaiming the processing capacity lost to fragmentation, by\nsplitting the execution of parallel tasks over different existing clusters, in\na manner inspired by semi-partitioned C=D scheduling (originally devised for\nnon-parallel tasks). In the experiments with synthetic DAG task sets, our\nSegmented-Flattened-and-Split scheduling approach achieves a significantly\nhigher scheduling success ratio than federated scheduling.\n","authors":["Shardul Lendve","Konstantinos Bletsas","Pedro F. Souto"],"pdf_url":"https://arxiv.org/pdf/2410.17563v1.pdf","comment":"Version submitted to RTNS 2024, on 16/08/2024 (with some typos fixed)"}],"Databases":[{"id":"http://arxiv.org/abs/2410.16929v2","updated":"2024-10-23T08:17:23Z","published":"2024-10-22T12:00:15Z","title":"CUBIT: Concurrent Updatable Bitmap Indexing (Extended Version)","summary":"  Bitmap indexes are widely used for read-intensive analytical workloads\nbecause they are clustered and offer efficient reads with a small memory\nfootprint. However, they are notoriously inefficient to update. As analytical\napplications are increasingly fused with transactional applications, leading to\nthe emergence of hybrid transactional/analytical processing (HTAP), it is\ndesirable that bitmap indexes support efficient concurrent real-time updates.\nIn this paper, we propose Concurrent Updatable Bitmap indexing (CUBIT) that\noffers efficient real-time updates that scale with the number of CPU cores used\nand do not interfere with queries. Our design relies on three principles.\nFirst, we employ a horizontal bitwise representation of updated bits, which\nenables efficient atomic updates without locking entire bitvectors. Second, we\npropose a lightweight snapshotting mechanism that allows queries (including\nrange queries) to run on separate snapshots and provides a wait-free progress\nguarantee. Third, we consolidate updates in a latch-free manner, providing a\nstrong progress guarantee. Our evaluation shows that CUBIT offers 3x - 16x\nhigher throughput and 3x - 220x lower latency than state-of-the-art updatable\nbitmap indexes.\n  CUBIT's update-friendly nature widens the applicability of bitmap indexing.\nExperimenting with OLAP workloads with standard, batched updates shows that\nCUBIT overcomes the maintenance downtime and outperforms DuckDB by 1.2x - 2.7x\non TPC-H. For HTAP workloads with real-time updates, CUBIT achieves 2x - 11x\nperformance improvement over the state-of-the-art approaches.\n","authors":["Junchang Wang","Manos Athanassoulis"],"pdf_url":"https://arxiv.org/pdf/2410.16929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07898v4","updated":"2024-10-23T15:27:30Z","published":"2023-10-11T21:10:15Z","title":"Multiversion Hindsight Logging for Continuous Training","summary":"  Production Machine Learning involves continuous training: hosting multiple\nversions of models over time, often with many model versions running at once.\nWhen model performance does not meet expectations, Machine Learning Engineers\n(MLEs) debug issues by exploring and analyzing numerous prior versions of code\nand training data to identify root causes and mitigate problems. Traditional\ndebugging and logging tools often fall short in managing this experimental,\nmulti-version context. FlorDB introduces Multiversion Hindsight Logging, which\nallows engineers to use the most recent version's logging statements to query\npast versions, even when older versions logged different data. Log statement\npropagation enables consistent injection of logging statements into past code\nversions, regardless of changes to the codebase. Once log statements are\npropagated across code versions, the remaining challenge in Multiversion\nHindsight Logging is to efficiently replay the new log statements based on\ncheckpoints from previous runs. Finally, a coherent user experience is required\nto help MLEs debug across all versions of code and data. To this end, FlorDB\npresents a unified relational model for efficient handling of historical\nqueries, offering a comprehensive view of the log history to simplify the\nexploration of past code iterations. We present a performance evaluation on\ndiverse benchmarks confirming its scalability and the ability to deliver\nreal-time query responses, leveraging query-based filtering and\ncheckpoint-based parallelism for efficient replay.\n","authors":["Rolando Garcia","Anusha Dandamudi","Gabriel Matute","Lehan Wan","Joseph Gonzalez","Joseph M. Hellerstein","Koushik Sen"],"pdf_url":"https://arxiv.org/pdf/2310.07898v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17748v1","updated":"2024-10-23T10:23:53Z","published":"2024-10-23T10:23:53Z","title":"Can Uncertainty Quantification Enable Better Learning-based Index\n  Tuning?","summary":"  Index tuning is crucial for optimizing database performance by selecting\noptimal indexes based on workload. The key to this process lies in an accurate\nand efficient benefit estimator. Traditional methods relying on what-if tools\noften suffer from inefficiency and inaccuracy. In contrast, learning-based\nmodels provide a promising alternative but face challenges such as instability,\nlack of interpretability, and complex management. To overcome these\nlimitations, we adopt a novel approach: quantifying the uncertainty in\nlearning-based models' results, thereby combining the strengths of both\ntraditional and learning-based methods for reliable index tuning. We propose\nBeauty, the first uncertainty-aware framework that enhances learning-based\nmodels with uncertainty quantification and uses what-if tools as a\ncomplementary mechanism to improve reliability and reduce management\ncomplexity. Specifically, we introduce a novel method that combines AutoEncoder\nand Monte Carlo Dropout to jointly quantify uncertainty, tailored to the\ncharacteristics of benefit estimation tasks. In experiments involving sixteen\nmodels, our approach outperformed existing uncertainty quantification methods\nin the majority of cases. We also conducted index tuning tests on six datasets.\nBy applying the Beauty framework, we eliminated worst-case scenarios and more\nthan tripled the occurrence of best-case scenarios.\n","authors":["Tao Yu","Zhaonian Zou","Hao Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.17748v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.17600v1","updated":"2024-10-23T06:54:03Z","published":"2024-10-23T06:54:03Z","title":"Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective","summary":"  Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion.\n","authors":["Rui Yang","Boming Yang","Aosong Feng","Sixun Ouyang","Moritz Blum","Tianwei She","Yuang Jiang","Freddy Lecue","Jinghui Lu","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2410.17600v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.10794"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2410.17195v2","updated":"2024-10-23T07:02:09Z","published":"2024-10-22T17:13:38Z","title":"Non-myopic Generation of Language Model for Reasoning and Planning","summary":"  Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.\n","authors":["Chang Ma","Haiteng Zhao","Junlei Zhang","Junxian He","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.17195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17031v2","updated":"2024-10-23T13:52:51Z","published":"2024-10-22T13:57:55Z","title":"GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks","summary":"  The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation.\n","authors":["Shuyang Hou","Zhangxiao Shen","Anqi Zhao","Jianyuan Liang","Zhipeng Gui","Xuefeng Guan","Rui Li","Huayi Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16700v2","updated":"2024-10-23T02:29:24Z","published":"2024-10-22T05:11:54Z","title":"AskBeacon -- Performing genomic data exchange and analytics with natural\n  language","summary":"  Enabling clinicians and researchers to directly interact with global genomic\ndata resources by removing technological barriers is vital for medical\ngenomics. AskBeacon enables Large Language Models to be applied to securely\nshared cohorts via the GA4GH Beacon protocol. By simply \"asking\" Beacon,\nactionable insights can be gained, analyzed and made publication-ready.\n","authors":["Anuradha Wickramarachchi","Shakila Tonni","Sonali Majumdar","Sarvnaz Karimi","Sulev Kõks","Brendan Hosking","Jordi Rambla","Natalie A. Twine","Yatish Jain","Denis C. Bauer"],"pdf_url":"https://arxiv.org/pdf/2410.16700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16638v2","updated":"2024-10-23T03:41:49Z","published":"2024-10-22T02:27:57Z","title":"LLMScan: Causal Scan for LLM Misbehavior Detection","summary":"  Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.\n","authors":["Mengdi Zhang","Kai Kiat Goh","Peixin Zhang","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.16638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16194v3","updated":"2024-10-23T05:47:21Z","published":"2024-05-25T11:53:23Z","title":"Diffusion-Reward Adversarial Imitation Learning","summary":"  Imitation learning aims to learn a policy from observing expert\ndemonstrations without access to reward signals from environments. Generative\nadversarial imitation learning (GAIL) formulates imitation learning as\nadversarial learning, employing a generator policy learning to imitate expert\nbehaviors and discriminator learning to distinguish the expert demonstrations\nfrom agent trajectories. Despite its encouraging results, GAIL training is\noften brittle and unstable. Inspired by the recent dominance of diffusion\nmodels in generative modeling, we propose Diffusion-Reward Adversarial\nImitation Learning (DRAIL), which integrates a diffusion model into GAIL,\naiming to yield more robust and smoother rewards for policy learning.\nSpecifically, we propose a diffusion discriminative classifier to construct an\nenhanced discriminator, and design diffusion rewards based on the classifier's\noutput for policy learning. Extensive experiments are conducted in navigation,\nmanipulation, and locomotion, verifying DRAIL's effectiveness compared to prior\nimitation learning methods. Moreover, additional experimental results\ndemonstrate the generalizability and data efficiency of DRAIL. Visualized\nlearned reward functions of GAIL and DRAIL suggest that DRAIL can produce more\nrobust and smoother rewards. Project page:\nhttps://nturobotlearninglab.github.io/DRAIL/\n","authors":["Chun-Mao Lai","Hsiang-Chun Wang","Ping-Chun Hsieh","Yu-Chiang Frank Wang","Min-Hung Chen","Shao-Hua Sun"],"pdf_url":"https://arxiv.org/pdf/2405.16194v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18077v1","updated":"2024-10-23T17:58:49Z","published":"2024-10-23T17:58:49Z","title":"ALTA: Compiler-Based Analysis of Transformers","summary":"  We propose a new programming language called ALTA and a compiler that can map\nALTA programs to Transformer weights. ALTA is inspired by RASP, a language\nproposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler\nfrom RASP programs to Transformer weights. ALTA complements and extends this\nprior work, offering the ability to express loops and to compile programs to\nUniversal Transformers, among other advantages. ALTA allows us to\nconstructively show how Transformers can represent length-invariant algorithms\nfor computing parity and addition, as well as a solution to the SCAN benchmark\nof compositional generalization tasks, without requiring intermediate\nscratchpad decoding steps. We also propose tools to analyze cases where the\nexpressibility of an algorithm is established, but end-to-end training on a\ngiven training set fails to induce behavior consistent with the desired\nalgorithm. To this end, we explore training from ALTA execution traces as a\nmore fine-grained supervision signal. This enables additional experiments and\ntheoretical analyses relating the learnability of various algorithms to data\navailability and modeling decisions, such as positional encodings. We make the\nALTA framework -- language specification, symbolic interpreter, and weight\ncompiler -- available to the community to enable further applications and\ninsights.\n","authors":["Peter Shaw","James Cohan","Jacob Eisenstein","Kenton Lee","Jonathan Berant","Kristina Toutanova"],"pdf_url":"https://arxiv.org/pdf/2410.18077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18076v1","updated":"2024-10-23T17:58:45Z","published":"2024-10-23T17:58:45Z","title":"Leveraging Skills from Unlabeled Prior Data for Efficient Online\n  Exploration","summary":"  Unsupervised pretraining has been transformative in many supervised domains.\nHowever, applying such ideas to reinforcement learning (RL) presents a unique\nchallenge in that fine-tuning does not involve mimicking task-specific data,\nbut rather exploring and locating the solution through iterative\nself-improvement. In this work, we study how unlabeled prior trajectory data\ncan be leveraged to learn efficient exploration strategies. While prior data\ncan be used to pretrain a set of low-level skills, or as additional off-policy\ndata for online RL, it has been unclear how to combine these ideas effectively\nfor online exploration. Our method SUPE (Skills from Unlabeled Prior data for\nExploration) demonstrates that a careful combination of these ideas compounds\ntheir benefits. Our method first extracts low-level skills using a variational\nautoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an\noptimistic reward model, transforming prior data into high-level, task-relevant\nexamples. Finally, SUPE uses these transformed examples as additional\noff-policy data for online RL to learn a high-level policy that composes\npretrained low-level skills to explore efficiently. We empirically show that\nSUPE reliably outperforms prior strategies, successfully solving a suite of\nlong-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.\n","authors":["Max Wilcoxson","Qiyang Li","Kevin Frans","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.18076v1.pdf","comment":"23 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.18071v1","updated":"2024-10-23T17:54:43Z","published":"2024-10-23T17:54:43Z","title":"TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing\n  Prompts","summary":"  Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks.\n","authors":["Yuxuan Xie","Tianhua Li","Wenqi Shao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12568v2","updated":"2024-10-23T17:53:24Z","published":"2024-08-22T17:35:18Z","title":"Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune\n  CNNs and Transformers","summary":"  To solve ever more complex problems, Deep Neural Networks are scaled to\nbillions of parameters, leading to huge computational costs. An effective\napproach to reduce computational requirements and increase efficiency is to\nprune unnecessary components of these often over-parameterized networks.\nPrevious work has shown that attribution methods from the field of eXplainable\nAI serve as effective means to extract and prune the least relevant network\ncomponents in a few-shot fashion. We extend the current state by proposing to\nexplicitly optimize hyperparameters of attribution methods for the task of\npruning, and further include transformer-based networks in our analysis. Our\napproach yields higher model compression rates of large transformer- and\nconvolutional architectures (VGG, ResNet, ViT) compared to previous works,\nwhile still attaining high performance on ImageNet classification tasks. Here,\nour experiments indicate that transformers have a higher degree of\nover-parameterization compared to convolutional neural networks. Code is\navailable at https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.\n","authors":["Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2408.12568v2.pdf","comment":"Accepted as a workshop paper at ECCV 2024, 26 pages (11 pages\n  manuscript, 3 pages references, 12 pages appendix)"},{"id":"http://arxiv.org/abs/2410.18070v1","updated":"2024-10-23T17:53:11Z","published":"2024-10-23T17:53:11Z","title":"Training Free Guided Flow Matching with Optimal Control","summary":"  Controlled generation with pre-trained Diffusion and Flow Matching models has\nvast applications. One strategy for guiding ODE-based generative models is\nthrough optimizing a target loss $R(x_1)$ while staying close to the prior\ndistribution. Along this line, some recent work showed the effectiveness of\nguiding flow model by differentiating through its ODE sampling process. Despite\nthe superior performance, the theoretical understanding of this line of methods\nis still preliminary, leaving space for algorithm improvement. Moreover,\nexisting methods predominately focus on Euclidean data manifold, and there is a\ncompelling need for guided flow methods on complex geometries such as SO(3),\nwhich prevails in high-stake scientific applications like protein design. We\npresent OC-Flow, a general and theoretically grounded training-free framework\nfor guided flow matching using optimal control. Building upon advances in\noptimal control theory, we develop effective and practical algorithms for\nsolving optimal control in guided ODE-based generation and provide a systematic\ntheoretical analysis of the convergence guarantee in both Euclidean and SO(3).\nWe show that existing backprop-through-ODE methods can be interpreted as\nspecial cases of Euclidean OC-Flow. OC-Flow achieved superior performance in\nextensive experiments on text-guided image manipulation, conditional molecule\ngeneration, and all-atom peptide design.\n","authors":["Luran Wang","Chaoran Cheng","Yizhen Liao","Yanru Qu","Ge Liu"],"pdf_url":"https://arxiv.org/pdf/2410.18070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03185v2","updated":"2024-10-23T17:52:57Z","published":"2024-03-05T18:22:15Z","title":"Correlated Proxies: A New Definition and Improved Mitigation for Reward\n  Hacking","summary":"  Because it is difficult to precisely specify complex objectives,\nreinforcement learning policies are often optimized using flawed proxy rewards\nthat seem to capture the true objective. However, optimizing proxy rewards\nfrequently leads to reward hacking: the optimized reward function ceases to be\na good proxy, and the resulting policy performs poorly with respect to the\nunspecified true reward. Principled solutions to reward hacking have been\nimpeded by the lack of a good definition for the problem. To address this, we\nintroduce a definition of reward hacking based on the correlation between proxy\nand true rewards for states and actions seen by a \"base policy\" that breaks\ndown under optimization. We show that this definition captures reward hacking\nbehavior across several realistic settings, including in reinforcement learning\nfrom human feedback (RLHF). We then show theoretically that regularization to\nthe base policy can effectively prevent reward hacking. While current RLHF\napproaches apply a KL penalty between the action distributions of policies, our\ntheory suggests that it is more effective to regularize using the $\\chi^2$\ndivergence between the policies' occupancy measures. We intuitively show why\nthis type of regularization is superior and demonstrate that it better\nmitigates reward hacking in practice across four realistic domains, including\nRLHF for LLMs. Our code is available at https://github.com/cassidylaidlaw/orpo.\n","authors":["Cassidy Laidlaw","Shivam Singhal","Anca Dragan"],"pdf_url":"https://arxiv.org/pdf/2403.03185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13577v2","updated":"2024-10-23T17:50:54Z","published":"2023-11-22T18:32:03Z","title":"Physical Reasoning and Object Planning for Household Embodied Agents","summary":"  In this study, we explore the sophisticated domain of task planning for\nrobust household embodied agents, with a particular emphasis on the intricate\ntask of selecting substitute objects. We introduce the CommonSense Object\nAffordance Task (COAT), a novel framework designed to analyze reasoning\ncapabilities in commonsense scenarios. This approach is centered on\nunderstanding how these agents can effectively identify and utilize alternative\nobjects when executing household tasks, thereby offering insights into the\ncomplexities of practical decision-making in real-world environments. Drawing\ninspiration from factors affecting human decision-making, we explore how large\nlanguage models tackle this challenge through four meticulously crafted\ncommonsense question-and-answer datasets featuring refined rules and human\nannotations. Our evaluation of state-of-the-art language models on these\ndatasets sheds light on three pivotal considerations: 1) aligning an object's\ninherent utility with the task at hand, 2) navigating contextual dependencies\n(societal norms, safety, appropriateness, and efficiency), and 3) accounting\nfor the current physical state of the object. To maintain accessibility, we\nintroduce five abstract variables reflecting an object's physical condition,\nmodulated by human insights, to simulate diverse household scenarios. Our\ncontributions include insightful human preference mappings for all three\nfactors and four extensive QA datasets (2K, 15k, 60k, 70K questions) probing\nthe intricacies of utility dependencies, contextual dependencies and object\nphysical states. The datasets, along with our findings, are accessible at:\nhttps://github.com/Ayush8120/COAT. This research not only advances our\nunderstanding of physical commonsense reasoning in language models but also\npaves the way for future improvements in household agent intelligence.\n","authors":["Ayush Agrawal","Raghav Prabhakar","Anirudh Goyal","Dianbo Liu"],"pdf_url":"https://arxiv.org/pdf/2311.13577v2.pdf","comment":"Journal: TMLR(May/2024) Total: 39 pages (17 pages main content, 15\n  Figures)"},{"id":"http://arxiv.org/abs/2410.18067v1","updated":"2024-10-23T17:48:28Z","published":"2024-10-23T17:48:28Z","title":"Beyond position: how rotary embeddings shape representations and memory\n  in autoregressive transfomers","summary":"  Rotary Positional Embeddings (RoPE) enhance positional encoding in\nTransformer models, yet their full impact on model dynamics remains\nunderexplored. This paper studies how RoPE introduces position-dependent\nrotations, causing phase shifts in token embeddings that influence\nhigher-frequency components within the model's internal representations.\nThrough spectral analysis, we demonstrate that RoPE's rotation matrices induce\noscillatory behaviors in embeddings, affecting information retention across\nlayers and shaping temporal modeling capabilities. We show that activation\nfunctions in feed-forward networks interact with RoPE-modulated embeddings to\ngenerate harmonics, leading to constructive or destructive interference based\non phase alignment. Our findings reveal that phase alignment amplifies\nactivations and sharpens attention, while misalignment weakens activations and\ndisrupts focus on positional patterns. This study underscores the importance of\nfrequency components as intrinsic elements of model behavior, offering new\ninsights beyond traditional analyses.\n","authors":["Valeria Ruscio","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2410.18067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15240v2","updated":"2024-10-23T17:47:58Z","published":"2024-09-23T17:38:41Z","title":"MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue\n  Generation","summary":"  Long-term memory is important for chatbots and dialogue systems (DS) to\ncreate consistent and human-like conversations, evidenced by numerous developed\nmemory-augmented DS (MADS). To evaluate the effectiveness of such MADS,\nexisting commonly used evaluation metrics, like retrieval accuracy and\nperplexity (PPL), mainly focus on query-oriented factualness and language\nquality assessment. However, these metrics often lack practical value.\nMoreover, the evaluation dimensions are insufficient for human-like assessment\nin DS. Regarding memory-recalling paradigms, current evaluation schemes only\nconsider passive memory retrieval while ignoring diverse memory recall with\nrich triggering factors, e.g., emotions and surroundings, which can be\nessential in emotional support scenarios. To bridge the gap, we construct a\nnovel Memory-Augmented Dialogue Benchmark (MADail-Bench) covering various\nmemory-recalling paradigms based on cognitive science and psychology theories.\nThe benchmark assesses two tasks separately: memory retrieval and memory\nrecognition with the incorporation of both passive and proactive memory recall\ndata. We introduce new scoring criteria to the evaluation, including memory\ninjection, emotion support (ES) proficiency, and intimacy, to comprehensively\nassess generated responses. Results from cutting-edge embedding models and\nlarge language models on this benchmark indicate the potential for further\nadvancement. Extensive testing further reveals correlations between memory\ninjection, ES proficiency, and intimacy.\n","authors":["Junqing He","Liang Zhu","Rui Wang","Xi Wang","Reza Haffari","Jiaxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.15240v2.pdf","comment":"Submitted to NAACL 2025"},{"id":"http://arxiv.org/abs/2407.15762v2","updated":"2024-10-23T17:42:39Z","published":"2024-07-22T16:13:38Z","title":"Conditional Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning","summary":"  Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge is to develop\nsteerable language models that trade-off multiple (conflicting) objectives in a\nflexible and efficient manner. This paper presents Conditional Language Policy\n(CLP), a general framework for finetuning language models on multiple\nobjectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through extensive experiments and ablations on two\nsummarization datasets, we show that CLP learns steerable language models that\noutperform and Pareto-dominate the existing approaches for multi-objective\nfinetuning.\n","authors":["Kaiwen Wang","Rahul Kidambi","Ryan Sullivan","Alekh Agarwal","Christoph Dann","Andrea Michi","Marco Gelmi","Yunxuan Li","Raghav Gupta","Avinava Dubey","Alexandre Ramé","Johan Ferret","Geoffrey Cideron","Le Hou","Hongkun Yu","Amr Ahmed","Aranyak Mehta","Léonard Hussenot","Olivier Bachem","Edouard Leurent"],"pdf_url":"https://arxiv.org/pdf/2407.15762v2.pdf","comment":"40 pages. Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.18065v1","updated":"2024-10-23T17:42:07Z","published":"2024-10-23T17:42:07Z","title":"SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for\n  Long-Horizon Manipulation","summary":"  Robot learning has proven to be a general and effective technique for\nprogramming manipulators. Imitation learning is able to teach robots solely\nfrom human demonstrations but is bottlenecked by the capabilities of the\ndemonstrations. Reinforcement learning uses exploration to discover better\nbehaviors; however, the space of possible improvements can be too large to\nstart from scratch. And for both techniques, the learning difficulty increases\nproportional to the length of the manipulation task. Accounting for this, we\npropose SPIRE, a system that first uses Task and Motion Planning (TAMP) to\ndecompose tasks into smaller learning subproblems and second combines imitation\nand reinforcement learning to maximize their strengths. We develop novel\nstrategies to train learning agents when deployed in the context of a planning\nsystem. We evaluate SPIRE on a suite of long-horizon and contact-rich robot\nmanipulation problems. We find that SPIRE outperforms prior approaches that\nintegrate imitation learning, reinforcement learning, and planning by 35% to\n50% in average task performance, is 6 times more data efficient in the number\nof human demonstrations needed to train proficient agents, and learns to\ncomplete tasks nearly twice as efficiently. View\nhttps://sites.google.com/view/spire-corl-2024 for more details.\n","authors":["Zihan Zhou","Animesh Garg","Dieter Fox","Caelan Garrett","Ajay Mandlekar"],"pdf_url":"https://arxiv.org/pdf/2410.18065v1.pdf","comment":"Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2405.18246v2","updated":"2024-10-23T17:33:57Z","published":"2024-05-28T14:58:07Z","title":"Utilitarian Algorithm Configuration for Infinite Parameter Spaces","summary":"  Utilitarian algorithm configuration is a general-purpose technique for\nautomatically searching the parameter space of a given algorithm to optimize\nits performance, as measured by a given utility function, on a given set of\ninputs. Recently introduced utilitarian configuration procedures offer\noptimality guarantees about the returned parameterization while provably\nadapting to the hardness of the underlying problem. However, the applicability\nof these approaches is severely limited by the fact that they only search a\nfinite, relatively small set of parameters. They cannot effectively search the\nconfiguration space of algorithms with continuous or uncountable parameters. In\nthis paper we introduce a new procedure, which we dub COUP (Continuous,\nOptimistic Utilitarian Procrastination). COUP is designed to search infinite\nparameter spaces efficiently to find good configurations quickly. Furthermore,\nCOUP maintains the theoretical benefits of previous utilitarian configuration\nprocedures when applied to finite parameter spaces but is significantly faster,\nboth provably and experimentally.\n","authors":["Devon Graham","Kevin Leyton-Brown"],"pdf_url":"https://arxiv.org/pdf/2405.18246v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18060v1","updated":"2024-10-23T17:33:27Z","published":"2024-10-23T17:33:27Z","title":"Explaining Bayesian Networks in Natural Language using Factor Arguments.\n  Evaluation in the medical domain","summary":"  In this paper, we propose a model for building natural language explanations\nfor Bayesian Network Reasoning in terms of factor arguments, which are\nargumentation graphs of flowing evidence, relating the observed evidence to a\ntarget variable we want to learn about. We introduce the notion of factor\nargument independence to address the outstanding question of defining when\narguments should be presented jointly or separately and present an algorithm\nthat, starting from the evidence nodes and a target node, produces a list of\nall independent factor arguments ordered by their strength. Finally, we\nimplemented a scheme to build natural language explanations of Bayesian\nReasoning using this approach. Our proposal has been validated in the medical\ndomain through a human-driven evaluation study where we compare the Bayesian\nNetwork Reasoning explanations obtained using factor arguments with an\nalternative explanation method. Evaluation results indicate that our proposed\nexplanation approach is deemed by users as significantly more useful for\nunderstanding Bayesian Network Reasoning than another existing explanation\nmethod it is compared to.\n","authors":["Jaime Sevilla","Nikolay Babakov","Ehud Reiter","Alberto Bugarin"],"pdf_url":"https://arxiv.org/pdf/2410.18060v1.pdf","comment":"First Workshop on Explainable Artificial Intelligence for the medical\n  domain - EXPLIMED. THE 27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE"},{"id":"http://arxiv.org/abs/2410.02916v2","updated":"2024-10-23T17:26:06Z","published":"2024-10-03T19:07:53Z","title":"Safeguard is a Double-edged Sword: Denial-of-service Attack on Large\n  Language Models","summary":"  Safety is a paramount concern of large language models (LLMs) in their open\ndeployment. To this end, safeguard methods aim to enforce the ethical and\nresponsible use of LLMs through safety alignment or guardrail mechanisms.\nHowever, we found that the malicious attackers could exploit false positives of\nsafeguards, i.e., fooling the safeguard model to block safe content mistakenly,\nleading to a new denial-of-service (DoS) attack on LLMs. Specifically, by\nsoftware or phishing attacks on user client software, attackers insert a short,\nseemingly innocuous adversarial prompt into to user prompt templates in\nconfiguration files; thus, this prompt appears in final user requests without\nvisibility in the user interface and is not trivial to identify. By designing\nan optimization process that utilizes gradient and attention information, our\nattack can automatically generate seemingly safe adversarial prompts,\napproximately only 30 characters long, that universally block over 97\\% of user\nrequests on Llama Guard 3. The attack presents a new dimension of evaluating\nLLM safeguards focusing on false positives, fundamentally different from the\nclassic jailbreak.\n","authors":["Qingzhao Zhang","Ziyang Xiong","Z. Morley Mao"],"pdf_url":"https://arxiv.org/pdf/2410.02916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18040v1","updated":"2024-10-23T17:07:32Z","published":"2024-10-23T17:07:32Z","title":"Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for\n  Russian Scientific Keyphrases","summary":"  Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts.\n","authors":["Anna Glazkova","Dmitry Morozov","Timur Garipov"],"pdf_url":"https://arxiv.org/pdf/2410.18040v1.pdf","comment":"The 12th International Conference on Analysis of Images, Social\n  Networks and Texts (AIST'2024)"},{"id":"http://arxiv.org/abs/2410.18032v1","updated":"2024-10-23T17:02:59Z","published":"2024-10-23T17:02:59Z","title":"GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration","summary":"  Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.\n","authors":["Xin Li","Qizhi Chu","Yubin Chen","Yang Liu","Yaoqi Liu","Zekai Yu","Weize Chen","Chen Qian","Chuan Shi","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.18032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12025v2","updated":"2024-10-23T17:01:05Z","published":"2024-08-21T22:35:19Z","title":"Exploring Large Language Models for Feature Selection: A Data-centric\n  Perspective","summary":"  The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires numerical values of samples to do\nstatistical inference and text-based feature selection which utilizes prior\nknowledge of LLMs to do semantical associations using descriptive context. We\nconduct experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field.\n","authors":["Dawei Li","Zhen Tan","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2408.12025v2.pdf","comment":"Accepted by SIGKDD Explorations (December 2024)"},{"id":"http://arxiv.org/abs/2410.18027v1","updated":"2024-10-23T17:00:13Z","published":"2024-10-23T17:00:13Z","title":"Cross-lingual Transfer of Reward Models in Multilingual Alignment","summary":"  Reinforcement learning with human feedback (RLHF) is shown to largely benefit\nfrom precise reward models (RMs). However, recent studies in reward modeling\nschemes are skewed towards English, limiting the applicability of RLHF in\nmultilingual alignments. In this work, we investigate the cross-lingual\ntransfer of RMs trained in diverse languages, primarily from English. Our\nexperimental results demonstrate the strong cross-lingual transfer of English\nRMs, exceeding target language RMs by 3~4% average increase in Multilingual\nRewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through\nthe representation shifts. Finally, we perform multilingual alignment to\nexemplify how cross-lingual transfer in RM propagates to enhanced multilingual\ninstruction-following capability, along with extensive analyses on\noff-the-shelf RMs. We release the code, model, and data.\n","authors":["Jiwoo Hong","Noah Lee","Rodrigo Martínez-Castaño","César Rodríguez","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2410.18027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11757v4","updated":"2024-10-23T16:41:45Z","published":"2024-06-17T17:16:45Z","title":"STAR: SocioTechnical Approach to Red Teaming Language Models","summary":"  This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.\n","authors":["Laura Weidinger","John Mellor","Bernat Guillen Pegueroles","Nahema Marchal","Ravin Kumar","Kristian Lum","Canfer Akbulut","Mark Diaz","Stevie Bergman","Mikel Rodriguez","Verena Rieser","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2406.11757v4.pdf","comment":"8 pages, 5 figures, 5 pages appendix. * denotes equal contribution"},{"id":"http://arxiv.org/abs/2409.19841v2","updated":"2024-10-23T16:27:27Z","published":"2024-09-30T00:47:13Z","title":"Counter-Current Learning: A Biologically Plausible Dual Network Approach\n  for Deep Learning","summary":"  Despite its widespread use in neural networks, error backpropagation has\nfaced criticism for its lack of biological plausibility, suffering from issues\nsuch as the backward locking problem and the weight transport problem. These\nlimitations have motivated researchers to explore more biologically plausible\nlearning algorithms that could potentially shed light on how biological neural\nsystems adapt and learn. Inspired by the counter-current exchange mechanisms\nobserved in biological systems, we propose counter-current learning (CCL), a\nbiologically plausible framework for credit assignment in neural networks. This\nframework employs a feedforward network to process input data and a feedback\nnetwork to process targets, with each network enhancing the other through\nanti-parallel signal propagation. By leveraging the more informative signals\nfrom the bottom layer of the feedback network to guide the updates of the top\nlayer of the feedforward network and vice versa, CCL enables the simultaneous\ntransformation of source inputs to target outputs and the dynamic mutual\ninfluence of these transformations. Experimental results on MNIST,\nFashionMNIST, CIFAR10, and CIFAR100 datasets using multi-layer perceptrons and\nconvolutional neural networks demonstrate that CCL achieves comparable\nperformance to other biologically plausible algorithms while offering a more\nbiologically realistic learning mechanism. Furthermore, we showcase the\napplicability of our approach to an autoencoder task, underscoring its\npotential for unsupervised representation learning. Our work presents a\ndirection for biologically inspired and plausible learning algorithms, offering\nan alternative mechanism of learning and adaptation in neural networks.\n","authors":["Chia-Hsiang Kao","Bharath Hariharan"],"pdf_url":"https://arxiv.org/pdf/2409.19841v2.pdf","comment":"Accepted at NeurIPS 2024. Code available at\n  https://github.com/IandRover/CCL-NeurIPS24"},{"id":"http://arxiv.org/abs/2409.17270v2","updated":"2024-10-23T16:27:20Z","published":"2024-09-25T18:35:45Z","title":"Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.\n","authors":["Debargha Ganguly","Srinivasan Iyengar","Vipin Chaudhary","Shivkumar Kalyanaraman"],"pdf_url":"https://arxiv.org/pdf/2409.17270v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) System 2 Reasoning At Scale Workshop"},{"id":"http://arxiv.org/abs/2410.18001v1","updated":"2024-10-23T16:24:23Z","published":"2024-10-23T16:24:23Z","title":"Benchmarking Foundation Models on Exceptional Cases: Dataset Creation\n  and Validation","summary":"  Foundation models (FMs) have achieved significant success across various\ntasks, leading to research on benchmarks for reasoning abilities. However,\nthere is a lack of studies on FMs performance in exceptional scenarios, which\nwe define as out-of-distribution (OOD) reasoning tasks. This paper is the first\nto address these cases, developing a novel dataset for evaluation of FMs across\nmultiple modalities, including graphic novels, calligraphy, news articles, and\nlyrics. It includes tasks for instance classification, character recognition,\ntoken prediction, and text generation. The paper also proposes prompt\nengineering techniques like Chain-of-Thought (CoT) and CoT+Few-Shot to enhance\nperformance. Validation of FMs using various methods revealed improvements. The\ncode repository is accessible at:\nhttps://github.com/MLAI-Yonsei/ExceptionalBenchmark\n","authors":["Suho Kang","Jungyang Park","Joonseo Ha","SoMin Kim","JinHyeong Kim","Subeen Park","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2410.18001v1.pdf","comment":"EMNLP 2024 Workshop\n  Genbench(https://genbench.org/workshop_programme/)"},{"id":"http://arxiv.org/abs/2410.17991v1","updated":"2024-10-23T16:08:00Z","published":"2024-10-23T16:08:00Z","title":"AI driven health recommender","summary":"  As AI emerged as highest valued technology, We used that to create a web\napplication that makes a patient work easier .It detects the disease name based\non the symptoms given by the patient and recommends medication for respective\ndisease, precautions to take, diet to follow and workouts to do, so the disease\ncan be minimized. The web application is made with clean and Realtime data by\nusing Machine learning as root. We used flask to create a user-friendly\nplatform.\n","authors":["K. Vignesh","B. Pranavi","Ch. Sreenidhi"],"pdf_url":"https://arxiv.org/pdf/2410.17991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02131v4","updated":"2024-10-23T16:05:11Z","published":"2024-06-04T09:18:20Z","title":"CondTSF: One-line Plugin of Dataset Condensation for Time Series\n  Forecasting","summary":"  Dataset condensation is a newborn technique that generates a small dataset\nthat can be used in training deep neural networks to lower training costs. The\nobjective of dataset condensation is to ensure that the model trained with the\nsynthetic dataset can perform comparably to the model trained with full\ndatasets. However, existing methods predominantly concentrate on classification\ntasks, posing challenges in their adaptation to time series forecasting\n(TS-forecasting). This challenge arises from disparities in the evaluation of\nsynthetic data. In classification, the synthetic data is considered\nwell-distilled if the model trained with the full dataset and the model trained\nwith the synthetic dataset yield identical labels for the same input,\nregardless of variations in output logits distribution. Conversely, in\nTS-forecasting, the effectiveness of synthetic data distillation is determined\nby the distance between predictions of the two models. The synthetic data is\ndeemed well-distilled only when all data points within the predictions are\nsimilar. Consequently, TS-forecasting has a more rigorous evaluation\nmethodology compared to classification. To mitigate this gap, we theoretically\nanalyze the optimization objective of dataset condensation for TS-forecasting\nand propose a new one-line plugin of dataset condensation designated as Dataset\nCondensation for Time Series Forecasting (CondTSF) based on our analysis.\nPlugging CondTSF into previous dataset condensation methods facilitates a\nreduction in the distance between the predictions of the model trained with the\nfull dataset and the model trained with the synthetic dataset, thereby\nenhancing performance. We conduct extensive experiments on eight commonly used\ntime series datasets. CondTSF consistently improves the performance of all\nprevious dataset condensation methods across all datasets, particularly at low\ncondensing ratios.\n","authors":["Jianrong Ding","Zhanyu Liu","Guanjie Zheng","Haiming Jin","Linghe Kong"],"pdf_url":"https://arxiv.org/pdf/2406.02131v4.pdf","comment":"Accepted by NeurIPS 2024, the project can be found at\n  https://github.com/RafaDD/CondTSF"},{"id":"http://arxiv.org/abs/2410.17986v1","updated":"2024-10-23T16:00:14Z","published":"2024-10-23T16:00:14Z","title":"Federated Transformer: Multi-Party Vertical Federated Learning on\n  Practical Fuzzily Linked Data","summary":"  Federated Learning (FL) is an evolving paradigm that enables multiple parties\nto collaboratively train models without sharing raw data. Among its variants,\nVertical Federated Learning (VFL) is particularly relevant in real-world,\ncross-organizational collaborations, where distinct features of a shared\ninstance group are contributed by different parties. In these scenarios,\nparties are often linked using fuzzy identifiers, leading to a common practice\ntermed as multi-party fuzzy VFL. Existing models generally address either\nmulti-party VFL or fuzzy VFL between two parties. Extending these models to\npractical multi-party fuzzy VFL typically results in significant performance\ndegradation and increased costs for maintaining privacy. To overcome these\nlimitations, we introduce the Federated Transformer (FeT), a novel framework\nthat supports multi-party VFL with fuzzy identifiers. FeT innovatively encodes\nthese identifiers into data representations and employs a transformer\narchitecture distributed across different parties, incorporating three new\ntechniques to enhance performance. Furthermore, we have developed a multi-party\nprivacy framework for VFL that integrates differential privacy with secure\nmulti-party computation, effectively protecting local representations while\nminimizing associated utility costs. Our experiments demonstrate that the FeT\nsurpasses the baseline models by up to 46\\% in terms of accuracy when scaled to\n50 parties. Additionally, in two-party fuzzy VFL settings, FeT also shows\nimproved performance and privacy over cutting-edge VFL models.\n","authors":["Zhaomin Wu","Junyi Hou","Yiqun Diao","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2410.17986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14979v2","updated":"2024-10-23T15:43:28Z","published":"2024-10-19T05:01:56Z","title":"Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration","summary":"  Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.\n","authors":["Wei Xie","Shuoyoucheng Ma","Zhenhua Wang","Enze Wang","Baosheng Wang","Jinshu Su"],"pdf_url":"https://arxiv.org/pdf/2410.14979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17971v1","updated":"2024-10-23T15:36:43Z","published":"2024-10-23T15:36:43Z","title":"Dynamic Spectrum Access for Ambient Backscatter Communication-assisted\n  D2D Systems with Quantum Reinforcement Learning","summary":"  Spectrum access is an essential problem in device-to-device (D2D)\ncommunications. However, with the recent growth in the number of mobile\ndevices, the wireless spectrum is becoming scarce, resulting in low spectral\nefficiency for D2D communications. To address this problem, this paper aims to\nintegrate the ambient backscatter communication technology into D2D devices to\nallow them to backscatter ambient RF signals to transmit their data when the\nshared spectrum is occupied by mobile users. To obtain the optimal spectrum\naccess policy, i.e., stay idle or access the shared spectrum and perform active\ntransmissions or backscattering ambient RF signals for transmissions, to\nmaximize the average throughput for D2D users, deep reinforcement learning\n(DRL) can be adopted. However, DRL-based solutions may require long training\ntime due to the curse of dimensionality issue as well as complex deep neural\nnetwork architectures. For that, we develop a novel quantum reinforcement\nlearning (RL) algorithm that can achieve a faster convergence rate with fewer\ntraining parameters compared to DRL thanks to the quantum superposition and\nquantum entanglement principles. Specifically, instead of using conventional\ndeep neural networks, the proposed quantum RL algorithm uses a parametrized\nquantum circuit to approximate an optimal policy. Extensive simulations then\ndemonstrate that the proposed solution not only can significantly improve the\naverage throughput of D2D devices when the shared spectrum is busy but also can\nachieve much better performance in terms of convergence rate and learning\ncomplexity compared to existing DRL-based methods.\n","authors":["Nguyen Van Huynh","Bolun Zhang","Dinh-Hieu Tran","Dinh Thai Hoang","Diep N. Nguyen","Gan Zheng","Dusit Niyato","Quoc-Viet Pham"],"pdf_url":"https://arxiv.org/pdf/2410.17971v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17961v1","updated":"2024-10-23T15:30:13Z","published":"2024-10-23T15:30:13Z","title":"Closed-form merging of parameter-efficient modules for Federated\n  Continual Learning","summary":"  Model merging has emerged as a crucial technique in Deep Learning, enabling\nthe integration of multiple models into a unified system while preserving\nperformance and scalability. In this respect, the compositional properties of\nlow-rank adaptation techniques (e.g., LoRA) have proven beneficial, as simple\naveraging LoRA modules yields a single model that mostly integrates the\ncapabilities of all individual modules. Building on LoRA, we take a step\nfurther by imposing that the merged model matches the responses of all learned\nmodules. Solving this objective in closed form yields an indeterminate system\nwith A and B as unknown variables, indicating the existence of infinitely many\nclosed-form solutions. To address this challenge, we introduce LoRM, an\nalternating optimization strategy that trains one LoRA matrix at a time. This\nallows solving for each unknown variable individually, thus finding a unique\nsolution. We apply our proposed methodology to Federated Class-Incremental\nLearning (FCIL), ensuring alignment of model responses both between clients and\nacross tasks. Our method demonstrates state-of-the-art performance across a\nrange of FCIL scenarios.\n","authors":["Riccardo Salami","Pietro Buzzega","Matteo Mosconi","Jacopo Bonato","Luigi Sabetta","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2410.17961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05101v3","updated":"2024-10-23T15:28:45Z","published":"2024-04-07T22:53:43Z","title":"StockGPT: A GenAI Model for Stock Prediction and Trading","summary":"  This paper introduces StockGPT, an autoregressive ``number'' model trained\nand tested on 70 million daily U.S.\\ stock returns over nearly 100 years.\nTreating each return series as a sequence of tokens, StockGPT automatically\nlearns the hidden patterns predictive of future returns via its attention\nmechanism. On a held-out test sample from 2001 to 2023, daily and monthly\nrebalanced long-short portfolios formed from StockGPT predictions yield strong\nperformance. The StockGPT-based portfolios span momentum and long-/short-term\nreversals, eliminating the need for manually crafted price-based strategies,\nand yield highly significant alphas against leading stock market factors,\nsuggesting a novel AI pricing effect. This highlights the immense promise of\ngenerative AI in surpassing human in making complex financial investment\ndecisions.\n","authors":["Dat Mai"],"pdf_url":"https://arxiv.org/pdf/2404.05101v3.pdf","comment":"26 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.17957v1","updated":"2024-10-23T15:27:37Z","published":"2024-10-23T15:27:37Z","title":"MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers","summary":"  In this paper, we propose MCUBERT to enable language models like BERT on tiny\nmicrocontroller units (MCUs) through network and scheduling co-optimization. We\nobserve the embedding table contributes to the major storage bottleneck for\ntiny BERT models. Hence, at the network level, we propose an MCU-aware\ntwo-stage neural architecture search algorithm based on clustered low-rank\napproximation for embedding compression. To reduce the inference memory\nrequirements, we further propose a novel fine-grained MCU-friendly scheduling\nstrategy. Through careful computation tiling and re-ordering as well as kernel\ndesign, we drastically increase the input sequence lengths supported on MCUs\nwithout any latency or accuracy penalty. MCUBERT reduces the parameter size of\nBERT-tiny and BERT-mini by 5.7$\\times$ and 3.0$\\times$ and the execution memory\nby 3.5$\\times$ and 4.3$\\times$, respectively. MCUBERT also achieves 1.5$\\times$\nlatency reduction. For the first time, MCUBERT enables lightweight BERT models\non commodity MCUs and processing more than 512 tokens with less than 256KB of\nmemory.\n","authors":["Zebin Yang","Renze Chen","Taiqiang Wu","Ngai Wong","Yun Liang","Runsheng Wang","Ru Huang","Meng Li"],"pdf_url":"https://arxiv.org/pdf/2410.17957v1.pdf","comment":"ICCAD 2024"},{"id":"http://arxiv.org/abs/2410.17954v1","updated":"2024-10-23T15:24:54Z","published":"2024-10-23T15:24:54Z","title":"ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference","summary":"  Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.\n","authors":["Xin He","Shunkang Zhang","Yuxin Wang","Haiyan Yin","Zihao Zeng","Shaohuai Shi","Zhenheng Tang","Xiaowen Chu","Ivor Tsang","Ong Yew Soon"],"pdf_url":"https://arxiv.org/pdf/2410.17954v1.pdf","comment":"Mixture-of-Experts, Inference, Offloading"},{"id":"http://arxiv.org/abs/2410.17952v1","updated":"2024-10-23T15:24:16Z","published":"2024-10-23T15:24:16Z","title":"SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains","summary":"  Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.\n","authors":["Ran Xu","Hui Liu","Sreyashi Nag","Zhenwei Dai","Yaochen Xie","Xianfeng Tang","Chen Luo","Yang Li","Joyce C. Ho","Carl Yang","Qi He"],"pdf_url":"https://arxiv.org/pdf/2410.17952v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.17950v1","updated":"2024-10-23T15:23:23Z","published":"2024-10-23T15:23:23Z","title":"Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for\n  Enhanced LLM Function Calling","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in various\ndomains, yet their economic impact has been limited by challenges in tool use\nand function calling. This paper introduces ThorV2, a novel architecture that\nsignificantly enhances LLMs' function calling abilities. We develop a\ncomprehensive benchmark focused on HubSpot CRM operations to evaluate ThorV2\nagainst leading models from OpenAI and Anthropic. Our results demonstrate that\nThorV2 outperforms existing models in accuracy, reliability, latency, and cost\nefficiency for both single and multi-API calling tasks. We also show that\nThorV2 is far more reliable and scales better to multistep tasks compared to\ntraditional models. Our work offers the tantalizing possibility of more\naccurate function-calling compared to today's best-performing models using\nsignificantly smaller LLMs. These advancements have significant implications\nfor the development of more capable AI assistants and the broader application\nof LLMs in real-world scenarios.\n","authors":["Nirav Bhan","Shival Gupta","Sai Manaswini","Ritik Baba","Narun Yadav","Hillori Desai","Yash Choudhary","Aman Pawar","Sarthak Shrivastava","Sudipta Biswas"],"pdf_url":"https://arxiv.org/pdf/2410.17950v1.pdf","comment":"15 pages for main paper, 21 pages in total including references and\n  appendix, 10 figures"},{"id":"http://arxiv.org/abs/2410.17943v1","updated":"2024-10-23T15:15:56Z","published":"2024-10-23T15:15:56Z","title":"Optimizing Travel Itineraries with AI Algorithms in a Microservices\n  Architecture: Balancing Cost, Time, Preferences, and Sustainability","summary":"  The objective of this research is how an implementation of AI algorithms in\nthe microservices architecture enhances travel itineraries by cost, time, user\npreferences, and environmental sustainability. It uses machine learning models\nfor both cost forecasting and personalization, genetic algorithm for\noptimization of the itinerary, and heuristics for sustainability checking.\nPrimary evaluated parameters consist of latency, ability to satisfy user\npreferences, cost and environmental concern. The experimental results\ndemonstrate an average of 4.5 seconds of response time on 1000 concurrent users\nand 92% of user preferences accuracy. The cost efficiency is proved, with 95%\nof provided trips being within the limits of the budget declared by the user.\nThe system also implements some measures to alleviate negative externalities\nrelated to travel and 60% of offered travel plans had green options\nincorporated, resulting in the average 15% lower carbon emissions than the\ntraditional travel plans offered. The genetic algorithm with time complexity\nO(g.p.f) provides the optimal solution in 100 generations. Every iteration\nimproves the quality of the solution by 5%, thus enabling its effective use in\noptimization problems where time is measured in seconds. Finally, the system is\ndesigned to be fault-tolerant with functional 99.9% availability which allows\nthe provision of services even when requirements are exceeded. Travel\noptimization platform is turned dynamic and efficient by this microservices\nbased architecture which provides enhanced scaling, allows asynchronous\ncommunication and real time changes. Because of the incorporation of Ai, cost\ncontrol and eco-friendliness approaches, the system addresses the different\nuser needs in the present days travel business.\n","authors":["Biman Barua","M. Shamim Kaiser"],"pdf_url":"https://arxiv.org/pdf/2410.17943v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.03093v2","updated":"2024-10-23T15:01:34Z","published":"2024-08-06T10:48:15Z","title":"Certifiably Robust Policies for Uncertain Parametric Environments","summary":"  We present a data-driven approach for producing policies that are provably\nrobust across unknown stochastic environments. Existing approaches can learn\nmodels of a single environment as an interval Markov decision processes (IMDP)\nand produce a robust policy with a probably approximately correct (PAC)\nguarantee on its performance. However these are unable to reason about the\nimpact of environmental parameters underlying the uncertainty. We propose a\nframework based on parametric Markov decision processes (MDPs) with unknown\ndistributions over parameters. We learn and analyse IMDPs for a set of unknown\nsample environments induced by parameters. The key challenge is then to produce\nmeaningful performance guarantees that combine the two layers of uncertainty:\n(1) multiple environments induced by parameters with an unknown distribution;\n(2) unknown induced environments which are approximated by IMDPs. We present a\nnovel approach based on scenario optimisation that yields a single PAC\nguarantee quantifying the risk level for which a specified performance level\ncan be assured in unseen environments, plus a means to trade-off risk and\nperformance. We implement and evaluate our framework using multiple robust\npolicy generation methods on a range of benchmarks. We show that our approach\nproduces tight bounds on a policy's performance with high confidence.\n","authors":["Yannik Schnitzer","Alessandro Abate","David Parker"],"pdf_url":"https://arxiv.org/pdf/2408.03093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17933v1","updated":"2024-10-23T14:55:53Z","published":"2024-10-23T14:55:53Z","title":"Multi-Continental Healthcare Modelling Using Blockchain-Enabled\n  Federated Learning","summary":"  One of the biggest challenges of building artificial intelligence (AI) model\nin healthcare area is the data sharing. Since healthcare data is private,\nsensitive, and heterogeneous, collecting sufficient data for modelling is\nexhausted, costly, and sometimes impossible. In this paper, we propose a\nframework for global healthcare modelling using datasets from multi-continents\n(Europe, North America and Asia) while without sharing the local datasets, and\nchoose glucose management as a study model to verify its effectiveness.\nTechnically, blockchain-enabled federated learning is implemented with adaption\nto make it meet with the privacy and safety requirements of healthcare data,\nmeanwhile rewards honest participation and penalize malicious activities using\nits on-chain incentive mechanism. Experimental results show that the proposed\nframework is effective, efficient, and privacy preserved. Its prediction\naccuracy is much better than the models trained from limited personal data and\nis similar to, and even slightly better than, the results from a centralized\ndataset. This work paves the way for international collaborations on healthcare\nprojects, where additional data is crucial for reducing bias and providing\nbenefits to humanity.\n","authors":["Rui Sun","Zhipeng Wang","Hengrui Zhang","Ming Jiang","Yizhe Wen","Jiqun Zhang","Jiahao Sun","Shuoying Zhang","Erwu Liu","Kezhi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17933v1.pdf","comment":"Accepted by IEEE Global Blockchain Conference"},{"id":"http://arxiv.org/abs/2410.02240v4","updated":"2024-10-23T14:53:38Z","published":"2024-10-03T06:25:53Z","title":"SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial\n  Attack","summary":"  Deep neural network based systems deployed in sensitive environments are\nvulnerable to adversarial attacks. Unrestricted adversarial attacks typically\nmanipulate the semantic content of an image (e.g., color or texture) to create\nadversarial examples that are both effective and photorealistic. Recent works\nhave utilized the diffusion inversion process to map images into a latent\nspace, where high-level semantics are manipulated by introducing perturbations.\nHowever, they often results in substantial semantic distortions in the denoised\noutput and suffers from low efficiency. In this study, we propose a novel\nframework called Semantic-Consistent Unrestricted Adversarial Attacks (SCA),\nwhich employs an inversion method to extract edit-friendly noise maps and\nutilizes Multimodal Large Language Model (MLLM) to provide semantic guidance\nthroughout the process. Under the condition of rich semantic information\nprovided by MLLM, we perform the DDPM denoising process of each step using a\nseries of edit-friendly noise maps, and leverage DPM Solver++ to accelerate\nthis process, enabling efficient sampling with semantic consistency. Compared\nto existing methods, our framework enables the efficient generation of\nadversarial examples that exhibit minimal discernible semantic changes.\nConsequently, we for the first time introduce Semantic-Consistent Adversarial\nExamples (SCAE). Extensive experiments and visualizations have demonstrated the\nhigh efficiency of SCA, particularly in being on average 12 times faster than\nthe state-of-the-art attacks. Our research can further draw attention to the\nsecurity of multimedia information.\n","authors":["Zihao Pan","Weibin Wu","Yuhang Cao","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.02240v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08401v3","updated":"2024-10-23T14:48:44Z","published":"2024-04-12T11:15:15Z","title":"PnLCalib: Sports Field Registration via Points and Lines Optimization","summary":"  Camera calibration in broadcast sports videos presents numerous challenges\nfor accurate sports field registration due to multiple camera angles, varying\ncamera parameters, and frequent occlusions of the field. Traditional\nsearch-based methods depend on initial camera pose estimates, which can\nstruggle in non-standard positions and dynamic environments. In response, we\npropose an optimization-based calibration pipeline that leverages a 3D soccer\nfield model and a predefined set of keypoints to overcome these limitations.\nOur method also introduces a novel refinement module that improves initial\ncalibration by using detected field lines in a non-linear optimization process.\nThis approach outperforms existing techniques in both multi-view and\nsingle-view 3D camera calibration tasks, while maintaining competitive\nperformance in homography estimation. Extensive experimentation on real-world\nsoccer datasets, including SoccerNet-Calibration, WorldCup 2014, and\nTS-WorldCup, highlights the robustness and accuracy of our method across\ndiverse broadcast scenarios. Our approach offers significant improvements in\ncamera calibration precision and reliability.\n","authors":["Marc Gutiérrez-Pérez","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2404.08401v3.pdf","comment":"Extended version of \"No Bells, Just Whistles: Sports Field\n  Registration Leveraging Geometric Properties\""},{"id":"http://arxiv.org/abs/2310.10107v4","updated":"2024-10-23T14:47:42Z","published":"2023-10-16T06:41:13Z","title":"Posterior Sampling-based Online Learning for Episodic POMDPs","summary":"  Learning in POMDPs is known to be significantly harder than in MDPs. In this\npaper, we consider the online learning problem for episodic POMDPs with unknown\ntransition and observation models. We propose a Posterior Sampling-based\nreinforcement learning algorithm for POMDPs (PS4POMDPs), which is much simpler\nand more implementable compared to state-of-the-art optimism-based online\nlearning algorithms for POMDPs. We show that the Bayesian regret of the\nproposed algorithm scales as the square root of the number of episodes and is\npolynomial in the other parameters. In a general setting, the regret scales\nexponentially in the horizon length $H$, and we show that this is inevitable by\nproviding a lower bound. However, when the POMDP is undercomplete and weakly\nrevealing (a common assumption in the recent literature), we establish a\npolynomial Bayesian regret bound. We finally propose a posterior sampling\nalgorithm for multi-agent POMDPs, and show it too has sublinear regret.\n","authors":["Dengwang Tang","Dongze Ye","Rahul Jain","Ashutosh Nayyar","Pierluigi Nuzzo"],"pdf_url":"https://arxiv.org/pdf/2310.10107v4.pdf","comment":"41 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.05349v3","updated":"2024-10-23T14:47:10Z","published":"2023-12-08T20:12:26Z","title":"PixLore: A Dataset-driven Approach to Rich Image Captioning","summary":"  In the domain of vision-language integration, generating detailed image\ncaptions poses a significant challenge due to the lack of curated and rich\ndatasets. This study introduces PixLore, a novel method that leverages Querying\nTransformers through the fine-tuning of the BLIP-2 model using the LoRa method\non a standard commercial GPU. The followed approach, which involves training on\na carefully assembled dataset from state-of-the-art Computer Vision models\ncombined and augmented by ChatGPT, addresses the question of whether intricate\nimage understanding can be achieved with an ensemble of smaller-scale models,\nreferred to as Knowledge Stitching. Comparative evaluations against major\nmodels such as GPT-4 and Google Bard demonstrate that PixLore-2.7B, despite\nhaving considerably fewer parameters, is rated higher than the existing\nState-of-the-Art models in over half of the assessments. Precisely, PixLore\noutperform Bard and BLIP-2, which score approximately 35.18% and 27.98% lower\nthan PixLore in the task of image captioning. This research not only presents a\ngroundbreaking approach but also highlights the importance of well-curated\ndatasets in enhancing the performance of smaller models.\n","authors":["Diego Bonilla-Salvador","Marcelino Martínez-Sober","Joan Vila-Francés","Antonio José Serrano-López","Pablo Rodríguez-Belenguer","Fernando Mateo"],"pdf_url":"https://arxiv.org/pdf/2312.05349v3.pdf","comment":"Paper in preprint pending of publication"},{"id":"http://arxiv.org/abs/2410.17922v1","updated":"2024-10-23T14:40:37Z","published":"2024-10-23T14:40:37Z","title":"Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced\n  Defense in Large Language Models","summary":"  With the extensive deployment of Large Language Models (LLMs), ensuring their\nsafety has become increasingly critical. However, existing defense methods\noften struggle with two key issues: (i) inadequate defense capabilities,\nparticularly in domain-specific scenarios like chemistry, where a lack of\nspecialized knowledge can lead to the generation of harmful responses to\nmalicious queries. (ii) over-defensiveness, which compromises the general\nutility and responsiveness of LLMs. To mitigate these issues, we introduce a\nmulti-agents-based defense framework, Guide for Defense (G4D), which leverages\naccurate external information to provide an unbiased summary of user intentions\nand analytically grounded safety response guidance. Extensive experiments on\npopular jailbreak attacks and benign datasets show that our G4D can enhance\nLLM's robustness against jailbreak attacks on general and domain-specific\nscenarios without compromising the model's general functionality.\n","authors":["He Cao","Weidi Luo","Yu Wang","Zijing Liu","Bing Feng","Yuan Yao","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2410.17922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17918v1","updated":"2024-10-23T14:34:39Z","published":"2024-10-23T14:34:39Z","title":"Addressing Asynchronicity in Clinical Multimodal Fusion via\n  Individualized Chest X-ray Generation","summary":"  Integrating multi-modal clinical data, such as electronic health records\n(EHR) and chest X-ray images (CXR), is particularly beneficial for clinical\nprediction tasks. However, in a temporal setting, multi-modal data are often\ninherently asynchronous. EHR can be continuously collected but CXR is generally\ntaken with a much longer interval due to its high cost and radiation dose. When\nclinical prediction is needed, the last available CXR image might have been\noutdated, leading to suboptimal predictions. To address this challenge, we\npropose DDL-CXR, a method that dynamically generates an up-to-date latent\nrepresentation of the individualized CXR images. Our approach leverages latent\ndiffusion models for patient-specific generation strategically conditioned on a\nprevious CXR image and EHR time series, providing information regarding\nanatomical structures and disease progressions, respectively. In this way, the\ninteraction across modalities could be better captured by the latent CXR\ngeneration process, ultimately improving the prediction performance.\nExperiments using MIMIC datasets show that the proposed model could effectively\naddress asynchronicity in multimodal fusion and consistently outperform\nexisting methods.\n","authors":["Wenfang Yao","Chen Liu","Kejing Yin","William K. Cheung","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2410.17918v1.pdf","comment":"Accepted by NeurIPS-24"},{"id":"http://arxiv.org/abs/2408.04377v3","updated":"2024-10-23T14:29:56Z","published":"2024-08-08T11:22:52Z","title":"Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon","summary":"  Anomaly detection in time series data is a critical challenge across various\ndomains. Traditional methods typically focus on identifying anomalies in\nimmediate subsequent steps, often underestimating the significance of temporal\ndynamics such as delay time and horizons of anomalies, which generally require\nextensive post-analysis. This paper introduces a novel approach for time series\nanomaly prediction, incorporating temporal information directly into the\nprediction results. We propose a new dataset specifically designed to evaluate\nthis approach and conduct comprehensive experiments using several\nstate-of-the-art methods. Our results demonstrate the efficacy of our approach\nin providing timely and accurate anomaly predictions, setting a new benchmark\nfor future research in this field.\n","authors":["Jiang You","Arben Cela","René Natowicz","Jacob Ouanounou","Patrick Siarry"],"pdf_url":"https://arxiv.org/pdf/2408.04377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17906v1","updated":"2024-10-23T14:26:35Z","published":"2024-10-23T14:26:35Z","title":"Leveraging Deep Learning for Time Series Extrinsic Regression in\n  predicting photometric metallicity of Fundamental-mode RR Lyrae Stars","summary":"  Astronomy is entering an unprecedented era of Big Data science, driven by\nmissions like the ESA's Gaia telescope, which aims to map the Milky Way in\nthree dimensions. Gaia's vast dataset presents a monumental challenge for\ntraditional analysis methods. The sheer scale of this data exceeds the\ncapabilities of manual exploration, necessitating the utilization of advanced\ncomputational techniques. In response to this challenge, we developed a novel\napproach leveraging deep learning to estimate the metallicity of fundamental\nmode (ab-type) RR Lyrae stars from their light curves in the Gaia optical\nG-band. Our study explores applying deep learning techniques, particularly\nadvanced neural network architectures, in predicting photometric metallicity\nfrom time-series data. Our deep learning models demonstrated notable predictive\nperformance, with a low mean absolute error (MAE) of 0.0565, the root mean\nsquare error (RMSE) achieved is 0.0765 and a high $R^2$ regression performance\nof 0.9401 measured by cross-validation. The weighted mean absolute error (wMAE)\nis 0.0563, while the weighted root mean square error (wRMSE) is 0.0763. These\nresults showcase the effectiveness of our approach in accurately estimating\nmetallicity values. Our work underscores the importance of deep learning in\nastronomical research, particularly with large datasets from missions like\nGaia. By harnessing the power of deep learning methods, we can provide\nprecision in analyzing vast datasets, contributing to more precise and\ncomprehensive insights into complex astronomical phenomena.\n","authors":["Lorenzo Monti","Tatiana Muraveva","Gisella Clementini","Alessia Garofalo"],"pdf_url":"https://arxiv.org/pdf/2410.17906v1.pdf","comment":"Sensors 2024, 24(16), 5203; (23 pages)"},{"id":"http://arxiv.org/abs/2405.16164v3","updated":"2024-10-23T14:24:50Z","published":"2024-05-25T10:15:51Z","title":"Acquiring Better Load Estimates by Combining Anomaly and Change Point\n  Detection in Power Grid Time-series Measurements","summary":"  In this paper we present novel methodology for automatic anomaly and switch\nevent filtering to improve load estimation in power grid systems. By leveraging\nunsupervised methods with supervised optimization, our approach prioritizes\ninterpretability while ensuring robust and generalizable performance on unseen\ndata. Through experimentation, a combination of binary segmentation for change\npoint detection and statistical process control for anomaly detection emerges\nas the most effective strategy, specifically when ensembled in a novel\nsequential manner. Results indicate the clear wasted potential when filtering\nis not applied. The automatic load estimation is also fairly accurate, with\napproximately 90% of estimates falling within a 10% error margin, with only a\nsingle significant failure in both the minimum and maximum load estimates\nacross 60 measurements in the test set. Our methodology's interpretability\nmakes it particularly suitable for critical infrastructure planning, thereby\nenhancing decision-making processes.\n","authors":["Roel Bouman","Linda Schmeitz","Luco Buise","Jacco Heres","Yuliya Shapovalova","Tom Heskes"],"pdf_url":"https://arxiv.org/pdf/2405.16164v3.pdf","comment":"All code can be found at: https://github.com/RoelBouman/StormPhase2"},{"id":"http://arxiv.org/abs/2410.17904v1","updated":"2024-10-23T14:22:49Z","published":"2024-10-23T14:22:49Z","title":"Reinforcement Learning under Latent Dynamics: Toward Statistical and\n  Algorithmic Modularity","summary":"  Real-world applications of reinforcement learning often involve environments\nwhere agents operate on complex, high-dimensional observations, but the\nunderlying (''latent'') dynamics are comparatively simple. However, outside of\nrestrictive settings such as small latent spaces, the fundamental statistical\nrequirements and algorithmic principles for reinforcement learning under latent\ndynamics are poorly understood.\n  This paper addresses the question of reinforcement learning under\n$\\textit{general}$ latent dynamics from a statistical and algorithmic\nperspective. On the statistical side, our main negative result shows that most\nwell-studied settings for reinforcement learning with function approximation\nbecome intractable when composed with rich observations; we complement this\nwith a positive result, identifying latent pushforward coverability as a\ngeneral condition that enables statistical tractability. Algorithmically, we\ndevelop provably efficient observable-to-latent reductions -- that is,\nreductions that transform an arbitrary algorithm for the latent MDP into an\nalgorithm that can operate on rich observations -- in two settings: one where\nthe agent has access to hindsight observations of the latent dynamics [LADZ23],\nand one where the agent can estimate self-predictive latent models [SAGHCB20].\nTogether, our results serve as a first step toward a unified statistical and\nalgorithmic theory for reinforcement learning under latent dynamics.\n","authors":["Philip Amortila","Dylan J. Foster","Nan Jiang","Akshay Krishnamurthy","Zakaria Mhammedi"],"pdf_url":"https://arxiv.org/pdf/2410.17904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14703v2","updated":"2024-10-23T14:01:14Z","published":"2024-06-20T19:50:56Z","title":"Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics","summary":"  Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.\n","authors":["Seungbeen Lee","Seungwon Lim","Seungju Han","Giyeong Oh","Hyungjoo Chae","Jiwan Chung","Minju Kim","Beong-woo Kwak","Yeonsoo Lee","Dongha Lee","Jinyoung Yeo","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.14703v2.pdf","comment":"Preprint; Under review"},{"id":"http://arxiv.org/abs/2410.17885v1","updated":"2024-10-23T13:58:39Z","published":"2024-10-23T13:58:39Z","title":"R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric\n  Reasoning in Large Multimodal Models","summary":"  Existing Large Multimodal Models (LMMs) struggle with mathematical geometric\nreasoning due to a lack of high-quality image-text paired data. Current\ngeometric data generation approaches, which apply preset templates to generate\ngeometric data or use Large Language Models (LLMs) to rephrase questions and\nanswers (Q&A), unavoidably limit data accuracy and diversity. To synthesize\nhigher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT)\ngeometry problem generation pipeline. First, we introduce GeoChain to produce\nhigh-fidelity geometric images and corresponding descriptions highlighting\nrelations among geometric elements. We then design a Reverse A&Q method that\nreasons step-by-step based on the descriptions and generates questions in\nreverse from the reasoning results. Experiments demonstrate that the proposed\nmethod brings significant and consistent improvements on multiple LMM\nbaselines, achieving new performance records in the 2B, 7B, and 8B settings.\nNotably, R-CoT-8B significantly outperforms previous state-of-the-art\nopen-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while\nalso surpassing the closed-source model GPT-4o by an average of 13% across both\ndatasets. The code is available at https://github.com/dle666/R-CoT.\n","authors":["Linger Deng","Yuliang Liu","Bohan Li","Dongliang Luo","Liang Wu","Chengquan Zhang","Pengyuan Lyu","Ziyang Zhang","Gang Zhang","Errui Ding","Yingying Zhu","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2410.17885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17883v1","updated":"2024-10-23T13:57:00Z","published":"2024-10-23T13:57:00Z","title":"Lightweight Neural App Control","summary":"  This paper introduces a novel mobile phone control architecture, termed ``app\nagents\", for efficient interactions and controls across various Android apps.\nThe proposed Lightweight Multi-modal App Control (LiMAC) takes as input a\ntextual goal and a sequence of past mobile observations, such as screenshots\nand corresponding UI trees, to generate precise actions. To address the\ncomputational constraints inherent to smartphones, within LiMAC, we introduce a\nsmall Action Transformer (AcT) integrated with a fine-tuned vision-language\nmodel (VLM) for real-time decision-making and task execution. We evaluate LiMAC\non two open-source mobile control datasets, demonstrating the superior\nperformance of our small-form-factor approach against fine-tuned versions of\nopen-source VLMs, such as Florence2 and Qwen2-VL. It also significantly\noutperforms prompt engineering baselines utilising closed-source foundation\nmodels like GPT-4o. More specifically, LiMAC increases the overall action\naccuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to\nprompt-engineering baselines.\n","authors":["Filippos Christianos","Georgios Papoudakis","Thomas Coste","Jianye Hao","Jun Wang","Kun Shao"],"pdf_url":"https://arxiv.org/pdf/2410.17883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17875v1","updated":"2024-10-23T13:47:05Z","published":"2024-10-23T13:47:05Z","title":"Understanding Layer Significance in LLM Alignment","summary":"  Aligning large language models (LLMs) through fine-tuning is essential for\ntailoring them to specific applications. Therefore, understanding what LLMs\nlearn during the alignment process is crucial. Recent studies suggest that\nalignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To delve deeper into LLM alignment, we propose to\nidentify which layers within LLMs are most critical to the alignment process,\nthereby uncovering how alignment influences model behavior at a granular level.\nWe propose a novel approach to identify the important layers for LLM alignment\n(ILA). It involves learning a binary mask for each incremental weight matrix in\nthe LoRA algorithm, indicating the significance of each layer. ILA consistently\nidentifies important layers across various alignment datasets, with nearly 90%\noverlap even with substantial dataset differences, highlighting fundamental\npatterns in LLM alignment. Experimental results indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss.\n","authors":["Guangyuan Shi","Zexin Lu","Xiaoyu Dong","Wenlong Zhang","Xuanyu Zhang","Yujie Feng","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03489v2","updated":"2024-10-23T13:38:29Z","published":"2024-10-04T14:59:39Z","title":"Gradient-based Jailbreak Images for Multimodal Fusion Models","summary":"  Augmenting language models with image inputs may enable more effective\njailbreak attacks through continuous optimization, unlike text inputs that\nrequire discrete optimization. However, new multimodal fusion models tokenize\nall input modalities using non-differentiable functions, which hinders\nstraightforward attacks. In this work, we introduce the notion of a tokenizer\nshortcut that approximates tokenization with a continuous function and enables\ncontinuous optimization. We use tokenizer shortcuts to create the first\nend-to-end gradient image attacks against multimodal fusion models. We evaluate\nour attacks on Chameleon models and obtain jailbreak images that elicit harmful\ninformation for 72.5% of prompts. Jailbreak images outperform text jailbreaks\noptimized with the same objective and require 3x lower compute budget to\noptimize 50x more input tokens. Finally, we find that representation\nengineering defenses, like Circuit Breakers, trained only on text attacks can\neffectively transfer to adversarial image inputs.\n","authors":["Javier Rando","Hannah Korevaar","Erik Brinkman","Ivan Evtimov","Florian Tramèr"],"pdf_url":"https://arxiv.org/pdf/2410.03489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04953v3","updated":"2024-10-23T13:36:37Z","published":"2022-12-09T16:03:34Z","title":"TargetCall: Eliminating the Wasted Computation in Basecalling via\n  Pre-Basecalling Filtering","summary":"  Basecalling is an essential step in nanopore sequencing analysis where the\nraw signals of nanopore sequencers are converted into nucleotide sequences,\ni.e., reads. State-of-the-art basecallers employ complex deep learning models\nto achieve high basecalling accuracy. This makes basecalling computationally\ninefficient and memory-hungry, bottlenecking the entire genome analysis\npipeline. However, for many applications, the majority of reads do no match the\nreference genome of interest (i.e., target reference) and thus are discarded in\nlater steps in the genomics pipeline, wasting the basecalling computation. To\novercome this issue, we propose TargetCall, the first pre-basecalling filter to\neliminate the wasted computation in basecalling. TargetCall's key idea is to\ndiscard reads that will not match the target reference (i.e., off-target reads)\nprior to basecalling. TargetCall consists of two main components: (1)\nLightCall, a lightweight neural network basecaller that produces noisy reads;\nand (2) Similarity Check, which labels each of these noisy reads as on-target\nor off-target by matching them to the target reference. Our thorough\nexperimental evaluations show that TargetCall 1) improves the end-to-end\nbasecalling runtime performance of the state-of-the-art basecaller by 3.31x\nwhile maintaining high (98.88%) recall in keeping on-target reads, 2) maintains\nhigh accuracy in downstream analysis, and 3) achieves better runtime\nperformance, throughput, recall, precision, and generality compared to prior\nworks. TargetCall is available at https://github.com/CMU-SAFARI/TargetCall.\n","authors":["Meryem Banu Cavlak","Gagandeep Singh","Mohammed Alser","Can Firtina","Joël Lindegger","Mohammad Sadrosadati","Nika Mansouri Ghiasi","Can Alkan","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2212.04953v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17859v1","updated":"2024-10-23T13:30:02Z","published":"2024-10-23T13:30:02Z","title":"DataTales: A Benchmark for Real-World Intelligent Data Narration","summary":"  We introduce DataTales, a novel benchmark designed to assess the proficiency\nof language models in data narration, a task crucial for transforming complex\ntabular data into accessible narratives. Existing benchmarks often fall short\nin capturing the requisite analytical complexity for practical applications.\nDataTales addresses this gap by offering 4.9k financial reports paired with\ncorresponding market data, showcasing the demand for models to create clear\nnarratives and analyze large datasets while understanding specialized\nterminology in the field. Our findings highlights the significant challenge\nthat language models face in achieving the necessary precision and analytical\ndepth for proficient data narration, suggesting promising avenues for future\nmodel development and evaluation methodologies.\n","authors":["Yajing Yang","Qian Liu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2410.17859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17856v1","updated":"2024-10-23T13:26:59Z","published":"2024-10-23T13:26:59Z","title":"ROCKET-1: Master Open-World Interaction with Visual-Temporal Context\n  Prompting","summary":"  Vision-language models (VLMs) have excelled in multimodal tasks, but adapting\nthem to embodied decision-making in open-world environments presents\nchallenges. A key issue is the difficulty in smoothly connecting individual\nentities in low-level observations with abstract concepts required for\nplanning. A common approach to address this problem is through the use of\nhierarchical agents, where VLMs serve as high-level reasoners that break down\ntasks into executable sub-tasks, typically specified using language and\nimagined observations. However, language often fails to effectively convey\nspatial information, while generating future images with sufficient accuracy\nremains challenging. To address these limitations, we propose visual-temporal\ncontext prompting, a novel communication protocol between VLMs and policy\nmodels. This protocol leverages object segmentation from both past and present\nobservations to guide policy-environment interactions. Using this approach, we\ntrain ROCKET-1, a low-level policy that predicts actions based on concatenated\nvisual observations and segmentation masks, with real-time object tracking\nprovided by SAM-2. Our method unlocks the full potential of VLMs\nvisual-language reasoning abilities, enabling them to solve complex creative\ntasks, especially those heavily reliant on spatial understanding. Experiments\nin Minecraft demonstrate that our approach allows agents to accomplish\npreviously unattainable tasks, highlighting the effectiveness of\nvisual-temporal context prompting in embodied decision-making. Codes and demos\nwill be available on the project page: https://craftjarvis.github.io/ROCKET-1.\n","authors":["Shaofei Cai","Zihao Wang","Kewei Lian","Zhancun Mu","Xiaojian Ma","Anji Liu","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2410.17856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17855v1","updated":"2024-10-23T13:26:19Z","published":"2024-10-23T13:26:19Z","title":"TAGE: Trustworthy Attribute Group Editing for Stable Few-shot Image\n  Generation","summary":"  Generative Adversarial Networks (GANs) have emerged as a prominent research\nfocus for image editing tasks, leveraging the powerful image generation\ncapabilities of the GAN framework to produce remarkable results.However,\nprevailing approaches are contingent upon extensive training datasets and\nexplicit supervision, presenting a significant challenge in manipulating the\ndiverse attributes of new image classes with limited sample availability. To\nsurmount this hurdle, we introduce TAGE, an innovative image generation network\ncomprising three integral modules: the Codebook Learning Module (CLM), the Code\nPrediction Module (CPM) and the Prompt-driven Semantic Module (PSM). The CPM\nmodule delves into the semantic dimensions of category-agnostic attributes,\nencapsulating them within a discrete codebook. This module is predicated on the\nconcept that images are assemblages of attributes, and thus, by editing these\ncategory-independent attributes, it is theoretically possible to generate\nimages from unseen categories. Subsequently, the CPM module facilitates\nnaturalistic image editing by predicting indices of category-independent\nattribute vectors within the codebook. Additionally, the PSM module generates\nsemantic cues that are seamlessly integrated into the Transformer architecture\nof the CPM, enhancing the model's comprehension of the targeted attributes for\nediting. With these semantic cues, the model can generate images that\naccentuate desired attributes more prominently while maintaining the integrity\nof the original category, even with a limited number of samples. We have\nconducted extensive experiments utilizing the Animal Faces, Flowers, and\nVGGFaces datasets. The results of these experiments demonstrate that our\nproposed method not only achieves superior performance but also exhibits a high\ndegree of stability when compared to other few-shot image generation\ntechniques.\n","authors":["Ruicheng Zhang","Guoheng Huang","Yejing Huo","Xiaochen Yuan","Zhizhen Zhou","Xuhang Chen","Guo Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.17855v1.pdf","comment":"Accepted by International Conference on Signal Processing Systems\n  Conference"},{"id":"http://arxiv.org/abs/2410.17851v1","updated":"2024-10-23T13:20:42Z","published":"2024-10-23T13:20:42Z","title":"The Probabilistic Tsetlin Machine: A Novel Approach to Uncertainty\n  Quantification","summary":"  Tsetlin Machines (TMs) have emerged as a compelling alternative to\nconventional deep learning methods, offering notable advantages such as smaller\nmemory footprint, faster inference, fault-tolerant properties, and\ninterpretability. Although various adaptations of TMs have expanded their\napplicability across diverse domains, a fundamental gap remains in\nunderstanding how TMs quantify uncertainty in their predictions. In response,\nthis paper introduces the Probabilistic Tsetlin Machine (PTM) framework, aimed\nat providing a robust, reliable, and interpretable approach for uncertainty\nquantification. Unlike the original TM, the PTM learns the probability of\nstaying on each state of each Tsetlin Automaton (TA) across all clauses. These\nprobabilities are updated using the feedback tables that are part of the TM\nframework: Type I and Type II feedback. During inference, TAs decide their\nactions by sampling states based on learned probability distributions, akin to\nBayesian neural networks when generating weight values. In our experimental\nanalysis, we first illustrate the spread of the probabilities across TA states\nfor the noisy-XOR dataset. Then we evaluate the PTM alongside benchmark models\nusing both simulated and real-world datasets. The experiments on the simulated\ndataset reveal the PTM's effectiveness in uncertainty quantification,\nparticularly in delineating decision boundaries and identifying regions of high\nuncertainty. Moreover, when applied to multiclass classification tasks using\nthe Iris dataset, the PTM demonstrates competitive performance in terms of\npredictive entropy and expected calibration error, showcasing its potential as\na reliable tool for uncertainty estimation. Our findings underscore the\nimportance of selecting appropriate models for accurate uncertainty\nquantification in predictive tasks, with the PTM offering a particularly\ninterpretable and effective solution.\n","authors":["K. Darshana Abeyrathna","Sara El Mekkaoui","Andreas Hafver","Christian Agrell"],"pdf_url":"https://arxiv.org/pdf/2410.17851v1.pdf","comment":"12 pages, 5 figures, 6 tables, accepted and presented at ICAAI 2024,\n  London"},{"id":"http://arxiv.org/abs/2402.15055v2","updated":"2024-10-23T13:20:15Z","published":"2024-02-23T02:15:47Z","title":"Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions","summary":"  Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.\n","authors":["Clement Neo","Shay B. Cohen","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.15055v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.15956v2","updated":"2024-10-23T13:00:27Z","published":"2024-10-21T12:34:17Z","title":"Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs","summary":"  Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.\n","authors":["Yanzhu Guo","Simone Conia","Zelin Zhou","Min Li","Saloni Potdar","Henry Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.15956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12376v2","updated":"2024-10-23T12:58:14Z","published":"2024-10-16T08:48:27Z","title":"ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated\n  Shapefile Processing","summary":"  Vector data is one of the two core data structures in geographic information\nscience (GIS), essential for accurately storing and representing geospatial\ninformation. Shapefile, the most widely used vector data format, has become the\nindustry standard supported by all major geographic information systems.\nHowever, processing this data typically requires specialized GIS knowledge and\nskills, creating a barrier for researchers from other fields and impeding\ninterdisciplinary research in spatial data analysis. Moreover, while large\nlanguage models (LLMs) have made significant advancements in natural language\nprocessing and task automation, they still face challenges in handling the\ncomplex spatial and topological relationships inherent in GIS vector data. To\naddress these challenges, we propose ShapefileGPT, an innovative framework\npowered by LLMs, specifically designed to automate Shapefile tasks.\nShapefileGPT utilizes a multi-agent architecture, in which the planner agent is\nresponsible for task decomposition and supervision, while the worker agent\nexecutes the tasks. We developed a specialized function library for handling\nShapefiles and provided comprehensive API documentation, enabling the worker\nagent to operate Shapefiles efficiently through function calling. For\nevaluation, we developed a benchmark dataset based on authoritative textbooks,\nencompassing tasks in categories such as geometric operations and spatial\nqueries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the\nGPT series models. In comparison to traditional LLMs, ShapefileGPT effectively\nhandles complex vector data analysis tasks, overcoming the limitations of\ntraditional LLMs in spatial analysis. This breakthrough opens new pathways for\nadvancing automation and intelligence in the GIS field, with significant\npotential in interdisciplinary data analysis and application contexts.\n","authors":["Qingming Lin","Rui Hu","Huaxia Li","Sensen Wu","Yadong Li","Kai Fang","Hailin Feng","Zhenhong Du","Liuchang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.12376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13152v3","updated":"2024-10-23T12:56:05Z","published":"2024-05-21T18:45:18Z","title":"Enhancing Interaction Modeling with Agent Selection and Physical\n  Coefficient for Trajectory Prediction","summary":"  A thorough understanding of the interaction between the target agent and\nsurrounding agents is a prerequisite for accurate trajectory prediction.\nAlthough many methods have been explored, they all assign correlation\ncoefficients to surrounding agents in a purely learning-based manner. In this\nstudy, we present ASPILin, which manually selects interacting agents and\ncalculates their correlations instead of attention scores. Surprisingly, these\nsimple modifications can significantly improve prediction performance and\nsubstantially reduce computational costs. Additionally, ASPILin models the\ninteracting agents at each past time step separately, rather than only modeling\nthe interacting agents at the current time step. This clarifies the causal\nchain of the target agent's historical trajectory and helps the model better\nunderstand dynamic interactions. We intentionally simplified our model in other\naspects, such as map encoding. Remarkably, experiments conducted on the\nINTERACTION, highD, and CitySim datasets demonstrate that our method is\nefficient and straightforward, outperforming other state-of-the-art methods.\n","authors":["Shiji Huang","Lei Ye","Min Chen","Wenhai Luo","Dihong Wang","Chenqi Xu","Deyuan Liang"],"pdf_url":"https://arxiv.org/pdf/2405.13152v3.pdf","comment":"code:https://github.com/kkk00714/ASPILin"},{"id":"http://arxiv.org/abs/2410.17827v1","updated":"2024-10-23T12:40:33Z","published":"2024-10-23T12:40:33Z","title":"RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models\n  for Multi-label Chest X-ray Classification","summary":"  In this paper we introduce RE-tune, a novel approach for fine-tuning\npre-trained Multimodal Biomedical Vision-Language models (VLMs) in Incremental\nLearning scenarios for multi-label chest disease diagnosis. RE-tune freezes the\nbackbones and only trains simple adaptors on top of the Image and Text encoders\nof the VLM. By engineering positive and negative text prompts for diseases, we\nleverage the ability of Large Language Models to steer the training trajectory.\nWe evaluate RE-tune in three realistic incremental learning scenarios:\nclass-incremental, label-incremental, and data-incremental. Our results\ndemonstrate that Biomedical VLMs are natural continual learners and prevent\ncatastrophic forgetting. RE-tune not only achieves accurate multi-label\nclassification results, but also prioritizes patient privacy and it\ndistinguishes itself through exceptional computational efficiency, rendering it\nhighly suitable for broad adoption in real-world healthcare settings.\n","authors":["Marco Mistretta","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2410.17827v1.pdf","comment":"Accepted for publication at Medical Imaging meets NeurIPS (NeurIPS23)"},{"id":"http://arxiv.org/abs/2404.01335v2","updated":"2024-10-23T12:38:40Z","published":"2024-03-30T13:25:11Z","title":"Generative AI Models for Different Steps in Architectural Design: A\n  Literature Review","summary":"  Recent advances in generative artificial intelligence (AI) technologies have\nbeen significantly driven by models such as generative adversarial networks\n(GANs), variational autoencoders (VAEs), and denoising diffusion probabilistic\nmodels (DDPMs). Although architects recognize the potential of generative AI in\ndesign, personal barriers often restrict their access to the latest\ntechnological developments, thereby causing the application of generative AI in\narchitectural design to lag behind. Therefore, it is essential to comprehend\nthe principles and advancements of generative AI models and analyze their\nrelevance in architecture applications. This paper first provides an overview\nof generative AI technologies, with a focus on probabilistic diffusion models\n(DDPMs), 3D generative models, and foundation models, highlighting their recent\ndevelopments and main application scenarios. Then, the paper explains how the\nabovementioned models could be utilized in architecture. We subdivide the\narchitectural design process into six steps and review related research\nprojects in each step from 2020 to the present. Lastly, this paper discusses\npotential future directions for applying generative AI in the architectural\ndesign steps. This research can help architects quickly understand the\ndevelopment and latest progress of generative AI and contribute to the further\ndevelopment of intelligent architecture.\n","authors":["Chengyuan Li","Tianyu Zhang","Xusheng Du","Ye Zhang","Haoran Xie"],"pdf_url":"https://arxiv.org/pdf/2404.01335v2.pdf","comment":"34 pages, 14 figures, accepted by Frontiers of Architectural Research"},{"id":"http://arxiv.org/abs/2409.10568v2","updated":"2024-10-23T12:37:10Z","published":"2024-09-14T04:17:24Z","title":"On the limits of agency in agent-based models","summary":"  Agent-based modeling (ABM) seeks to understand the behavior of complex\nsystems by simulating a collection of agents that act and interact within an\nenvironment. Their practical utility requires capturing realistic environment\ndynamics and adaptive agent behavior while efficiently simulating million-size\npopulations. Recent advancements in large language models (LLMs) present an\nopportunity to enhance ABMs by using LLMs as agents with further potential to\ncapture adaptive behavior. However, the computational infeasibility of using\nLLMs for large populations has hindered their widespread adoption. In this\npaper, we introduce AgentTorch -- a framework that scales ABMs to millions of\nagents while capturing high-resolution agent behavior using LLMs. We benchmark\nthe utility of LLMs as ABM agents, exploring the trade-off between simulation\nscale and individual agency. Using the COVID-19 pandemic as a case study, we\ndemonstrate how AgentTorch can simulate 8.4 million agents representing New\nYork City, capturing the impact of isolation and employment behavior on health\nand economic outcomes. We compare the performance of different agent\narchitectures based on heuristic and LLM agents in predicting disease waves and\nunemployment rates. Furthermore, we showcase AgentTorch's capabilities for\nretrospective, counterfactual, and prospective analyses, highlighting how\nadaptive agent behavior can help overcome the limitations of historical data in\npolicy design. AgentTorch is an open-source project actively being used for\npolicy-making and scientific discovery around the world. The framework is\navailable here: github.com/AgentTorch/AgentTorch.\n","authors":["Ayush Chopra","Shashank Kumar","Nurullah Giray-Kuru","Ramesh Raskar","Arnau Quera-Bofarull"],"pdf_url":"https://arxiv.org/pdf/2409.10568v2.pdf","comment":"19 pages, 5 appendices, 5 figures"},{"id":"http://arxiv.org/abs/2410.17812v1","updated":"2024-10-23T12:17:03Z","published":"2024-10-23T12:17:03Z","title":"PGDiffSeg: Prior-Guided Denoising Diffusion Model with Parameter-Shared\n  Attention for Breast Cancer Segmentation","summary":"  Early detection through imaging and accurate diagnosis is crucial in\nmitigating the high mortality rate associated with breast cancer. However,\nlocating tumors from low-resolution and high-noise medical images is extremely\nchallenging. Therefore, this paper proposes a novel PGDiffSeg (Prior-Guided\nDiffusion Denoising Model with Parameter-Shared Attention) that applies\ndiffusion denoising methods to breast cancer medical image segmentation,\naccurately recovering the affected areas from Gaussian noise. Firstly, we\ndesign a parallel pipeline for noise processing and semantic information\nprocessing and propose a parameter-shared attention module (PSA) in multi-layer\nthat seamlessly integrates these two pipelines. This integration empowers\nPGDiffSeg to incorporate semantic details at multiple levels during the\ndenoising process, producing highly accurate segmentation maps. Secondly, we\nintroduce a guided strategy that leverages prior knowledge to simulate the\ndecision-making process of medical professionals, thereby enhancing the model's\nability to locate tumor positions precisely. Finally, we provide the first-ever\ndiscussion on the interpretability of the generative diffusion model in the\ncontext of breast cancer segmentation. Extensive experiments have demonstrated\nthe superiority of our model over the current state-of-the-art approaches,\nconfirming its effectiveness as a flexible diffusion denoising method suitable\nfor medical image research. Our code will be publicly available later.\n","authors":["Feiyan Feng","Tianyu Liu","Hong Wang","Jun Zhao","Wei Li","Yanshen Sun"],"pdf_url":"https://arxiv.org/pdf/2410.17812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17799v1","updated":"2024-10-23T11:58:58Z","published":"2024-10-23T11:58:58Z","title":"OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation","summary":"  Full-duplex spoken dialogue systems significantly advance over traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex communication\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text-based large language model (LLM) backbone into a speech-text\ndialogue LLM, capable of generating text and speech in real time, without\nmodifying the architecture of the backbone LLM. The training process comprises\nthree stages: modality alignment, half-duplex dialogue learning, and\nfull-duplex dialogue learning. Throughout all training stages, we standardize\nthe data using a flattening operation, which allows us to unify the training\nmethods and the model architecture across different modalities and tasks. Our\napproach offers a straightforward modeling technique and a promising research\ndirection for developing efficient and natural end-to-end full-duplex spoken\ndialogue systems. Audio samples of dialogues generated by OmniFlatten can be\nfound at this web site (https://omniflatten.github.io/).\n","authors":["Qinglin Zhang","Luyao Cheng","Chong Deng","Qian Chen","Wen Wang","Siqi Zheng","Jiaqing Liu","Hai Yu","Chaohong Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17799v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.17792v1","updated":"2024-10-23T11:47:04Z","published":"2024-10-23T11:47:04Z","title":"Enhancing Federated Learning Convergence with Dynamic Data Queue and\n  Data Entropy-driven Participant Selection","summary":"  Federated Learning (FL) is a decentralized approach for collaborative model\ntraining on edge devices. This distributed method of model training offers\nadvantages in privacy, security, regulatory compliance, and cost-efficiency.\nOur emphasis in this research lies in addressing statistical complexity in FL,\nespecially when the data stored locally across devices is not identically and\nindependently distributed (non-IID). We have observed an accuracy reduction of\nup to approximately 10\\% to 30\\%, particularly in skewed scenarios where each\nedge device trains with only 1 class of data. This reduction is attributed to\nweight divergence, quantified using the Euclidean distance between device-level\nclass distributions and the population distribution, resulting in a bias term\n(\\(\\delta_k\\)). As a solution, we present a method to improve convergence in FL\nby creating a global subset of data on the server and dynamically distributing\nit across devices using a Dynamic Data queue-driven Federated Learning (DDFL).\nNext, we leverage Data Entropy metrics to observe the process during each\ntraining round and enable reasonable device selection for aggregation.\nFurthermore, we provide a convergence analysis of our proposed DDFL to justify\ntheir viability in practical FL scenarios, aiming for better device selection,\na non-sub-optimal global model, and faster convergence. We observe that our\napproach results in a substantial accuracy boost of approximately 5\\% for the\nMNIST dataset, around 18\\% for CIFAR-10, and 20\\% for CIFAR-100 with a 10\\%\nglobal subset of data, outperforming the state-of-the-art (SOTA) aggregation\nalgorithms.\n","authors":["Charuka Herath","Xiaolan Liu","Sangarapillai Lambotharan","Yogachandran Rahulamathavan"],"pdf_url":"https://arxiv.org/pdf/2410.17792v1.pdf","comment":"The Journal is submitted to IEEE Transactions in the Internet of\n  Things"},{"id":"http://arxiv.org/abs/2410.17787v1","updated":"2024-10-23T11:37:20Z","published":"2024-10-23T11:37:20Z","title":"Large Language Models Engineer Too Many Simple Features For Tabular Data","summary":"  Tabular machine learning problems often require time-consuming and\nlabor-intensive feature engineering. Recent efforts have focused on using large\nlanguage models (LLMs) to capitalize on their potential domain knowledge. At\nthe same time, researchers have observed ethically concerning negative biases\nin other LLM-related use cases, such as text generation. These developments\nmotivated us to investigate whether LLMs exhibit a bias that negatively impacts\nthe performance of feature engineering. While not ethically concerning, such a\nbias could hinder practitioners from fully utilizing LLMs for automated data\nscience. Therefore, we propose a method to detect potential biases by detecting\nanomalies in the frequency of operators (e.g., adding two features) suggested\nby LLMs when engineering new features. Our experiments evaluate the bias of\nfour LLMs, two big frontier and two small open-source models, across 27 tabular\ndatasets. Our results indicate that LLMs are biased toward simple operators,\nsuch as addition, and can fail to utilize more complex operators, such as\ngrouping followed by aggregations. Furthermore, the bias can negatively impact\nthe predictive performance when using LLM-generated features. Our results call\nfor mitigating bias when using LLMs for feature engineering.\n","authors":["Jaris Küken","Lennart Purucker","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2410.17787v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.05804v4","updated":"2024-10-23T11:36:57Z","published":"2024-06-09T14:42:55Z","title":"A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning","summary":"  Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work.\n","authors":["Xinzhe Li"],"pdf_url":"https://arxiv.org/pdf/2406.05804v4.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.17784v1","updated":"2024-10-23T11:34:29Z","published":"2024-10-23T11:34:29Z","title":"Holon Programming Model -- A Software-Defined Approach for System of\n  Systems","summary":"  As Systems of Systems evolve into increasingly complex networks, harnessing\ntheir collective potential becomes paramount. Traditional SoS engineering\napproaches lack the necessary programmability to develop third party SoS level\nbehaviors. To address this challenge, we propose a software defined approach to\nenable flexible and adaptive programming of SoS. We introduce the Holon\nProgramming Model, a software-defined framework designed to meet these needs.\nThe Holon Programming Model empowers developers to design and orchestrate\ncomplex system behaviors effectively, as illustrated in our disaster management\nscenario. This research outlines the Holon Programming Model theoretical\nunderpinnings and practical applications, with the aim of driving further\nexploration and advancement in the field of software defined SoS\n","authors":["Muhammad Ashfaq","Ahmed R. Sadik","Tommi Mikkonen","Muhammad Waseem","Niko Makitalo"],"pdf_url":"https://arxiv.org/pdf/2410.17784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17781v1","updated":"2024-10-23T11:31:52Z","published":"2024-10-23T11:31:52Z","title":"Evaluating Explanations Through LLMs: Beyond Traditional User Studies","summary":"  As AI becomes fundamental in sectors like healthcare, explainable AI (XAI)\ntools are essential for trust and transparency. However, traditional user\nstudies used to evaluate these tools are often costly, time consuming, and\ndifficult to scale. In this paper, we explore the use of Large Language Models\n(LLMs) to replicate human participants to help streamline XAI evaluation. We\nreproduce a user study comparing counterfactual and causal explanations,\nreplicating human participants with seven LLMs under various settings. Our\nresults show that (i) LLMs can replicate most conclusions from the original\nstudy, (ii) different LLMs yield varying levels of alignment in the results,\nand (iii) experimental factors such as LLM memory and output variability affect\nalignment with human responses. These initial findings suggest that LLMs could\nprovide a scalable and cost-effective way to simplify qualitative XAI\nevaluation.\n","authors":["Francesco Bombassei De Bona","Gabriele Dominici","Tim Miller","Marc Langheinrich","Martin Gjoreski"],"pdf_url":"https://arxiv.org/pdf/2410.17781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17772v1","updated":"2024-10-23T11:19:48Z","published":"2024-10-23T11:19:48Z","title":"Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation\n  Models","summary":"  A central challenge towards developing robots that can relate human language\nto their perception and actions is the scarcity of natural language annotations\nin diverse robot datasets. Moreover, robot policies that follow natural\nlanguage instructions are typically trained on either templated language or\nexpensive human-labeled instructions, hindering their scalability. To this end,\nwe introduce NILS: Natural language Instruction Labeling for Scalability. NILS\nautomatically labels uncurated, long-horizon robot data at scale in a zero-shot\nmanner without any human intervention. NILS combines pretrained vision-language\nfoundation models in order to detect objects in a scene, detect object-centric\nchanges, segment tasks from large datasets of unlabelled interaction data and\nultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a\nkitchen play dataset show that NILS can autonomously annotate diverse robot\ndemonstrations of unlabeled and unstructured datasets while alleviating several\nshortcomings of crowdsourced human annotations, such as low data quality and\ndiversity. We use NILS to label over 115k trajectories obtained from over 430\nhours of robot data. We open-source our auto-labeling code and generated\nannotations on our website: http://robottasklabeling.github.io.\n","authors":["Nils Blank","Moritz Reuss","Marcel Rühle","Ömer Erdinç Yağmurlu","Fabian Wenzel","Oier Mees","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.17772v1.pdf","comment":"Project Website at https://robottasklabeling.github.io/"},{"id":"http://arxiv.org/abs/2404.19232v7","updated":"2024-10-23T11:19:02Z","published":"2024-04-30T03:29:30Z","title":"GRAMMAR: Grounded and Modular Methodology for Assessment of\n  Closed-Domain Retrieval-Augmented Language Model","summary":"  Retrieval-Augmented Generation (RAG) systems are widely used across various\nindustries for querying closed-domain and in-house knowledge bases. However,\nevaluating these systems presents significant challenges due to the private\nnature of closed-domain data and a scarcity of queries with verifiable ground\ntruths. Moreover, there is a lack of analytical methods to diagnose problematic\nmodules and identify types of failure, such as those caused by knowledge\ndeficits or issues with robustness. To address these challenges, we introduce\nGRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation\nframework comprising a grounded data generation process and an evaluation\nprotocol that effectively pinpoints defective modules. Our validation\nexperiments reveal that GRAMMAR provides a reliable approach for identifying\nvulnerable modules and supports hypothesis testing for textual form\nvulnerabilities. An open-source tool accompanying this framework is available\nin our GitHub repository (see https://github.com/xinzhel/grammar), allowing for\neasy reproduction of our results and enabling reliable and modular evaluation\nin closed-domain settings.\n","authors":["Xinzhe Li","Ming Liu","Shang Gao"],"pdf_url":"https://arxiv.org/pdf/2404.19232v7.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.13247v2","updated":"2024-10-23T11:09:57Z","published":"2024-10-17T06:14:34Z","title":"Collaborative AI in Sentiment Analysis: System Architecture, Data\n  Prediction and Deployment Strategies","summary":"  The advancement of large language model (LLM) based artificial intelligence\ntechnologies has been a game-changer, particularly in sentiment analysis. This\nprogress has enabled a shift from highly specialized research environments to\npractical, widespread applications within the industry. However, integrating\ndiverse AI models for processing complex multimodal data and the associated\nhigh costs of feature extraction presents significant challenges. Motivated by\nthe marketing oriented software development +needs, our study introduces a\ncollaborative AI framework designed to efficiently distribute and resolve tasks\nacross various AI systems to address these issues. Initially, we elucidate the\nkey solutions derived from our development process, highlighting the role of\ngenerative AI models like \\emph{chatgpt}, \\emph{google gemini} in simplifying\nintricate sentiment analysis tasks into manageable, phased objectives.\nFurthermore, we present a detailed case study utilizing our collaborative AI\nsystem in edge and cloud, showcasing its effectiveness in analyzing sentiments\nacross diverse online media channels.\n","authors":["Chaofeng Zhang","Jia Hou","Xueting Tan","Gaolei Li","Caijuan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.13247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17764v1","updated":"2024-10-23T11:02:59Z","published":"2024-10-23T11:02:59Z","title":"Beyond Backpropagation: Optimization with Multi-Tangent Forward\n  Gradients","summary":"  The gradients used to train neural networks are typically computed using\nbackpropagation. While an efficient way to obtain exact gradients,\nbackpropagation is computationally expensive, hinders parallelization, and is\nbiologically implausible. Forward gradients are an approach to approximate the\ngradients from directional derivatives along random tangents computed by\nforward-mode automatic differentiation. So far, research has focused on using a\nsingle tangent per step. This paper provides an in-depth analysis of\nmulti-tangent forward gradients and introduces an improved approach to\ncombining the forward gradients from multiple tangents based on orthogonal\nprojections. We demonstrate that increasing the number of tangents improves\nboth approximation quality and optimization performance across various tasks.\n","authors":["Katharina Flügel","Daniel Coquelin","Marie Weiel","Achim Streit","Markus Götz"],"pdf_url":"https://arxiv.org/pdf/2410.17764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12676v2","updated":"2024-10-23T11:01:45Z","published":"2023-12-20T00:31:43Z","title":"Bayesian Analysis of Combinatorial Gaussian Process Bandits","summary":"  We consider the combinatorial volatile Gaussian process (GP) semi-bandit\nproblem. Each round, an agent is provided a set of available base arms and must\nselect a subset of them to maximize the long-term cumulative reward. We study\nthe Bayesian setting and provide novel Bayesian cumulative regret bounds for\nthree GP-based algorithms: GP-UCB, GP-BayesUCB and GP-TS. Our bounds extend\nprevious results for GP-UCB and GP-TS to the infinite, volatile and\ncombinatorial setting, and to the best of our knowledge, we provide the first\nregret bound for GP-BayesUCB. Volatile arms encompass other widely considered\nbandit problems such as contextual bandits. Furthermore, we employ our\nframework to address the challenging real-world problem of online\nenergy-efficient navigation, where we demonstrate its effectiveness compared to\nthe alternatives.\n","authors":["Jack Sandberg","Niklas Åkerblom","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2312.12676v2.pdf","comment":"32 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.17758v1","updated":"2024-10-23T10:50:07Z","published":"2024-10-23T10:50:07Z","title":"Escaping the Forest: Sparse Interpretable Neural Networks for Tabular\n  Data","summary":"  Tabular datasets are widely used in scientific disciplines such as biology.\nWhile these disciplines have already adopted AI methods to enhance their\nfindings and analysis, they mainly use tree-based methods due to their\ninterpretability. At the same time, artificial neural networks have been shown\nto offer superior flexibility and depth for rich and complex non-tabular\nproblems, but they are falling behind tree-based models for tabular data in\nterms of performance and interpretability. Although sparsity has been shown to\nimprove the interpretability and performance of ANN models for complex\nnon-tabular datasets, enforcing sparsity structurally and formatively for\ntabular data before training the model, remains an open question. To address\nthis question, we establish a method that infuses sparsity in neural networks\nby utilising attention mechanisms to capture the features' importance in\ntabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with\nattention mechanisms, are more effective than tree-based models, reaching the\nstate-of-the-art on biological datasets. They further permit the extraction of\ninsights from these datasets and achieve better performance than post-hoc\nmethods like SHAP.\n","authors":["Salvatore Raieli","Abdulrahman Altahhan","Nathalie Jeanray","Stéphane Gerart","Sebastien Vachenc"],"pdf_url":"https://arxiv.org/pdf/2410.17758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17751v1","updated":"2024-10-23T10:28:17Z","published":"2024-10-23T10:28:17Z","title":"VISAGE: Video Synthesis using Action Graphs for Surgery","summary":"  Surgical data science (SDS) is a field that analyzes patient data before,\nduring, and after surgery to improve surgical outcomes and skills. However,\nsurgical data is scarce, heterogeneous, and complex, which limits the\napplicability of existing machine learning methods. In this work, we introduce\nthe novel task of future video generation in laparoscopic surgery. This task\ncan augment and enrich the existing surgical data and enable various\napplications, such as simulation, analysis, and robot-aided surgery.\nUltimately, it involves not only understanding the current state of the\noperation but also accurately predicting the dynamic and often unpredictable\nnature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis\nusing Action Graphs for Surgery), leverages the power of action scene graphs to\ncapture the sequential nature of laparoscopic procedures and utilizes diffusion\nmodels to synthesize temporally coherent video sequences. VISAGE predicts the\nfuture frames given only a single initial frame, and the action graph triplets.\nBy incorporating domain-specific knowledge through the action graph, VISAGE\nensures the generated videos adhere to the expected visual and motion patterns\nobserved in real laparoscopic procedures. The results of our experiments\ndemonstrate high-fidelity video generation for laparoscopy procedures, which\nenables various applications in SDS.\n","authors":["Yousef Yeganeh","Rachmadio Lazuardi","Amir Shamseddin","Emine Dari","Yash Thirani","Nassir Navab Azade Farshad"],"pdf_url":"https://arxiv.org/pdf/2410.17751v1.pdf","comment":"Accepted at MICCAI 2024 Embodied AI and Robotics for HealTHcare\n  (EARTH) Workshop"},{"id":"http://arxiv.org/abs/2410.17744v1","updated":"2024-10-23T10:17:13Z","published":"2024-10-23T10:17:13Z","title":"Learning Versatile Skills with Curriculum Masking","summary":"  Masked prediction has emerged as a promising pretraining paradigm in offline\nreinforcement learning (RL) due to its versatile masking schemes, enabling\nflexible inference across various downstream tasks with a unified model.\nDespite the versatility of masked prediction, it remains unclear how to balance\nthe learning of skills at different levels of complexity. To address this, we\npropose CurrMask, a curriculum masking pretraining paradigm for sequential\ndecision making. Motivated by how humans learn by organizing knowledge in a\ncurriculum, CurrMask adjusts its masking scheme during pretraining for learning\nversatile skills. Through extensive experiments, we show that CurrMask exhibits\nsuperior zero-shot performance on skill prompting tasks, goal-conditioned\nplanning tasks, and competitive finetuning performance on offline RL tasks.\nAdditionally, our analysis of training dynamics reveals that CurrMask gradually\nacquires skills of varying complexity by dynamically adjusting its masking\nscheme.\n","authors":["Yao Tang","Zhihui Xie","Zichuan Lin","Deheng Ye","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2410.17744v1.pdf","comment":"NeurIPS 2024 poster, 21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17740v1","updated":"2024-10-23T10:14:37Z","published":"2024-10-23T10:14:37Z","title":"Emotion Recognition with Facial Attention and Objective Activation\n  Functions","summary":"  In this paper, we study the effect of introducing channel and spatial\nattention mechanisms, namely SEN-Net, ECA-Net, and CBAM, to existing CNN\nvision-based models such as VGGNet, ResNet, and ResNetV2 to perform the Facial\nEmotion Recognition task. We show that not only attention can significantly\nimprove the performance of these models but also that combining them with a\ndifferent activation function can further help increase the performance of\nthese models.\n","authors":["Andrzej Miskow","Abdulrahman Altahhan"],"pdf_url":"https://arxiv.org/pdf/2410.17740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17735v1","updated":"2024-10-23T10:11:39Z","published":"2024-10-23T10:11:39Z","title":"New Insight in Cervical Cancer Diagnosis Using Convolution Neural\n  Network Architecture","summary":"  The Pap smear is a screening method for early cervical cancer diagnosis. The\nselection of the right optimizer in the convolutional neural network (CNN)\nmodel is key to the success of the CNN in image classification, including the\nclassification of cervical cancer Pap smear images. In this study, stochastic\ngradient descent (SGD), RMSprop, Adam, AdaGrad, AdaDelta, Adamax, and Nadam\noptimizers were used to classify cervical cancer Pap smear images from the\nSipakMed dataset. Resnet-18, Resnet-34, and VGG-16 are the CNN architectures\nused in this study, and each architecture uses a transfer-learning model. Based\non the test results, we conclude that the transfer learning model performs\nbetter on all CNNs and optimization techniques and that in the transfer\nlearning model, the optimization has little influence on the training of the\nmodel. Adamax, with accuracy values of 72.8% and 66.8%, had the best accuracy\nfor the VGG-16 and Resnet-18 architectures, respectively. Resnet-34 had 54.0%.\nThis is 0.034% lower than Nadam. Overall, Adamax is a suitable optimizer for\nCNN in cervical cancer classification on Resnet-18, Resnet-34, and VGG-16\narchitectures. This study provides new insights into the configuration of CNN\nmodels for Pap smear image analysis.\n","authors":["Ach. Khozaimi","Wayan Firdaus Mahmudy"],"pdf_url":"https://arxiv.org/pdf/2410.17735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03094v2","updated":"2024-10-23T10:09:10Z","published":"2024-07-03T13:34:33Z","title":"Conformal Prediction for Causal Effects of Continuous Treatments","summary":"  Uncertainty quantification of causal effects is crucial for safety-critical\napplications such as personalized medicine. A powerful approach for this is\nconformal prediction, which has several practical benefits due to\nmodel-agnostic finite-sample guarantees. Yet, existing methods for conformal\nprediction of causal effects are limited to binary/discrete treatments and make\nhighly restrictive assumptions such as known propensity scores. In this work,\nwe provide a novel conformal prediction method for potential outcomes of\ncontinuous treatments. We account for the additional uncertainty introduced\nthrough propensity estimation so that our conformal prediction intervals are\nvalid even if the propensity score is unknown. Our contributions are\nthree-fold: (1) We derive finite-sample prediction intervals for potential\noutcomes of continuous treatments. (2) We provide an algorithm for calculating\nthe derived intervals. (3) We demonstrate the effectiveness of the conformal\nprediction intervals in experiments on synthetic and real-world datasets. To\nthe best of our knowledge, we are the first to propose conformal prediction for\ncontinuous treatments when the propensity score is unknown and must be\nestimated from data.\n","authors":["Maresa Schröder","Dennis Frauen","Jonas Schweisthal","Konstantin Heß","Valentyn Melnychuk","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2407.03094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14622v2","updated":"2024-10-23T10:06:10Z","published":"2024-02-22T15:10:45Z","title":"From Keywords to Structured Summaries: Streamlining Scholarly\n  Information Access","summary":"  This paper highlights the growing importance of information retrieval (IR)\nengines in the scientific community, addressing the inefficiency of traditional\nkeyword-based search engines due to the rising volume of publications. The\nproposed solution involves structured records, underpinning advanced\ninformation technology (IT) tools, including visualization dashboards, to\nrevolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the \"reproductive number estimate of infectious diseases\"\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation information access system as\nan IR method accessible at https://orkg.org/usecases/r0-estimates.\n","authors":["Mahsa Shamsabadi","Jennifer D'Souza"],"pdf_url":"https://arxiv.org/pdf/2402.14622v2.pdf","comment":"8 pages, 3 figures | Accepted for publication as a poster paper at\n  the International Semantic Web Conference (ISWC 2024)"},{"id":"http://arxiv.org/abs/2410.17732v1","updated":"2024-10-23T10:06:08Z","published":"2024-10-23T10:06:08Z","title":"FuzzWiz -- Fuzzing Framework for Efficient Hardware Coverage","summary":"  Ever-increasing design complexity of System-on-Chips (SoCs) led to\nsignificant verification challenges. Unlike software, bugs in hardware design\nare vigorous and eternal i.e., once the hardware is fabricated, it cannot be\nrepaired with any patch. Despite being one of the powerful techniques used in\nverification, the dynamic random approach cannot give confidence to complex\nRegister Transfer Leve (RTL) designs during the pre-silicon design phase. In\nparticular, achieving coverage targets and exposing bugs is a complicated task\nwith random simulations. In this paper, we leverage an existing testing\nsolution available in the software world known as fuzzing and apply it to\nhardware verification in order to achieve coverage targets in quick time. We\ncreated an automated hardware fuzzing framework FuzzWiz using metamodeling and\nPython to achieve coverage goals faster. It includes parsing the RTL design\nmodule, converting it into C/C++ models, creating generic testbench with\nassertions, fuzzer-specific compilation, linking, and fuzzing. Furthermore, it\nis configurable and provides the debug flow if any crash is detected during the\nfuzzing process. The proposed framework is applied on four IP blocks from\nGoogle's OpenTitan chip with various fuzzing engines to show its scalability\nand compatibility. Our benchmarking results show that we could achieve around\n90% of the coverage 10 times faster than traditional simulation regression\nbased approach.\n","authors":["Deepak Narayan Gadde","Aman Kumar","Djones Lettnin","Sebastian Simon"],"pdf_url":"https://arxiv.org/pdf/2410.17732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06310v2","updated":"2024-10-23T09:59:15Z","published":"2024-08-12T17:24:19Z","title":"OWL2Vec4OA: Tailoring Knowledge Graph Embeddings for Ontology Alignment","summary":"  Ontology alignment is integral to achieving semantic interoperability as the\nnumber of available ontologies covering intersecting domains is increasing.\nThis paper proposes OWL2Vec4OA, an extension of the ontology embedding system\nOWL2Vec*. While OWL2Vec* has emerged as a powerful technique for ontology\nembedding, it currently lacks a mechanism to tailor the embedding to the\nontology alignment task. OWL2Vec4OA incorporates edge confidence values from\nseed mappings to guide the random walk strategy. We present the theoretical\nfoundations, implementation details, and experimental evaluation of our\nproposed extension, demonstrating its potential effectiveness for ontology\nalignment tasks.\n","authors":["Sevinj Teymurova","Ernesto Jiménez-Ruiz","Tillman Weyde","Jiaoyan Chen"],"pdf_url":"https://arxiv.org/pdf/2408.06310v2.pdf","comment":"Accepted to the 6th Knowledge Graph and Semantic Web Conference"},{"id":"http://arxiv.org/abs/2410.05354v3","updated":"2024-10-23T09:51:11Z","published":"2024-10-07T13:44:49Z","title":"Over-the-Air Federated Learning in Cell-Free MIMO with Long-term Power\n  Constraint","summary":"  Wireless networks supporting artificial intelligence have gained significant\nattention, with Over-the-Air Federated Learning emerging as a key application\ndue to its unique transmission and distributed computing characteristics. This\npaper derives error bounds for Over-the-Air Federated Learning in a Cell-free\nMIMO system and formulates an optimization problem to minimize optimality gap\nvia joint optimization of power control and beamforming. We introduce the\nMOP-LOFPC algorithm, which employs Lyapunov optimization to decouple long-term\nconstraints across rounds while requiring only causal channel state\ninformation. Experimental results demonstrate that MOP-LOFPC achieves a better\nand more flexible trade-off between the model's training loss and adherence to\nlong-term power constraints compared to existing baselines.\n","authors":["Yifan Wang","Cheng Zhang","Yuanndon Zhuang","Mingzeng Dai","Haiming Wang","Yongming Huang"],"pdf_url":"https://arxiv.org/pdf/2410.05354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11642v2","updated":"2024-10-23T09:43:03Z","published":"2024-10-15T14:31:54Z","title":"Improve Value Estimation of Q Function and Reshape Reward with Monte\n  Carlo Tree Search","summary":"  Reinforcement learning has achieved remarkable success in perfect information\ngames such as Go and Atari, enabling agents to compete at the highest levels\nagainst human players. However, research in reinforcement learning for\nimperfect information games has been relatively limited due to the more complex\ngame structures and randomness. Traditional methods face challenges in training\nand improving performance in imperfect information games due to issues like\ninaccurate Q value estimation and reward sparsity. In this paper, we focus on\nUno, an imperfect information game, and aim to address these problems by\nreducing Q value overestimation and reshaping reward function. We propose a\nnovel algorithm that utilizes Monte Carlo Tree Search to average the value\nestimations in Q function. Even though we choose Double Deep Q Learning as the\nfoundational framework in this paper, our method can be generalized and used in\nany algorithm which needs Q value estimation, such as the Actor-Critic.\nAdditionally, we employ Monte Carlo Tree Search to reshape the reward structure\nin the game environment. We compare our algorithm with several traditional\nmethods applied to games such as Double Deep Q Learning, Deep Monte Carlo and\nNeural Fictitious Self Play, and the experiments demonstrate that our algorithm\nconsistently outperforms these approaches, especially as the number of players\nin Uno increases, indicating a higher level of difficulty.\n","authors":["Jiamian Li"],"pdf_url":"https://arxiv.org/pdf/2410.11642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14282v3","updated":"2024-10-23T09:42:59Z","published":"2024-06-20T13:07:38Z","title":"Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs","summary":"  Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.\n","authors":["Junjie Wang","Mingyang Chen","Binbin Hu","Dan Yang","Ziqi Liu","Yue Shen","Peng Wei","Zhiqiang Zhang","Jinjie Gu","Jun Zhou","Jeff Z. Pan","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14282v3.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.17714v1","updated":"2024-10-23T09:40:15Z","published":"2024-10-23T09:40:15Z","title":"CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient\n  Semantic Steering in Large Language Models","summary":"  Despite their impressive capabilities, large language models (LLMs) often\nlack interpretability and can generate toxic content. While using LLMs as\nfoundation models and applying semantic steering methods are widely practiced,\nwe believe that efficient methods should be based on a thorough understanding\nof LLM behavior. To this end, we propose using eye movement measures to\ninterpret LLM behavior across layers. We find that LLMs exhibit patterns\nsimilar to human gaze across layers and different layers function differently.\nInspired by these findings, we introduce a heuristic steering layer selection\nand apply it to layer intervention methods via fine-tuning and inference. Using\nlanguage toxification and detoxification as test beds, we demonstrate that our\nproposed CogSteer methods achieve better results in terms of toxicity scores\nwhile efficiently saving 97% of the computational resources and 60% of the\ntraining time. Our model-agnostic approach can be adopted into various LLMs,\ncontributing to their interpretability and promoting trustworthiness for safe\ndeployment.\n","authors":["Xintong Wang","Jingheng Pan","Longqin Jiang","Liang Ding","Xingshan Li","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2410.17714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17712v1","updated":"2024-10-23T09:39:26Z","published":"2024-10-23T09:39:26Z","title":"A Data-Driven Odyssey in Solar Vehicles","summary":"  Solar vehicles, which simultaneously produce and consume energy, require\nmeticulous energy management. However, potential users often feel uncertain\nabout their operation compared to conventional vehicles. This study presents a\nsimulator designed to help users understand long-distance travel in solar\nvehicles and recognize the importance of proper energy management. By utilizing\nGoogle Maps data and weather information, the simulator replicates real-world\ndriving conditions and provides a dashboard displaying vehicle status, updated\nhourly based on user-inputted speed. Users can explore various speed policy\nscenarios and receive recommendations for optimal driving strategies. The\nsimulator's effectiveness was validated using the route of the World Solar\nChallenge (WSC). This research enables users to monitor energy dynamics before\na journey, enhancing their understanding of energy management and informing\nappropriate speed decisions.\n","authors":["Do Young Kim","Kyunghyun Kim","Gyeongseop Lee","Niloy Das","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17711v1","updated":"2024-10-23T09:36:21Z","published":"2024-10-23T09:36:21Z","title":"Beware of Calibration Data for Pruning Large Language Models","summary":"  As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL).\n","authors":["Yixin Ji","Yang Xiang","Juntao Li","Qingrong Xia","Ping Li","Xinyu Duan","Zhefeng Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17711v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.17700v1","updated":"2024-10-23T09:22:43Z","published":"2024-10-23T09:22:43Z","title":"Scalable Random Feature Latent Variable Models","summary":"  Random feature latent variable models (RFLVMs) represent the state-of-the-art\nin latent variable models, capable of handling non-Gaussian likelihoods and\neffectively uncovering patterns in high-dimensional data. However, their heavy\nreliance on Monte Carlo sampling results in scalability issues which makes it\ndifficult to use these models for datasets with a massive number of\nobservations. To scale up RFLVMs, we turn to the optimization-based variational\nBayesian inference (VBI) algorithm which is known for its scalability compared\nto sampling-based methods. However, implementing VBI for RFLVMs poses\nchallenges, such as the lack of explicit probability distribution functions\n(PDFs) for the Dirichlet process (DP) in the kernel learning component, and the\nincompatibility of existing VBI algorithms with RFLVMs. To address these\nissues, we introduce a stick-breaking construction for DP to obtain an explicit\nPDF and a novel VBI algorithm called ``block coordinate descent variational\ninference\" (BCD-VI). This enables the development of a scalable version of\nRFLVMs, or in short, SRFLVM. Our proposed method shows scalability,\ncomputational efficiency, superior performance in generating informative latent\nrepresentations and the ability of imputing missing data across various\nreal-world datasets, outperforming state-of-the-art competitors.\n","authors":["Ying Li","Zhidi Lin","Yuhao Liu","Michael Minyi Zhang","Pablo M. Olmos","Petar M. Djurić"],"pdf_url":"https://arxiv.org/pdf/2410.17700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17694v1","updated":"2024-10-23T09:14:57Z","published":"2024-10-23T09:14:57Z","title":"An Adaptive Framework for Generating Systematic Explanatory Answer in\n  Online Q&A Platforms","summary":"  Question Answering (QA) systems face challenges in handling complex questions\nthat require multi-domain knowledge synthesis. The naive RAG models, although\neffective in information retrieval, struggle with complex questions that\nrequire comprehensive and in-depth answers. The pioneering task is defined as\nexplanatory answer generation, which entails handling identified challenges\nsuch as the requirement for comprehensive information and logical coherence\nwithin the generated context. To address these issues, we refer to systematic\nthinking theory and propose SynthRAG, an innovative framework designed to\nenhance QA performance. SynthRAG improves on conventional models by employing\nadaptive outlines for dynamic content structuring, generating systematic\ninformation to ensure detailed coverage, and producing customized answers\ntailored to specific user inquiries. This structured approach guarantees\nlogical coherence and thorough integration of information, yielding responses\nthat are both insightful and methodically organized. Empirical evaluations\nunderscore SynthRAG's effectiveness, demonstrating its superiority in handling\ncomplex questions, overcoming the limitations of naive RAG models, and\nsignificantly improving answer quality and depth. Furthermore, an online\ndeployment on the Zhihu platform revealed that SynthRAG's answers achieved\nnotable user engagement, with each response averaging 5.73 upvotes and\nsurpassing the performance of 79.8% of human contributors, highlighting the\npractical relevance and impact of the proposed framework. Our code is available\nat https://github.com/czy1999/SynthRAG .\n","authors":["Ziyang Chen","Xiaobin Wang","Yong Jiang","Jinzhi Liao","Pengjun Xie","Fei Huang","Xiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.17694v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.04892v2","updated":"2024-10-23T09:04:57Z","published":"2024-02-07T14:24:04Z","title":"Probabilistic ML Verification via Weighted Model Integration","summary":"  In machine learning (ML) verification, the majority of procedures are\nnon-quantitative and therefore cannot be used for verifying probabilistic\nmodels, or be applied in domains where hard guarantees are practically\nunachievable. The probabilistic formal verification (PFV) of ML models is in\nits infancy, with the existing approaches limited to specific ML models,\nproperties, or both. This contrasts with standard formal methods techniques,\nwhose successful adoption in real-world scenarios is also due to their support\nfor a wide range of properties and diverse systems. We propose a unifying\nframework for the PFV of ML systems based on Weighted Model Integration (WMI),\na relatively recent formalism for probabilistic inference with algebraic and\nlogical constraints. Crucially, reducing the PFV of ML models to WMI enables\nthe verification of many properties of interest over a wide range of systems,\naddressing multiple limitations of deterministic verification and ad-hoc\nalgorithms. We substantiate the generality of the approach on prototypical\ntasks involving the verification of group fairness, monotonicity, robustness to\nnoise, probabilistic local robustness and equivalence among predictors. We\ncharacterize the challenges related to the scalability of the approach and,\nthrough our WMI-based perspective, we show how successful scaling techniques in\nthe ML verification literature can be generalized beyond their original scope.\n","authors":["Paolo Morettin","Andrea Passerini","Roberto Sebastiani"],"pdf_url":"https://arxiv.org/pdf/2402.04892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10192v3","updated":"2024-10-23T08:39:00Z","published":"2024-02-15T18:48:32Z","title":"Multi-Excitation Projective Simulation with a Many-Body Physics Inspired\n  Inductive Bias","summary":"  With the impressive progress of deep learning, applications relying on\nmachine learning are increasingly being integrated into daily life. However,\nmost deep learning models have an opaque, oracle-like nature making it\ndifficult to interpret and understand their decisions. This problem led to the\ndevelopment of the field known as eXplainable Artificial Intelligence (XAI).\nOne method in this field known as Projective Simulation (PS) models a\nchain-of-thought as a random walk of a particle on a graph with vertices that\nhave concepts attached to them. While this description has various benefits,\nincluding the possibility of quantization, it cannot be naturally used to model\nthoughts that combine several concepts simultaneously. To overcome this\nlimitation, we introduce Multi-Excitation Projective Simulation (mePS), a\ngeneralization that considers a chain-of-thought to be a random walk of several\nparticles on a hypergraph. A definition for a dynamic hypergraph is put forward\nto describe the agent's training history along with applications to AI and\nhypergraph visualization. An inductive bias inspired by the remarkably\nsuccessful few-body interaction models used in quantum many-body physics is\nformalized for our classical mePS framework and employed to tackle the\nexponential complexity associated with naive implementations of hypergraphs. We\nprove that our inductive bias reduces the complexity from exponential to\npolynomial, with the exponent representing the cutoff on how many particles can\ninteract. We numerically apply our method to two toy environments and a more\ncomplex scenario modelling the diagnosis of a broken computer. These\nenvironments demonstrate the resource savings provided by an appropriate choice\nof inductive bias, as well as showcasing aspects of interpretability. A quantum\nmodel for mePS is also briefly outlined and some future directions for it are\ndiscussed.\n","authors":["Philip A. LeMaitre","Marius Krumm","Hans J. Briegel"],"pdf_url":"https://arxiv.org/pdf/2402.10192v3.pdf","comment":"26 pages, 8 figures; Code repository at\n  https://github.com/MariusKrumm/ManyBodyMEPS. Reorganized main text for better\n  readability"},{"id":"http://arxiv.org/abs/2410.17661v1","updated":"2024-10-23T08:24:47Z","published":"2024-10-23T08:24:47Z","title":"PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a\n  resource-limited Context","summary":"  Following their success in natural language processing (NLP), there has been\na shift towards transformer models in computer vision. While transformers\nperform well and offer promising multi-tasking performance, due to their high\ncompute requirements, many resource-constrained applications still rely on\nconvolutional or hybrid models that combine the benefits of convolution and\nattention layers and achieve the best results in the sub 100M parameter range.\nSimultaneously, task adaptation techniques that allow for the use of one shared\ntransformer backbone for multiple downstream tasks, resulting in great storage\nsavings at negligible cost in performance, have not yet been adopted for hybrid\ntransformers. In this work, we investigate how to achieve the best\ntask-adaptation performance and introduce PETAH: Parameter Efficient Task\nAdaptation for Hybrid Transformers. We further combine PETAH adaptation with\npruning to achieve highly performant and storage friendly models for\nmulti-tasking. In our extensive evaluation on classification and other vision\ntasks, we demonstrate that our PETAH-adapted hybrid models outperform\nestablished task-adaptation techniques for ViTs while requiring fewer\nparameters and being more efficient on mobile hardware.\n","authors":["Maximilian Augustin","Syed Shakib Sarwar","Mostafa Elhoushi","Sai Qian Zhang","Yuecheng Li","Barbara De Salvo"],"pdf_url":"https://arxiv.org/pdf/2410.17661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10216v2","updated":"2024-10-23T08:22:44Z","published":"2024-06-14T17:49:59Z","title":"Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs","summary":"  Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm.\n","authors":["Rui Yang","Ruomeng Ding","Yong Lin","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10216v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17656v1","updated":"2024-10-23T08:18:38Z","published":"2024-10-23T08:18:38Z","title":"AutoRNet: Automatically Optimizing Heuristics for Robust Network Design\n  via Large Language Models","summary":"  Achieving robust networks is a challenging problem due to its NP-hard nature\nand complex solution space. Current methods, from handcrafted feature\nextraction to deep learning, have made progress but remain rigid, requiring\nmanual design and large labeled datasets. To address these issues, we propose\nAutoRNet, a framework that integrates large language models (LLMs) with\nevolutionary algorithms to generate heuristics for robust network design. We\ndesign network optimization strategies to provide domain-specific prompts for\nLLMs, utilizing domain knowledge to generate advanced heuristics. Additionally,\nwe introduce an adaptive fitness function to balance convergence and diversity\nwhile maintaining degree distributions. AutoRNet is evaluated on sparse and\ndense scale-free networks, outperforming current methods by reducing the need\nfor manual design and large datasets.\n","authors":["He Yu","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17655v1","updated":"2024-10-23T08:18:26Z","published":"2024-10-23T08:18:26Z","title":"Mapping the Media Landscape: Predicting Factual Reporting and Political\n  Bias Through Web Interactions","summary":"  Bias assessment of news sources is paramount for professionals,\norganizations, and researchers who rely on truthful evidence for information\ngathering and reporting. While certain bias indicators are discernible from\ncontent analysis, descriptors like political bias and fake news pose greater\nchallenges. In this paper, we propose an extension to a recently presented news\nmedia reliability estimation method that focuses on modeling outlets and their\nlongitudinal web interactions. Concretely, we assess the classification\nperformance of four reinforcement learning strategies on a large news media\nhyperlink graph. Our experiments, targeting two challenging bias descriptors,\nfactual reporting and political bias, showed a significant performance\nimprovement at the source media level. Additionally, we validate our methods on\nthe CLEF 2023 CheckThat! Lab challenge, outperforming the reported results in\nboth, F1-score and the official MAE metric. Furthermore, we contribute by\nreleasing the largest annotated dataset of news source media, categorized with\nfactual reporting and political bias labels. Our findings suggest that\nprofiling news media sources based on their hyperlink interactions over time is\nfeasible, offering a bird's-eye view of evolving media landscapes.\n","authors":["Dairazalia Sánchez-Cortés","Sergio Burdisso","Esaú Villatoro-Tello","Petr Motlicek"],"pdf_url":"https://arxiv.org/pdf/2410.17655v1.pdf","comment":"Accepted to CLEF 2024"},{"id":"http://arxiv.org/abs/2410.17637v1","updated":"2024-10-23T07:56:48Z","published":"2024-10-23T07:56:48Z","title":"MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large\n  Vision-Language Models","summary":"  Visual preference alignment involves training Large Vision-Language Models\n(LVLMs) to predict human preferences between visual inputs. This is typically\nachieved by using labeled datasets of chosen/rejected pairs and employing\noptimization algorithms like direct preference optimization (DPO). Existing\nvisual alignment methods, primarily designed for single-image scenarios,\nstruggle to effectively handle the complexity of multi-image tasks due to the\nscarcity of diverse training data and the high cost of annotating\nchosen/rejected pairs. We present Multi-Image Augmented Direct Preference\nOptimization (MIA-DPO), a visual preference alignment approach that effectively\nhandles multi-image inputs. MIA-DPO mitigates the scarcity of diverse\nmulti-image training data by extending single-image data with unrelated images\narranged in grid collages or pic-in-pic formats, significantly reducing the\ncosts associated with multi-image data annotations. Our observation reveals\nthat attention values of LVLMs vary considerably across different images. We\nuse attention values to identify and filter out rejected responses the model\nmay have mistakenly focused on. Our attention-aware selection for constructing\nthe chosen/rejected pairs without relying on (i) human annotation, (ii) extra\ndata, and (iii) external models or APIs. MIA-DPO is compatible with various\narchitectures and outperforms existing methods on five multi-image benchmarks,\nachieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the\nrecent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's\nability to understand single images.\n","authors":["Ziyu Liu","Yuhang Zang","Xiaoyi Dong","Pan Zhang","Yuhang Cao","Haodong Duan","Conghui He","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17637v1.pdf","comment":"Project URL: https://github.com/Liuziyu77/MIA-DPO"},{"id":"http://arxiv.org/abs/2410.17635v1","updated":"2024-10-23T07:53:29Z","published":"2024-10-23T07:53:29Z","title":"Markov Chain of Thought for Efficient Mathematical Reasoning","summary":"  Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.\n","authors":["Wen Yang","Kai Fan","Minpeng Liao"],"pdf_url":"https://arxiv.org/pdf/2410.17635v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.17632v1","updated":"2024-10-23T07:48:51Z","published":"2024-10-23T07:48:51Z","title":"LMLPA: Language Model Linguistic Personality Assessment","summary":"  Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing.\n","authors":["Jingyao Zheng","Xian Wang","Simo Hosio","Xiaoxian Xu","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2410.17632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17629v1","updated":"2024-10-23T07:44:56Z","published":"2024-10-23T07:44:56Z","title":"Graph Signal Adaptive Message Passing","summary":"  This paper proposes Graph Signal Adaptive Message Passing (GSAMP), a novel\nmessage passing method that simultaneously conducts online prediction, missing\ndata imputation, and noise removal on time-varying graph signals. Unlike\nconventional Graph Signal Processing methods that apply the same filter to the\nentire graph, the spatiotemporal updates of GSAMP employ a distinct approach\nthat utilizes localized computations at each node. This update is based on an\nadaptive solution obtained from an optimization problem designed to minimize\nthe discrepancy between observed and estimated values. GSAMP effectively\nprocesses real-world, time-varying graph signals under Gaussian and impulsive\nnoise conditions.\n","authors":["Yi Yan","Changran Peng","Ercan Engin Kuruoglu"],"pdf_url":"https://arxiv.org/pdf/2410.17629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05623v2","updated":"2024-10-23T07:28:19Z","published":"2024-10-08T02:11:35Z","title":"Understanding Gradient Boosting Classifier: Training, Prediction, and\n  the Role of $γ_j$","summary":"  The Gradient Boosting Classifier (GBC) is a widely used machine learning\nalgorithm for binary classification, which builds decision trees iteratively to\nminimize prediction errors. This document explains the GBC's training and\nprediction processes, focusing on the computation of terminal node values\n$\\gamma_j$, which are crucial to optimizing the logistic loss function. We\nderive $\\gamma_j$ through a Taylor series approximation and provide a\nstep-by-step pseudocode for the algorithm's implementation. The guide explains\nthe theory of GBC and its practical application, demonstrating its\neffectiveness in binary classification tasks. We provide a step-by-step example\nin the appendix to help readers understand.\n","authors":["Hung-Hsuan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17621v1","updated":"2024-10-23T07:22:33Z","published":"2024-10-23T07:22:33Z","title":"Process Supervision-Guided Policy Optimization for Code Generation","summary":"  Reinforcement Learning (RL) with unit test feedback has enhanced large\nlanguage models (LLMs) code generation, but relies on sparse rewards provided\nonly after complete code evaluation, limiting learning efficiency and\nincremental improvements. When generated code fails all unit tests, no learning\nsignal is received, hindering progress on complex tasks. To address this, we\npropose a Process Reward Model (PRM) that delivers dense, line-level feedback\non code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs\nand integrating them into the RL framework, finding that using PRMs both as\ndense rewards and for value function initialization significantly boosts\nperformance. Our approach increases our in-house LLM's pass rate from 28.2% to\n29.8% on LiveCodeBench and from 31.8% to 35.8% on our internal benchmark. Our\nexperimental results highlight the effectiveness of PRMs in enhancing RL-driven\ncode generation, especially for long-horizon scenarios.\n","authors":["Ning Dai","Zheng Wu","Renjie Zheng","Ziyun Wei","Wenlei Shi","Xing Jin","Guanlin Liu","Chen Dun","Liang Huang","Lin Yan"],"pdf_url":"https://arxiv.org/pdf/2410.17621v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.17619v1","updated":"2024-10-23T07:17:31Z","published":"2024-10-23T07:17:31Z","title":"From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database\n  Management","summary":"  This study investigates the effectiveness of Large Language Models (LLMs) in\nprocessing semi-structured data from PDF documents into structured formats,\nspecifically examining their application in updating the Finnish Sports Clubs\nDatabase. Through action research methodology, we developed and evaluated an\nAI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus\nmodels to process data from 72 sports federation membership reports. The system\nachieved a 90% success rate in automated processing, successfully handling 65\nof 72 files without errors and converting over 7,900 rows of data. While the\ninitial development time was comparable to traditional manual processing (three\nmonths), the implemented system shows potential for reducing future processing\ntime by approximately 90%. Key challenges included handling multilingual\ncontent, processing multi-page datasets, and managing extraneous information.\nThe findings suggest that while LLMs demonstrate significant potential for\nautomating semi-structured data processing tasks, optimal results are achieved\nthrough a hybrid approach combining AI automation with selective human\noversight. This research contributes to the growing body of literature on\npractical LLM applications in organizational data management and provides\ninsights into the transformation of traditional data processing workflows.\n","authors":["Juhani Merilehto"],"pdf_url":"https://arxiv.org/pdf/2410.17619v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2404.07554v2","updated":"2024-10-23T07:16:42Z","published":"2024-04-11T08:36:13Z","title":"CAT: Contrastive Adapter Training for Personalized Image Generation","summary":"  The emergence of various adapters, including Low-Rank Adaptation (LoRA)\napplied from the field of natural language processing, has allowed diffusion\nmodels to personalize image generation at a low cost. However, due to the\nvarious challenges including limited datasets and shortage of regularization\nand computation resources, adapter training often results in unsatisfactory\noutcomes, leading to the corruption of the backbone model's prior knowledge.\nOne of the well known phenomena is the loss of diversity in object generation,\nespecially within the same class which leads to generating almost identical\nobjects with minor variations. This poses challenges in generation\ncapabilities. To solve this issue, we present Contrastive Adapter Training\n(CAT), a simple yet effective strategy to enhance adapter training through the\napplication of CAT loss. Our approach facilitates the preservation of the base\nmodel's original knowledge when the model initiates adapters. Furthermore, we\nintroduce the Knowledge Preservation Score (KPS) to evaluate CAT's ability to\nkeep the former information. We qualitatively and quantitatively compare CAT's\nimprovement. Finally, we mention the possibility of CAT in the aspects of\nmulti-concept adapter and optimization.\n","authors":["Jae Wan Park","Sang Hyun Park","Jun Young Koh","Junha Lee","Min Song"],"pdf_url":"https://arxiv.org/pdf/2404.07554v2.pdf","comment":"CVPRW 2024"},{"id":"http://arxiv.org/abs/2305.12987v3","updated":"2024-10-23T07:13:49Z","published":"2023-05-22T12:47:48Z","title":"GPT-SW3: An Autoregressive Language Model for the Nordic Languages","summary":"  This paper details the process of developing the first native large\ngenerative language model for the Nordic languages, GPT-SW3. We cover all parts\nof the development process, from data collection and processing, training\nconfiguration and instruction finetuning, to evaluation and considerations for\nrelease strategies. We hope that this paper can serve as a guide and reference\nfor other researchers that undertake the development of large generative models\nfor smaller languages.\n","authors":["Ariel Ekgren","Amaru Cuba Gyllensten","Felix Stollenwerk","Joey Öhman","Tim Isbister","Evangelia Gogoulou","Fredrik Carlsson","Alice Heiman","Judit Casademont","Magnus Sahlgren"],"pdf_url":"https://arxiv.org/pdf/2305.12987v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17610v1","updated":"2024-10-23T07:06:08Z","published":"2024-10-23T07:06:08Z","title":"ImDy: Human Inverse Dynamics from Imitated Observations","summary":"  Inverse dynamics (ID), which aims at reproducing the driven torques from\nhuman kinematic observations, has been a critical tool for gait analysis.\nHowever, it is hindered from wider application to general motion due to its\nlimited scalability. Conventional optimization-based ID requires expensive\nlaboratory setups, restricting its availability. To alleviate this problem, we\npropose to exploit the recently progressive human motion imitation algorithms\nto learn human inverse dynamics in a data-driven manner. The key insight is\nthat the human ID knowledge is implicitly possessed by motion imitators, though\nnot directly applicable. In light of this, we devise an efficient data\ncollection pipeline with state-of-the-art motion imitation algorithms and\nphysics simulators, resulting in a large-scale human inverse dynamics benchmark\nas Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint\ntorque and full-body ground reaction force data. With ImDy, we train a\ndata-driven human inverse dynamics solver ImDyS(olver) in a fully supervised\nmanner, which conducts ID and ground reaction force estimation simultaneously.\nExperiments on ImDy and real-world data demonstrate the impressive competency\nof ImDyS in human inverse dynamics and ground reaction force estimation.\nMoreover, the potential of ImDy(-S) as a fundamental motion analysis tool is\nexhibited with downstream applications. The project page is\nhttps://foruck.github.io/ImDy/.\n","authors":["Xinpeng Liu","Junxuan Liang","Zili Lin","Haowen Hou","Yong-Lu Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2410.17610v1.pdf","comment":"Yong-Lu Li and Cewu Lu are the corresponding authors"},{"id":"http://arxiv.org/abs/2406.14514v2","updated":"2024-10-23T07:05:18Z","published":"2024-06-20T17:24:13Z","title":"Solving a Stackelberg Game on Transportation Networks in a Dynamic Crime\n  Scenario: A Mixed Approach on Multi-Layer Networks","summary":"  Interdicting a criminal with limited police resources is a challenging task\nas the criminal changes location over time. The size of the large\ntransportation network further adds to the difficulty of this scenario. To\ntackle this issue, we consider the concept of a layered graph. At each time\nstamp, we create a copy of the entire transportation network to track the\npossible movements of both players, the attacker and the defenders. We consider\na Stackelberg game in a dynamic crime scenario where the attacker changes\nlocation over time while the defenders attempt to interdict the attacker on his\nescape route. Given a set of defender strategies, the optimal attacker strategy\nis determined by applying Dijkstra's algorithm on the layered networks. Here,\nthe attacker aims to minimize while the defenders aim to maximize the\nprobability of interdiction. We develop an approximation algorithm on the\nlayered networks to find near-optimal strategy for defenders. The efficacy of\nthe developed approach is compared with the adopted MILP approach. We compare\nthe results in terms of computational time and solution quality. The quality of\nthe results demonstrates the need for the developed approach, as it effectively\nsolves the complex problem within a short amount of time.\n","authors":["Sukanya Samanta","Kei Kimura","Makoto Yokoo"],"pdf_url":"https://arxiv.org/pdf/2406.14514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17606v1","updated":"2024-10-23T07:01:16Z","published":"2024-10-23T07:01:16Z","title":"Towards Effective Data-Free Knowledge Distillation via Diverse Diffusion\n  Augmentation","summary":"  Data-free knowledge distillation (DFKD) has emerged as a pivotal technique in\nthe domain of model compression, substantially reducing the dependency on the\noriginal training data. Nonetheless, conventional DFKD methods that employ\nsynthesized training data are prone to the limitations of inadequate diversity\nand discrepancies in distribution between the synthesized and original\ndatasets. To address these challenges, this paper introduces an innovative\napproach to DFKD through diverse diffusion augmentation (DDA). Specifically, we\nrevise the paradigm of common data synthesis in DFKD to a composite process\nthrough leveraging diffusion models subsequent to data synthesis for\nself-supervised augmentation, which generates a spectrum of data samples with\nsimilar distributions while retaining controlled variations. Furthermore, to\nmitigate excessive deviation in the embedding space, we introduce an image\nfiltering technique grounded in cosine similarity to maintain fidelity during\nthe knowledge distillation process. Comprehensive experiments conducted on\nCIFAR-10, CIFAR-100, and Tiny-ImageNet datasets showcase the superior\nperformance of our method across various teacher-student network\nconfigurations, outperforming the contemporary state-of-the-art DFKD methods.\nCode will be available at:https://github.com/SLGSP/DDA.\n","authors":["Muquan Li","Dongyang Zhang","Tao He","Xiurui Xie","Yuan-Fang Li","Ke Qin"],"pdf_url":"https://arxiv.org/pdf/2410.17606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17602v1","updated":"2024-10-23T06:56:53Z","published":"2024-10-23T06:56:53Z","title":"Integrating Large Language Models for UAV Control in Simulated\n  Environments: A Modular Interaction Approach","summary":"  The intersection of LLMs (Large Language Models) and UAV (Unoccupied Aerial\nVehicles) technology represents a promising field of research with the\npotential to enhance UAV capabilities significantly. This study explores the\napplication of LLMs in UAV control, focusing on the opportunities for\nintegrating advanced natural language processing into autonomous aerial\nsystems. By enabling UAVs to interpret and respond to natural language\ncommands, LLMs simplify the UAV control and usage, making them accessible to a\nbroader user base and facilitating more intuitive human-machine interactions.\nThe paper discusses several key areas where LLMs can impact UAV technology,\nincluding autonomous decision-making, dynamic mission planning, enhanced\nsituational awareness, and improved safety protocols. Through a comprehensive\nreview of current developments and potential future directions, this study aims\nto highlight how LLMs can transform UAV operations, making them more adaptable,\nresponsive, and efficient in complex environments. A template development\nframework for integrating LLMs in UAV control is also described. Proof of\nConcept results that integrate existing LLM models and popular robotic\nsimulation platforms are demonstrated. The findings suggest that while there\nare substantial technical and ethical challenges to address, integrating LLMs\ninto UAV control holds promising implications for advancing autonomous aerial\nsystems.\n","authors":["Abhishek Phadke","Alihan Hadimlioglu","Tianxing Chu","Chandra N Sekharan"],"pdf_url":"https://arxiv.org/pdf/2410.17602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17600v1","updated":"2024-10-23T06:54:03Z","published":"2024-10-23T06:54:03Z","title":"Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective","summary":"  Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion.\n","authors":["Rui Yang","Boming Yang","Aosong Feng","Sixun Ouyang","Moritz Blum","Tianwei She","Yuang Jiang","Freddy Lecue","Jinghui Lu","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2410.17600v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.10794"},{"id":"http://arxiv.org/abs/2407.06813v4","updated":"2024-10-23T06:39:57Z","published":"2024-07-09T12:37:54Z","title":"Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy","summary":"  Diplomacy is one of the most sophisticated activities in human society,\ninvolving complex interactions among multiple parties that require skills in\nsocial reasoning, negotiation, and long-term strategic planning. Previous AI\nagents have demonstrated their ability to handle multi-step games and large\naction spaces in multi-agent tasks. However, diplomacy involves a staggering\nmagnitude of decision spaces, especially considering the negotiation stage\nrequired. While recent agents based on large language models (LLMs) have shown\npotential in various applications, they still struggle with extended planning\nperiods in complex multi-agent settings. Leveraging recent technologies for\nLLM-based agents, we aim to explore AI's potential to create a human-like agent\ncapable of executing comprehensive multi-agent missions by integrating three\nfundamental capabilities: 1) strategic planning with memory and reflection; 2)\ngoal-oriented negotiation with social reasoning; and 3) augmenting memory\nthrough self-play games for self-evolution without human in the loop.\n","authors":["Zhenyu Guan","Xiangyu Kong","Fangwei Zhong","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2407.06813v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08035v2","updated":"2024-10-23T06:37:01Z","published":"2024-06-12T09:36:52Z","title":"LVBench: An Extreme Long Video Understanding Benchmark","summary":"  Recent progress in multimodal large language models has markedly enhanced the\nunderstanding of short videos (typically under one minute), and several\nevaluation datasets have emerged accordingly. However, these advancements fall\nshort of meeting the demands of real-world applications such as embodied\nintelligence for long-term decision-making, in-depth movie reviews and\ndiscussions, and live sports commentary, all of which require comprehension of\nlong videos spanning several hours. To address this gap, we introduce LVBench,\na benchmark specifically designed for long video understanding. Our dataset\ncomprises publicly sourced videos and encompasses a diverse set of tasks aimed\nat long video comprehension and information extraction. LVBench is designed to\nchallenge multimodal models to demonstrate long-term memory and extended\ncomprehension capabilities. Our extensive evaluations reveal that current\nmultimodal models still underperform on these demanding long video\nunderstanding tasks. Through LVBench, we aim to spur the development of more\nadvanced models capable of tackling the complexities of long video\ncomprehension. Our data and code are publicly available at:\nhttps://lvbench.github.io.\n","authors":["Weihan Wang","Zehai He","Wenyi Hong","Yean Cheng","Xiaohan Zhang","Ji Qi","Xiaotao Gu","Shiyu Huang","Bin Xu","Yuxiao Dong","Ming Ding","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2406.08035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17589v1","updated":"2024-10-23T06:35:41Z","published":"2024-10-23T06:35:41Z","title":"Challenge on Sound Scene Synthesis: Evaluating Text-to-Audio Generation","summary":"  Despite significant advancements in neural text-to-audio generation,\nchallenges persist in controllability and evaluation. This paper addresses\nthese issues through the Sound Scene Synthesis challenge held as part of the\nDetection and Classification of Acoustic Scenes and Events 2024. We present an\nevaluation protocol combining objective metric, namely Fr\\'echet Audio\nDistance, with perceptual assessments, utilizing a structured prompt format to\nenable diverse captions and effective evaluation. Our analysis reveals varying\nperformance across sound categories and model architectures, with larger models\ngenerally excelling but innovative lightweight approaches also showing promise.\nThe strong correlation between objective metrics and human ratings validates\nour evaluation approach. We discuss outcomes in terms of audio quality,\ncontrollability, and architectural considerations for text-to-audio\nsynthesizers, providing direction for future research.\n","authors":["Junwon Lee","Modan Tailleur","Laurie M. Heller","Keunwoo Choi","Mathieu Lagrange","Brian McFee","Keisuke Imoto","Yuki Okamoto"],"pdf_url":"https://arxiv.org/pdf/2410.17589v1.pdf","comment":"accepted to NeurIPS 2024 Workshop: Audio Imagination"},{"id":"http://arxiv.org/abs/2402.12617v2","updated":"2024-10-23T06:28:19Z","published":"2024-02-20T00:51:05Z","title":"Generative AI Security: Challenges and Countermeasures","summary":"  Generative AI's expanding footprint across numerous industries has led to\nboth excitement and increased scrutiny. This paper delves into the unique\nsecurity challenges posed by Generative AI, and outlines potential research\ndirections for managing these risks.\n","authors":["Banghua Zhu","Norman Mu","Jiantao Jiao","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2402.12617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15573v2","updated":"2024-10-23T06:21:09Z","published":"2024-10-21T01:36:42Z","title":"OpenMU: Your Swiss Army Knife for Music Understanding","summary":"  We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.\n","authors":["Mengjie Zhao","Zhi Zhong","Zhuoyuan Mao","Shiqi Yang","Wei-Hsiang Liao","Shusuke Takahashi","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.15573v2.pdf","comment":"Resources: https://github.com/mzhaojp22/openmu"},{"id":"http://arxiv.org/abs/2410.17584v1","updated":"2024-10-23T06:19:48Z","published":"2024-10-23T06:19:48Z","title":"Exploring Tokenization Methods for Multitrack Sheet Music Generation","summary":"  This study explores the tokenization of multitrack sheet music in ABC\nnotation, introducing two methods--bar-stream and line-stream patching. We\ncompare these methods against existing techniques, including bar patching, byte\npatching, and Byte Pair Encoding (BPE). In terms of both computational\nefficiency and the musicality of the generated compositions, experimental\nresults show that bar-stream patching performs best overall compared to the\nothers, which makes it a promising tokenization strategy for sheet music\ngeneration.\n","authors":["Yashan Wang","Shangda Wu","Xingjian Du","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.17584v1.pdf","comment":"3 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2410.17579v1","updated":"2024-10-23T06:08:45Z","published":"2024-10-23T06:08:45Z","title":"Bonsai: Gradient-free Graph Distillation for Node Classification","summary":"  Graph distillation has emerged as a promising avenue to enable scalable\ntraining of GNNs by compressing the training dataset while preserving essential\ngraph characteristics. Our study uncovers significant shortcomings in current\ngraph distillation techniques. First, the majority of the algorithms\nparadoxically require training on the full dataset to perform distillation.\nSecond, due to their gradient-emulating approach, these methods require fresh\ndistillation for any change in hyperparameters or GNN architecture, limiting\ntheir flexibility and reusability. Finally, they fail to achieve substantial\nsize reduction due to synthesizing fully-connected, edge-weighted graphs. To\naddress these challenges, we present Bonsai, a novel graph distillation method\nempowered by the observation that \\textit{computation trees} form the\nfundamental processing units of message-passing GNNs. Bonsai distills datasets\nby encoding a careful selection of \\textit{exemplar} trees that maximize the\nrepresentation of all computation trees in the training set. This unique\napproach imparts Bonsai as the first linear-time, model-agnostic graph\ndistillation algorithm for node classification that outperforms existing\nbaselines across $6$ real-world datasets on accuracy, while being $22$ times\nfaster on average. Bonsai is grounded in rigorous mathematical guarantees on\nthe adopted approximation strategies making it robust to GNN architectures,\ndatasets, and parameters.\n","authors":["Mridul Gupta","Samyak Jain","Vansh Ramani","Hariprasad Kodamana","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2410.17579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17576v1","updated":"2024-10-23T05:59:55Z","published":"2024-10-23T05:59:55Z","title":"Real-time Vehicle-to-Vehicle Communication Based Network Cooperative\n  Control System through Distributed Database and Multimodal Perception:\n  Demonstrated in Crossroads","summary":"  The autonomous driving industry is rapidly advancing, with Vehicle-to-Vehicle\n(V2V) communication systems highlighting as a key component of enhanced road\nsafety and traffic efficiency. This paper introduces a novel Real-time\nVehicle-to-Vehicle Communication Based Network Cooperative Control System\n(VVCCS), designed to revolutionize macro-scope traffic planning and collision\navoidance in autonomous driving. Implemented on Quanser Car (Qcar) hardware\nplatform, our system integrates the distributed databases into individual\nautonomous vehicles and an optional central server. We also developed a\ncomprehensive multi-modal perception system with multi-objective tracking and\nradar sensing. Through a demonstration within a physical crossroad environment,\nour system showcases its potential to be applied in congested and complex urban\nenvironments.\n","authors":["Xinwen Zhu","Zihao Li","Yuxuan Jiang","Jiazhen Xu","Jie Wang","Xuyang Bai"],"pdf_url":"https://arxiv.org/pdf/2410.17576v1.pdf","comment":"ICICT 2024, 18 pages"},{"id":"http://arxiv.org/abs/2408.04870v5","updated":"2024-10-23T05:55:31Z","published":"2024-08-09T05:20:05Z","title":"ConfusedPilot: Confused Deputy Risks in RAG-based LLMs","summary":"  Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.\n","authors":["Ayush RoyChowdhury","Mulong Luo","Prateek Sahu","Sarbartha Banerjee","Mohit Tiwari"],"pdf_url":"https://arxiv.org/pdf/2408.04870v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15815v2","updated":"2024-10-23T05:32:34Z","published":"2024-07-22T17:29:02Z","title":"Learning to Manipulate Anywhere: A Visual Generalizable Framework For\n  Reinforcement Learning","summary":"  Can we endow visuomotor robots with generalization capabilities to operate in\ndiverse open-world scenarios? In this paper, we propose \\textbf{Maniwhere}, a\ngeneralizable framework tailored for visual reinforcement learning, enabling\nthe trained robot policies to generalize across a combination of multiple\nvisual disturbance types. Specifically, we introduce a multi-view\nrepresentation learning approach fused with Spatial Transformer Network (STN)\nmodule to capture shared semantic information and correspondences among\ndifferent viewpoints. In addition, we employ a curriculum-based randomization\nand augmentation approach to stabilize the RL training process and strengthen\nthe visual generalization ability. To exhibit the effectiveness of Maniwhere,\nwe meticulously design 8 tasks encompassing articulate objects, bi-manual, and\ndexterous hand manipulation tasks, demonstrating Maniwhere's strong visual\ngeneralization and sim2real transfer abilities across 3 hardware platforms. Our\nexperiments show that Maniwhere significantly outperforms existing\nstate-of-the-art methods. Videos are provided at\nhttps://gemcollector.github.io/maniwhere/.\n","authors":["Zhecheng Yuan","Tianming Wei","Shuiqi Cheng","Gu Zhang","Yuanpei Chen","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2407.15815v2.pdf","comment":"Webpage: https://gemcollector.github.io/maniwhere/"},{"id":"http://arxiv.org/abs/2410.17566v1","updated":"2024-10-23T05:19:51Z","published":"2024-10-23T05:19:51Z","title":"Differentially Private Learning Needs Better Model Initialization and\n  Self-Distillation","summary":"  Differentially private SGD (DPSGD) enables privacy-preserving training of\nlanguage models, but often reduces utility, diversity, and linguistic quality.\nWe introduce DPRefine, a three-phase method that initializes a model using data\nsynthesis from a small pre-trained LM with rigorous filtering, applies DP\nfinetuning on private data, and performs self-distillation to refine outputs.\nThis approach significantly outperforms vanilla DPSGD, with AlpacaEval\npreferring DPRefine's generations in 78.4% of cases across all datasets. Our\nanalysis reveals that DPRefine reduces linguistic errors in generated text by\n84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.\nIt also reduces inconsistencies of non-private models, such as hallucinated\ndetails and misattributed quotes. We find that small models like GPT-2 can be\neffective for initialization and distillation, highlighting their potential in\nenabling scalable and efficient deployment of privacy-preserving language.\n","authors":["Ivoline C. Ngong","Joseph P. Near","Niloofar Mireshghallah"],"pdf_url":"https://arxiv.org/pdf/2410.17566v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2405.03239v2","updated":"2024-10-23T05:18:11Z","published":"2024-05-06T07:48:34Z","title":"Deep Learning for Detecting and Early Predicting Chronic Obstructive\n  Pulmonary Disease from Spirogram Time Series","summary":"  Chronic Obstructive Pulmonary Disease (COPD) is a chronic lung disease that\ncauses airflow obstruction. Current methods can only detect COPD from prominent\nfeatures in spirogram (Volume-Flow time series) but cannot predict future COPD\nrisk from subtle data patterns. We propose a deep learning-based method,\nDeepSpiro, for early prediction of future COPD risk. DeepSpiro consists of four\nkey components: SpiroSmoother for stabilizing the Volume-Flow curve,\nSpiroEncoder for capturing volume evolution through key patches of varying\nlengths, SpiroExplainer for integrating heterogeneous data and explaining\npredictions through volume attention, and SpiroPredictor for predicting the\ndisease risk of undiagnosed high-risk patients based on key patch concavity,\nwith prediction horizons of 1, 2, 3, 4, 5 years, or even longer. Evaluated on\nthe UK Biobank dataset, DeepSpiro achieved an AUC of 0.8328 for COPD detection\nand demonstrated strong predictive performance for future COPD risk (p-value <\n0.001). DeepSpiro effectively predicts the long-term progression of the\ndisease.\n","authors":["Shuhao Mei","Xin Li","Yuxi Zhou","Jiahao Xu","Yong Zhang","Yuxuan Wan","Shan Cao","Qinghao Zhao","Shijia Geng","Junqing Xie","Shengyong Chen","Shenda Hong"],"pdf_url":"https://arxiv.org/pdf/2405.03239v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15715v2","updated":"2024-10-23T05:02:39Z","published":"2024-10-21T07:34:04Z","title":"Timetable Nodes for Public Transport Network","summary":"  Faster pathfinding in time-dependent transport networks is an important and\nchallenging problem in navigation systems. There are two main types of\ntransport networks: road networks for car driving and public transport route\nnetwork. The solutions that work well in road networks, such as Time-dependent\nContraction Hierarchies and other graph-based approaches, do not usually apply\nin transport networks. In transport networks, non-graph solutions such as CSA\nand RAPTOR show the best results compared to graph-based techniques. In our\nwork, we propose a method that advances graph-based approaches by using\ndifferent optimization techniques from computational geometry to speed up the\nsearch process in transport networks. We apply a new pre-computation step,\nwhich we call timetable nodes (TTN). Our inspiration comes from an iterative\nsearch problem in computational geometry. We implement two versions of the TTN:\none uses a Combined Search Tree (TTN-CST), and the second uses Fractional\nCascading (TTN-FC). Both of these approaches decrease the asymptotic complexity\nof reaching new nodes from $O(k\\times \\log|C|)$ to $O(k + \\log(k) +\n\\log(|C|))$, where $k$ is the number of outgoing edges from a node and $|C|$ is\nthe size of the timetable information (total outgoing edges). Our solution\nsuits any other time-dependent networks and can be integrated into other\npathfinding algorithms. Our experiments indicate that this pre-computation\nsignificantly enhances the performance on high-density graphs. This study\nshowcases how leveraging computational geometry can enhance pathfinding in\ntransport networks, enabling faster pathfinding in scenarios involving large\nnumbers of outgoing edges.\n","authors":["Andrii Rohovyi","Peter J. Stuckey","Toby Walsh"],"pdf_url":"https://arxiv.org/pdf/2410.15715v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17558v1","updated":"2024-10-23T04:55:08Z","published":"2024-10-23T04:55:08Z","title":"CLR-Bench: Evaluating Large Language Models in College-level Reasoning","summary":"  Large language models (LLMs) have demonstrated their remarkable performance\nacross various language understanding tasks. While emerging benchmarks have\nbeen proposed to evaluate LLMs in various domains such as mathematics and\ncomputer science, they merely measure the accuracy in terms of the final\nprediction on multi-choice questions. However, it remains insufficient to\nverify the essential understanding of LLMs given a chosen choice. To fill this\ngap, we present CLR-Bench to comprehensively evaluate the LLMs in complex\ncollege-level reasoning. Specifically, (i) we prioritize 16 challenging college\ndisciplines in computer science and artificial intelligence. The dataset\ncontains 5 types of questions, while each question is associated with detailed\nexplanations from experts. (ii) To quantify a fair evaluation of LLMs'\nreasoning ability, we formalize the criteria with two novel metrics.\nQ$\\rightarrow$A is utilized to measure the performance of direct answer\nprediction, and Q$\\rightarrow$AR effectively considers the joint ability to\nanswer the question and provide rationale simultaneously. Extensive experiments\nare conducted with 40 LLMs over 1,018 discipline-specific questions. The\nresults demonstrate the key insights that LLMs, even the best closed-source\nLLM, i.e., GPT-4 turbo, tend to `guess' the college-level answers. It shows a\ndramatic decrease in accuracy from 63.31% Q$\\rightarrow$A to 39.00%\nQ$\\rightarrow$AR, indicating an unsatisfactory reasoning ability.\n","authors":["Junnan Dong","Zijin Hong","Yuanchen Bei","Feiran Huang","Xinrun Wang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2410.17558v1.pdf","comment":"18 pages, 6 figures, dataset and evaluation framework will be\n  opensourced"},{"id":"http://arxiv.org/abs/2410.17555v1","updated":"2024-10-23T04:43:03Z","published":"2024-10-23T04:43:03Z","title":"FairDgcl: Fairness-aware Recommendation with Dynamic Graph Contrastive\n  Learning","summary":"  As trustworthy AI continues to advance, the fairness issue in recommendations\nhas received increasing attention. A recommender system is considered unfair\nwhen it produces unequal outcomes for different user groups based on\nuser-sensitive attributes (e.g., age, gender). Some researchers have proposed\ndata augmentation-based methods aiming at alleviating user-level unfairness by\naltering the skewed distribution of training data among various user groups.\nDespite yielding promising results, they often rely on fairness-related\nassumptions that may not align with reality, potentially reducing the data\nquality and negatively affecting model effectiveness. To tackle this issue, in\nthis paper, we study how to implement high-quality data augmentation to improve\nrecommendation fairness. Specifically, we propose FairDgcl, a dynamic graph\nadversarial contrastive learning framework aiming at improving fairness in\nrecommender system. First, FairDgcl develops an adversarial contrastive network\nwith a view generator and a view discriminator to learn generating fair\naugmentation strategies in an adversarial style. Then, we propose two dynamic,\nlearnable models to generate contrastive views within contrastive learning\nframework, which automatically fine-tune the augmentation strategies.\nMeanwhile, we theoretically show that FairDgcl can simultaneously generate\nenhanced representations that possess both fairness and accuracy. Lastly,\ncomprehensive experiments conducted on four real-world datasets demonstrate the\neffectiveness of the proposed FairDgcl.\n","authors":["Wei Chen","Meng Yuan","Zhao Zhang","Ruobing Xie","Fuzhen Zhuang","Deqing Wang","Rui Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17555v1.pdf","comment":"12 pages, submitted to TKDE"},{"id":"http://arxiv.org/abs/2409.05280v2","updated":"2024-10-23T04:41:51Z","published":"2024-09-09T02:18:50Z","title":"RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac\n  Segmentation","summary":"  Cardiovascular disease remains a predominant global health concern,\nresponsible for a significant portion of mortality worldwide. Accurate\nsegmentation of cardiac medical imaging data is pivotal in mitigating fatality\nrates associated with cardiovascular conditions. However, existing\nstate-of-the-art (SOTA) neural networks, including both CNN-based and\nTransformer-based approaches, exhibit limitations in practical applicability\ndue to their inability to effectively capture inter-slice connections alongside\nintra-slice information. This deficiency is particularly evident in datasets\nfeaturing intricate, long-range details along the z-axis, such as coronary\narteries in axial views. Additionally, SOTA methods fail to differentiate\nnon-cardiac components from myocardium in segmentation, leading to the\n\"spraying\" phenomenon. To address these challenges, we present\nRotCAtt-TransUNet++, a novel architecture tailored for robust segmentation of\ncomplex cardiac structures. Our approach emphasizes modeling global contexts by\naggregating multiscale features with nested skip connections in the encoder. It\nintegrates transformer layers to capture interactions between patches and\nemploys a rotatory attention mechanism to capture connectivity between multiple\nslices (inter-slice information). Additionally, a channel-wise cross-attention\ngate guides the fused multi-scale channel-wise information and features from\ndecoder stages to bridge semantic gaps. Experimental results demonstrate that\nour proposed model outperforms existing SOTA approaches across four cardiac\ndatasets and one abdominal dataset. Importantly, coronary arteries and\nmyocardium are annotated with near-perfect accuracy during inference. An\nablation study shows that the rotatory attention mechanism effectively\ntransforms embedded vectorized patches in the semantic dimensional space,\nenhancing segmentation accuracy.\n","authors":["Quoc-Bao Nguyen-Le","Tuan-Hy Le","Anh-Triet Do","Quoc-Huy Trinh"],"pdf_url":"https://arxiv.org/pdf/2409.05280v2.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2404.00424v2","updated":"2024-10-23T04:27:26Z","published":"2024-03-30T17:18:00Z","title":"Quantformer: from attention to profit with a quantitative transformer\n  trading strategy","summary":"  In traditional quantitative trading practice, navigating the complicated and\ndynamic financial market presents a persistent challenge. Fully capturing\nvarious market variables, including long-term information, as well as essential\nsignals that may lead to profit remains a difficult task for learning\nalgorithms. In order to tackle this challenge, this paper introduces\nquantformer, an enhanced neural network architecture based on transformers, to\nbuild investment factors. By transfer learning from sentiment analysis,\nquantformer not only exploits its original inherent advantages in capturing\nlong-range dependencies and modeling complex data relationships, but is also\nable to solve tasks with numerical inputs and accurately forecast future\nreturns over a given period. This work collects more than 5,000,000 rolling\ndata of 4,601 stocks in the Chinese capital market from 2010 to 2019. The\nresults of this study demonstrated the model's superior performance in\npredicting stock trends compared with other 100 factor-based quantitative\nstrategies. Notably, the model's innovative use of transformer-liked model to\nestablish factors, in conjunction with market sentiment information, has been\nshown to enhance the accuracy of trading signals significantly, thereby\noffering promising implications for the future of quantitative trading\nstrategies.\n","authors":["Zhaofeng Zhang","Banghao Chen","Shengxin Zhu","Nicolas Langrené"],"pdf_url":"https://arxiv.org/pdf/2404.00424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09990v4","updated":"2024-10-23T04:00:40Z","published":"2022-05-20T06:53:03Z","title":"Set-based Meta-Interpolation for Few-Task Meta-Learning","summary":"  Meta-learning approaches enable machine learning systems to adapt to new\ntasks given few examples by leveraging knowledge from related tasks. However, a\nlarge number of meta-training tasks are still required for generalization to\nunseen tasks during meta-testing, which introduces a critical bottleneck for\nreal-world problems that come with only few tasks, due to various reasons\nincluding the difficulty and cost of constructing tasks. Recently, several task\naugmentation methods have been proposed to tackle this issue using\ndomain-specific knowledge to design augmentation techniques to densify the\nmeta-training task distribution. However, such reliance on domain-specific\nknowledge renders these methods inapplicable to other domains. While Manifold\nMixup based task augmentation methods are domain-agnostic, we empirically find\nthem ineffective on non-image domains. To tackle these limitations, we propose\na novel domain-agnostic task augmentation method, Meta-Interpolation, which\nutilizes expressive neural set functions to densify the meta-training task\ndistribution using bilevel optimization. We empirically validate the efficacy\nof Meta-Interpolation on eight datasets spanning across various domains such as\nimage classification, molecule property prediction, text classification and\nspeech recognition. Experimentally, we show that Meta-Interpolation\nconsistently outperforms all the relevant baselines. Theoretically, we prove\nthat task interpolation with the set function regularizes the meta-learner to\nimprove generalization.\n","authors":["Seanie Lee","Bruno Andreis","Kenji Kawaguchi","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2205.09990v4.pdf","comment":"First two authors contributed equally. Name order decided by a coin\n  toss"},{"id":"http://arxiv.org/abs/2305.19599v4","updated":"2024-10-23T03:59:05Z","published":"2023-05-31T06:59:21Z","title":"RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine\n  Semantic Re-alignment","summary":"  Recent advances in text-to-image diffusion models have achieved remarkable\nsuccess in generating high-quality, realistic images from textual descriptions.\nHowever, these approaches have faced challenges in precisely aligning the\ngenerated visual content with the textual concepts described in the prompts. In\nthis paper, we propose a two-stage coarse-to-fine semantic re-alignment method,\nnamed RealignDiff, aimed at improving the alignment between text and images in\ntext-to-image diffusion models. In the coarse semantic re-alignment phase, a\nnovel caption reward, leveraging the BLIP-2 model, is proposed to evaluate the\nsemantic discrepancy between the generated image caption and the given text\nprompt. Subsequently, the fine semantic re-alignment stage employs a local\ndense caption generation module and a re-weighting attention modulation module\nto refine the previously generated images from a local semantic view.\nExperimental results on the MS-COCO and ViLG-300 datasets demonstrate that the\nproposed two-stage coarse-to-fine semantic re-alignment method outperforms\nother baseline re-alignment techniques by a substantial margin in both visual\nquality and semantic similarity with the input prompt.\n","authors":["Guian Fang","Zutao Jiang","Jianhua Han","Guansong Lu","Hang Xu","Shengcai Liao","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2305.19599v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15116v2","updated":"2024-10-23T03:55:34Z","published":"2024-05-24T00:14:16Z","title":"Quantifying the Gain in Weak-to-Strong Generalization","summary":"  Recent advances in large language models have shown capabilities that are\nextraordinary and near-superhuman. These models operate with such complexity\nthat reliably evaluating and aligning them proves challenging for humans. This\nleads to the natural question: can guidance from weak models (like humans)\nadequately direct the capabilities of strong models? In a recent and somewhat\nsurprising work, Burns et al. (2023) empirically demonstrated that when strong\nmodels (like GPT-4) are finetuned using labels generated by weak supervisors\n(like GPT-2), the strong models outperform their weaker counterparts -- a\nphenomenon they term weak-to-strong generalization.\n  In this work, we present a theoretical framework for understanding\nweak-to-strong generalization. Specifically, we show that the improvement in\nperformance achieved by strong models over their weaker counterparts is\nquantified by the misfit error incurred by the strong model on labels generated\nby the weaker model. Our theory reveals several curious algorithmic insights.\nFor instance, we can predict the amount by which the strong model will improve\nover the weak model, and also choose among different weak models to train the\nstrong model, based on its misfit error. We validate our theoretical findings\nthrough various empirical assessments.\n","authors":["Moses Charikar","Chirag Pabbaraju","Kirankumar Shiragur"],"pdf_url":"https://arxiv.org/pdf/2405.15116v2.pdf","comment":"19 pages; NeurIPS 2024 camera-ready version with additional\n  experiments, references and discussion"},{"id":"http://arxiv.org/abs/2410.17546v1","updated":"2024-10-23T03:53:46Z","published":"2024-10-23T03:53:46Z","title":"ProtoLens: Advancing Prototype Learning for Fine-Grained\n  Interpretability in Text Classification","summary":"  Deep neural networks have achieved remarkable performance in various\ntext-based tasks but often lack interpretability, making them less suitable for\napplications where transparency is critical. To address this, we propose\nProtoLens, a novel prototype-based model that provides fine-grained,\nsub-sentence level interpretability for text classification. ProtoLens uses a\nPrototype-aware Span Extraction module to identify relevant text spans\nassociated with learned prototypes and a Prototype Alignment mechanism to\nensure prototypes are semantically meaningful throughout training. By aligning\nthe prototype embeddings with human-understandable examples, ProtoLens provides\ninterpretable predictions while maintaining competitive accuracy. Extensive\nexperiments demonstrate that ProtoLens outperforms both prototype-based and\nnon-interpretable baselines on multiple text classification benchmarks. Code\nand data are available at\n\\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}.\n","authors":["Bowen Wei","Ziwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.17546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05741v2","updated":"2024-10-23T03:39:00Z","published":"2024-02-08T15:19:50Z","title":"Real-World Robot Applications of Foundation Models: A Review","summary":"  Recent developments in foundation models, like Large Language Models (LLMs)\nand Vision-Language Models (VLMs), trained on extensive data, facilitate\nflexible application across different tasks and modalities. Their impact spans\nvarious fields, including healthcare, education, and robotics. This paper\nprovides an overview of the practical application of foundation models in\nreal-world robotics, with a primary emphasis on the replacement of specific\ncomponents within existing robot systems. The summary encompasses the\nperspective of input-output relationships in foundation models, as well as\ntheir role in perception, motion planning, and control within the field of\nrobotics. This paper concludes with a discussion of future challenges and\nimplications for practical robot applications.\n","authors":["Kento Kawaharazuka","Tatsuya Matsushima","Andrew Gambardella","Jiaxian Guo","Chris Paxton","Andy Zeng"],"pdf_url":"https://arxiv.org/pdf/2402.05741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17538v1","updated":"2024-10-23T03:38:31Z","published":"2024-10-23T03:38:31Z","title":"Primal-Dual Spectral Representation for Off-policy Evaluation","summary":"  Off-policy evaluation (OPE) is one of the most fundamental problems in\nreinforcement learning (RL) to estimate the expected long-term payoff of a\ngiven target policy with only experiences from another behavior policy that is\npotentially unknown. The distribution correction estimation (DICE) family of\nestimators have advanced the state of the art in OPE by breaking the curse of\nhorizon. However, the major bottleneck of applying DICE estimators lies in the\ndifficulty of solving the saddle-point optimization involved, especially with\nneural network implementations. In this paper, we tackle this challenge by\nestablishing a linear representation of value function and stationary\ndistribution correction ratio, i.e., primal and dual variables in the DICE\nframework, using the spectral decomposition of the transition operator. Such\nprimal-dual representation not only bypasses the non-convex non-concave\noptimization in vanilla DICE, therefore enabling an computational efficient\nalgorithm, but also paves the way for more efficient utilization of historical\ndata. We highlight that our algorithm, SpectralDICE, is the first to leverage\nthe linear representation of primal-dual variables that is both computation and\nsample efficient, the performance of which is supported by a rigorous\ntheoretical sample complexity guarantee and a thorough empirical evaluation on\nvarious benchmarks.\n","authors":["Yang Hu","Tianyi Chen","Na Li","Kai Wang","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2410.17538v1.pdf","comment":"29 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.17532v1","updated":"2024-10-23T03:19:15Z","published":"2024-10-23T03:19:15Z","title":"Responsible Multilingual Large Language Models: A Survey of Development,\n  Applications, and Societal Impact","summary":"  Multilingual Large Language Models (MLLMs) represent a pivotal advancement in\ndemocratizing artificial intelligence across linguistic boundaries. While\ntheoretical foundations are well-established, practical implementation\nguidelines remain scattered. This work bridges this gap by providing a\ncomprehensive end-to-end framework for developing and deploying MLLMs in\nproduction environments. We make three distinctive contributions: First, we\npresent an actionable pipeline from data pre-processing through deployment,\nintegrating insights from academic research and industrial applications.\nSecond, using Llama2 as a case study, we provide detailed optimization\nstrategies for enhancing multilingual capabilities, including curriculum\nlearning approaches for balancing high-resource and low-resource languages,\ntokenization strategies, and effective sampling methods. Third, we offer an\ninterdisciplinary analysis that considers technical, linguistic, and cultural\nperspectives in MLLM development. Our findings reveal critical challenges in\nsupporting linguistic diversity, with 88.38% of world languages categorized as\nlow-resource, affecting over a billion speakers. We examine practical solutions\nthrough real-world applications in customer service, search engines, and\nmachine translation. By synthesizing theoretical frameworks with\nproduction-ready implementation strategies, this survey provides essential\nguidance for practitioners and researchers working to develop more inclusive\nand effective multilingual AI systems.\n","authors":["Junhua Liu","Bin Fu"],"pdf_url":"https://arxiv.org/pdf/2410.17532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11303v2","updated":"2024-10-23T03:00:41Z","published":"2024-10-15T05:54:17Z","title":"TSDS: Data Selection for Task-Specific Model Finetuning","summary":"  Finetuning foundation models for specific tasks is an emerging paradigm in\nmodern machine learning. The efficacy of task-specific finetuning largely\ndepends on the selection of appropriate training data. We present TSDS\n(Task-Specific Data Selection), a framework to select data for task-specific\nmodel finetuning, guided by a small but representative set of examples from the\ntarget task. To do so, we formulate data selection for task-specific finetuning\nas an optimization problem with a distribution alignment loss based on optimal\ntransport to capture the discrepancy between the selected data and the target\ndistribution. In addition, we add a regularizer to encourage the diversity of\nthe selected data and incorporate kernel density estimation into the\nregularizer to reduce the negative effects of near-duplicates among the\ncandidate data. We connect our optimization problem to nearest neighbor search\nand design efficient algorithms to compute the optimal solution based on\napproximate nearest neighbor search techniques. We evaluate our method on data\nselection for both continued pretraining and instruction tuning of language\nmodels. We show that instruction tuning using data selected by our method with\na 1% selection ratio often outperforms using the full dataset and beats the\nbaseline selection methods by 1.5 points in F1 score on average.\n","authors":["Zifan Liu","Amin Karbasi","Theodoros Rekatsinas"],"pdf_url":"https://arxiv.org/pdf/2410.11303v2.pdf","comment":"31 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.01023v2","updated":"2024-10-23T02:55:20Z","published":"2024-10-01T19:32:57Z","title":"Can visual language models resolve textual ambiguity with visual cues?\n  Let visual puns tell you!","summary":"  Humans possess multimodal literacy, allowing them to actively integrate\ninformation from various modalities to form reasoning. Faced with challenges\nlike lexical ambiguity in text, we supplement this with other modalities, such\nas thumbnail images or textbook illustrations. Is it possible for machines to\nachieve a similar multimodal understanding capability? In response, we present\nUnderstanding Pun with Image Explanations (UNPIE), a novel benchmark designed\nto assess the impact of multimodal inputs in resolving lexical ambiguities.\nPuns serve as the ideal subject for this evaluation due to their intrinsic\nambiguity. Our dataset includes 1,000 puns, each accompanied by an image that\nexplains both meanings. We pose three multimodal challenges with the\nannotations to assess different aspects of multimodal literacy; Pun Grounding,\nDisambiguation, and Reconstruction. The results indicate that various Socratic\nModels and Visual-Language Models improve over the text-only models when given\nvisual context, particularly as the complexity of the tasks increases.\n","authors":["Jiwan Chung","Seungwon Lim","Jaehyun Jeon","Seungbeen Lee","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01023v2.pdf","comment":"Accepted as main paper in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17517v1","updated":"2024-10-23T02:49:37Z","published":"2024-10-23T02:49:37Z","title":"Bridging Swarm Intelligence and Reinforcement Learning","summary":"  Swarm intelligence (SI) explores how large groups of simple individuals\n(e.g., insects, fish, birds) collaborate to produce complex behaviors,\nexemplifying that the whole is greater than the sum of its parts. A fundamental\ntask in SI is Collective Decision-Making (CDM), where a group selects the best\noption among several alternatives, such as choosing an optimal foraging site.\nIn this work, we demonstrate a theoretical and empirical equivalence between\nCDM and single-agent reinforcement learning (RL) in multi-armed bandit\nproblems, utilizing concepts from opinion dynamics, evolutionary game theory,\nand RL. This equivalence bridges the gap between SI and RL and leads us to\nintroduce a novel abstract RL update rule called Maynard-Cross Learning.\nAdditionally, it provides a new population-based perspective on common RL\npractices like learning rate adjustment and batching. Our findings enable\ncross-disciplinary fertilization between RL and SI, allowing techniques from\none field to enhance the understanding and methodologies of the other.\n","authors":["Karthik Soma","Yann Bouteiller","Heiko Hamann","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2410.17517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07840v3","updated":"2024-10-23T02:43:03Z","published":"2023-07-15T16:16:22Z","title":"RegExplainer: Generating Explanations for Graph Neural Networks in\n  Regression Task","summary":"  Graph regression is a fundamental task and has received increasing attention\nin a wide range of graph learning tasks. However, the inference process is\noften not interpretable. Most existing explanation techniques are limited to\nunderstanding GNN behaviors in classification tasks. In this work, we seek an\nexplanation to interpret the graph regression models (XAIG-R). We show that\nexisting methods overlook the distribution shifting and continuously ordered\ndecision boundary, which hinders them away from being applied in the regression\ntasks. To address these challenges, we propose a novel objective based on the\ninformation bottleneck theory and introduce a new mix-up framework, which could\nsupport various GNNs in a model-agnostic manner. We further present a\ncontrastive learning strategy to tackle the continuously ordered labels in\nregression task. To empirically verify the effectiveness of the proposed\nmethod, we introduce three benchmark datasets and a real-life dataset for\nevaluation. Extensive experiments show the effectiveness of the proposed method\nin interpreting GNN models in regression tasks.\n","authors":["Jiaxing Zhang","Zhuomin Chen","Hao Mei","Dongsheng Luo","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2307.07840v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.20279v2","updated":"2024-10-23T02:38:44Z","published":"2024-05-30T17:33:10Z","title":"CV-VAE: A Compatible Video VAE for Latent Generative Video Models","summary":"  Spatio-temporal compression of videos, utilizing networks such as Variational\nAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other\nvideo generative models. For instance, many LLM-like video models learn the\ndistribution of discrete tokens derived from 3D VAEs within the VQVAE\nframework, while most diffusion-based video models capture the distribution of\ncontinuous latent extracted by 2D VAEs without quantization. The temporal\ncompression is simply realized by uniform frame sampling which results in\nunsmooth motion between consecutive frames. Currently, there lacks of a\ncommonly used continuous video (3D) VAE for latent diffusion-based video models\nin the research community. Moreover, since current diffusion-based approaches\nare often implemented using pre-trained text-to-image (T2I) models, directly\ntraining a video VAE without considering the compatibility with existing T2I\nmodels will result in a latent space gap between them, which will take huge\ncomputational resources for training to bridge the gap even with the T2I models\nas initialization. To address this issue, we propose a method for training a\nvideo VAE of latent video models, namely CV-VAE, whose latent space is\ncompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion\n(SD). The compatibility is achieved by the proposed novel latent space\nregularization, which involves formulating a regularization loss using the\nimage VAE. Benefiting from the latent space compatibility, video models can be\ntrained seamlessly from pre-trained T2I or video models in a truly\nspatio-temporally compressed latent space, rather than simply sampling video\nframes at equal intervals. With our CV-VAE, existing video models can generate\nfour times more frames with minimal finetuning. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed video VAE.\n","authors":["Sijie Zhao","Yong Zhang","Xiaodong Cun","Shaoshu Yang","Muyao Niu","Xiaoyu Li","Wenbo Hu","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2405.20279v2.pdf","comment":"Project Page: https://ailab-cvc.github.io/cvvae/index.html"},{"id":"http://arxiv.org/abs/2410.17511v1","updated":"2024-10-23T02:29:50Z","published":"2024-10-23T02:29:50Z","title":"Time and Frequency Synergy for Source-Free Time-Series Domain\n  Adaptations","summary":"  The issue of source-free time-series domain adaptations still gains scarce\nresearch attentions. On the other hand, existing approaches rely solely on\ntime-domain features ignoring frequency components providing complementary\ninformation. This paper proposes Time Frequency Domain Adaptation (TFDA), a\nmethod to cope with the source-free time-series domain adaptation problems.\nTFDA is developed with a dual branch network structure fully utilizing both\ntime and frequency features in delivering final predictions. It induces\npseudo-labels based on a neighborhood concept where predictions of a sample\ngroup are aggregated to generate reliable pseudo labels. The concept of\ncontrastive learning is carried out in both time and frequency domains with\npseudo label information and a negative pair exclusion strategy to make valid\nneighborhood assumptions. In addition, the time-frequency consistency technique\nis proposed using the self-distillation strategy while the uncertainty\nreduction strategy is implemented to alleviate uncertainties due to the domain\nshift problem. Last but not least, the curriculum learning strategy is\nintegrated to combat noisy pseudo labels. Our experiments demonstrate the\nadvantage of our approach over prior arts with noticeable margins in benchmark\nproblems.\n","authors":["Muhammad Tanzil Furqon","Mahardhika Pratama","Ary Mazharuddin Shiddiqi","Lin Liu","Habibullah Habibullah","Kutluyil Dogancay"],"pdf_url":"https://arxiv.org/pdf/2410.17511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17510v1","updated":"2024-10-23T02:25:53Z","published":"2024-10-23T02:25:53Z","title":"Congestion Forecast for Trains with Railroad-Graph-based Semi-Supervised\n  Learning using Sparse Passenger Reports","summary":"  Forecasting rail congestion is crucial for efficient mobility in transport\nsystems. We present rail congestion forecasting using reports from passengers\ncollected through a transit application. Although reports from passengers have\nreceived attention from researchers, ensuring a sufficient volume of reports is\nchallenging due to passenger's reluctance. The limited number of reports\nresults in the sparsity of the congestion label, which can be an issue in\nbuilding a stable prediction model. To address this issue, we propose a\nsemi-supervised method for congestion forecasting for trains, or SURCONFORT.\nOur key idea is twofold: firstly, we adopt semi-supervised learning to leverage\nsparsely labeled data and many unlabeled data. Secondly, in order to complement\nthe unlabeled data from nearby stations, we design a railway network-oriented\ngraph and apply the graph to semi-supervised graph regularization. Empirical\nexperiments with actual reporting data show that SURCONFORT improved the\nforecasting performance by 14.9% over state-of-the-art methods under the label\nsparsity.\n","authors":["Soto Anno","Kota Tsubouchi","Masamichi Shimosaka"],"pdf_url":"https://arxiv.org/pdf/2410.17510v1.pdf","comment":"Accepted in ACM SIGSPATIAL 2024"},{"id":"http://arxiv.org/abs/2410.13334v2","updated":"2024-10-23T02:15:52Z","published":"2024-10-17T08:46:09Z","title":"Do LLMs Have Political Correctness? Analyzing Ethical Biases and\n  Jailbreak Vulnerabilities in AI Systems","summary":"  Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content. To address\nthese issues, many LLM developers have implemented various safety measures to\nalign these models. This alignment involves several techniques, including data\nfiltering during pre-training, supervised fine-tuning, reinforcement learning\nfrom human feedback, and red-teaming exercises. These methods often introduce\ndeliberate and intentional biases similar to Political Correctness (PC) to\nensure the ethical behavior of LLMs. In this paper, we delve into the\nintentional biases injected into LLMs for safety purposes and examine methods\nto circumvent these safety alignment techniques. Notably, these intentional\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20% between non-binary and cisgender keywords and by 16% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of PCJailbreak, highlighting the inherent risks posed by\nthese safety-induced biases. Additionally, we propose an efficient defense\nmethod PCDefense, which prevents jailbreak attempts by injecting defense\nprompts prior to generation. PCDefense stands as an appealing alternative to\nGuard Models, such as Llama-Guard, that require additional inference cost after\ntext generation. Our findings emphasize the urgent need for LLM developers to\nadopt a more responsible approach when designing and implementing safety\nmeasures.\n","authors":["Isack Lee","Haebin Seong"],"pdf_url":"https://arxiv.org/pdf/2410.13334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17506v1","updated":"2024-10-23T02:09:02Z","published":"2024-10-23T02:09:02Z","title":"Mitigating Graph Covariate Shift via Score-based Out-of-distribution\n  Augmentation","summary":"  Distribution shifts between training and testing datasets significantly\nimpair the model performance on graph learning. A commonly-taken causal view in\ngraph invariant learning suggests that stable predictive features of graphs are\ncausally associated with labels, whereas varying environmental features lead to\ndistribution shifts. In particular, covariate shifts caused by unseen\nenvironments in test graphs underscore the critical need for\nout-of-distribution (OOD) generalization. Existing graph augmentation methods\ndesigned to address the covariate shift often disentangle the stable and\nenvironmental features in the input space, and selectively perturb or mixup the\nenvironmental features. However, such perturbation-based methods heavily rely\non an accurate separation of stable and environmental features, and their\nexploration ability is confined to existing environmental features in the\ntraining distribution. To overcome these limitations, we introduce a novel\napproach using score-based graph generation strategies that synthesize unseen\nenvironmental features while preserving the validity and stable features of\noverall graph patterns. Our comprehensive empirical evaluations demonstrate the\nenhanced effectiveness of our method in improving graph OOD generalization.\n","authors":["Bohan Wang","Yurui Chang","Lu Lin"],"pdf_url":"https://arxiv.org/pdf/2410.17506v1.pdf","comment":"17 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.17504v1","updated":"2024-10-23T02:03:49Z","published":"2024-10-23T02:03:49Z","title":"An Ontology-Enabled Approach For User-Centered and Knowledge-Enabled\n  Explanations of AI Systems","summary":"  Explainable Artificial Intelligence (AI) focuses on helping humans understand\nthe working of AI systems or their decisions and has been a cornerstone of AI\nfor decades. Recent research in explainability has focused on explaining the\nworkings of AI models or model explainability. There have also been several\nposition statements and review papers detailing the needs of end-users for\nuser-centered explainability but fewer implementations. Hence, this thesis\nseeks to bridge some gaps between model and user-centered explainability. We\ncreate an explanation ontology (EO) to represent literature-derived explanation\ntypes via their supporting components. We implement a knowledge-augmented\nquestion-answering (QA) pipeline to support contextual explanations in a\nclinical setting. Finally, we are implementing a system to combine explanations\nfrom different AI methods and data modalities. Within the EO, we can represent\nfifteen different explanation types, and we have tested these representations\nin six exemplar use cases. We find that knowledge augmentations improve the\nperformance of base large language models in the contextualized QA, and the\nperformance is variable across disease groups. In the same setting, clinicians\nalso indicated that they prefer to see actionability as one of the main foci in\nexplanations. In our explanations combination method, we plan to use similarity\nmetrics to determine the similarity of explanations in a chronic disease\ndetection setting. Overall, through this thesis, we design methods that can\nsupport knowledge-enabled explanations across different use cases, accounting\nfor the methods in today's AI era that can generate the supporting components\nof these explanations and domain knowledge sources that can enhance them.\n","authors":["Shruthi Chari"],"pdf_url":"https://arxiv.org/pdf/2410.17504v1.pdf","comment":"Doctoral dissertation. Some chapters appeared as individual papers -\n  arXiv:2302.05752 is one such chapters"},{"id":"http://arxiv.org/abs/2410.17500v1","updated":"2024-10-23T01:47:55Z","published":"2024-10-23T01:47:55Z","title":"Learning Fair and Preferable Allocations through Neural Network","summary":"  The fair allocation of indivisible resources is a fundamental problem.\nExisting research has developed various allocation mechanisms or algorithms to\nsatisfy different fairness notions. For example, round robin (RR) was proposed\nto meet the fairness criterion known as envy-freeness up to one good (EF1).\nExpert algorithms without mathematical formulations are used in real-world\nresource allocation problems to find preferable outcomes for users. Therefore,\nwe aim to design mechanisms that strictly satisfy good properties with\nreplicating expert knowledge. However, this problem is challenging because such\nheuristic rules are often difficult to formalize mathematically, complicating\ntheir integration into theoretical frameworks. Additionally, formal algorithms\nstruggle to find preferable outcomes, and directly replicating these implicit\nrules can result in unfair allocations because human decision-making can\nintroduce biases. In this paper, we aim to learn implicit allocation mechanisms\nfrom examples while strictly satisfying fairness constraints, specifically\nfocusing on learning EF1 allocation mechanisms through supervised learning on\nexamples of reported valuations and corresponding allocation outcomes produced\nby implicit rules. To address this, we developed a neural RR (NRR), a novel\nneural network that parameterizes RR. NRR is built from a differentiable\nrelaxation of RR and can be trained to learn the agent ordering used for RR. We\nconducted experiments to learn EF1 allocation mechanisms from examples,\ndemonstrating that our method outperforms baselines in terms of the proximity\nof predicted allocations and other metrics.\n","authors":["Ryota Maruo","Koh Takeuchi","Hisashi Kashima"],"pdf_url":"https://arxiv.org/pdf/2410.17500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17498v1","updated":"2024-10-23T01:38:10Z","published":"2024-10-23T01:38:10Z","title":"Mechanisms of Symbol Processing for In-Context Learning in Transformer\n  Networks","summary":"  Large Language Models (LLMs) have demonstrated impressive abilities in symbol\nprocessing through in-context learning (ICL). This success flies in the face of\ndecades of predictions that artificial neural networks cannot master abstract\nsymbol manipulation. We seek to understand the mechanisms that can enable\nrobust symbol processing in transformer networks, illuminating both the\nunanticipated success, and the significant limitations, of transformers in\nsymbol processing. Borrowing insights from symbolic AI on the power of\nProduction System architectures, we develop a high-level language, PSL, that\nallows us to write symbolic programs to do complex, abstract symbol processing,\nand create compilers that precisely implement PSL programs in transformer\nnetworks which are, by construction, 100% mechanistically interpretable. We\ndemonstrate that PSL is Turing Universal, so the work can inform the\nunderstanding of transformer ICL in general. The type of transformer\narchitecture that we compile from PSL programs suggests a number of paths for\nenhancing transformers' capabilities at symbol processing. (Note: The first\nsection of the paper gives an extended synopsis of the entire paper.)\n","authors":["Paul Smolensky","Roland Fernandez","Zhenghao Herbert Zhou","Mattia Opper","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2410.17498v1.pdf","comment":"101 pages (including 30 pages of Appendices), 18 figures"},{"id":"http://arxiv.org/abs/2408.08470v2","updated":"2024-10-23T01:36:25Z","published":"2024-08-16T01:12:21Z","title":"Context-Aware Assistant Selection for Improved Inference Acceleration\n  with Large Language Models","summary":"  Despite their widespread adoption, large language models (LLMs) remain\nprohibitive to use under resource constraints, with their ever growing sizes\nonly increasing the barrier for use. One noted issue is the high latency\nassociated with auto-regressive generation, rendering large LLMs use dependent\non advanced computing infrastructure. Assisted decoding, where a smaller draft\nmodel guides a larger target model's generation, has helped alleviate this, but\nremains dependent on alignment between the two models. Thus if the draft model\nis insufficiently capable on some domain relative to the target model,\nperformance can degrade. Alternatively, one can leverage multiple draft models\nto better cover the expertise of the target, but when multiple black-box draft\nmodels are available, selecting an assistant without details about its\nconstruction can be difficult. To better understand this decision making\nproblem, we observe it as a contextual bandit, where a policy must choose a\ndraft model based on a context. We show that even without prior knowledge of\nthe draft models, creating an offline dataset from only outputs of independent\ndraft/target models and training a policy over the alignment of these outputs\ncan accelerate performance on multiple domains provided the candidates are\neffective. Further results show this to hold on various settings with multiple\nassisted decoding candidates, highlighting its flexibility and the advantageous\nrole that such decision making can play.\n","authors":["Jerry Huang","Prasanna Parthasarathi","Mehdi Rezagholizadeh","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2408.08470v2.pdf","comment":"2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP); 14 pages (9 pages main content + references + appendix)"},{"id":"http://arxiv.org/abs/2410.17489v1","updated":"2024-10-23T00:59:27Z","published":"2024-10-23T00:59:27Z","title":"Unsupervised Domain Adaptation for Action Recognition via\n  Self-Ensembling and Conditional Embedding Alignment","summary":"  Recent advancements in deep learning-based wearable human action recognition\n(wHAR) have improved the capture and classification of complex motions, but\nadoption remains limited due to the lack of expert annotations and domain\ndiscrepancies from user variations. Limited annotations hinder the model's\nability to generalize to out-of-distribution samples. While data augmentation\ncan improve generalizability, unsupervised augmentation techniques must be\napplied carefully to avoid introducing noise. Unsupervised domain adaptation\n(UDA) addresses domain discrepancies by aligning conditional distributions with\nlabeled target samples, but vanilla pseudo-labeling can lead to error\npropagation. To address these challenges, we propose $\\mu$DAR, a novel joint\noptimization architecture comprised of three functions: (i) consistency\nregularizer between augmented samples to improve model classification\ngeneralizability, (ii) temporal ensemble for robust pseudo-label generation and\n(iii) conditional distribution alignment to improve domain generalizability.\nThe temporal ensemble works by aggregating predictions from past epochs to\nsmooth out noisy pseudo-label predictions, which are then used in the\nconditional distribution alignment module to minimize kernel-based class-wise\nconditional maximum mean discrepancy ($k$CMMD) between the source and target\nfeature space to learn a domain invariant embedding. The\nconsistency-regularized augmentations ensure that multiple augmentations of the\nsame sample share the same labels; this results in (a) strong generalization\nwith limited source domain samples and (b) consistent pseudo-label generation\nin target samples. The novel integration of these three modules in $\\mu$DAR\nresults in a range of $\\approx$ 4-12% average macro-F1 score improvement over\nsix state-of-the-art UDA methods in four benchmark wHAR datasets\n","authors":["Indrajeet Ghosh","Garvit Chugh","Abu Zaher Md Faridee","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2410.17489v1.pdf","comment":"This work has been accepted to the Proceedings of the IEEE\n  International Conference on Data Mining, 2024"},{"id":"http://arxiv.org/abs/2402.01909v2","updated":"2024-10-23T00:40:23Z","published":"2024-02-02T21:21:55Z","title":"On Catastrophic Inheritance of Large Foundation Models","summary":"  Large foundation models (LFMs) are claiming incredible performances. Yet\ngreat concerns have been raised about their mythic and uninterpreted potentials\nnot only in machine learning, but also in various other disciplines. In this\nposition paper, we propose to identify a neglected issue deeply rooted in LFMs:\nCatastrophic Inheritance, describing the weaknesses and limitations inherited\nfrom biased large-scale pre-training data to behaviors of LFMs on the\ndownstream tasks, including samples that are corrupted, long-tailed, noisy,\nout-of-distributed, to name a few. Such inheritance can potentially cause\ncatastrophes to downstream applications, such as bias, lack of generalization,\ndeteriorated performance, security vulnerability, privacy leakage, and value\nmisalignment. We discuss the challenges behind this issue and propose UIM, a\nframework to Understand the catastrophic inheritance of LFMs from both\npre-training and downstream adaptation, Interpret the implications of\ncatastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims\nto unite both the machine learning and social sciences communities for more\nresponsible and promising AI development and deployment.\n","authors":["Hao Chen","Bhiksha Raj","Xing Xie","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2402.01909v2.pdf","comment":"Accepted by DMLR"},{"id":"http://arxiv.org/abs/2402.10601v2","updated":"2024-10-23T00:38:14Z","published":"2024-02-16T11:37:05Z","title":"When \"Competency\" in Reasoning Opens the Door to Vulnerability:\n  Jailbreaking LLMs via Novel Complex Ciphers","summary":"  Recent advancements in the safety of Large Language Models (LLMs) have\nprimarily focused on mitigating attacks crafted in natural language or in\ncommon encryption techniques like Base64. However, new models which often\npossess better reasoning capabilities, open the door to new attack vectors that\nwere previously non-existent in older models. This seems counter-intuitive at\nfirst glance, but these advanced models can decipher more complex cryptic\nqueries that previous models could not, making them susceptible to attacks\nusing such prompts. To exploit this vulnerability, we propose Attacks using\nCustom Encryptions (ACE), a novel method to jailbreak LLMs by leveraging custom\nencryption schemes. We evaluate the effectiveness of ACE on four\nstate-of-the-art LLMs, achieving Attack Success Rates (ASR) of up to 66% on\nclose-source models and 88% on open-source models. Building upon this, we\nintroduce Layered Attacks using Custom Encryptions (LACE), which employs\nmultiple layers of encryption through our custom ciphers to further enhance the\nASR. Our findings demonstrate that LACE significantly enhances the ability to\njailbreak LLMs, increasing the ASR of GPT-4o from 40% to 78%, a 38%\nimprovement. Our results highlight that the advanced capabilities of LLMs\nintroduce unforeseen vulnerabilities to complex attacks. Specifically complex\nand layered ciphers increase the chance of jailbreaking.\n","authors":["Divij Handa","Zehua Zhang","Amir Saeidi","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2402.10601v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17481v1","updated":"2024-10-23T00:05:33Z","published":"2024-10-23T00:05:33Z","title":"AI, Global Governance, and Digital Sovereignty","summary":"  This essay examines how Artificial Intelligence (AI) systems are becoming\nmore integral to international affairs by affecting how global governors exert\npower and pursue digital sovereignty. We first introduce a taxonomy of\nmultifaceted AI payoffs for governments and corporations related to\ninstrumental, structural, and discursive power in the domains of violence,\nmarkets, and rights. We next leverage different institutional and practice\nperspectives on sovereignty to assess how digital sovereignty is variously\nimplicated in AI-empowered global governance. States both seek sovereign\ncontrol over AI infrastructures in the institutional approach, while\nestablishing sovereign competence through AI infrastructures in the practice\napproach. Overall, we present the digital sovereignty stakes of AI as related\nto entanglements of public and private power. Rather than foreseeing technology\ncompanies as replacing states, we argue that AI systems will embed in global\ngovernance to create dueling dynamics of public/private cooperation and\ncontestation. We conclude with sketching future directions for IR research on\nAI and global governance.\n","authors":["Swati Srivastava","Justin Bullock"],"pdf_url":"https://arxiv.org/pdf/2410.17481v1.pdf","comment":"21 pages, 2 tables"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.17128v2","updated":"2024-10-23T06:51:54Z","published":"2024-10-22T16:00:44Z","title":"Understanding Transfer Learning via Mean-field Analysis","summary":"  We propose a novel framework for exploring generalization errors of transfer\nlearning through the lens of differential calculus on the space of probability\nmeasures. In particular, we consider two main transfer learning scenarios,\n$\\alpha$-ERM and fine-tuning with the KL-regularized empirical risk\nminimization and establish generic conditions under which the generalization\nerror and the population risk convergence rates for these scenarios are\nstudied. Based on our theoretical results, we show the benefits of transfer\nlearning with a one-hidden-layer neural network in the mean-field regime under\nsome suitable integrability and regularity assumptions on the loss and\nactivation functions.\n","authors":["Gholamali Aminian","Łukasz Szpruch","Samuel N. Cohen"],"pdf_url":"https://arxiv.org/pdf/2410.17128v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.17055v2","updated":"2024-10-23T12:55:39Z","published":"2024-10-22T14:36:44Z","title":"Optimal Design for Reward Modeling in RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) has become a popular\napproach to align language models (LMs) with human preferences. This method\ninvolves collecting a large dataset of human pairwise preferences across\nvarious text generations and using it to infer (implicitly or explicitly) a\nreward model. Numerous methods have been proposed to learn the reward model and\nalign a LM with it. However, the costly process of collecting human preferences\nhas received little attention and could benefit from theoretical insights. This\npaper addresses this issue and aims to formalize the reward training model in\nRLHF. We frame the selection of an effective dataset as a simple regret\nminimization task, using a linear contextual dueling bandit method. Given the\npotentially large number of arms, this approach is more coherent than the\nbest-arm identification setting. We then propose an offline framework for\nsolving this problem. Under appropriate assumptions - linearity of the reward\nmodel in the embedding space, and boundedness of the reward parameter - we\nderive bounds on the simple regret. Finally, we provide a lower bound that\nmatches our upper bound up to constant and logarithmic terms. To our knowledge,\nthis is the first theoretical contribution in this area to provide an offline\napproach as well as worst-case guarantees.\n","authors":["Antoine Scheid","Etienne Boursier","Alain Durmus","Michael I. Jordan","Pierre Ménard","Eric Moulines","Michal Valko"],"pdf_url":"https://arxiv.org/pdf/2410.17055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16928v2","updated":"2024-10-23T08:13:11Z","published":"2024-10-22T11:59:36Z","title":"xLSTM-Mixer: Multivariate Time Series Forecasting by Mixing via Scalar\n  Memories","summary":"  Time series data is prevalent across numerous fields, necessitating the\ndevelopment of robust and accurate forecasting models. Capturing patterns both\nwithin and between temporal and multivariate components is crucial for reliable\npredictions. We introduce xLSTM-Mixer, a model designed to effectively\nintegrate temporal sequences, joint time-variate information, and multiple\nperspectives for robust forecasting. Our approach begins with a linear forecast\nshared across variates, which is then refined by xLSTM blocks. These blocks\nserve as key elements for modeling the complex dynamics of challenging time\nseries data. xLSTM-Mixer ultimately reconciles two distinct views to produce\nthe final forecast. Our extensive evaluations demonstrate xLSTM-Mixer's\nsuperior long-term forecasting performance compared to recent state-of-the-art\nmethods. A thorough model analysis provides further insights into its key\ncomponents and confirms its robustness and effectiveness. This work contributes\nto the resurgence of recurrent models in time series forecasting.\n","authors":["Maurice Kraus","Felix Divo","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2410.16928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16811v2","updated":"2024-10-23T05:57:12Z","published":"2024-10-22T08:38:46Z","title":"Masked Clinical Modelling: A Framework for Synthetic and Augmented\n  Survival Data Generation","summary":"  Access to real clinical data is often restricted due to privacy obligations,\ncreating significant barriers for healthcare research. Synthetic datasets\nprovide a promising solution, enabling secure data sharing and model\ndevelopment. However, most existing approaches focus on data realism rather\nthan utility -- ensuring that models trained on synthetic data yield clinically\nmeaningful insights comparable to those trained on real data. In this paper, we\npresent Masked Clinical Modelling (MCM), a framework inspired by masked\nlanguage modelling, designed for both data synthesis and conditional data\naugmentation. We evaluate this prototype on the WHAS500 dataset using Cox\nProportional Hazards models, focusing on the preservation of hazard ratios as\nkey clinical metrics. Our results show that data generated using the MCM\nframework improves both discrimination and calibration in survival analysis,\noutperforming existing methods. MCM demonstrates strong potential to support\nsurvival data analysis and broader healthcare applications.\n","authors":["Nicholas I-Hsien Kuo","Blanca Gallego","Louisa Jorm"],"pdf_url":"https://arxiv.org/pdf/2410.16811v2.pdf","comment":"Re-archived due to incorrect ORCiD. Last edited: 2024-10-23"},{"id":"http://arxiv.org/abs/2410.16638v2","updated":"2024-10-23T03:41:49Z","published":"2024-10-22T02:27:57Z","title":"LLMScan: Causal Scan for LLM Misbehavior Detection","summary":"  Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.\n","authors":["Mengdi Zhang","Kai Kiat Goh","Peixin Zhang","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.16638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16194v3","updated":"2024-10-23T05:47:21Z","published":"2024-05-25T11:53:23Z","title":"Diffusion-Reward Adversarial Imitation Learning","summary":"  Imitation learning aims to learn a policy from observing expert\ndemonstrations without access to reward signals from environments. Generative\nadversarial imitation learning (GAIL) formulates imitation learning as\nadversarial learning, employing a generator policy learning to imitate expert\nbehaviors and discriminator learning to distinguish the expert demonstrations\nfrom agent trajectories. Despite its encouraging results, GAIL training is\noften brittle and unstable. Inspired by the recent dominance of diffusion\nmodels in generative modeling, we propose Diffusion-Reward Adversarial\nImitation Learning (DRAIL), which integrates a diffusion model into GAIL,\naiming to yield more robust and smoother rewards for policy learning.\nSpecifically, we propose a diffusion discriminative classifier to construct an\nenhanced discriminator, and design diffusion rewards based on the classifier's\noutput for policy learning. Extensive experiments are conducted in navigation,\nmanipulation, and locomotion, verifying DRAIL's effectiveness compared to prior\nimitation learning methods. Moreover, additional experimental results\ndemonstrate the generalizability and data efficiency of DRAIL. Visualized\nlearned reward functions of GAIL and DRAIL suggest that DRAIL can produce more\nrobust and smoother rewards. Project page:\nhttps://nturobotlearninglab.github.io/DRAIL/\n","authors":["Chun-Mao Lai","Hsiang-Chun Wang","Ping-Chun Hsieh","Yu-Chiang Frank Wang","Min-Hung Chen","Shao-Hua Sun"],"pdf_url":"https://arxiv.org/pdf/2405.16194v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18082v1","updated":"2024-10-23T17:59:52Z","published":"2024-10-23T17:59:52Z","title":"Prioritized Generative Replay","summary":"  Sample-efficient online reinforcement learning often uses replay buffers to\nstore experience for reuse when updating the value function. However, uniform\nreplay is inefficient, since certain classes of transitions can be more\nrelevant to learning. While prioritization of more useful samples is helpful,\nthis strategy can also lead to overfitting, as useful samples are likely to be\nmore rare. In this work, we instead propose a prioritized, parametric version\nof an agent's memory, using generative models to capture online experience.\nThis paradigm enables (1) densification of past experience, with new\ngenerations that benefit from the generative model's generalization capacity\nand (2) guidance via a family of \"relevance functions\" that push these\ngenerations towards more useful parts of an agent's acquired history. We show\nthis recipe can be instantiated using conditional diffusion models and simple\nrelevance functions such as curiosity- or value-based metrics. Our approach\nconsistently improves performance and sample efficiency in both state- and\npixel-based domains. We expose the mechanisms underlying these gains, showing\nhow guidance promotes diversity in our generated transitions and reduces\noverfitting. We also showcase how our approach can train policies with even\nhigher update-to-data ratios than before, opening up avenues to better scale\nonline RL agents.\n","authors":["Renhao Wang","Kevin Frans","Pieter Abbeel","Sergey Levine","Alexei A. Efros"],"pdf_url":"https://arxiv.org/pdf/2410.18082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18077v1","updated":"2024-10-23T17:58:49Z","published":"2024-10-23T17:58:49Z","title":"ALTA: Compiler-Based Analysis of Transformers","summary":"  We propose a new programming language called ALTA and a compiler that can map\nALTA programs to Transformer weights. ALTA is inspired by RASP, a language\nproposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler\nfrom RASP programs to Transformer weights. ALTA complements and extends this\nprior work, offering the ability to express loops and to compile programs to\nUniversal Transformers, among other advantages. ALTA allows us to\nconstructively show how Transformers can represent length-invariant algorithms\nfor computing parity and addition, as well as a solution to the SCAN benchmark\nof compositional generalization tasks, without requiring intermediate\nscratchpad decoding steps. We also propose tools to analyze cases where the\nexpressibility of an algorithm is established, but end-to-end training on a\ngiven training set fails to induce behavior consistent with the desired\nalgorithm. To this end, we explore training from ALTA execution traces as a\nmore fine-grained supervision signal. This enables additional experiments and\ntheoretical analyses relating the learnability of various algorithms to data\navailability and modeling decisions, such as positional encodings. We make the\nALTA framework -- language specification, symbolic interpreter, and weight\ncompiler -- available to the community to enable further applications and\ninsights.\n","authors":["Peter Shaw","James Cohan","Jacob Eisenstein","Kenton Lee","Jonathan Berant","Kristina Toutanova"],"pdf_url":"https://arxiv.org/pdf/2410.18077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18076v1","updated":"2024-10-23T17:58:45Z","published":"2024-10-23T17:58:45Z","title":"Leveraging Skills from Unlabeled Prior Data for Efficient Online\n  Exploration","summary":"  Unsupervised pretraining has been transformative in many supervised domains.\nHowever, applying such ideas to reinforcement learning (RL) presents a unique\nchallenge in that fine-tuning does not involve mimicking task-specific data,\nbut rather exploring and locating the solution through iterative\nself-improvement. In this work, we study how unlabeled prior trajectory data\ncan be leveraged to learn efficient exploration strategies. While prior data\ncan be used to pretrain a set of low-level skills, or as additional off-policy\ndata for online RL, it has been unclear how to combine these ideas effectively\nfor online exploration. Our method SUPE (Skills from Unlabeled Prior data for\nExploration) demonstrates that a careful combination of these ideas compounds\ntheir benefits. Our method first extracts low-level skills using a variational\nautoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an\noptimistic reward model, transforming prior data into high-level, task-relevant\nexamples. Finally, SUPE uses these transformed examples as additional\noff-policy data for online RL to learn a high-level policy that composes\npretrained low-level skills to explore efficiently. We empirically show that\nSUPE reliably outperforms prior strategies, successfully solving a suite of\nlong-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.\n","authors":["Max Wilcoxson","Qiyang Li","Kevin Frans","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.18076v1.pdf","comment":"23 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.18075v1","updated":"2024-10-23T17:57:14Z","published":"2024-10-23T17:57:14Z","title":"ProFL: Performative Robust Optimal Federated Learning","summary":"  Performative prediction (PP) is a framework that captures distribution shifts\nthat occur during the training of machine learning models due to their\ndeployment. As the trained model is used, its generated data could cause the\nmodel to evolve, leading to deviations from the original data distribution. The\nimpact of such model-induced distribution shifts in the federated learning (FL)\nsetup remains unexplored despite being increasingly likely to transpire in\nreal-life use cases. Although Jin et al. (2024) recently extended PP to FL in a\nstraightforward manner, the resulting model only converges to a performative\nstable point, which may be far from optimal. The methods in Izzo et al. (2021);\nMiller et al. (2021) can find a performative optimal point in centralized\nsettings, but they require the performative risk to be convex and the training\ndata to be noiseless, assumptions often violated in realistic FL systems. This\npaper overcomes all of these shortcomings and proposes Performative robust\noptimal Federated Learning (ProFL), an algorithm that finds performative\noptimal points in FL from noisy and contaminated data. We present the\nconvergence analysis under the Polyak-Lojasiewicz condition, which applies to\nnon-convex objectives. Extensive experiments on multiple datasets validate our\nproposed algorithms' efficiency.\n","authors":["Xue Zheng","Tian Xie","Xuwei Tan","Aylin Yener","Xueru Zhang","Ali Payani","Myungjin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18075v1.pdf","comment":"27 pages with Appendix, 18 figures. The paper has been submitted and\n  is currently under review"},{"id":"http://arxiv.org/abs/2410.18074v1","updated":"2024-10-23T17:56:33Z","published":"2024-10-23T17:56:33Z","title":"UnCLe: Unsupervised Continual Learning of Depth Completion","summary":"  We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform.\n","authors":["Suchisrit Gangopadhyay","Xien Chen","Michael Chu","Patrick Rim","Hyoungseob Park","Alex Wong"],"pdf_url":"https://arxiv.org/pdf/2410.18074v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.12568v2","updated":"2024-10-23T17:53:24Z","published":"2024-08-22T17:35:18Z","title":"Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune\n  CNNs and Transformers","summary":"  To solve ever more complex problems, Deep Neural Networks are scaled to\nbillions of parameters, leading to huge computational costs. An effective\napproach to reduce computational requirements and increase efficiency is to\nprune unnecessary components of these often over-parameterized networks.\nPrevious work has shown that attribution methods from the field of eXplainable\nAI serve as effective means to extract and prune the least relevant network\ncomponents in a few-shot fashion. We extend the current state by proposing to\nexplicitly optimize hyperparameters of attribution methods for the task of\npruning, and further include transformer-based networks in our analysis. Our\napproach yields higher model compression rates of large transformer- and\nconvolutional architectures (VGG, ResNet, ViT) compared to previous works,\nwhile still attaining high performance on ImageNet classification tasks. Here,\nour experiments indicate that transformers have a higher degree of\nover-parameterization compared to convolutional neural networks. Code is\navailable at https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.\n","authors":["Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2408.12568v2.pdf","comment":"Accepted as a workshop paper at ECCV 2024, 26 pages (11 pages\n  manuscript, 3 pages references, 12 pages appendix)"},{"id":"http://arxiv.org/abs/2410.18070v1","updated":"2024-10-23T17:53:11Z","published":"2024-10-23T17:53:11Z","title":"Training Free Guided Flow Matching with Optimal Control","summary":"  Controlled generation with pre-trained Diffusion and Flow Matching models has\nvast applications. One strategy for guiding ODE-based generative models is\nthrough optimizing a target loss $R(x_1)$ while staying close to the prior\ndistribution. Along this line, some recent work showed the effectiveness of\nguiding flow model by differentiating through its ODE sampling process. Despite\nthe superior performance, the theoretical understanding of this line of methods\nis still preliminary, leaving space for algorithm improvement. Moreover,\nexisting methods predominately focus on Euclidean data manifold, and there is a\ncompelling need for guided flow methods on complex geometries such as SO(3),\nwhich prevails in high-stake scientific applications like protein design. We\npresent OC-Flow, a general and theoretically grounded training-free framework\nfor guided flow matching using optimal control. Building upon advances in\noptimal control theory, we develop effective and practical algorithms for\nsolving optimal control in guided ODE-based generation and provide a systematic\ntheoretical analysis of the convergence guarantee in both Euclidean and SO(3).\nWe show that existing backprop-through-ODE methods can be interpreted as\nspecial cases of Euclidean OC-Flow. OC-Flow achieved superior performance in\nextensive experiments on text-guided image manipulation, conditional molecule\ngeneration, and all-atom peptide design.\n","authors":["Luran Wang","Chaoran Cheng","Yizhen Liao","Yanru Qu","Ge Liu"],"pdf_url":"https://arxiv.org/pdf/2410.18070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03185v2","updated":"2024-10-23T17:52:57Z","published":"2024-03-05T18:22:15Z","title":"Correlated Proxies: A New Definition and Improved Mitigation for Reward\n  Hacking","summary":"  Because it is difficult to precisely specify complex objectives,\nreinforcement learning policies are often optimized using flawed proxy rewards\nthat seem to capture the true objective. However, optimizing proxy rewards\nfrequently leads to reward hacking: the optimized reward function ceases to be\na good proxy, and the resulting policy performs poorly with respect to the\nunspecified true reward. Principled solutions to reward hacking have been\nimpeded by the lack of a good definition for the problem. To address this, we\nintroduce a definition of reward hacking based on the correlation between proxy\nand true rewards for states and actions seen by a \"base policy\" that breaks\ndown under optimization. We show that this definition captures reward hacking\nbehavior across several realistic settings, including in reinforcement learning\nfrom human feedback (RLHF). We then show theoretically that regularization to\nthe base policy can effectively prevent reward hacking. While current RLHF\napproaches apply a KL penalty between the action distributions of policies, our\ntheory suggests that it is more effective to regularize using the $\\chi^2$\ndivergence between the policies' occupancy measures. We intuitively show why\nthis type of regularization is superior and demonstrate that it better\nmitigates reward hacking in practice across four realistic domains, including\nRLHF for LLMs. Our code is available at https://github.com/cassidylaidlaw/orpo.\n","authors":["Cassidy Laidlaw","Shivam Singhal","Anca Dragan"],"pdf_url":"https://arxiv.org/pdf/2403.03185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18067v1","updated":"2024-10-23T17:48:28Z","published":"2024-10-23T17:48:28Z","title":"Beyond position: how rotary embeddings shape representations and memory\n  in autoregressive transfomers","summary":"  Rotary Positional Embeddings (RoPE) enhance positional encoding in\nTransformer models, yet their full impact on model dynamics remains\nunderexplored. This paper studies how RoPE introduces position-dependent\nrotations, causing phase shifts in token embeddings that influence\nhigher-frequency components within the model's internal representations.\nThrough spectral analysis, we demonstrate that RoPE's rotation matrices induce\noscillatory behaviors in embeddings, affecting information retention across\nlayers and shaping temporal modeling capabilities. We show that activation\nfunctions in feed-forward networks interact with RoPE-modulated embeddings to\ngenerate harmonics, leading to constructive or destructive interference based\non phase alignment. Our findings reveal that phase alignment amplifies\nactivations and sharpens attention, while misalignment weakens activations and\ndisrupts focus on positional patterns. This study underscores the importance of\nfrequency components as intrinsic elements of model behavior, offering new\ninsights beyond traditional analyses.\n","authors":["Valeria Ruscio","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2410.18067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18066v1","updated":"2024-10-23T17:42:54Z","published":"2024-10-23T17:42:54Z","title":"The Double-Edged Sword of Behavioral Responses in Strategic\n  Classification: Theory and User Studies","summary":"  When humans are subject to an algorithmic decision system, they can\nstrategically adjust their behavior accordingly (``game'' the system). While a\ngrowing line of literature on strategic classification has used game-theoretic\nmodeling to understand and mitigate such gaming, these existing works consider\nstandard models of fully rational agents. In this paper, we propose a strategic\nclassification model that considers behavioral biases in human responses to\nalgorithms. We show how misperceptions of a classifier (specifically, of its\nfeature weights) can lead to different types of discrepancies between biased\nand rational agents' responses, and identify when behavioral agents over- or\nunder-invest in different features. We also show that strategic agents with\nbehavioral biases can benefit or (perhaps, unexpectedly) harm the firm compared\nto fully rational strategic agents. We complement our analytical results with\nuser studies, which support our hypothesis of behavioral biases in human\nresponses to the algorithm. Together, our findings highlight the need to\naccount for human (cognitive) biases when designing AI systems, and providing\nexplanations of them, to strategic human in the loop.\n","authors":["Raman Ebrahimi","Kristen Vaccaro","Parinaz Naghizadeh"],"pdf_url":"https://arxiv.org/pdf/2410.18066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15762v2","updated":"2024-10-23T17:42:39Z","published":"2024-07-22T16:13:38Z","title":"Conditional Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning","summary":"  Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge is to develop\nsteerable language models that trade-off multiple (conflicting) objectives in a\nflexible and efficient manner. This paper presents Conditional Language Policy\n(CLP), a general framework for finetuning language models on multiple\nobjectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through extensive experiments and ablations on two\nsummarization datasets, we show that CLP learns steerable language models that\noutperform and Pareto-dominate the existing approaches for multi-objective\nfinetuning.\n","authors":["Kaiwen Wang","Rahul Kidambi","Ryan Sullivan","Alekh Agarwal","Christoph Dann","Andrea Michi","Marco Gelmi","Yunxuan Li","Raghav Gupta","Avinava Dubey","Alexandre Ramé","Johan Ferret","Geoffrey Cideron","Le Hou","Hongkun Yu","Amr Ahmed","Aranyak Mehta","Léonard Hussenot","Olivier Bachem","Edouard Leurent"],"pdf_url":"https://arxiv.org/pdf/2407.15762v2.pdf","comment":"40 pages. Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.18065v1","updated":"2024-10-23T17:42:07Z","published":"2024-10-23T17:42:07Z","title":"SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for\n  Long-Horizon Manipulation","summary":"  Robot learning has proven to be a general and effective technique for\nprogramming manipulators. Imitation learning is able to teach robots solely\nfrom human demonstrations but is bottlenecked by the capabilities of the\ndemonstrations. Reinforcement learning uses exploration to discover better\nbehaviors; however, the space of possible improvements can be too large to\nstart from scratch. And for both techniques, the learning difficulty increases\nproportional to the length of the manipulation task. Accounting for this, we\npropose SPIRE, a system that first uses Task and Motion Planning (TAMP) to\ndecompose tasks into smaller learning subproblems and second combines imitation\nand reinforcement learning to maximize their strengths. We develop novel\nstrategies to train learning agents when deployed in the context of a planning\nsystem. We evaluate SPIRE on a suite of long-horizon and contact-rich robot\nmanipulation problems. We find that SPIRE outperforms prior approaches that\nintegrate imitation learning, reinforcement learning, and planning by 35% to\n50% in average task performance, is 6 times more data efficient in the number\nof human demonstrations needed to train proficient agents, and learns to\ncomplete tasks nearly twice as efficiently. View\nhttps://sites.google.com/view/spire-corl-2024 for more details.\n","authors":["Zihan Zhou","Animesh Garg","Dieter Fox","Caelan Garrett","Ajay Mandlekar"],"pdf_url":"https://arxiv.org/pdf/2410.18065v1.pdf","comment":"Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2410.18038v1","updated":"2024-10-23T17:06:56Z","published":"2024-10-23T17:06:56Z","title":"POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM\n  Inference","summary":"  Each request in LLM inference goes through two phases: compute-bound prefill\nand memory-bandwidth-bound decode. To improve GPU utilization, recent systems\nuse hybrid batching that combines the prefill and decode phases of different\nrequests into the same batch. Hybrid batching works well for linear operations\nas it amortizes the cost of loading model weights from HBM. However, attention\ncomputation in hybrid batches remains inefficient because existing attention\nkernels are optimized for either prefill or decode.\n  In this paper, we present POD-Attention -- the first GPU kernel that\nefficiently computes attention for hybrid batches. POD-Attention aims to\nmaximize the utilization of both compute and memory bandwidth by carefully\nallocating the GPU's resources such that prefill and decode operations happen\nconcurrently on the same multiprocessor. We integrate POD-Attention in a\nstate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up\nattention computation by up to 75% (mean 28%) and increases LLM serving\nthroughput by up to 22% in offline inference. In online inference,\nPOD-Attention enables lower time-to-first-token (TTFT), time-between-tokens\n(TBT), and request execution latency versus Sarathi-Serve.\n","authors":["Aditya K Kamath","Ramya Prabhu","Jayashree Mohan","Simon Peter","Ramachandran Ramjee","Ashish Panwar"],"pdf_url":"https://arxiv.org/pdf/2410.18038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04429v2","updated":"2024-10-23T16:42:06Z","published":"2024-09-06T17:49:56Z","title":"VILA-U: a Unified Foundation Model Integrating Visual Understanding and\n  Generation","summary":"  VILA-U is a Unified foundation model that integrates Video, Image, Language\nunderstanding and generation. Traditional visual language models (VLMs) use\nseparate modules for understanding and generating visual content, which can\nlead to misalignment and increased complexity. In contrast, VILA-U employs a\nsingle autoregressive next-token prediction framework for both tasks,\neliminating the need for additional components like diffusion models. This\napproach not only simplifies the model but also achieves near state-of-the-art\nperformance in visual language understanding and generation. The success of\nVILA-U is attributed to two main factors: the unified vision tower that aligns\ndiscrete visual tokens with textual inputs during pretraining, which enhances\nvisual perception, and autoregressive image generation can achieve similar\nquality as diffusion models with high-quality dataset. This allows VILA-U to\nperform comparably to more complex models using a fully token-based\nautoregressive framework.\n","authors":["Yecheng Wu","Zhuoyang Zhang","Junyu Chen","Haotian Tang","Dacheng Li","Yunhao Fang","Ligeng Zhu","Enze Xie","Hongxu Yin","Li Yi","Song Han","Yao Lu"],"pdf_url":"https://arxiv.org/pdf/2409.04429v2.pdf","comment":"Code: https://github.com/mit-han-lab/vila-u. The first two authors\n  contributed equally to this work"},{"id":"http://arxiv.org/abs/2409.19841v2","updated":"2024-10-23T16:27:27Z","published":"2024-09-30T00:47:13Z","title":"Counter-Current Learning: A Biologically Plausible Dual Network Approach\n  for Deep Learning","summary":"  Despite its widespread use in neural networks, error backpropagation has\nfaced criticism for its lack of biological plausibility, suffering from issues\nsuch as the backward locking problem and the weight transport problem. These\nlimitations have motivated researchers to explore more biologically plausible\nlearning algorithms that could potentially shed light on how biological neural\nsystems adapt and learn. Inspired by the counter-current exchange mechanisms\nobserved in biological systems, we propose counter-current learning (CCL), a\nbiologically plausible framework for credit assignment in neural networks. This\nframework employs a feedforward network to process input data and a feedback\nnetwork to process targets, with each network enhancing the other through\nanti-parallel signal propagation. By leveraging the more informative signals\nfrom the bottom layer of the feedback network to guide the updates of the top\nlayer of the feedforward network and vice versa, CCL enables the simultaneous\ntransformation of source inputs to target outputs and the dynamic mutual\ninfluence of these transformations. Experimental results on MNIST,\nFashionMNIST, CIFAR10, and CIFAR100 datasets using multi-layer perceptrons and\nconvolutional neural networks demonstrate that CCL achieves comparable\nperformance to other biologically plausible algorithms while offering a more\nbiologically realistic learning mechanism. Furthermore, we showcase the\napplicability of our approach to an autoencoder task, underscoring its\npotential for unsupervised representation learning. Our work presents a\ndirection for biologically inspired and plausible learning algorithms, offering\nan alternative mechanism of learning and adaptation in neural networks.\n","authors":["Chia-Hsiang Kao","Bharath Hariharan"],"pdf_url":"https://arxiv.org/pdf/2409.19841v2.pdf","comment":"Accepted at NeurIPS 2024. Code available at\n  https://github.com/IandRover/CCL-NeurIPS24"},{"id":"http://arxiv.org/abs/2409.17270v2","updated":"2024-10-23T16:27:20Z","published":"2024-09-25T18:35:45Z","title":"Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.\n","authors":["Debargha Ganguly","Srinivasan Iyengar","Vipin Chaudhary","Shivkumar Kalyanaraman"],"pdf_url":"https://arxiv.org/pdf/2409.17270v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) System 2 Reasoning At Scale Workshop"},{"id":"http://arxiv.org/abs/2410.18003v1","updated":"2024-10-23T16:25:36Z","published":"2024-10-23T16:25:36Z","title":"Inferring stability properties of chaotic systems on autoencoders'\n  latent spaces","summary":"  The data-driven learning of solutions of partial differential equations can\nbe based on a divide-and-conquer strategy. First, the high dimensional data is\ncompressed to a latent space with an autoencoder; and, second, the temporal\ndynamics are inferred on the latent space with a form of recurrent neural\nnetwork. In chaotic systems and turbulence, convolutional autoencoders and echo\nstate networks (CAE-ESN) successfully forecast the dynamics, but little is\nknown about whether the stability properties can also be inferred. We show that\nthe CAE-ESN model infers the invariant stability properties and the geometry of\nthe tangent space in the low-dimensional manifold (i.e. the latent space)\nthrough Lyapunov exponents and covariant Lyapunov vectors. This work opens up\nnew opportunities for inferring the stability of high-dimensional chaotic\nsystems in latent spaces.\n","authors":["Elise Özalp","Luca Magri"],"pdf_url":"https://arxiv.org/pdf/2410.18003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10992v2","updated":"2024-10-23T16:19:06Z","published":"2024-06-24T09:29:14Z","title":"AlleNoise: large-scale text classification benchmark dataset with\n  real-world label noise","summary":"  Label noise remains a challenge for training robust classification models.\nMost methods for mitigating label noise have been benchmarked using primarily\ndatasets with synthetic noise. While the need for datasets with realistic noise\ndistribution has partially been addressed by web-scraped benchmarks such as\nWebVision and Clothing1M, those benchmarks are restricted to the computer\nvision domain. With the growing importance of Transformer-based models, it is\ncrucial to establish text classification benchmarks for learning with noisy\nlabels. In this paper, we present AlleNoise, a new curated text classification\nbenchmark dataset with real-world instance-dependent label noise, containing\nover 500,000 examples across approximately 5,600 classes, complemented with a\nmeaningful, hierarchical taxonomy of categories. The noise distribution comes\nfrom actual users of a major e-commerce marketplace, so it realistically\nreflects the semantics of human mistakes. In addition to the noisy labels, we\nprovide human-verified clean labels, which help to get a deeper insight into\nthe noise distribution, unlike web-scraped datasets typically used in the\nfield. We demonstrate that a representative selection of established methods\nfor learning with noisy labels is inadequate to handle such real-world noise.\nIn addition, we show evidence that these algorithms do not alleviate excessive\nmemorization. As such, with AlleNoise, we set the bar high for the development\nof label noise methods that can handle real-world label noise in text\nclassification tasks. The code and dataset are available for download at\nhttps://github.com/allegro/AlleNoise.\n","authors":["Alicja Rączkowska","Aleksandra Osowska-Kurczab","Jacek Szczerbiński","Kalina Jasinska-Kobus","Klaudia Nazarko"],"pdf_url":"https://arxiv.org/pdf/2407.10992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17998v1","updated":"2024-10-23T16:12:59Z","published":"2024-10-23T16:12:59Z","title":"Estimating the Spectral Moments of the Kernel Integral Operator from\n  Finite Sample Matrices","summary":"  Analyzing the structure of sampled features from an input data distribution\nis challenging when constrained by limited measurements in both the number of\ninputs and features. Traditional approaches often rely on the eigenvalue\nspectrum of the sample covariance matrix derived from finite measurement\nmatrices; however, these spectra are sensitive to the size of the measurement\nmatrix, leading to biased insights. In this paper, we introduce a novel\nalgorithm that provides unbiased estimates of the spectral moments of the\nkernel integral operator in the limit of infinite inputs and features from\nfinitely sampled measurement matrices. Our method, based upon dynamic\nprogramming, is efficient and capable of estimating the moments of the operator\nspectrum. We demonstrate the accuracy of our estimator on radial basis function\n(RBF) kernels, highlighting its consistency with the theoretical spectra.\nFurthermore, we showcase the practical utility and robustness of our method in\nunderstanding the geometry of learned representations in neural networks.\n","authors":["Chanwoo Chun","SueYeon Chung","Daniel D. Lee"],"pdf_url":"https://arxiv.org/pdf/2410.17998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02131v4","updated":"2024-10-23T16:05:11Z","published":"2024-06-04T09:18:20Z","title":"CondTSF: One-line Plugin of Dataset Condensation for Time Series\n  Forecasting","summary":"  Dataset condensation is a newborn technique that generates a small dataset\nthat can be used in training deep neural networks to lower training costs. The\nobjective of dataset condensation is to ensure that the model trained with the\nsynthetic dataset can perform comparably to the model trained with full\ndatasets. However, existing methods predominantly concentrate on classification\ntasks, posing challenges in their adaptation to time series forecasting\n(TS-forecasting). This challenge arises from disparities in the evaluation of\nsynthetic data. In classification, the synthetic data is considered\nwell-distilled if the model trained with the full dataset and the model trained\nwith the synthetic dataset yield identical labels for the same input,\nregardless of variations in output logits distribution. Conversely, in\nTS-forecasting, the effectiveness of synthetic data distillation is determined\nby the distance between predictions of the two models. The synthetic data is\ndeemed well-distilled only when all data points within the predictions are\nsimilar. Consequently, TS-forecasting has a more rigorous evaluation\nmethodology compared to classification. To mitigate this gap, we theoretically\nanalyze the optimization objective of dataset condensation for TS-forecasting\nand propose a new one-line plugin of dataset condensation designated as Dataset\nCondensation for Time Series Forecasting (CondTSF) based on our analysis.\nPlugging CondTSF into previous dataset condensation methods facilitates a\nreduction in the distance between the predictions of the model trained with the\nfull dataset and the model trained with the synthetic dataset, thereby\nenhancing performance. We conduct extensive experiments on eight commonly used\ntime series datasets. CondTSF consistently improves the performance of all\nprevious dataset condensation methods across all datasets, particularly at low\ncondensing ratios.\n","authors":["Jianrong Ding","Zhanyu Liu","Guanjie Zheng","Haiming Jin","Linghe Kong"],"pdf_url":"https://arxiv.org/pdf/2406.02131v4.pdf","comment":"Accepted by NeurIPS 2024, the project can be found at\n  https://github.com/RafaDD/CondTSF"},{"id":"http://arxiv.org/abs/2304.10650v2","updated":"2024-10-23T16:03:19Z","published":"2023-04-20T21:25:33Z","title":"Learning a quantum computer's capability","summary":"  Accurately predicting a quantum computer's capability -- which circuits it\ncan run and how well it can run them -- is a foundational goal of quantum\ncharacterization and benchmarking. As modern quantum computers become\nincreasingly hard to simulate, we must develop accurate and scalable predictive\ncapability models to help researchers and stakeholders decide which quantum\ncomputers to build and use. In this work, we propose a hardware-agnostic method\nto efficiently construct scalable predictive models of a quantum computer's\ncapability for almost any class of circuits, and demonstrate our method using\nconvolutional neural networks (CNNs). Our CNN-based approach works by\nefficiently representing a circuit as a three-dimensional tensor and then using\na CNN to predict its success rate. Our CNN capability models obtain\napproximately a $1\\%$ average absolute prediction error when modeling\nprocessors experiencing both Markovian and non-Markovian stochastic Pauli\nerrors. We also apply our CNNs to model the capabilities of cloud-access\nquantum computing systems, obtaining moderate prediction accuracy (average\nabsolute error around $2-5\\%$), and we highlight the challenges to building\nbetter neural network capability models.\n","authors":["Daniel Hothem","Kevin Young","Tommie Catanach","Timothy Proctor"],"pdf_url":"https://arxiv.org/pdf/2304.10650v2.pdf","comment":"20 pages, 11 figures, plus appendices"},{"id":"http://arxiv.org/abs/2410.17986v1","updated":"2024-10-23T16:00:14Z","published":"2024-10-23T16:00:14Z","title":"Federated Transformer: Multi-Party Vertical Federated Learning on\n  Practical Fuzzily Linked Data","summary":"  Federated Learning (FL) is an evolving paradigm that enables multiple parties\nto collaboratively train models without sharing raw data. Among its variants,\nVertical Federated Learning (VFL) is particularly relevant in real-world,\ncross-organizational collaborations, where distinct features of a shared\ninstance group are contributed by different parties. In these scenarios,\nparties are often linked using fuzzy identifiers, leading to a common practice\ntermed as multi-party fuzzy VFL. Existing models generally address either\nmulti-party VFL or fuzzy VFL between two parties. Extending these models to\npractical multi-party fuzzy VFL typically results in significant performance\ndegradation and increased costs for maintaining privacy. To overcome these\nlimitations, we introduce the Federated Transformer (FeT), a novel framework\nthat supports multi-party VFL with fuzzy identifiers. FeT innovatively encodes\nthese identifiers into data representations and employs a transformer\narchitecture distributed across different parties, incorporating three new\ntechniques to enhance performance. Furthermore, we have developed a multi-party\nprivacy framework for VFL that integrates differential privacy with secure\nmulti-party computation, effectively protecting local representations while\nminimizing associated utility costs. Our experiments demonstrate that the FeT\nsurpasses the baseline models by up to 46\\% in terms of accuracy when scaled to\n50 parties. Additionally, in two-party fuzzy VFL settings, FeT also shows\nimproved performance and privacy over cutting-edge VFL models.\n","authors":["Zhaomin Wu","Junyi Hou","Yiqun Diao","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2410.17986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17980v1","updated":"2024-10-23T15:51:13Z","published":"2024-10-23T15:51:13Z","title":"Stick-breaking Attention","summary":"  The self-attention mechanism traditionally relies on the softmax operator,\nnecessitating positional embeddings like RoPE, or position biases to account\nfor token order. But current methods using still face length generalisation\nchallenges. We propose an alternative attention mechanism based on the\nstick-breaking process: For each token before the current, we determine a break\npoint $\\beta_{i,j}$, which represents the proportion of the remaining stick to\nallocate to the current token. We repeat the process until the stick is fully\nallocated, resulting in a sequence of attention weights. This process naturally\nincorporates recency bias, which has linguistic motivations for grammar parsing\n(Shen et. al., 2017). We study the implications of replacing the conventional\nsoftmax-based attention mechanism with stick-breaking attention. We then\ndiscuss implementation of numerically stable stick-breaking attention and adapt\nFlash Attention to accommodate this mechanism. When used as a drop-in\nreplacement for current softmax+RoPE attention systems, we find that\nstick-breaking attention performs competitively with current methods on length\ngeneralisation and downstream tasks. Stick-breaking also performs well at\nlength generalisation, allowing a model trained with $2^{11}$ context window to\nperform well at $2^{14}$ with perplexity improvements.\n","authors":["Shawn Tan","Yikang Shen","Songlin Yang","Aaron Courville","Rameswar Panda"],"pdf_url":"https://arxiv.org/pdf/2410.17980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02447v3","updated":"2024-10-23T15:48:45Z","published":"2024-06-04T16:12:27Z","title":"Federated Class-Incremental Learning with Hierarchical Generative\n  Prototypes","summary":"  Federated Learning (FL) aims at unburdening the training of deep models by\ndistributing computation across multiple devices (clients) while safeguarding\ndata privacy. On top of that, Federated Continual Learning (FCL) also accounts\nfor data distribution evolving over time, mirroring the dynamic nature of\nreal-world environments. While previous studies have identified Catastrophic\nForgetting and Client Drift as primary causes of performance degradation in\nFCL, we shed light on the importance of Incremental Bias and Federated Bias,\nwhich cause models to prioritize classes that are recently introduced or\nlocally predominant, respectively. Our proposal constrains both biases in the\nlast layer by efficiently finetuning a pre-trained backbone using learnable\nprompts, resulting in clients that produce less biased representations and more\nbiased classifiers. Therefore, instead of solely relying on parameter\naggregation, we leverage generative prototypes to effectively balance the\npredictions of the global model. Our method significantly improves the current\nState Of The Art, providing an average increase of +7.8% in accuracy. Code to\nreproduce the results is provided in the suppl. material.\n","authors":["Riccardo Salami","Pietro Buzzega","Matteo Mosconi","Mattia Verasani","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2406.02447v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14979v2","updated":"2024-10-23T15:43:28Z","published":"2024-10-19T05:01:56Z","title":"Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration","summary":"  Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.\n","authors":["Wei Xie","Shuoyoucheng Ma","Zhenhua Wang","Enze Wang","Baosheng Wang","Jinshu Su"],"pdf_url":"https://arxiv.org/pdf/2410.14979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17976v1","updated":"2024-10-23T15:39:08Z","published":"2024-10-23T15:39:08Z","title":"metasnf: Meta Clustering with Similarity Network Fusion in R","summary":"  metasnf is an R package that enables users to apply meta clustering, a method\nfor efficiently searching a broad space of cluster solutions by clustering the\nsolutions themselves, to clustering workflows based on similarity network\nfusion (SNF). SNF is a multi-modal data integration algorithm commonly used for\nbiomedical subtype discovery. The package also contains functions to assist\nwith cluster visualization, characterization, and validation. This package can\nhelp researchers identify SNF-derived cluster solutions that are guided by\ncontext-specific utility over context-agnostic measures of quality.\n","authors":["Prashanth S Velayudhan","Xiaoqiao Xu","Prajkta Kallurkar","Ana Patricia Balbon","Maria T Secara","Adam Taback","Denise Sabac","Nicholas Chan","Shihao Ma","Bo Wang","Daniel Felsky","Stephanie H Ameis","Brian Cox","Colin Hawco","Lauren Erdman","Anne L Wheeler"],"pdf_url":"https://arxiv.org/pdf/2410.17976v1.pdf","comment":"72 pages, 22 figures, submitted to Journal of Statistical Software"},{"id":"http://arxiv.org/abs/2410.17970v1","updated":"2024-10-23T15:36:08Z","published":"2024-10-23T15:36:08Z","title":"Optical Generative Models","summary":"  Generative models cover various application areas, including image, video and\nmusic synthesis, natural language processing, and molecular design, among many\nothers. As digital generative models become larger, scalable inference in a\nfast and energy-efficient manner becomes a challenge. Here, we present optical\ngenerative models inspired by diffusion models, where a shallow and fast\ndigital encoder first maps random noise into phase patterns that serve as\noptical generative seeds for a desired data distribution; a jointly-trained\nfree-space-based reconfigurable decoder all-optically processes these\ngenerative seeds to create novel images (never seen before) following the\ntarget data distribution. Except for the illumination power and the random seed\ngeneration through a shallow encoder, these optical generative models do not\nconsume computing power during the synthesis of novel images. We report the\noptical generation of monochrome and multi-color novel images of handwritten\ndigits, fashion products, butterflies, and human faces, following the data\ndistributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets,\nrespectively, achieving an overall performance comparable to digital neural\nnetwork-based generative models. To experimentally demonstrate optical\ngenerative models, we used visible light to generate, in a snapshot, novel\nimages of handwritten digits and fashion products. These optical generative\nmodels might pave the way for energy-efficient, scalable and rapid inference\ntasks, further exploiting the potentials of optics and photonics for artificial\nintelligence-generated content.\n","authors":["Shiqi Chen","Yuhang Li","Hanlong Chen","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2410.17970v1.pdf","comment":"24 Pages, 9 Figures"},{"id":"http://arxiv.org/abs/2410.11709v2","updated":"2024-10-23T15:35:57Z","published":"2024-10-15T15:46:03Z","title":"On the potential of Optimal Transport in Geospatial Data Science","summary":"  Prediction problems in geographic information science and transportation are\noften motivated by the possibility to enhance operational efficiency and\nthereby reduce emissions. Examples range from predicting car sharing demand for\nrelocation planning to forecasting traffic congestion for navigation purposes.\nHowever, conventional accuracy metrics ignore the spatial distribution of the\nerrors, despite its relevance for operations. Here, we put forward a spatially\naware evaluation metric and loss function based on Optimal Transport (OT). Our\nframework leverages partial OT and can minimize relocation costs in any spatial\nprediction problem. We showcase the advantages of OT-based evaluation over\nconventional metrics and further demonstrate the application of an OT loss\nfunction for improving forecasts of bike sharing demand and charging station\noccupancy. Thus, our framework not only aligns with operational considerations,\nbut also signifies a step forward in refining predictions within geospatial\napplications. All code is available at https://github.com/mie-lab/geospatialOT.\n","authors":["Nina Wiedemann","Théo Uscidda","Martin Raubal"],"pdf_url":"https://arxiv.org/pdf/2410.11709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17967v1","updated":"2024-10-23T15:34:11Z","published":"2024-10-23T15:34:11Z","title":"POMDP-Driven Cognitive Massive MIMO Radar: Joint Target\n  Detection-Tracking In Unknown Disturbances","summary":"  The joint detection and tracking of a moving target embedded in an unknown\ndisturbance represents a key feature that motivates the development of the\ncognitive radar paradigm. Building upon recent advancements in robust target\ndetection with multiple-input multiple-output (MIMO) radars, this work explores\nthe application of a Partially Observable Markov Decision Process (POMDP)\nframework to enhance the tracking and detection tasks in a statistically\nunknown environment. In the POMDP setup, the radar system is considered as an\nintelligent agent that continuously senses the surrounding environment,\noptimizing its actions to maximize the probability of detection $(P_D)$ and\nimprove the target position and velocity estimation, all this while keeping a\nconstant probability of false alarm $(P_{FA})$. The proposed approach employs\nan online algorithm that does not require any apriori knowledge of the noise\nstatistics, and it relies on a much more general observation model than the\ntraditional range-azimuth-elevation model employed by conventional tracking\nalgorithms. Simulation results clearly show substantial performance improvement\nof the POMDP-based algorithm compared to the State-Action-Reward-State-Action\n(SARSA)-based one that has been recently investigated in the context of massive\nMIMO (MMIMO) radar systems.\n","authors":["Imad Bouhou","Stefano Fortunati","Leila Gharsalli","Alexandre Renaux"],"pdf_url":"https://arxiv.org/pdf/2410.17967v1.pdf","comment":"The paper has been submitted to ieee Transactions on radar systems"},{"id":"http://arxiv.org/abs/2401.11576v3","updated":"2024-10-23T15:30:50Z","published":"2024-01-21T19:53:17Z","title":"Quantum Architecture Search with Unsupervised Representation Learning","summary":"  Unsupervised representation learning presents new opportunities for advancing\nQuantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ)\ndevices. QAS is designed to optimize quantum circuits for Variational Quantum\nAlgorithms (VQAs). Most QAS algorithms tightly couple the search space and\nsearch algorithm, typically requiring the evaluation of numerous quantum\ncircuits, resulting in high computational costs and limiting scalability to\nlarger quantum circuits. Predictor-based QAS algorithms mitigate this issue by\nestimating circuit performance based on structure or embedding. However, these\nmethods often demand time-intensive labeling to optimize gate parameters across\nmany circuits, which is crucial for training accurate predictors. Inspired by\nthe classical neural architecture search algorithm Arch2vec, we investigate the\npotential of unsupervised representation learning for QAS without relying on\npredictors. Our framework decouples unsupervised architecture representation\nlearning from the search process, enabling the learned representations to be\napplied across various downstream tasks. Additionally, it integrates an\nimproved quantum circuit graph encoding scheme, addressing the limitations of\nexisting representations and enhancing search efficiency. This predictor-free\napproach removes the need for large labeled datasets. During the search, we\nemploy REINFORCE and Bayesian Optimization to explore the latent representation\nspace and compare their performance against baseline methods. Our results\ndemonstrate that the framework efficiently identifies high-performing quantum\ncircuits with fewer search iterations.\n","authors":["Yize Sun","Zixin Wu","Yunpu Ma","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2401.11576v3.pdf","comment":"9 Pages, quantum architecture search, unsupervised representation\n  learning"},{"id":"http://arxiv.org/abs/2410.17963v1","updated":"2024-10-23T15:30:37Z","published":"2024-10-23T15:30:37Z","title":"A Time-Aware Approach to Early Detection of Anorexia: UNSL at eRisk 2024","summary":"  The eRisk laboratory aims to address issues related to early risk detection\non the Web. In this year's edition, three tasks were proposed, where Task 2 was\nabout early detection of signs of anorexia. Early risk detection is a problem\nwhere precision and speed are two crucial objectives. Our research group solved\nTask 2 by defining a CPI+DMC approach, addressing both objectives\nindependently, and a time-aware approach, where precision and speed are\nconsidered a combined single-objective. We implemented the last approach by\nexplicitly integrating time during the learning process, considering the\nERDE{\\theta} metric as the training objective. It also allowed us to\nincorporate temporal metrics to validate and select the optimal models. We\nachieved outstanding results for the ERDE50 metric and ranking-based metrics,\ndemonstrating consistency in solving ERD problems.\n","authors":["Horacio Thompson","Marcelo Errecalde"],"pdf_url":"https://arxiv.org/pdf/2410.17963v1.pdf","comment":"In Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble,\n  France"},{"id":"http://arxiv.org/abs/2410.17961v1","updated":"2024-10-23T15:30:13Z","published":"2024-10-23T15:30:13Z","title":"Closed-form merging of parameter-efficient modules for Federated\n  Continual Learning","summary":"  Model merging has emerged as a crucial technique in Deep Learning, enabling\nthe integration of multiple models into a unified system while preserving\nperformance and scalability. In this respect, the compositional properties of\nlow-rank adaptation techniques (e.g., LoRA) have proven beneficial, as simple\naveraging LoRA modules yields a single model that mostly integrates the\ncapabilities of all individual modules. Building on LoRA, we take a step\nfurther by imposing that the merged model matches the responses of all learned\nmodules. Solving this objective in closed form yields an indeterminate system\nwith A and B as unknown variables, indicating the existence of infinitely many\nclosed-form solutions. To address this challenge, we introduce LoRM, an\nalternating optimization strategy that trains one LoRA matrix at a time. This\nallows solving for each unknown variable individually, thus finding a unique\nsolution. We apply our proposed methodology to Federated Class-Incremental\nLearning (FCIL), ensuring alignment of model responses both between clients and\nacross tasks. Our method demonstrates state-of-the-art performance across a\nrange of FCIL scenarios.\n","authors":["Riccardo Salami","Pietro Buzzega","Matteo Mosconi","Jacopo Bonato","Luigi Sabetta","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2410.17961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08649v2","updated":"2024-10-23T15:29:28Z","published":"2024-06-12T21:18:14Z","title":"MOTIVE: A Drug-Target Interaction Graph For Inductive Link Prediction","summary":"  Drug-target interaction (DTI) prediction is crucial for identifying new\ntherapeutics and detecting mechanisms of action. While structure-based methods\naccurately model physical interactions between a drug and its protein target,\ncell-based assays such as Cell Painting can better capture complex DTI\ninteractions. This paper introduces MOTIVE, a Morphological cOmpound Target\nInteraction Graph dataset comprising Cell Painting features for 11,000 genes\nand 3,600 compounds, along with their relationships extracted from seven\npublicly available databases. We provide random, cold-source (new drugs), and\ncold-target (new genes) data splits to enable rigorous evaluation under\nrealistic use cases. Our benchmark results show that graph neural networks that\nuse Cell Painting features consistently outperform those that learn from graph\nstructure alone, feature-based models, and topological heuristics. MOTIVE\naccelerates both graph ML research and drug discovery by promoting the\ndevelopment of more reliable DTI prediction models. MOTIVE resources are\navailable at https://github.com/carpenter-singh-lab/motive.\n","authors":["John Arevalo","Ellen Su","Anne E Carpenter","Shantanu Singh"],"pdf_url":"https://arxiv.org/pdf/2406.08649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09063v3","updated":"2024-10-23T15:28:59Z","published":"2023-05-15T23:12:15Z","title":"Bounded KRnet and its applications to density estimation and\n  approximation","summary":"  In this paper, we develop an invertible mapping, called B-KRnet, on a bounded\ndomain and apply it to density estimation/approximation for data or the\nsolutions of PDEs such as the Fokker-Planck equation and the Keller-Segel\nequation. Similar to KRnet, the structure of B-KRnet adapts the\npseudo-triangular structure into a normalizing flow model. The main difference\nbetween B-KRnet and KRnet is that B-KRnet is defined on a hypercube while KRnet\nis defined on the whole space, in other words, a new mechanism is introduced in\nB-KRnet to maintain the exact invertibility. Using B-KRnet as a transport map,\nwe obtain an explicit probability density function (PDF) model that corresponds\nto the pushforward of a prior (uniform) distribution on the hypercube. It can\nbe directly applied to density estimation when only data are available. By\ncoupling KRnet and B-KRnet, we define a deep generative model on a\nhigh-dimensional domain where some dimensions are bounded and other dimensions\nare unbounded. A typical case is the solution of the stationary kinetic\nFokker-Planck equation, which is a PDF of position and momentum. Based on\nB-KRnet, we develop an adaptive learning approach to approximate partial\ndifferential equations whose solutions are PDFs or can be treated as PDFs. A\nvariety of numerical experiments is presented to demonstrate the effectiveness\nof B-KRnet.\n","authors":["Li Zeng","Xiaoliang Wan","Tao Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.09063v3.pdf","comment":"26 pages, 13 figures"},{"id":"http://arxiv.org/abs/2201.12091v5","updated":"2024-10-23T15:28:38Z","published":"2022-01-28T13:00:17Z","title":"Linear Adversarial Concept Erasure","summary":"  Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem. We formulate the problem of identifying and erasing a linear subspace\nthat corresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear maximin\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, \\method, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod is highly expressive, effectively mitigating bias in deep nonlinear\nclassifiers while maintaining tractability and interpretability.\n","authors":["Shauli Ravfogel","Michael Twiton","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12091v5.pdf","comment":"Accepted in ICML 2022; a revised version"},{"id":"http://arxiv.org/abs/2410.17959v1","updated":"2024-10-23T15:28:25Z","published":"2024-10-23T15:28:25Z","title":"Medical Imaging Complexity and its Effects on GAN Performance","summary":"  The proliferation of machine learning models in diverse clinical applications\nhas led to a growing need for high-fidelity, medical image training data. Such\ndata is often scarce due to cost constraints and privacy concerns. Alleviating\nthis burden, medical image synthesis via generative adversarial networks (GANs)\nemerged as a powerful method for synthetically generating photo-realistic\nimages based on existing sets of real medical images. However, the exact image\nset size required to efficiently train such a GAN is unclear. In this work, we\nexperimentally establish benchmarks that measure the relationship between a\nsample dataset size and the fidelity of the generated images, given the\ndataset's distribution of image complexities. We analyze statistical metrics\nbased on delentropy, an image complexity measure rooted in Shannon's entropy in\ninformation theory. For our pipeline, we conduct experiments with two\nstate-of-the-art GANs, StyleGAN 3 and SPADE-GAN, trained on multiple medical\nimaging datasets with variable sample sizes. Across both GANs, general\nperformance improved with increasing training set size but suffered with\nincreasing complexity.\n","authors":["William Cagas","Chan Ko","Blake Hsiao","Shryuk Grandhi","Rishi Bhattacharya","Kevin Zhu","Michael Lam"],"pdf_url":"https://arxiv.org/pdf/2410.17959v1.pdf","comment":"Accepted to ACCV, Workshop on Generative AI for Synthetic Medical\n  Data"},{"id":"http://arxiv.org/abs/2410.17957v1","updated":"2024-10-23T15:27:37Z","published":"2024-10-23T15:27:37Z","title":"MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers","summary":"  In this paper, we propose MCUBERT to enable language models like BERT on tiny\nmicrocontroller units (MCUs) through network and scheduling co-optimization. We\nobserve the embedding table contributes to the major storage bottleneck for\ntiny BERT models. Hence, at the network level, we propose an MCU-aware\ntwo-stage neural architecture search algorithm based on clustered low-rank\napproximation for embedding compression. To reduce the inference memory\nrequirements, we further propose a novel fine-grained MCU-friendly scheduling\nstrategy. Through careful computation tiling and re-ordering as well as kernel\ndesign, we drastically increase the input sequence lengths supported on MCUs\nwithout any latency or accuracy penalty. MCUBERT reduces the parameter size of\nBERT-tiny and BERT-mini by 5.7$\\times$ and 3.0$\\times$ and the execution memory\nby 3.5$\\times$ and 4.3$\\times$, respectively. MCUBERT also achieves 1.5$\\times$\nlatency reduction. For the first time, MCUBERT enables lightweight BERT models\non commodity MCUs and processing more than 512 tokens with less than 256KB of\nmemory.\n","authors":["Zebin Yang","Renze Chen","Taiqiang Wu","Ngai Wong","Yun Liang","Runsheng Wang","Ru Huang","Meng Li"],"pdf_url":"https://arxiv.org/pdf/2410.17957v1.pdf","comment":"ICCAD 2024"},{"id":"http://arxiv.org/abs/2410.17952v1","updated":"2024-10-23T15:24:16Z","published":"2024-10-23T15:24:16Z","title":"SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains","summary":"  Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.\n","authors":["Ran Xu","Hui Liu","Sreyashi Nag","Zhenwei Dai","Yaochen Xie","Xianfeng Tang","Chen Luo","Yang Li","Joyce C. Ho","Carl Yang","Qi He"],"pdf_url":"https://arxiv.org/pdf/2410.17952v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.17948v1","updated":"2024-10-23T15:22:21Z","published":"2024-10-23T15:22:21Z","title":"Generalized Resubstitution for Regression Error Estimation","summary":"  We propose generalized resubstitution error estimators for regression, a\nbroad family of estimators, each corresponding to a choice of empirical\nprobability measures and loss function. The usual sum of squares criterion is a\nspecial case corresponding to the standard empirical probability measure and\nthe quadratic loss. Other choices of empirical probability measure lead to more\ngeneral estimators with superior bias and variance properties. We prove that\nthese error estimators are consistent under broad assumptions. In addition,\nprocedures for choosing the empirical measure based on the method of moments\nand maximum pseudo-likelihood are proposed and investigated. Detailed\nexperimental results using polynomial regression demonstrate empirically the\nsuperior finite-sample bias and variance properties of the proposed estimators.\nThe R code for the experiments is provided.\n","authors":["Diego Marcondes","Ulisses Braga-Neto"],"pdf_url":"https://arxiv.org/pdf/2410.17948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17945v1","updated":"2024-10-23T15:18:07Z","published":"2024-10-23T15:18:07Z","title":"Theoretically Grounded Pruning of Large Ground Sets for Constrained,\n  Discrete Optimization","summary":"  Modern instances of combinatorial optimization problems often exhibit\nbillion-scale ground sets, which have many uninformative or redundant elements.\nIn this work, we develop light-weight pruning algorithms to quickly discard\nelements that are unlikely to be part of an optimal solution. Under mild\nassumptions on the instance, we prove theoretical guarantees on the fraction of\nthe optimal value retained and the size of the resulting pruned ground set.\nThrough extensive experiments on real-world datasets for various applications,\nwe demonstrate that our algorithm, QuickPrune, efficiently prunes over 90% of\nthe ground set and outperforms state-of-the-art classical and machine learning\nheuristics for pruning.\n","authors":["Ankur Nath","Alan Kuhnle"],"pdf_url":"https://arxiv.org/pdf/2410.17945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17943v1","updated":"2024-10-23T15:15:56Z","published":"2024-10-23T15:15:56Z","title":"Optimizing Travel Itineraries with AI Algorithms in a Microservices\n  Architecture: Balancing Cost, Time, Preferences, and Sustainability","summary":"  The objective of this research is how an implementation of AI algorithms in\nthe microservices architecture enhances travel itineraries by cost, time, user\npreferences, and environmental sustainability. It uses machine learning models\nfor both cost forecasting and personalization, genetic algorithm for\noptimization of the itinerary, and heuristics for sustainability checking.\nPrimary evaluated parameters consist of latency, ability to satisfy user\npreferences, cost and environmental concern. The experimental results\ndemonstrate an average of 4.5 seconds of response time on 1000 concurrent users\nand 92% of user preferences accuracy. The cost efficiency is proved, with 95%\nof provided trips being within the limits of the budget declared by the user.\nThe system also implements some measures to alleviate negative externalities\nrelated to travel and 60% of offered travel plans had green options\nincorporated, resulting in the average 15% lower carbon emissions than the\ntraditional travel plans offered. The genetic algorithm with time complexity\nO(g.p.f) provides the optimal solution in 100 generations. Every iteration\nimproves the quality of the solution by 5%, thus enabling its effective use in\noptimization problems where time is measured in seconds. Finally, the system is\ndesigned to be fault-tolerant with functional 99.9% availability which allows\nthe provision of services even when requirements are exceeded. Travel\noptimization platform is turned dynamic and efficient by this microservices\nbased architecture which provides enhanced scaling, allows asynchronous\ncommunication and real time changes. Because of the incorporation of Ai, cost\ncontrol and eco-friendliness approaches, the system addresses the different\nuser needs in the present days travel business.\n","authors":["Biman Barua","M. Shamim Kaiser"],"pdf_url":"https://arxiv.org/pdf/2410.17943v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17941v1","updated":"2024-10-23T15:09:02Z","published":"2024-10-23T15:09:02Z","title":"Spiking Graph Neural Network on Riemannian Manifolds","summary":"  Graph neural networks (GNNs) have become the dominant solution for learning\non graphs, the typical non-Euclidean structures. Conventional GNNs, constructed\nwith the Artificial Neuron Network (ANN), have achieved impressive performance\nat the cost of high computation and energy consumption. In parallel, spiking\nGNNs with brain-like spiking neurons are drawing increasing research attention\nowing to the energy efficiency. So far, existing spiking GNNs consider graphs\nin Euclidean space, ignoring the structural geometry, and suffer from the high\nlatency issue due to Back-Propagation-Through-Time (BPTT) with the surrogate\ngradient. In light of the aforementioned issues, we are devoted to exploring\nspiking GNN on Riemannian manifolds, and present a Manifold-valued Spiking GNN\n(MSG). In particular, we design a new spiking neuron on geodesically complete\nmanifolds with the diffeomorphism, so that BPTT regarding the spikes is\nreplaced by the proposed differentiation via manifold. Theoretically, we show\nthat MSG approximates a solver of the manifold ordinary differential equation.\nExtensive experiments on common graphs show the proposed MSG achieves superior\nperformance to previous spiking GNNs and energy efficiency to conventional\nGNNs.\n","authors":["Li Sun","Zhenhao Huang","Qiqi Wan","Hao Peng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2410.17941v1.pdf","comment":"Accepted by NeurIPS 2024, 30 pages"},{"id":"http://arxiv.org/abs/2406.14856v2","updated":"2024-10-23T15:08:59Z","published":"2024-06-21T04:02:19Z","title":"Accessible, At-Home Detection of Parkinson's Disease via Multi-task\n  Video Analysis","summary":"  Limited accessibility to neurological care leads to underdiagnosed\nParkinson's Disease (PD), preventing early intervention. Existing AI-based PD\ndetection methods primarily focus on unimodal analysis of motor or speech\ntasks, overlooking the multifaceted nature of the disease. To address this, we\nintroduce a large-scale, multi-task video dataset consisting of 1102 sessions\n(each containing videos of finger tapping, facial expression, and speech tasks\ncaptured via webcam) from 845 participants (272 with PD). We propose a novel\nUncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal\ndata to enhance diagnostic accuracy. UFNet employs independent task-specific\nnetworks, trained with Monte Carlo Dropout for uncertainty quantification,\nfollowed by self-attended fusion of features, with attention weights\ndynamically adjusted based on task-specific uncertainties. To ensure\npatient-centered evaluation, the participants were randomly split into three\nsets: 60% for training, 20% for model selection, and 20% for final performance\nevaluation. UFNet significantly outperformed single-task models in terms of\naccuracy, area under the ROC curve (AUROC), and sensitivity while maintaining\nnon-inferior specificity. Withholding uncertain predictions further boosted the\nperformance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9%\nsensitivity, and 92.6+-0.3% specificity, at the expense of not being able to\npredict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further\nanalysis suggests that the trained model does not exhibit any detectable bias\nacross sex and ethnic subgroups and is most effective for individuals aged\nbetween 50 and 80. Requiring only a webcam and microphone, our approach\nfacilitates accessible home-based PD screening, especially in regions with\nlimited healthcare resources.\n","authors":["Md Saiful Islam","Tariq Adnan","Jan Freyberg","Sangwu Lee","Abdelrahman Abdelkader","Meghan Pawlik","Cathe Schwartz","Karen Jaffe","Ruth B. Schneider","E Ray Dorsey","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2406.14856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03093v2","updated":"2024-10-23T15:01:34Z","published":"2024-08-06T10:48:15Z","title":"Certifiably Robust Policies for Uncertain Parametric Environments","summary":"  We present a data-driven approach for producing policies that are provably\nrobust across unknown stochastic environments. Existing approaches can learn\nmodels of a single environment as an interval Markov decision processes (IMDP)\nand produce a robust policy with a probably approximately correct (PAC)\nguarantee on its performance. However these are unable to reason about the\nimpact of environmental parameters underlying the uncertainty. We propose a\nframework based on parametric Markov decision processes (MDPs) with unknown\ndistributions over parameters. We learn and analyse IMDPs for a set of unknown\nsample environments induced by parameters. The key challenge is then to produce\nmeaningful performance guarantees that combine the two layers of uncertainty:\n(1) multiple environments induced by parameters with an unknown distribution;\n(2) unknown induced environments which are approximated by IMDPs. We present a\nnovel approach based on scenario optimisation that yields a single PAC\nguarantee quantifying the risk level for which a specified performance level\ncan be assured in unseen environments, plus a means to trade-off risk and\nperformance. We implement and evaluate our framework using multiple robust\npolicy generation methods on a range of benchmarks. We show that our approach\nproduces tight bounds on a policy's performance with high confidence.\n","authors":["Yannik Schnitzer","Alessandro Abate","David Parker"],"pdf_url":"https://arxiv.org/pdf/2408.03093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17935v1","updated":"2024-10-23T15:00:30Z","published":"2024-10-23T15:00:30Z","title":"Semi-Implicit Functional Gradient Flow","summary":"  Particle-based variational inference methods (ParVIs) use non-parametric\nvariational families represented by particles to approximate the target\ndistribution according to the kernelized Wasserstein gradient flow for the\nKullback-Leibler (KL) divergence. Recent works introduce functional gradient\nflows to substitute the kernel for better flexibility. However, the\ndeterministic updating mechanism may suffer from limited exploration and\nrequire expensive repetitive runs for new samples. In this paper, we propose\nSemi-Implicit Functional Gradient flow (SIFG), a functional gradient ParVI\nmethod that uses perturbed particles as the approximation family. The\ncorresponding functional gradient flow, which can be estimated via denoising\nscore matching, exhibits strong theoretical convergence guarantee. We also\npresent an adaptive version of our method to automatically choose the suitable\nnoise magnitude. Extensive experiments demonstrate the effectiveness and\nefficiency of the proposed framework on both simulated and real data problems.\n","authors":["Shiyue Zhang","Ziheng Cheng","Cheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17935v1.pdf","comment":"31 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.17934v1","updated":"2024-10-23T14:59:06Z","published":"2024-10-23T14:59:06Z","title":"Retrieving snow depth distribution by downscaling ERA5 Reanalysis with\n  ICESat-2 laser altimetry","summary":"  Estimating the variability of seasonal snow cover, in particular snow depth\nin remote areas, poses significant challenges due to limited spatial and\ntemporal data availability. This study uses snow depth measurements from the\nICESat-2 satellite laser altimeter, which are sparse in both space and time,\nand incorporates them with climate reanalysis data into a\ndownscaling-calibration scheme to produce monthly gridded snow depth maps at\nmicroscale (10 m). Snow surface elevation measurements from ICESat-2 along\nprofiles are compared to a digital elevation model to determine snow depth at\neach point. To efficiently turn sparse measurements into snow depth maps, a\nregression model is fitted to establish a relationship between the retrieved\nsnow depth and the corresponding ERA5 Land snow depth. This relationship,\nreferred to as subgrid variability, is then applied to downscale the monthly\nERA5 Land snow depth data. The method can provide timeseries of monthly snow\ndepth maps for the entire ERA5 time range (since 1950). The validation of\ndownscaled snow depth data was performed at an intermediate scale (100 m x 500\nm) using datasets from airborne laser scanning (ALS) in the Hardangervidda\nregion of southern Norway. Results show that snow depth prediction achieved R2\nvalues ranging from 0.74 to 0.88 (post-calibration). The method relies on\nglobally available data and is applicable to other snow regions above the\ntreeline. Though requiring area-specific calibration, our approach has the\npotential to provide snow depth maps in areas where no such data exist and can\nbe used to extrapolate existing snow surveys in time and over larger areas.\nWith this, it can offer valuable input data for hydrological, ecological or\npermafrost modeling tasks.\n","authors":["Zhihao Liu","Simon Filhol","Désirée Treichler"],"pdf_url":"https://arxiv.org/pdf/2410.17934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17933v1","updated":"2024-10-23T14:55:53Z","published":"2024-10-23T14:55:53Z","title":"Multi-Continental Healthcare Modelling Using Blockchain-Enabled\n  Federated Learning","summary":"  One of the biggest challenges of building artificial intelligence (AI) model\nin healthcare area is the data sharing. Since healthcare data is private,\nsensitive, and heterogeneous, collecting sufficient data for modelling is\nexhausted, costly, and sometimes impossible. In this paper, we propose a\nframework for global healthcare modelling using datasets from multi-continents\n(Europe, North America and Asia) while without sharing the local datasets, and\nchoose glucose management as a study model to verify its effectiveness.\nTechnically, blockchain-enabled federated learning is implemented with adaption\nto make it meet with the privacy and safety requirements of healthcare data,\nmeanwhile rewards honest participation and penalize malicious activities using\nits on-chain incentive mechanism. Experimental results show that the proposed\nframework is effective, efficient, and privacy preserved. Its prediction\naccuracy is much better than the models trained from limited personal data and\nis similar to, and even slightly better than, the results from a centralized\ndataset. This work paves the way for international collaborations on healthcare\nprojects, where additional data is crucial for reducing bias and providing\nbenefits to humanity.\n","authors":["Rui Sun","Zhipeng Wang","Hengrui Zhang","Ming Jiang","Yizhe Wen","Jiqun Zhang","Jiahao Sun","Shuoying Zhang","Erwu Liu","Kezhi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17933v1.pdf","comment":"Accepted by IEEE Global Blockchain Conference"},{"id":"http://arxiv.org/abs/2402.04033v3","updated":"2024-10-23T14:50:51Z","published":"2024-02-06T14:26:22Z","title":"On provable privacy vulnerabilities of graph representations","summary":"  Graph representation learning (GRL) is critical for extracting insights from\ncomplex network structures, but it also raises security concerns due to\npotential privacy vulnerabilities in these representations. This paper\ninvestigates the structural vulnerabilities in graph neural models where\nsensitive topological information can be inferred through edge reconstruction\nattacks. Our research primarily addresses the theoretical underpinnings of\nsimilarity-based edge reconstruction attacks (SERA), furnishing a\nnon-asymptotic analysis of their reconstruction capacities. Moreover, we\npresent empirical corroboration indicating that such attacks can perfectly\nreconstruct sparse graphs as graph size increases. Conversely, we establish\nthat sparsity is a critical factor for SERA's effectiveness, as demonstrated\nthrough analysis and experiments on (dense) stochastic block models. Finally,\nwe explore the resilience of private graph representations produced via noisy\naggregation (NAG) mechanism against SERA. Through theoretical analysis and\nempirical assessments, we affirm the mitigation of SERA using NAG . In\nparallel, we also empirically delineate instances wherein SERA demonstrates\nboth efficacy and deficiency in its capacity to function as an instrument for\nelucidating the trade-off between privacy and utility.\n","authors":["Ruofan Wu","Guanhua Fang","Qiying Pan","Mingyang Zhang","Tengfei Liu","Weiqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.04033v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01959v2","updated":"2024-10-23T14:49:39Z","published":"2024-06-04T04:39:51Z","title":"Adaptive Variance Reduction for Stochastic Optimization under Weaker\n  Assumptions","summary":"  This paper explores adaptive variance reduction methods for stochastic\noptimization based on the STORM technique. Existing adaptive extensions of\nSTORM rely on strong assumptions like bounded gradients and bounded function\nvalues, or suffer an additional $\\mathcal{O}(\\log T)$ term in the convergence\nrate. To address these limitations, we introduce a novel adaptive STORM method\nthat achieves an optimal convergence rate of $\\mathcal{O}(T^{-1/3})$ for\nnon-convex functions with our newly designed learning rate strategy. Compared\nwith existing approaches, our method requires weaker assumptions and attains\nthe optimal convergence rate without the additional $\\mathcal{O}(\\log T)$ term.\nWe also extend the proposed technique to stochastic compositional optimization,\nobtaining the same optimal rate of $\\mathcal{O}(T^{-1/3})$. Furthermore, we\ninvestigate the non-convex finite-sum problem and develop another innovative\nadaptive variance reduction method that achieves an optimal convergence rate of\n$\\mathcal{O}(n^{1/4} T^{-1/2} )$, where $n$ represents the number of component\nfunctions. Numerical experiments across various tasks validate the\neffectiveness of our method.\n","authors":["Wei Jiang","Sifan Yang","Yibo Wang","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.01959v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.00489"},{"id":"http://arxiv.org/abs/2310.10107v4","updated":"2024-10-23T14:47:42Z","published":"2023-10-16T06:41:13Z","title":"Posterior Sampling-based Online Learning for Episodic POMDPs","summary":"  Learning in POMDPs is known to be significantly harder than in MDPs. In this\npaper, we consider the online learning problem for episodic POMDPs with unknown\ntransition and observation models. We propose a Posterior Sampling-based\nreinforcement learning algorithm for POMDPs (PS4POMDPs), which is much simpler\nand more implementable compared to state-of-the-art optimism-based online\nlearning algorithms for POMDPs. We show that the Bayesian regret of the\nproposed algorithm scales as the square root of the number of episodes and is\npolynomial in the other parameters. In a general setting, the regret scales\nexponentially in the horizon length $H$, and we show that this is inevitable by\nproviding a lower bound. However, when the POMDP is undercomplete and weakly\nrevealing (a common assumption in the recent literature), we establish a\npolynomial Bayesian regret bound. We finally propose a posterior sampling\nalgorithm for multi-agent POMDPs, and show it too has sublinear regret.\n","authors":["Dengwang Tang","Dongze Ye","Rahul Jain","Ashutosh Nayyar","Pierluigi Nuzzo"],"pdf_url":"https://arxiv.org/pdf/2310.10107v4.pdf","comment":"41 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.00489v2","updated":"2024-10-23T14:42:35Z","published":"2024-06-01T16:38:43Z","title":"Efficient Sign-Based Optimization: Accelerating Convergence via Variance\n  Reduction","summary":"  Sign stochastic gradient descent (signSGD) is a communication-efficient\nmethod that transmits only the sign of stochastic gradients for parameter\nupdating. Existing literature has demonstrated that signSGD can achieve a\nconvergence rate of $\\mathcal{O}(d^{1/2}T^{-1/4})$, where $d$ represents the\ndimension and $T$ is the iteration number. In this paper, we improve this\nconvergence rate to $\\mathcal{O}(d^{1/2}T^{-1/3})$ by introducing the\nSign-based Stochastic Variance Reduction (SSVR) method, which employs variance\nreduction estimators to track gradients and leverages their signs to update.\nFor finite-sum problems, our method can be further enhanced to achieve a\nconvergence rate of $\\mathcal{O}(m^{1/4}d^{1/2}T^{-1/2})$, where $m$ denotes\nthe number of component functions. Furthermore, we investigate the\nheterogeneous majority vote in distributed settings and introduce two novel\nalgorithms that attain improved convergence rates of\n$\\mathcal{O}(d^{1/2}T^{-1/2} + dn^{-1/2})$ and $\\mathcal{O}(d^{1/4}T^{-1/4})$\nrespectively, outperforming the previous results of $\\mathcal{O}(dT^{-1/4} +\ndn^{-1/2})$ and $\\mathcal{O}(d^{3/8}T^{-1/8})$, where $n$ represents the number\nof nodes. Numerical experiments across different tasks validate the\neffectiveness of our proposed methods.\n","authors":["Wei Jiang","Sifan Yang","Wenhao Yang","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17918v1","updated":"2024-10-23T14:34:39Z","published":"2024-10-23T14:34:39Z","title":"Addressing Asynchronicity in Clinical Multimodal Fusion via\n  Individualized Chest X-ray Generation","summary":"  Integrating multi-modal clinical data, such as electronic health records\n(EHR) and chest X-ray images (CXR), is particularly beneficial for clinical\nprediction tasks. However, in a temporal setting, multi-modal data are often\ninherently asynchronous. EHR can be continuously collected but CXR is generally\ntaken with a much longer interval due to its high cost and radiation dose. When\nclinical prediction is needed, the last available CXR image might have been\noutdated, leading to suboptimal predictions. To address this challenge, we\npropose DDL-CXR, a method that dynamically generates an up-to-date latent\nrepresentation of the individualized CXR images. Our approach leverages latent\ndiffusion models for patient-specific generation strategically conditioned on a\nprevious CXR image and EHR time series, providing information regarding\nanatomical structures and disease progressions, respectively. In this way, the\ninteraction across modalities could be better captured by the latent CXR\ngeneration process, ultimately improving the prediction performance.\nExperiments using MIMIC datasets show that the proposed model could effectively\naddress asynchronicity in multimodal fusion and consistently outperform\nexisting methods.\n","authors":["Wenfang Yao","Chen Liu","Kejing Yin","William K. Cheung","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2410.17918v1.pdf","comment":"Accepted by NeurIPS-24"},{"id":"http://arxiv.org/abs/2410.17917v1","updated":"2024-10-23T14:34:36Z","published":"2024-10-23T14:34:36Z","title":"regAL: Python Package for Active Learning of Regression Problems","summary":"  Increasingly more research areas rely on machine learning methods to\naccelerate discovery while saving resources. Machine learning models, however,\nusually require large datasets of experimental or computational results, which\nin certain fields, such as (bio)chemistry, materials science, or medicine, are\nrarely given and often prohibitively expensive to obtain. To bypass that\nobstacle, active learning methods are employed to develop machine learning\nmodels with a desired performance while requiring the least possible number of\ncomputational or experimental results from the domain of application. For this\npurpose, the model's knowledge about certain regions of the application domain\nis estimated to guide the choice of the model's training set. Although active\nlearning is widely studied for classification problems (discrete outcomes),\ncomparatively few works handle this method for regression problems (continuous\noutcomes). In this work, we present our Python package regAL, which allows\nusers to evaluate different active learning strategies for regression problems.\nWith a minimal input of just the dataset in question, but many additional\ncustomization and insight options, this package is intended for anyone who aims\nto perform and understand active learning in their problem-specific scope.\n","authors":["Elizaveta Surzhikova","Jonny Proppe"],"pdf_url":"https://arxiv.org/pdf/2410.17917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17913v1","updated":"2024-10-23T14:33:11Z","published":"2024-10-23T14:33:11Z","title":"Deep learning for model correction of dynamical systems with data\n  scarcity","summary":"  We present a deep learning framework for correcting existing dynamical system\nmodels utilizing only a scarce high-fidelity data set. In many practical\nsituations, one has a low-fidelity model that can capture the dynamics\nreasonably well but lacks high resolution, due to the inherent limitation of\nthe model and the complexity of the underlying physics. When high resolution\ndata become available, it is natural to seek model correction to improve the\nresolution of the model predictions. We focus on the case when the amount of\nhigh-fidelity data is so small that most of the existing data driven modeling\nmethods cannot be applied. In this paper, we address these challenges with a\nmodel-correction method which only requires a scarce high-fidelity data set.\nOur method first seeks a deep neural network (DNN) model to approximate the\nexisting low-fidelity model. By using the scarce high-fidelity data, the method\nthen corrects the DNN model via transfer learning (TL). After TL, an improved\nDNN model with high prediction accuracy to the underlying dynamics is obtained.\nOne distinct feature of the propose method is that it does not assume a\nspecific form of the model correction terms. Instead, it offers an inherent\ncorrection to the low-fidelity model via TL. A set of numerical examples are\npresented to demonstrate the effectiveness of the proposed method.\n","authors":["Caroline Tatsuoka","Dongbin Xiu"],"pdf_url":"https://arxiv.org/pdf/2410.17913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04377v3","updated":"2024-10-23T14:29:56Z","published":"2024-08-08T11:22:52Z","title":"Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon","summary":"  Anomaly detection in time series data is a critical challenge across various\ndomains. Traditional methods typically focus on identifying anomalies in\nimmediate subsequent steps, often underestimating the significance of temporal\ndynamics such as delay time and horizons of anomalies, which generally require\nextensive post-analysis. This paper introduces a novel approach for time series\nanomaly prediction, incorporating temporal information directly into the\nprediction results. We propose a new dataset specifically designed to evaluate\nthis approach and conduct comprehensive experiments using several\nstate-of-the-art methods. Our results demonstrate the efficacy of our approach\nin providing timely and accurate anomaly predictions, setting a new benchmark\nfor future research in this field.\n","authors":["Jiang You","Arben Cela","René Natowicz","Jacob Ouanounou","Patrick Siarry"],"pdf_url":"https://arxiv.org/pdf/2408.04377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16164v3","updated":"2024-10-23T14:24:50Z","published":"2024-05-25T10:15:51Z","title":"Acquiring Better Load Estimates by Combining Anomaly and Change Point\n  Detection in Power Grid Time-series Measurements","summary":"  In this paper we present novel methodology for automatic anomaly and switch\nevent filtering to improve load estimation in power grid systems. By leveraging\nunsupervised methods with supervised optimization, our approach prioritizes\ninterpretability while ensuring robust and generalizable performance on unseen\ndata. Through experimentation, a combination of binary segmentation for change\npoint detection and statistical process control for anomaly detection emerges\nas the most effective strategy, specifically when ensembled in a novel\nsequential manner. Results indicate the clear wasted potential when filtering\nis not applied. The automatic load estimation is also fairly accurate, with\napproximately 90% of estimates falling within a 10% error margin, with only a\nsingle significant failure in both the minimum and maximum load estimates\nacross 60 measurements in the test set. Our methodology's interpretability\nmakes it particularly suitable for critical infrastructure planning, thereby\nenhancing decision-making processes.\n","authors":["Roel Bouman","Linda Schmeitz","Luco Buise","Jacco Heres","Yuliya Shapovalova","Tom Heskes"],"pdf_url":"https://arxiv.org/pdf/2405.16164v3.pdf","comment":"All code can be found at: https://github.com/RoelBouman/StormPhase2"},{"id":"http://arxiv.org/abs/2410.17904v1","updated":"2024-10-23T14:22:49Z","published":"2024-10-23T14:22:49Z","title":"Reinforcement Learning under Latent Dynamics: Toward Statistical and\n  Algorithmic Modularity","summary":"  Real-world applications of reinforcement learning often involve environments\nwhere agents operate on complex, high-dimensional observations, but the\nunderlying (''latent'') dynamics are comparatively simple. However, outside of\nrestrictive settings such as small latent spaces, the fundamental statistical\nrequirements and algorithmic principles for reinforcement learning under latent\ndynamics are poorly understood.\n  This paper addresses the question of reinforcement learning under\n$\\textit{general}$ latent dynamics from a statistical and algorithmic\nperspective. On the statistical side, our main negative result shows that most\nwell-studied settings for reinforcement learning with function approximation\nbecome intractable when composed with rich observations; we complement this\nwith a positive result, identifying latent pushforward coverability as a\ngeneral condition that enables statistical tractability. Algorithmically, we\ndevelop provably efficient observable-to-latent reductions -- that is,\nreductions that transform an arbitrary algorithm for the latent MDP into an\nalgorithm that can operate on rich observations -- in two settings: one where\nthe agent has access to hindsight observations of the latent dynamics [LADZ23],\nand one where the agent can estimate self-predictive latent models [SAGHCB20].\nTogether, our results serve as a first step toward a unified statistical and\nalgorithmic theory for reinforcement learning under latent dynamics.\n","authors":["Philip Amortila","Dylan J. Foster","Nan Jiang","Akshay Krishnamurthy","Zakaria Mhammedi"],"pdf_url":"https://arxiv.org/pdf/2410.17904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17898v1","updated":"2024-10-23T14:16:34Z","published":"2024-10-23T14:16:34Z","title":"Scalable Offline Reinforcement Learning for Mean Field Games","summary":"  Reinforcement learning algorithms for mean-field games offer a scalable\nframework for optimizing policies in large populations of interacting agents.\nExisting methods often depend on online interactions or access to system\ndynamics, limiting their practicality in real-world scenarios where such\ninteractions are infeasible or difficult to model. In this paper, we present\nOffline Munchausen Mirror Descent (Off-MMD), a novel mean-field RL algorithm\nthat approximates equilibrium policies in mean-field games using purely offline\ndata. By leveraging iterative mirror descent and importance sampling\ntechniques, Off-MMD estimates the mean-field distribution from static datasets\nwithout relying on simulation or environment dynamics. Additionally, we\nincorporate techniques from offline reinforcement learning to address common\nissues like Q-value overestimation, ensuring robust policy learning even with\nlimited data coverage. Our algorithm scales to complex environments and\ndemonstrates strong performance on benchmark tasks like crowd exploration or\nnavigation, highlighting its applicability to real-world multi-agent systems\nwhere online experimentation is infeasible. We empirically demonstrate the\nrobustness of Off-MMD to low-quality datasets and conduct experiments to\ninvestigate its sensitivity to hyperparameter choices.\n","authors":["Axel Brunnbauer","Julian Lemmel","Zahra Babaiee","Sophie Neubauer","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2410.17898v1.pdf","comment":"Submitted to AAMAS"},{"id":"http://arxiv.org/abs/2407.19353v2","updated":"2024-10-23T14:11:34Z","published":"2024-07-28T00:07:20Z","title":"A spring-block theory of feature learning in deep neural networks","summary":"  Feature-learning deep nets progressively collapse data to a regular\nlow-dimensional geometry. How this phenomenon emerges from collective action of\nnonlinearity, noise, learning rate, and other choices that shape the dynamics,\nhas eluded first-principles theories built from microscopic neuronal dynamics.\nWe exhibit a noise-nonlinearity phase diagram that identifies regimes where\nshallow or deep layers learn more effectively. We then propose a macroscopic\nmechanical theory that reproduces the diagram, explaining why some DNNs are\nlazy and some active, and linking feature learning across layers to\ngeneralization.\n","authors":["Cheng Shi","Liming Pan","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2407.19353v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15592v2","updated":"2024-10-23T14:08:10Z","published":"2024-10-21T02:21:56Z","title":"CPE-Pro: A Structure-Sensitive Deep Learning Method for Protein\n  Representation and Origin Evaluation","summary":"  Protein structures are important for understanding their functions and\ninteractions. Currently, many protein structure prediction methods are\nenriching the structure database. Discriminating the origin of structures is\ncrucial for distinguishing between experimentally resolved and computationally\npredicted structures, evaluating the reliability of prediction methods, and\nguiding downstream biological studies. Building on works in structure\nprediction, We developed a structure-sensitive supervised deep learning model,\nCrystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent\nand discriminate the origin of protein structures. CPE-Pro learns the\nstructural information of proteins and captures inter-structural differences to\nachieve accurate traceability on four data classes, and is expected to be\nextended to more. Simultaneously, we utilized Foldseek to encode protein\nstructures into \"structure-sequences\" and trained a protein Structural Sequence\nLanguage Model, SSLM. Preliminary experiments demonstrated that, compared to\nlarge-scale protein language models pre-trained on vast amounts of amino acid\nsequences, the \"structure-sequence\" enables the language model to learn more\ninformative protein features, enhancing and optimizing structural\nrepresentations. We have provided the code, model weights, and all related\nmaterials on https://github.com/GouWenrui/CPE-Pro-main.git.\n","authors":["Wenrui Gou","Wenhui Ge","Yang Tan","Mingchen Li","Guisheng Fan","Huiqun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.15592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19472v2","updated":"2024-10-23T14:02:12Z","published":"2024-09-28T22:41:49Z","title":"Towards Croppable Implicit Neural Representations","summary":"  Implicit Neural Representations (INRs) have peaked interest in recent years\ndue to their ability to encode natural signals using neural networks. While\nINRs allow for useful applications such as interpolating new coordinates and\nsignal compression, their black-box nature makes it difficult to modify them\npost-training. In this paper we explore the idea of editable INRs, and\nspecifically focus on the widely used cropping operation. To this end, we\npresent Local-Global SIRENs -- a novel INR architecture that supports cropping\nby design. Local-Global SIRENs are based on combining local and global feature\nextraction for signal encoding. What makes their design unique is the ability\nto effortlessly remove specific portions of an encoded signal, with a\nproportional weight decrease. This is achieved by eliminating the corresponding\nweights from the network, without the need for retraining. We further show how\nthis architecture can be used to support the straightforward extension of\npreviously encoded signals. Beyond signal editing, we examine how the\nLocal-Global approach can accelerate training, enhance encoding of various\nsignals, improve downstream performance, and be applied to modern INRs such as\nINCODE, highlighting its potential and flexibility. Code is available at\nhttps://github.com/maorash/Local-Global-INRs.\n","authors":["Maor Ashkenazi","Eran Treister"],"pdf_url":"https://arxiv.org/pdf/2409.19472v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.06927v2","updated":"2024-10-23T14:01:27Z","published":"2024-08-13T14:29:00Z","title":"Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class\n  Feature Compensator","summary":"  Dataset distillation has emerged as a technique aiming to condense\ninformative features from large, natural datasets into a compact and synthetic\nform. While recent advancements have refined this technique, its performance is\nbottlenecked by the prevailing class-specific synthesis paradigm. Under this\nparadigm, synthetic data is optimized exclusively for a pre-assigned one-hot\nlabel, creating an implicit class barrier in feature condensation. This leads\nto inefficient utilization of the distillation budget and oversight of\ninter-class feature distributions, which ultimately limits the effectiveness\nand efficiency, as demonstrated in our analysis. To overcome these constraints,\nthis paper presents the Inter-class Feature Compensator (INFER), an innovative\ndistillation approach that transcends the class-specific data-label framework\nwidely utilized in current dataset distillation methods. Specifically, INFER\nleverages a Universal Feature Compensator (UFC) to enhance feature integration\nacross classes, enabling the generation of multiple additional synthetic\ninstances from a single UFC input. This significantly improves the efficiency\nof the distillation budget. Moreover, INFER enriches inter-class interactions\nduring the distillation, thereby enhancing the effectiveness and\ngeneralizability of the distilled data. By allowing for the linear\ninterpolation of labels similar to those in the original dataset, INFER\nmeticulously optimizes the synthetic data and dramatically reduces the size of\nsoft labels in the synthetic dataset to almost zero, establishing a new\nbenchmark for efficiency and effectiveness in dataset distillation.\n","authors":["Xin Zhang","Jiawei Du","Ping Liu","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.06927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12819v2","updated":"2024-10-23T14:00:36Z","published":"2024-07-01T10:33:46Z","title":"I've Got 99 Problems But FLOPS Ain't One","summary":"  Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.\n","authors":["Alexandru M. Gherghescu","Vlad-Andrei Bădoiu","Alexandru Agache","Mihai-Valentin Dumitru","Iuliu Vasilescu","Radu Mantu","Costin Raiciu"],"pdf_url":"https://arxiv.org/pdf/2407.12819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17882v1","updated":"2024-10-23T13:55:42Z","published":"2024-10-23T13:55:42Z","title":"Identifiable Representation and Model Learning for Latent Dynamic\n  Systems","summary":"  Learning identifiable representations and models from low-level observations\nis useful for an intelligent spacecraft to reliability finish downstream tasks.\nFor temporal observations, to ensure that the data generating process is\nprovably inverted, most existing works either assume the noise variables in the\ndynamic mechanisms are (conditionally) independent, or require interventions\nwhich can directly affect each latent variable. However, in practice, the\nrelationship between the exogenous inputs/interventions and the latent\nvariables may follow some complex deterministic mechanisms. In this work, we\nstudy the problem of identifiable representation and model learning for latent\ndynamic systems. The key idea is that we use an inductive bias inspired by\ncontrollable canonical forms, which is invariant, sparse, and input dependent\nby definition. We prove that, for linear or affine nonlinear latent dynamic\nsystems, it is possible to identify the representations up to scaling and\ndetermine the models up to some simple transformations. The results have\npotential to provide some theoretical guarantees for developing more\ntrustworthy decision-making and control methods for intelligent spacecrafts.\n","authors":["Congxi Zhang","Yongchun Xie"],"pdf_url":"https://arxiv.org/pdf/2410.17882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17881v1","updated":"2024-10-23T13:53:26Z","published":"2024-10-23T13:53:26Z","title":"AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient\n  LLMs Training and Fine-Tuning","summary":"  Training and fine-tuning large language models (LLMs) come with challenges\nrelated to memory and computational requirements due to the increasing size of\nthe model weights and the optimizer states. Various techniques have been\ndeveloped to tackle these challenges, such as low-rank adaptation (LoRA), which\ninvolves introducing a parallel trainable low-rank matrix to the fixed\npre-trained weights at each layer. However, these methods often fall short\ncompared to the full-rank weight training approach, as they restrict the\nparameter search to a low-rank subspace. This limitation can disrupt training\ndynamics and require a full-rank warm start to mitigate the impact. In this\npaper, we introduce a new method inspired by a phenomenon we formally prove: as\ntraining progresses, the rank of the estimated layer gradients gradually\ndecreases, and asymptotically approaches rank one. Leveraging this, our\napproach involves adaptively reducing the rank of the gradients during Adam\noptimization steps, using an efficient online-updating low-rank projections\nrule. We further present a randomized SVD scheme for efficiently finding the\nprojection matrix. Our technique enables full-parameter fine-tuning with\nadaptive low-rank gradient updates, significantly reducing overall memory\nrequirements during training compared to state-of-the-art methods while\nimproving model performance in both pretraining and fine-tuning. Finally, we\nprovide a convergence analysis of our method and demonstrate its merits for\ntraining and fine-tuning language and biological foundation models.\n","authors":["Yehonathan Refael","Jonathan Svirsky","Boris Shustin","Wasim Huleihel","Ofir Lindenbaum"],"pdf_url":"https://arxiv.org/pdf/2410.17881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17878v1","updated":"2024-10-23T13:50:27Z","published":"2024-10-23T13:50:27Z","title":"Relaxed Equivariance via Multitask Learning","summary":"  Incorporating equivariance as an inductive bias into deep learning\narchitectures to take advantage of the data symmetry has been successful in\nmultiple applications, such as chemistry and dynamical systems. In particular,\nroto-translations are crucial for effectively modeling geometric graphs and\nmolecules, where understanding the 3D structures enhances generalization.\nHowever, equivariant models often pose challenges due to their high\ncomputational complexity. In this paper, we introduce REMUL, a training\nprocedure for approximating equivariance with multitask learning. We show that\nunconstrained models (which do not build equivariance into the architecture)\ncan learn approximate symmetries by minimizing an additional simple\nequivariance loss. By formulating equivariance as a new learning objective, we\ncan control the level of approximate equivariance in the model. Our method\nachieves competitive performance compared to equivariant baselines while being\n$10 \\times$ faster at inference and $2.5 \\times$ at training.\n","authors":["Ahmed A. Elhag","T. Konstantin Rusch","Francesco Di Giovanni","Michael Bronstein"],"pdf_url":"https://arxiv.org/pdf/2410.17878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04953v3","updated":"2024-10-23T13:36:37Z","published":"2022-12-09T16:03:34Z","title":"TargetCall: Eliminating the Wasted Computation in Basecalling via\n  Pre-Basecalling Filtering","summary":"  Basecalling is an essential step in nanopore sequencing analysis where the\nraw signals of nanopore sequencers are converted into nucleotide sequences,\ni.e., reads. State-of-the-art basecallers employ complex deep learning models\nto achieve high basecalling accuracy. This makes basecalling computationally\ninefficient and memory-hungry, bottlenecking the entire genome analysis\npipeline. However, for many applications, the majority of reads do no match the\nreference genome of interest (i.e., target reference) and thus are discarded in\nlater steps in the genomics pipeline, wasting the basecalling computation. To\novercome this issue, we propose TargetCall, the first pre-basecalling filter to\neliminate the wasted computation in basecalling. TargetCall's key idea is to\ndiscard reads that will not match the target reference (i.e., off-target reads)\nprior to basecalling. TargetCall consists of two main components: (1)\nLightCall, a lightweight neural network basecaller that produces noisy reads;\nand (2) Similarity Check, which labels each of these noisy reads as on-target\nor off-target by matching them to the target reference. Our thorough\nexperimental evaluations show that TargetCall 1) improves the end-to-end\nbasecalling runtime performance of the state-of-the-art basecaller by 3.31x\nwhile maintaining high (98.88%) recall in keeping on-target reads, 2) maintains\nhigh accuracy in downstream analysis, and 3) achieves better runtime\nperformance, throughput, recall, precision, and generality compared to prior\nworks. TargetCall is available at https://github.com/CMU-SAFARI/TargetCall.\n","authors":["Meryem Banu Cavlak","Gagandeep Singh","Mohammed Alser","Can Firtina","Joël Lindegger","Mohammad Sadrosadati","Nika Mansouri Ghiasi","Can Alkan","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2212.04953v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17865v1","updated":"2024-10-23T13:36:23Z","published":"2024-10-23T13:36:23Z","title":"Population stratification for prediction of mortality in post-AKI\n  patients","summary":"  Acute kidney injury (AKI) is a serious clinical condition that affects up to\n20% of hospitalised patients. AKI is associated with short term unplanned\nhospital readmission and post-discharge mortality risk. Patient risk and\nhealthcare expenditures can be minimised by followup planning grounded on\npredictive models and machine learning. Since AKI is multi-factorial,\npredictive models specialised in different categories of patients can increase\naccuracy of predictions. In the present article we present some results\nfollowing this approach.\n","authors":["Flavio S. Correa da Silva","Simon Sawhney"],"pdf_url":"https://arxiv.org/pdf/2410.17865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17863v1","updated":"2024-10-23T13:35:18Z","published":"2024-10-23T13:35:18Z","title":"CASCRNet: An Atrous Spatial Pyramid Pooling and Shared Channel Residual\n  based Network for Capsule Endoscopy","summary":"  This manuscript summarizes work on the Capsule Vision Challenge 2024 by\nMISAHUB. To address the multi-class disease classification task, which is\nchallenging due to the complexity and imbalance in the Capsule Vision challenge\ndataset, this paper proposes CASCRNet (Capsule endoscopy-Aspp-SCR-Network), a\nparameter-efficient and novel model that uses Shared Channel Residual (SCR)\nblocks and Atrous Spatial Pyramid Pooling (ASPP) blocks. Further, the\nperformance of the proposed model is compared with other well-known approaches.\nThe experimental results yield that proposed model provides better disease\nclassification results. The proposed model was successful in classifying\ndiseases with an F1 Score of 78.5% and a Mean AUC of 98.3%, which is promising\ngiven its compact architecture.\n","authors":["K V Srinanda","M Manvith Prabhu","Shyam Lal"],"pdf_url":"https://arxiv.org/pdf/2410.17863v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.17851v1","updated":"2024-10-23T13:20:42Z","published":"2024-10-23T13:20:42Z","title":"The Probabilistic Tsetlin Machine: A Novel Approach to Uncertainty\n  Quantification","summary":"  Tsetlin Machines (TMs) have emerged as a compelling alternative to\nconventional deep learning methods, offering notable advantages such as smaller\nmemory footprint, faster inference, fault-tolerant properties, and\ninterpretability. Although various adaptations of TMs have expanded their\napplicability across diverse domains, a fundamental gap remains in\nunderstanding how TMs quantify uncertainty in their predictions. In response,\nthis paper introduces the Probabilistic Tsetlin Machine (PTM) framework, aimed\nat providing a robust, reliable, and interpretable approach for uncertainty\nquantification. Unlike the original TM, the PTM learns the probability of\nstaying on each state of each Tsetlin Automaton (TA) across all clauses. These\nprobabilities are updated using the feedback tables that are part of the TM\nframework: Type I and Type II feedback. During inference, TAs decide their\nactions by sampling states based on learned probability distributions, akin to\nBayesian neural networks when generating weight values. In our experimental\nanalysis, we first illustrate the spread of the probabilities across TA states\nfor the noisy-XOR dataset. Then we evaluate the PTM alongside benchmark models\nusing both simulated and real-world datasets. The experiments on the simulated\ndataset reveal the PTM's effectiveness in uncertainty quantification,\nparticularly in delineating decision boundaries and identifying regions of high\nuncertainty. Moreover, when applied to multiclass classification tasks using\nthe Iris dataset, the PTM demonstrates competitive performance in terms of\npredictive entropy and expected calibration error, showcasing its potential as\na reliable tool for uncertainty estimation. Our findings underscore the\nimportance of selecting appropriate models for accurate uncertainty\nquantification in predictive tasks, with the PTM offering a particularly\ninterpretable and effective solution.\n","authors":["K. Darshana Abeyrathna","Sara El Mekkaoui","Andreas Hafver","Christian Agrell"],"pdf_url":"https://arxiv.org/pdf/2410.17851v1.pdf","comment":"12 pages, 5 figures, 6 tables, accepted and presented at ICAAI 2024,\n  London"},{"id":"http://arxiv.org/abs/2402.15055v2","updated":"2024-10-23T13:20:15Z","published":"2024-02-23T02:15:47Z","title":"Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions","summary":"  Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.\n","authors":["Clement Neo","Shay B. Cohen","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.15055v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.13563v2","updated":"2024-10-23T13:19:26Z","published":"2024-10-17T14:00:18Z","title":"Ornstein-Uhlenbeck Adaptation as a Mechanism for Learning in Brains and\n  Machines","summary":"  Learning is a fundamental property of intelligent systems, observed across\nbiological organisms and engineered systems. While modern intelligent systems\ntypically rely on gradient descent for learning, the need for exact gradients\nand complex information flow makes its implementation in biological and\nneuromorphic systems challenging. This has motivated the exploration of\nalternative learning mechanisms that can operate locally and do not rely on\nexact gradients. In this work, we introduce a novel approach that leverages\nnoise in the parameters of the system and global reinforcement signals. Using\nan Ornstein-Uhlenbeck process with adaptive dynamics, our method balances\nexploration and exploitation during learning, driven by deviations from error\npredictions, akin to reward prediction error. Operating in continuous time,\nOrstein-Uhlenbeck adaptation (OUA) is proposed as a general mechanism for\nlearning dynamic, time-evolving environments. We validate our approach across\ndiverse tasks, including supervised learning and reinforcement learning in\nfeedforward and recurrent systems. Additionally, we demonstrate that it can\nperform meta-learning, adjusting hyper-parameters autonomously. Our results\nindicate that OUA provides a viable alternative to traditional gradient-based\nmethods, with potential applications in neuromorphic computing. It also hints\nat a possible mechanism for noise-driven learning in the brain, where\nstochastic neurotransmitter release may guide synaptic adjustments.\n","authors":["Jesus Garcia Fernandez","Nasir Ahmad","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2410.13563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11397v2","updated":"2024-10-23T13:14:21Z","published":"2024-10-15T08:39:31Z","title":"FOOGD: Federated Collaboration for Both Out-of-distribution\n  Generalization and Detection","summary":"  Federated learning (FL) is a promising machine learning paradigm that\ncollaborates with client models to capture global knowledge. However, deploying\nFL models in real-world scenarios remains unreliable due to the coexistence of\nin-distribution data and unexpected out-of-distribution (OOD) data, such as\ncovariate-shift and semantic-shift data. Current FL researches typically\naddress either covariate-shift data through OOD generalization or\nsemantic-shift data via OOD detection, overlooking the simultaneous occurrence\nof various OOD shifts. In this work, we propose FOOGD, a method that estimates\nthe probability density of each client and obtains reliable global distribution\nas guidance for the subsequent FL process. Firstly, SM3D in FOOGD estimates\nscore model for arbitrary distributions without prior constraints, and detects\nsemantic-shift data powerfully. Then SAG in FOOGD provides invariant yet\ndiverse knowledge for both local covariate-shift generalization and client\nperformance generalization. In empirical validations, FOOGD significantly\nenjoys three main advantages: (1) reliably estimating non-normalized\ndecentralized distributions, (2) detecting semantic shift data via score\nvalues, and (3) generalizing to covariate-shift data by regularizing feature\nextractor. The prejoct is open in https://github.com/XeniaLLL/FOOGD-main.git.\n","authors":["Xinting Liao","Weiming Liu","Pengyang Zhou","Fengyuan Yu","Jiahe Xu","Jun Wang","Wenjie Wang","Chaochao Chen","Xiaolin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.11397v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17840v1","updated":"2024-10-23T13:05:46Z","published":"2024-10-23T13:05:46Z","title":"Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs","summary":"  Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces.\n","authors":["Ferdi Kossmann","Bruce Fontaine","Daya Khudia","Michael Cafarella","Samuel Madden"],"pdf_url":"https://arxiv.org/pdf/2410.17840v1.pdf","comment":"12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.14774v2","updated":"2024-10-23T13:01:14Z","published":"2024-03-21T18:28:43Z","title":"Few-Shot Adversarial Prompt Learning on Vision-Language Models","summary":"  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data. Code is available at:\nhttps://github.com/lionel-w2/FAP.\n","authors":["Yiwei Zhou","Xiaobo Xia","Zhiwei Lin","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14774v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17835v1","updated":"2024-10-23T12:54:04Z","published":"2024-10-23T12:54:04Z","title":"Optimal Streaming Algorithms for Multi-Armed Bandits","summary":"  This paper studies two variants of the best arm identification (BAI) problem\nunder the streaming model, where we have a stream of $n$ arms with reward\ndistributions supported on $[0,1]$ with unknown means. The arms in the stream\nare arriving one by one, and the algorithm cannot access an arm unless it is\nstored in a limited size memory.\n  We first study the streaming \\eps-$top$-$k$ arms identification problem,\nwhich asks for $k$ arms whose reward means are lower than that of the $k$-th\nbest arm by at most $\\eps$ with probability at least $1-\\delta$. For general\n$\\eps \\in (0,1)$, the existing solution for this problem assumes $k = 1$ and\nachieves the optimal sample complexity $O(\\frac{n}{\\eps^2} \\log\n\\frac{1}{\\delta})$ using $O(\\log^*(n))$ ($\\log^*(n)$ equals the number of times\nthat we need to apply the logarithm function on $n$ before the results is no\nmore than 1.) memory and a single pass of the stream. We propose an algorithm\nthat works for any $k$ and achieves the optimal sample complexity\n$O(\\frac{n}{\\eps^2} \\log\\frac{k}{\\delta})$ using a single-arm memory and a\nsingle pass of the stream.\n  Second, we study the streaming BAI problem, where the objective is to\nidentify the arm with the maximum reward mean with at least $1-\\delta$\nprobability, using a single-arm memory and as few passes of the input stream as\npossible. We present a single-arm-memory algorithm that achieves a near\ninstance-dependent optimal sample complexity within $O(\\log \\Delta_2^{-1})$\npasses, where $\\Delta_2$ is the gap between the mean of the best arm and that\nof the second best arm.\n","authors":["Tianyuan Jin","Keke Huang","Jing Tang","Xiaokui Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.17835v1.pdf","comment":"24pages"},{"id":"http://arxiv.org/abs/2410.17834v1","updated":"2024-10-23T12:53:58Z","published":"2024-10-23T12:53:58Z","title":"Non-intrusive Speech Quality Assessment with Diffusion Models Trained on\n  Clean Speech","summary":"  Diffusion models have found great success in generating high quality, natural\nsamples of speech, but their potential for density estimation for speech has so\nfar remained largely unexplored. In this work, we leverage an unconditional\ndiffusion model trained only on clean speech for the assessment of speech\nquality. We show that the quality of a speech utterance can be assessed by\nestimating the likelihood of a corresponding sample in the terminating Gaussian\ndistribution, obtained via a deterministic noising process. The resulting\nmethod is purely unsupervised, trained only on clean speech, and therefore does\nnot rely on annotations. Our diffusion-based approach leverages clean speech\npriors to assess quality based on how the input relates to the learned\ndistribution of clean data. Our proposed log-likelihoods show promising\nresults, correlating well with intrusive speech quality metrics such as POLQA\nand SI-SDR.\n","authors":["Danilo de Oliveira","Julius Richter","Jean-Marie Lemercier","Simon Welker","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2410.17834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01476v2","updated":"2024-10-23T12:53:49Z","published":"2024-10-02T12:30:05Z","title":"Reducing Variance in Meta-Learning via Laplace Approximation for\n  Regression Tasks","summary":"  Given a finite set of sample points, meta-learning algorithms aim to learn an\noptimal adaptation strategy for new, unseen tasks. Often, this data can be\nambiguous as it might belong to different tasks concurrently. This is\nparticularly the case in meta-regression tasks. In such cases, the estimated\nadaptation strategy is subject to high variance due to the limited amount of\nsupport data for each task, which often leads to sub-optimal generalization\nperformance. In this work, we address the problem of variance reduction in\ngradient-based meta-learning and formalize the class of problems prone to this,\na condition we refer to as \\emph{task overlap}. Specifically, we propose a\nnovel approach that reduces the variance of the gradient estimate by weighing\neach support point individually by the variance of its posterior over the\nparameters. To estimate the posterior, we utilize the Laplace approximation,\nwhich allows us to express the variance in terms of the curvature of the loss\nlandscape of our meta-learner. Experimental results demonstrate the\neffectiveness of the proposed method and highlight the importance of variance\nreduction in meta-learning.\n","authors":["Alfredo Reichlin","Gustaf Tegnér","Miguel Vasco","Hang Yin","Mårten Björkman","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2410.01476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01708v4","updated":"2024-10-23T12:50:18Z","published":"2022-10-04T16:08:54Z","title":"Conquering the Communication Constraints to Enable Large Pre-Trained\n  Models in Federated Learning","summary":"  Federated learning (FL) has emerged as a promising paradigm for enabling the\ncollaborative training of models without centralized access to the raw data on\nlocal devices. In the typical FL paradigm (e.g., FedAvg), model weights are\nsent to and from the server each round to participating clients. Recently, the\nuse of small pre-trained models has been shown effective in federated learning\noptimization and improving convergence. However, recent state-of-the-art\npre-trained models are getting more capable but also have more parameters. In\nconventional FL, sharing the enormous model weights can quickly put a massive\ncommunication burden on the system, especially if more capable models are\nemployed. Can we find a solution to enable those strong and readily-available\npre-trained models in FL to achieve excellent performance while simultaneously\nreducing the communication burden? To this end, we investigate the use of\nparameter-efficient fine-tuning in federated learning and thus introduce a new\nframework: FedPEFT. Specifically, we systemically evaluate the performance of\nFedPEFT across a variety of client stability, data distribution, and\ndifferential privacy settings. By only locally tuning and globally sharing a\nsmall portion of the model weights, significant reductions in the total\ncommunication overhead can be achieved while maintaining competitive or even\nbetter performance in a wide range of federated learning scenarios, providing\ninsight into a new paradigm for practical and effective federated systems.\n","authors":["Guangyu Sun","Umar Khalid","Matias Mendieta","Taojiannan Yang","Pu Wang","Minwoo Lee","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2210.01708v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01335v2","updated":"2024-10-23T12:38:40Z","published":"2024-03-30T13:25:11Z","title":"Generative AI Models for Different Steps in Architectural Design: A\n  Literature Review","summary":"  Recent advances in generative artificial intelligence (AI) technologies have\nbeen significantly driven by models such as generative adversarial networks\n(GANs), variational autoencoders (VAEs), and denoising diffusion probabilistic\nmodels (DDPMs). Although architects recognize the potential of generative AI in\ndesign, personal barriers often restrict their access to the latest\ntechnological developments, thereby causing the application of generative AI in\narchitectural design to lag behind. Therefore, it is essential to comprehend\nthe principles and advancements of generative AI models and analyze their\nrelevance in architecture applications. This paper first provides an overview\nof generative AI technologies, with a focus on probabilistic diffusion models\n(DDPMs), 3D generative models, and foundation models, highlighting their recent\ndevelopments and main application scenarios. Then, the paper explains how the\nabovementioned models could be utilized in architecture. We subdivide the\narchitectural design process into six steps and review related research\nprojects in each step from 2020 to the present. Lastly, this paper discusses\npotential future directions for applying generative AI in the architectural\ndesign steps. This research can help architects quickly understand the\ndevelopment and latest progress of generative AI and contribute to the further\ndevelopment of intelligent architecture.\n","authors":["Chengyuan Li","Tianyu Zhang","Xusheng Du","Ye Zhang","Haoran Xie"],"pdf_url":"https://arxiv.org/pdf/2404.01335v2.pdf","comment":"34 pages, 14 figures, accepted by Frontiers of Architectural Research"},{"id":"http://arxiv.org/abs/2410.17823v1","updated":"2024-10-23T12:32:21Z","published":"2024-10-23T12:32:21Z","title":"Att2CPC: Attention-Guided Lossy Attribute Compression of Point Clouds","summary":"  With the great progress of 3D sensing and acquisition technology, the volume\nof point cloud data has grown dramatically, which urges the development of\nefficient point cloud compression methods. In this paper, we focus on the task\nof learned lossy point cloud attribute compression (PCAC). We propose an\nefficient attention-based method for lossy compression of point cloud\nattributes leveraging on an autoencoder architecture. Specifically, at the\nencoding side, we conduct multiple downsampling to best exploit the local\nattribute patterns, in which effective External Cross Attention (ECA) is\ndevised to hierarchically aggregate features by intergrating attributes and\ngeometry contexts. At the decoding side, the attributes of the point cloud are\nprogressively reconstructed based on the multi-scale representation and the\nzero-padding upsampling tactic. To the best of our knowledge, this is the first\napproach to introduce attention mechanism to point-based lossy PCAC task. We\nverify the compression efficiency of our model on various sequences, including\nhuman body frames, sparse objects, and large-scale point cloud scenes.\nExperiments show that our method achieves an average improvement of 1.15 dB and\n2.13 dB in BD-PSNR of Y channel and YUV channel, respectively, when comparing\nwith the state-of-the-art point-based method Deep-PCAC. Codes of this paper are\navailable at https://github.com/I2-Multimedia-Lab/Att2CPC.\n","authors":["Kai Liu","Kang You","Pan Gao","Manoranjan Paul"],"pdf_url":"https://arxiv.org/pdf/2410.17823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04811v2","updated":"2024-10-23T12:27:12Z","published":"2024-07-05T18:49:07Z","title":"Simplifying Deep Temporal Difference Learning","summary":"  Q-learning played a foundational role in the field reinforcement learning\n(RL). However, TD algorithms with off-policy data, such as Q-learning, or\nnonlinear function approximation like deep neural networks require several\nadditional tricks to stabilise training, primarily a replay buffer and target\nnetworks. Unfortunately, the delayed updating of frozen network parameters in\nthe target network harms the sample efficiency and, similarly, the replay\nbuffer introduces memory and implementation overheads. In this paper, we\ninvestigate whether it is possible to accelerate and simplify TD training while\nmaintaining its stability. Our key theoretical result demonstrates for the\nfirst time that regularisation techniques such as LayerNorm can yield provably\nconvergent TD algorithms without the need for a target network, even with\noff-policy data. Empirically, we find that online, parallelised sampling\nenabled by vectorised environments stabilises training without the need of a\nreplay buffer. Motivated by these findings, we propose PQN, our simplified deep\nonline Q-Learning algorithm. Surprisingly, this simple algorithm is competitive\nwith more complex methods like: Rainbow in Atari, R2D2 in Hanabi, QMix in Smax,\nPPO-RNN in Craftax, and can be up to 50x faster than traditional DQN without\nsacrificing sample efficiency. In an era where PPO has become the go-to RL\nalgorithm, PQN reestablishes Q-learning as a viable alternative.\n","authors":["Matteo Gallici","Mattie Fellows","Benjamin Ellis","Bartomeu Pou","Ivan Masmitja","Jakob Nicolaus Foerster","Mario Martin"],"pdf_url":"https://arxiv.org/pdf/2407.04811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04372v3","updated":"2024-10-23T12:19:16Z","published":"2024-01-09T06:15:45Z","title":"Stable generative modeling using Schrödinger bridges","summary":"  We consider the problem of sampling from an unknown distribution for which\nonly a sufficiently large number of training samples are available. Such\nsettings have recently drawn considerable interest in the context of generative\nmodelling and Bayesian inference. In this paper, we propose a generative model\ncombining Schr\\\"odinger bridges and Langevin dynamics. Schr\\\"odinger bridges\nover an appropriate reversible reference process are used to approximate the\nconditional transition probability from the available training samples, which\nis then implemented in a discrete-time reversible Langevin sampler to generate\nnew samples. By setting the kernel bandwidth in the reference process to match\nthe time step size used in the unadjusted Langevin algorithm, our method\neffectively circumvents any stability issues typically associated with the\ntime-stepping of stiff stochastic differential equations. Moreover, we\nintroduce a novel split-step scheme, ensuring that the generated samples remain\nwithin the convex hull of the training samples. Our framework can be naturally\nextended to generate conditional samples and to Bayesian inference problems. We\ndemonstrate the performance of our proposed scheme through experiments on\nsynthetic datasets with increasing dimensions and on a stochastic subgrid-scale\nparametrization conditional sampling problem as well as generating sample\ntrajectories of a dynamical system using conditional sampling.\n","authors":["Georg A. Gottwald","Fengyi Li","Youssef Marzouk","Sebastian Reich"],"pdf_url":"https://arxiv.org/pdf/2401.04372v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11960v4","updated":"2024-10-23T12:18:49Z","published":"2024-03-18T16:57:16Z","title":"Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal\n  Time Series Imputation","summary":"  Spatiotemporal time series are usually collected via monitoring sensors\nplaced at different locations, which usually contain missing values due to\nvarious failures, such as mechanical damages and Internet outages. Imputing the\nmissing values is crucial for analyzing time series. When recovering a specific\ndata point, most existing methods consider all the information relevant to that\npoint regardless of the cause-and-effect relationship. During data collection,\nit is inevitable that some unknown confounders are included, e.g., background\nnoise in time series and non-causal shortcut edges in the constructed sensor\nnetwork. These confounders could open backdoor paths and establish non-causal\ncorrelations between the input and output. Over-exploiting these non-causal\ncorrelations could cause overfitting. In this paper, we first revisit\nspatiotemporal time series imputation from a causal perspective and show how to\nblock the confounders via the frontdoor adjustment. Based on the results of\nfrontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph\nNeural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and\na Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of\nconfounders and SCA could discover the sparse causal relationships among\nembeddings. Theoretical analysis reveals that SCA discovers causal\nrelationships based on the values of gradients. We evaluate Casper on three\nreal-world datasets, and the experimental results show that Casper could\noutperform the baselines and could effectively discover causal relationships.\n","authors":["Baoyu Jing","Dawei Zhou","Kan Ren","Carl Yang"],"pdf_url":"https://arxiv.org/pdf/2403.11960v4.pdf","comment":"Accepted by CIKM'2024. Fixed typos"},{"id":"http://arxiv.org/abs/2410.17814v1","updated":"2024-10-23T12:18:36Z","published":"2024-10-23T12:18:36Z","title":"Learning Lossless Compression for High Bit-Depth Volumetric Medical\n  Image","summary":"  Recent advances in learning-based methods have markedly enhanced the\ncapabilities of image compression. However, these methods struggle with high\nbit-depth volumetric medical images, facing issues such as degraded\nperformance, increased memory demand, and reduced processing speed. To address\nthese challenges, this paper presents the Bit-Division based Lossless\nVolumetric Image Compression (BD-LVIC) framework, which is tailored for high\nbit-depth medical volume compression. The BD-LVIC framework skillfully divides\nthe high bit-depth volume into two lower bit-depth segments: the Most\nSignificant Bit-Volume (MSBV) and the Least Significant Bit-Volume (LSBV). The\nMSBV concentrates on the most significant bits of the volumetric medical image,\ncapturing vital structural details in a compact manner. This reduction in\ncomplexity greatly improves compression efficiency using traditional codecs.\nConversely, the LSBV deals with the least significant bits, which encapsulate\nintricate texture details. To compress this detailed information effectively,\nwe introduce an effective learning-based compression model equipped with a\nTransformer-Based Feature Alignment Module, which exploits both intra-slice and\ninter-slice redundancies to accurately align features. Subsequently, a Parallel\nAutoregressive Coding Module merges these features to precisely estimate the\nprobability distribution of the least significant bit-planes. Our extensive\ntesting demonstrates that the BD-LVIC framework not only sets new performance\nbenchmarks across various datasets but also maintains a competitive coding\nspeed, highlighting its significant potential and practical utility in the\nrealm of volumetric medical image compression.\n","authors":["Kai Wang","Yuanchao Bai","Daxin Li","Deming Zhai","Junjun Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17814v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2408.04307v2","updated":"2024-10-23T12:08:33Z","published":"2024-08-08T08:40:15Z","title":"MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts\n  Model Training","summary":"  As large language models continue to scale up, distributed training systems\nhave expanded beyond 10k nodes, intensifying the importance of fault tolerance.\nCheckpoint has emerged as the predominant fault tolerance strategy, with\nextensive studies dedicated to optimizing its efficiency. However, the advent\nof the sparse Mixture-of-Experts (MoE) model presents new challenges due to the\nsubstantial increase in model size, despite comparable computational demands to\ndense models.\n  In this work, we propose the Mixture-of-Checkpoint System (MoC-System) to\norchestrate the vast array of checkpoint shards produced in distributed\ntraining systems. MoC-System features a novel Partial Experts Checkpointing\n(PEC) mechanism, an algorithm-system co-design that strategically saves a\nselected subset of experts, effectively reducing the MoE checkpoint size to\nlevels comparable with dense models. Incorporating hybrid parallel strategies,\nMoC-System involves fully sharded checkpointing strategies to evenly distribute\nthe workload across distributed ranks. Furthermore, MoC-System introduces a\ntwo-level checkpointing management method that asynchronously handles in-memory\nsnapshots and persistence processes.\n  We build MoC-System upon the Megatron-DeepSpeed framework, achieving up to a\n98.9% reduction in overhead for each checkpointing process compared to the\noriginal method, during MoE model training with ZeRO-2 data parallelism and\nexpert parallelism. Additionally, extensive empirical analyses substantiate\nthat our methods enhance efficiency while maintaining comparable model\naccuracy, even achieving an average accuracy increase of 1.08% on downstream\ntasks.\n","authors":["Weilin Cai","Le Qin","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.04307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12641v3","updated":"2024-10-23T11:54:42Z","published":"2024-03-19T11:24:14Z","title":"Automated Contrastive Learning Strategy Search for Time Series","summary":"  In recent years, Contrastive Learning (CL) has become a predominant\nrepresentation learning paradigm for time series. Most existing methods\nmanually build specific CL Strategies (CLS) by human heuristics for certain\ndatasets and tasks. However, manually developing CLS usually requires excessive\nprior knowledge about the data, and massive experiments to determine the\ndetailed CL configurations. In this paper, we present an Automated Machine\nLearning (AutoML) practice at Microsoft, which automatically learns CLS for\ntime series datasets and tasks, namely Automated Contrastive Learning (AutoCL).\nWe first construct a principled search space of size over $3\\times10^{12}$,\ncovering data augmentation, embedding transformation, contrastive pair\nconstruction, and contrastive losses. Further, we introduce an efficient\nreinforcement learning algorithm, which optimizes CLS from the performance on\nthe validation tasks, to obtain effective CLS within the space. Experimental\nresults on various real-world datasets demonstrate that AutoCL could\nautomatically find the suitable CLS for the given dataset and task. From the\ncandidate CLS found by AutoCL on several public datasets/tasks, we compose a\ntransferable Generally Good Strategy (GGS), which has a strong performance for\nother datasets. We also provide empirical analysis as a guide for the future\ndesign of CLS.\n","authors":["Baoyu Jing","Yansen Wang","Guoxin Sui","Jing Hong","Jingrui He","Yuqing Yang","Dongsheng Li","Kan Ren"],"pdf_url":"https://arxiv.org/pdf/2403.12641v3.pdf","comment":"Accepted by CIKM'2024. Fixed typos"},{"id":"http://arxiv.org/abs/2410.17796v1","updated":"2024-10-23T11:52:52Z","published":"2024-10-23T11:52:52Z","title":"A Comprehensive Analysis on the Learning Curve in Kernel Ridge\n  Regression","summary":"  This paper conducts a comprehensive study of the learning curves of kernel\nridge regression (KRR) under minimal assumptions. Our contributions are\nthree-fold: 1) we analyze the role of key properties of the kernel, such as its\nspectral eigen-decay, the characteristics of the eigenfunctions, and the\nsmoothness of the kernel; 2) we demonstrate the validity of the Gaussian\nEquivalent Property (GEP), which states that the generalization performance of\nKRR remains the same when the whitened features are replaced by standard\nGaussian vectors, thereby shedding light on the success of previous analyzes\nunder the Gaussian Design Assumption; 3) we derive novel bounds that improve\nover existing bounds across a broad range of setting such as (in)dependent\nfeature vectors and various combinations of eigen-decay rates in the\nover/underparameterized regimes.\n","authors":["Tin Sum Cheng","Aurelien Lucchi","Anastasis Kratsios","David Belius"],"pdf_url":"https://arxiv.org/pdf/2410.17796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17792v1","updated":"2024-10-23T11:47:04Z","published":"2024-10-23T11:47:04Z","title":"Enhancing Federated Learning Convergence with Dynamic Data Queue and\n  Data Entropy-driven Participant Selection","summary":"  Federated Learning (FL) is a decentralized approach for collaborative model\ntraining on edge devices. This distributed method of model training offers\nadvantages in privacy, security, regulatory compliance, and cost-efficiency.\nOur emphasis in this research lies in addressing statistical complexity in FL,\nespecially when the data stored locally across devices is not identically and\nindependently distributed (non-IID). We have observed an accuracy reduction of\nup to approximately 10\\% to 30\\%, particularly in skewed scenarios where each\nedge device trains with only 1 class of data. This reduction is attributed to\nweight divergence, quantified using the Euclidean distance between device-level\nclass distributions and the population distribution, resulting in a bias term\n(\\(\\delta_k\\)). As a solution, we present a method to improve convergence in FL\nby creating a global subset of data on the server and dynamically distributing\nit across devices using a Dynamic Data queue-driven Federated Learning (DDFL).\nNext, we leverage Data Entropy metrics to observe the process during each\ntraining round and enable reasonable device selection for aggregation.\nFurthermore, we provide a convergence analysis of our proposed DDFL to justify\ntheir viability in practical FL scenarios, aiming for better device selection,\na non-sub-optimal global model, and faster convergence. We observe that our\napproach results in a substantial accuracy boost of approximately 5\\% for the\nMNIST dataset, around 18\\% for CIFAR-10, and 20\\% for CIFAR-100 with a 10\\%\nglobal subset of data, outperforming the state-of-the-art (SOTA) aggregation\nalgorithms.\n","authors":["Charuka Herath","Xiaolan Liu","Sangarapillai Lambotharan","Yogachandran Rahulamathavan"],"pdf_url":"https://arxiv.org/pdf/2410.17792v1.pdf","comment":"The Journal is submitted to IEEE Transactions in the Internet of\n  Things"},{"id":"http://arxiv.org/abs/2410.17787v1","updated":"2024-10-23T11:37:20Z","published":"2024-10-23T11:37:20Z","title":"Large Language Models Engineer Too Many Simple Features For Tabular Data","summary":"  Tabular machine learning problems often require time-consuming and\nlabor-intensive feature engineering. Recent efforts have focused on using large\nlanguage models (LLMs) to capitalize on their potential domain knowledge. At\nthe same time, researchers have observed ethically concerning negative biases\nin other LLM-related use cases, such as text generation. These developments\nmotivated us to investigate whether LLMs exhibit a bias that negatively impacts\nthe performance of feature engineering. While not ethically concerning, such a\nbias could hinder practitioners from fully utilizing LLMs for automated data\nscience. Therefore, we propose a method to detect potential biases by detecting\nanomalies in the frequency of operators (e.g., adding two features) suggested\nby LLMs when engineering new features. Our experiments evaluate the bias of\nfour LLMs, two big frontier and two small open-source models, across 27 tabular\ndatasets. Our results indicate that LLMs are biased toward simple operators,\nsuch as addition, and can fail to utilize more complex operators, such as\ngrouping followed by aggregations. Furthermore, the bias can negatively impact\nthe predictive performance when using LLM-generated features. Our results call\nfor mitigating bias when using LLMs for feature engineering.\n","authors":["Jaris Küken","Lennart Purucker","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2410.17787v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.17772v1","updated":"2024-10-23T11:19:48Z","published":"2024-10-23T11:19:48Z","title":"Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation\n  Models","summary":"  A central challenge towards developing robots that can relate human language\nto their perception and actions is the scarcity of natural language annotations\nin diverse robot datasets. Moreover, robot policies that follow natural\nlanguage instructions are typically trained on either templated language or\nexpensive human-labeled instructions, hindering their scalability. To this end,\nwe introduce NILS: Natural language Instruction Labeling for Scalability. NILS\nautomatically labels uncurated, long-horizon robot data at scale in a zero-shot\nmanner without any human intervention. NILS combines pretrained vision-language\nfoundation models in order to detect objects in a scene, detect object-centric\nchanges, segment tasks from large datasets of unlabelled interaction data and\nultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a\nkitchen play dataset show that NILS can autonomously annotate diverse robot\ndemonstrations of unlabeled and unstructured datasets while alleviating several\nshortcomings of crowdsourced human annotations, such as low data quality and\ndiversity. We use NILS to label over 115k trajectories obtained from over 430\nhours of robot data. We open-source our auto-labeling code and generated\nannotations on our website: http://robottasklabeling.github.io.\n","authors":["Nils Blank","Moritz Reuss","Marcel Rühle","Ömer Erdinç Yağmurlu","Fabian Wenzel","Oier Mees","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.17772v1.pdf","comment":"Project Website at https://robottasklabeling.github.io/"},{"id":"http://arxiv.org/abs/2410.17770v1","updated":"2024-10-23T11:19:08Z","published":"2024-10-23T11:19:08Z","title":"Locating Information in Large Language Models via Random Matrix Theory","summary":"  As large language models (LLMs) become central to AI applications, gaining a\ndeeper understanding of their inner workings is increasingly important. In this\nwork, we analyze the weight matrices of pretrained transformer models --\nspecifically BERT and Llama -- using random matrix theory (RMT) as a\nzero-information hypothesis. While randomly initialized weights perfectly agree\nwith RMT predictions, deviations emerge after training, allowing us to locate\nlearned structures within the models. We identify layer-type specific behaviors\nthat are consistent across all blocks and architectures considered. By\npinpointing regions that deviate from RMT predictions, we highlight areas of\nfeature learning and confirm this through comparisons with the activation\ncovariance matrices of the corresponding layers. Our method provides a\ndiagnostic tool for identifying relevant regions in transformer weights using\nonly the trained matrices. Additionally, we address the ongoing debate\nregarding the significance of small singular values in the context of\nfine-tuning and alignment in LLMs. Our findings reveal that, after fine-tuning,\nsmall singular values play a crucial role in the models' capabilities,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment.\n","authors":["Max Staats","Matthias Thamm","Bernd Rosenow"],"pdf_url":"https://arxiv.org/pdf/2410.17770v1.pdf","comment":"17 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.17765v1","updated":"2024-10-23T11:06:36Z","published":"2024-10-23T11:06:36Z","title":"Faster Language Models with Better Multi-Token Prediction Using Tensor\n  Decomposition","summary":"  We propose a new model for multi-token prediction in transformers, aiming to\nenhance sampling efficiency without compromising accuracy. Motivated by recent\nwork that predicts the probabilities of subsequent tokens using multiple heads,\nwe connect this approach to rank-$1$ canonical tensor decomposition. By\ngeneralizing it to a rank-$r$ canonical probability decomposition, we develop\nan improved model that predicts multiple tokens simultaneously. This model can\nalso be interpreted as a mixture of experts, allowing us to leverage successful\ntechniques from that domain for efficient and robust training. Importantly, the\noverall overhead for training and sampling remains low. Our method demonstrates\nsignificant improvements in inference speed for both text and code generation\ntasks, proving particularly beneficial within the self-speculative decoding\nparadigm. It maintains its effectiveness across various model sizes and\ntraining epochs, highlighting its robustness and scalability.\n","authors":["Artem Basharin","Andrei Chertkov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2410.17765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17764v1","updated":"2024-10-23T11:02:59Z","published":"2024-10-23T11:02:59Z","title":"Beyond Backpropagation: Optimization with Multi-Tangent Forward\n  Gradients","summary":"  The gradients used to train neural networks are typically computed using\nbackpropagation. While an efficient way to obtain exact gradients,\nbackpropagation is computationally expensive, hinders parallelization, and is\nbiologically implausible. Forward gradients are an approach to approximate the\ngradients from directional derivatives along random tangents computed by\nforward-mode automatic differentiation. So far, research has focused on using a\nsingle tangent per step. This paper provides an in-depth analysis of\nmulti-tangent forward gradients and introduces an improved approach to\ncombining the forward gradients from multiple tangents based on orthogonal\nprojections. We demonstrate that increasing the number of tangents improves\nboth approximation quality and optimization performance across various tasks.\n","authors":["Katharina Flügel","Daniel Coquelin","Marie Weiel","Achim Streit","Markus Götz"],"pdf_url":"https://arxiv.org/pdf/2410.17764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12676v2","updated":"2024-10-23T11:01:45Z","published":"2023-12-20T00:31:43Z","title":"Bayesian Analysis of Combinatorial Gaussian Process Bandits","summary":"  We consider the combinatorial volatile Gaussian process (GP) semi-bandit\nproblem. Each round, an agent is provided a set of available base arms and must\nselect a subset of them to maximize the long-term cumulative reward. We study\nthe Bayesian setting and provide novel Bayesian cumulative regret bounds for\nthree GP-based algorithms: GP-UCB, GP-BayesUCB and GP-TS. Our bounds extend\nprevious results for GP-UCB and GP-TS to the infinite, volatile and\ncombinatorial setting, and to the best of our knowledge, we provide the first\nregret bound for GP-BayesUCB. Volatile arms encompass other widely considered\nbandit problems such as contextual bandits. Furthermore, we employ our\nframework to address the challenging real-world problem of online\nenergy-efficient navigation, where we demonstrate its effectiveness compared to\nthe alternatives.\n","authors":["Jack Sandberg","Niklas Åkerblom","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2312.12676v2.pdf","comment":"32 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.17762v1","updated":"2024-10-23T11:01:39Z","published":"2024-10-23T11:01:39Z","title":"Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted\n  Transformer Network","summary":"  Quality-of-Service (QoS) prediction is a critical task in the service\nlifecycle, enabling precise and adaptive service recommendations by\nanticipating performance variations over time in response to evolving network\nuncertainties and user preferences. However, contemporary QoS prediction\nmethods frequently encounter data sparsity and cold-start issues, which hinder\naccurate QoS predictions and limit the ability to capture diverse user\npreferences. Additionally, these methods often assume QoS data reliability,\nneglecting potential credibility issues such as outliers and the presence of\ngreysheep users and services with atypical invocation patterns. Furthermore,\ntraditional approaches fail to leverage diverse features, including\ndomain-specific knowledge and complex higher-order patterns, essential for\naccurate QoS predictions. In this paper, we introduce a real-time, trust-aware\nframework for temporal QoS prediction to address the aforementioned challenges,\nfeaturing an end-to-end deep architecture called the Hypergraph Convoluted\nTransformer Network (HCTN). HCTN combines a hypergraph structure with graph\nconvolution over hyper-edges to effectively address high-sparsity issues by\ncapturing complex, high-order correlations. Complementing this, the transformer\nnetwork utilizes multi-head attention along with parallel 1D convolutional\nlayers and fully connected dense blocks to capture both fine-grained and\ncoarse-grained dynamic patterns. Additionally, our approach includes a\nsparsity-resilient solution for detecting greysheep users and services,\nincorporating their unique characteristics to improve prediction accuracy.\nTrained with a robust loss function resistant to outliers, HCTN demonstrated\nstate-of-the-art performance on the large-scale WSDREAM-2 datasets for response\ntime and throughput.\n","authors":["Suraj Kumar","Soumi Chattopadhyay","Chandranath Adak"],"pdf_url":"https://arxiv.org/pdf/2410.17762v1.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.17760v1","updated":"2024-10-23T10:56:05Z","published":"2024-10-23T10:56:05Z","title":"Topology meets Machine Learning: An Introduction using the Euler\n  Characteristic Transform","summary":"  This overview article makes the case for how topological concepts can enrich\nresearch in machine learning. Using the Euler Characteristic Transform (ECT), a\ngeometrical-topological invariant, as a running example, I present different\nuse cases that result in more efficient models for analyzing point clouds,\ngraphs, and meshes. Moreover, I outline a vision for how topological concepts\ncould be used in the future, comprising (1) the learning of functions on\ntopological spaces, (2) the building of hybrid models that imbue neural\nnetworks with knowledge about the topological information in data, and (3) the\nanalysis of qualitative properties of neural networks. With current research\nalready addressing some of these aspects, this article thus serves as an\nintroduction and invitation to this nascent area of research.\n","authors":["Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.17760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17758v1","updated":"2024-10-23T10:50:07Z","published":"2024-10-23T10:50:07Z","title":"Escaping the Forest: Sparse Interpretable Neural Networks for Tabular\n  Data","summary":"  Tabular datasets are widely used in scientific disciplines such as biology.\nWhile these disciplines have already adopted AI methods to enhance their\nfindings and analysis, they mainly use tree-based methods due to their\ninterpretability. At the same time, artificial neural networks have been shown\nto offer superior flexibility and depth for rich and complex non-tabular\nproblems, but they are falling behind tree-based models for tabular data in\nterms of performance and interpretability. Although sparsity has been shown to\nimprove the interpretability and performance of ANN models for complex\nnon-tabular datasets, enforcing sparsity structurally and formatively for\ntabular data before training the model, remains an open question. To address\nthis question, we establish a method that infuses sparsity in neural networks\nby utilising attention mechanisms to capture the features' importance in\ntabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with\nattention mechanisms, are more effective than tree-based models, reaching the\nstate-of-the-art on biological datasets. They further permit the extraction of\ninsights from these datasets and achieve better performance than post-hoc\nmethods like SHAP.\n","authors":["Salvatore Raieli","Abdulrahman Altahhan","Nathalie Jeanray","Stéphane Gerart","Sebastien Vachenc"],"pdf_url":"https://arxiv.org/pdf/2410.17758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05549v3","updated":"2024-10-23T10:31:03Z","published":"2023-01-12T18:46:28Z","title":"On the explainability of quantum neural networks based on variational\n  quantum circuits","summary":"  Ridge functions are used to describe and study the lower bound of the\napproximation done by the neural networks which can be written as a linear\ncombination of activation functions. If the activation functions are also ridge\nfunctions, these networks are called explainable neural networks.\n  In this brief paper, we first show that quantum neural networks which are\nbased on variational quantum circuits can be written as a linear combination of\nridge functions by following matrix notations. Consequently, we show that the\ninterpretability and explainability of such quantum neural networks can be\ndirectly considered and studied as an approximation with the linear combination\nof ridge functions.\n","authors":["Ammar Daskin"],"pdf_url":"https://arxiv.org/pdf/2301.05549v3.pdf","comment":"a brief paper,a few missing references have been added"},{"id":"http://arxiv.org/abs/2410.17751v1","updated":"2024-10-23T10:28:17Z","published":"2024-10-23T10:28:17Z","title":"VISAGE: Video Synthesis using Action Graphs for Surgery","summary":"  Surgical data science (SDS) is a field that analyzes patient data before,\nduring, and after surgery to improve surgical outcomes and skills. However,\nsurgical data is scarce, heterogeneous, and complex, which limits the\napplicability of existing machine learning methods. In this work, we introduce\nthe novel task of future video generation in laparoscopic surgery. This task\ncan augment and enrich the existing surgical data and enable various\napplications, such as simulation, analysis, and robot-aided surgery.\nUltimately, it involves not only understanding the current state of the\noperation but also accurately predicting the dynamic and often unpredictable\nnature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis\nusing Action Graphs for Surgery), leverages the power of action scene graphs to\ncapture the sequential nature of laparoscopic procedures and utilizes diffusion\nmodels to synthesize temporally coherent video sequences. VISAGE predicts the\nfuture frames given only a single initial frame, and the action graph triplets.\nBy incorporating domain-specific knowledge through the action graph, VISAGE\nensures the generated videos adhere to the expected visual and motion patterns\nobserved in real laparoscopic procedures. The results of our experiments\ndemonstrate high-fidelity video generation for laparoscopy procedures, which\nenables various applications in SDS.\n","authors":["Yousef Yeganeh","Rachmadio Lazuardi","Amir Shamseddin","Emine Dari","Yash Thirani","Nassir Navab Azade Farshad"],"pdf_url":"https://arxiv.org/pdf/2410.17751v1.pdf","comment":"Accepted at MICCAI 2024 Embodied AI and Robotics for HealTHcare\n  (EARTH) Workshop"},{"id":"http://arxiv.org/abs/2410.17748v1","updated":"2024-10-23T10:23:53Z","published":"2024-10-23T10:23:53Z","title":"Can Uncertainty Quantification Enable Better Learning-based Index\n  Tuning?","summary":"  Index tuning is crucial for optimizing database performance by selecting\noptimal indexes based on workload. The key to this process lies in an accurate\nand efficient benefit estimator. Traditional methods relying on what-if tools\noften suffer from inefficiency and inaccuracy. In contrast, learning-based\nmodels provide a promising alternative but face challenges such as instability,\nlack of interpretability, and complex management. To overcome these\nlimitations, we adopt a novel approach: quantifying the uncertainty in\nlearning-based models' results, thereby combining the strengths of both\ntraditional and learning-based methods for reliable index tuning. We propose\nBeauty, the first uncertainty-aware framework that enhances learning-based\nmodels with uncertainty quantification and uses what-if tools as a\ncomplementary mechanism to improve reliability and reduce management\ncomplexity. Specifically, we introduce a novel method that combines AutoEncoder\nand Monte Carlo Dropout to jointly quantify uncertainty, tailored to the\ncharacteristics of benefit estimation tasks. In experiments involving sixteen\nmodels, our approach outperformed existing uncertainty quantification methods\nin the majority of cases. We also conducted index tuning tests on six datasets.\nBy applying the Beauty framework, we eliminated worst-case scenarios and more\nthan tripled the occurrence of best-case scenarios.\n","authors":["Tao Yu","Zhaonian Zou","Hao Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.17748v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.17744v1","updated":"2024-10-23T10:17:13Z","published":"2024-10-23T10:17:13Z","title":"Learning Versatile Skills with Curriculum Masking","summary":"  Masked prediction has emerged as a promising pretraining paradigm in offline\nreinforcement learning (RL) due to its versatile masking schemes, enabling\nflexible inference across various downstream tasks with a unified model.\nDespite the versatility of masked prediction, it remains unclear how to balance\nthe learning of skills at different levels of complexity. To address this, we\npropose CurrMask, a curriculum masking pretraining paradigm for sequential\ndecision making. Motivated by how humans learn by organizing knowledge in a\ncurriculum, CurrMask adjusts its masking scheme during pretraining for learning\nversatile skills. Through extensive experiments, we show that CurrMask exhibits\nsuperior zero-shot performance on skill prompting tasks, goal-conditioned\nplanning tasks, and competitive finetuning performance on offline RL tasks.\nAdditionally, our analysis of training dynamics reveals that CurrMask gradually\nacquires skills of varying complexity by dynamically adjusting its masking\nscheme.\n","authors":["Yao Tang","Zhihui Xie","Zichuan Lin","Deheng Ye","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2410.17744v1.pdf","comment":"NeurIPS 2024 poster, 21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.03094v2","updated":"2024-10-23T10:09:10Z","published":"2024-07-03T13:34:33Z","title":"Conformal Prediction for Causal Effects of Continuous Treatments","summary":"  Uncertainty quantification of causal effects is crucial for safety-critical\napplications such as personalized medicine. A powerful approach for this is\nconformal prediction, which has several practical benefits due to\nmodel-agnostic finite-sample guarantees. Yet, existing methods for conformal\nprediction of causal effects are limited to binary/discrete treatments and make\nhighly restrictive assumptions such as known propensity scores. In this work,\nwe provide a novel conformal prediction method for potential outcomes of\ncontinuous treatments. We account for the additional uncertainty introduced\nthrough propensity estimation so that our conformal prediction intervals are\nvalid even if the propensity score is unknown. Our contributions are\nthree-fold: (1) We derive finite-sample prediction intervals for potential\noutcomes of continuous treatments. (2) We provide an algorithm for calculating\nthe derived intervals. (3) We demonstrate the effectiveness of the conformal\nprediction intervals in experiments on synthetic and real-world datasets. To\nthe best of our knowledge, we are the first to propose conformal prediction for\ncontinuous treatments when the propensity score is unknown and must be\nestimated from data.\n","authors":["Maresa Schröder","Dennis Frauen","Jonas Schweisthal","Konstantin Heß","Valentyn Melnychuk","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2407.03094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05354v3","updated":"2024-10-23T09:51:11Z","published":"2024-10-07T13:44:49Z","title":"Over-the-Air Federated Learning in Cell-Free MIMO with Long-term Power\n  Constraint","summary":"  Wireless networks supporting artificial intelligence have gained significant\nattention, with Over-the-Air Federated Learning emerging as a key application\ndue to its unique transmission and distributed computing characteristics. This\npaper derives error bounds for Over-the-Air Federated Learning in a Cell-free\nMIMO system and formulates an optimization problem to minimize optimality gap\nvia joint optimization of power control and beamforming. We introduce the\nMOP-LOFPC algorithm, which employs Lyapunov optimization to decouple long-term\nconstraints across rounds while requiring only causal channel state\ninformation. Experimental results demonstrate that MOP-LOFPC achieves a better\nand more flexible trade-off between the model's training loss and adherence to\nlong-term power constraints compared to existing baselines.\n","authors":["Yifan Wang","Cheng Zhang","Yuanndon Zhuang","Mingzeng Dai","Haiming Wang","Yongming Huang"],"pdf_url":"https://arxiv.org/pdf/2410.05354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11642v2","updated":"2024-10-23T09:43:03Z","published":"2024-10-15T14:31:54Z","title":"Improve Value Estimation of Q Function and Reshape Reward with Monte\n  Carlo Tree Search","summary":"  Reinforcement learning has achieved remarkable success in perfect information\ngames such as Go and Atari, enabling agents to compete at the highest levels\nagainst human players. However, research in reinforcement learning for\nimperfect information games has been relatively limited due to the more complex\ngame structures and randomness. Traditional methods face challenges in training\nand improving performance in imperfect information games due to issues like\ninaccurate Q value estimation and reward sparsity. In this paper, we focus on\nUno, an imperfect information game, and aim to address these problems by\nreducing Q value overestimation and reshaping reward function. We propose a\nnovel algorithm that utilizes Monte Carlo Tree Search to average the value\nestimations in Q function. Even though we choose Double Deep Q Learning as the\nfoundational framework in this paper, our method can be generalized and used in\nany algorithm which needs Q value estimation, such as the Actor-Critic.\nAdditionally, we employ Monte Carlo Tree Search to reshape the reward structure\nin the game environment. We compare our algorithm with several traditional\nmethods applied to games such as Double Deep Q Learning, Deep Monte Carlo and\nNeural Fictitious Self Play, and the experiments demonstrate that our algorithm\nconsistently outperforms these approaches, especially as the number of players\nin Uno increases, indicating a higher level of difficulty.\n","authors":["Jiamian Li"],"pdf_url":"https://arxiv.org/pdf/2410.11642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17715v1","updated":"2024-10-23T09:42:17Z","published":"2024-10-23T09:42:17Z","title":"Continual Learning on a Data Diet","summary":"  Continual Learning (CL) methods usually learn from all available data.\nHowever, this is not the case in human cognition which efficiently focuses on\nkey experiences while disregarding the redundant information. Similarly, not\nall data points in a dataset have equal potential; some can be more informative\nthan others. This disparity may significantly impact the performance, as both\nthe quality and quantity of samples directly influence the model's\ngeneralizability and efficiency. Drawing inspiration from this, we explore the\npotential of learning from important samples and present an empirical study for\nevaluating coreset selection techniques in the context of CL to stimulate\nresearch in this unexplored area. We train different continual learners on\nincreasing amounts of selected samples and investigate the learning-forgetting\ndynamics by shedding light on the underlying mechanisms driving their improved\nstability-plasticity balance. We present several significant observations:\nlearning from selectively chosen samples (i) enhances incremental accuracy,\n(ii) improves knowledge retention of previous tasks, and (iii) refines learned\nrepresentations. This analysis contributes to a deeper understanding of\nselective learning strategies in CL scenarios.\n","authors":["Elif Ceren Gok Yildirim","Murat Onur Yildirim","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2410.17715v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.08745v5","updated":"2024-10-23T09:40:44Z","published":"2023-11-15T07:27:40Z","title":"Using Stochastic Gradient Descent to Smooth Nonconvex Functions:\n  Analysis of Implicit Graduated Optimization","summary":"  The graduated optimization approach is a heuristic method for finding global\noptimal solutions for nonconvex functions by using a function smoothing\noperation with stochastic noise. We show that stochastic noise in stochastic\ngradient descent (SGD) has the effect of smoothing the objective function, the\ndegree of which is determined by the learning rate, batch size, and variance of\nthe stochastic gradient. Using this finding, we propose and analyze a new\ngraduated optimization algorithm that varies the degree of smoothing by varying\nthe learning rate and batch size, and provide experimental results on image\nclassification tasks with ResNets that support our theoretical findings. We\nfurther show that there is an interesting correlation between the degree of\nsmoothing by SGD's stochastic noise, the well-studied ``sharpness'' indicator,\nand the generalization performance of the model.\n","authors":["Naoki Sato","Hideaki Iiduka"],"pdf_url":"https://arxiv.org/pdf/2311.08745v5.pdf","comment":"The latest version was updated in October 2024. Under review"},{"id":"http://arxiv.org/abs/2410.17711v1","updated":"2024-10-23T09:36:21Z","published":"2024-10-23T09:36:21Z","title":"Beware of Calibration Data for Pruning Large Language Models","summary":"  As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL).\n","authors":["Yixin Ji","Yang Xiang","Juntao Li","Qingrong Xia","Ping Li","Xinyu Duan","Zhefeng Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17711v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2402.09891v2","updated":"2024-10-23T09:29:39Z","published":"2024-02-15T11:34:38Z","title":"Do causal predictors generalize better to new domains?","summary":"  We study how well machine learning models trained on causal features\ngeneralize across domains. We consider 16 prediction tasks on tabular datasets\ncovering applications in health, employment, education, social benefits, and\npolitics. Each dataset comes with multiple domains, allowing us to test how\nwell a model trained in one domain performs in another. For each prediction\ntask, we select features that have a causal influence on the target of\nprediction. Our goal is to test the hypothesis that models trained on causal\nfeatures generalize better across domains. Without exception, we find that\npredictors using all available features, regardless of causality, have better\nin-domain and out-of-domain accuracy than predictors using causal features.\nMoreover, even the absolute drop in accuracy from one domain to the other is no\nbetter for causal predictors than for models that use all features. In\naddition, we show that recent causal machine learning methods for domain\ngeneralization do not perform better in our evaluation than standard predictors\ntrained on the set of causal features. Likewise, causal discovery algorithms\neither fail to run or select causal variables that perform no better than our\nselection. Extensive robustness checks confirm that our findings are stable\nunder variable misclassification.\n","authors":["Vivian Y. Nastl","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2402.09891v2.pdf","comment":"118 pages, 55 figures, accepted at NeurIPS'24"},{"id":"http://arxiv.org/abs/2408.14762v4","updated":"2024-10-23T09:25:58Z","published":"2024-08-27T03:30:01Z","title":"Explainable Hierarchical Urban Representation Learning for Commuting\n  Flow Prediction","summary":"  Commuting flow prediction is an essential task for municipal operations in\nthe real world. Previous studies have revealed that it is feasible to estimate\nthe commuting origin-destination (OD) demand within a city using multiple\nauxiliary data. However, most existing methods are not suitable to deal with a\nsimilar task at a large scale, namely within a prefecture or the whole nation,\nowing to the increased number of geographical units that need to be maintained.\nIn addition, region representation learning is a universal approach for gaining\nurban knowledge for diverse metropolitan downstream tasks. Although many\nresearchers have developed comprehensive frameworks to describe urban units\nfrom multi-source data, they have not clarified the relationship between the\nselected geographical elements. Furthermore, metropolitan areas naturally\npreserve ranked structures, like cities and their inclusive districts, which\nmakes elucidating relations between cross-level urban units necessary.\nTherefore, we develop a heterogeneous graph-based model to generate meaningful\nregion embeddings at multiple spatial resolutions for predicting different\ntypes of inter-level OD flows. To demonstrate the effectiveness of the proposed\nmethod, extensive experiments were conducted using real-world aggregated mobile\nphone datasets collected from Shizuoka Prefecture, Japan. The results indicate\nthat our proposed model outperforms existing models in terms of a uniform urban\nstructure. We extend the understanding of predicted results using reasonable\nexplanations to enhance the credibility of the model.\n","authors":["Mingfei Cai","Yanbo Pang","Yoshihide Sekimoto"],"pdf_url":"https://arxiv.org/pdf/2408.14762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17700v1","updated":"2024-10-23T09:22:43Z","published":"2024-10-23T09:22:43Z","title":"Scalable Random Feature Latent Variable Models","summary":"  Random feature latent variable models (RFLVMs) represent the state-of-the-art\nin latent variable models, capable of handling non-Gaussian likelihoods and\neffectively uncovering patterns in high-dimensional data. However, their heavy\nreliance on Monte Carlo sampling results in scalability issues which makes it\ndifficult to use these models for datasets with a massive number of\nobservations. To scale up RFLVMs, we turn to the optimization-based variational\nBayesian inference (VBI) algorithm which is known for its scalability compared\nto sampling-based methods. However, implementing VBI for RFLVMs poses\nchallenges, such as the lack of explicit probability distribution functions\n(PDFs) for the Dirichlet process (DP) in the kernel learning component, and the\nincompatibility of existing VBI algorithms with RFLVMs. To address these\nissues, we introduce a stick-breaking construction for DP to obtain an explicit\nPDF and a novel VBI algorithm called ``block coordinate descent variational\ninference\" (BCD-VI). This enables the development of a scalable version of\nRFLVMs, or in short, SRFLVM. Our proposed method shows scalability,\ncomputational efficiency, superior performance in generating informative latent\nrepresentations and the ability of imputing missing data across various\nreal-world datasets, outperforming state-of-the-art competitors.\n","authors":["Ying Li","Zhidi Lin","Yuhao Liu","Michael Minyi Zhang","Pablo M. Olmos","Petar M. Djurić"],"pdf_url":"https://arxiv.org/pdf/2410.17700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17696v1","updated":"2024-10-23T09:16:22Z","published":"2024-10-23T09:16:22Z","title":"Optimizing Load Scheduling in Power Grids Using Reinforcement Learning\n  and Markov Decision Processes","summary":"  Power grid load scheduling is a critical task that ensures the balance\nbetween electricity generation and consumption while minimizing operational\ncosts and maintaining grid stability. Traditional optimization methods often\nstruggle with the dynamic and stochastic nature of power systems, especially\nwhen faced with renewable energy sources and fluctuating demand. This paper\nproposes a reinforcement learning (RL) approach using a Markov Decision Process\n(MDP) framework to address the challenges of dynamic load scheduling. The MDP\nis defined by a state space representing grid conditions, an action space\ncovering control operations like generator adjustments and storage management,\nand a reward function balancing economic efficiency and system reliability. We\ninvestigate the application of various RL algorithms, from basic Q-Learning to\nmore advanced Deep Q-Networks (DQN) and Actor-Critic methods, to determine\noptimal scheduling policies. The proposed approach is evaluated through a\nsimulated power grid environment, demonstrating its potential to improve\nscheduling efficiency and adapt to variable demand patterns. Our results show\nthat the RL-based method provides a robust and scalable solution for real-time\nload scheduling, contributing to the efficient management of modern power\ngrids.\n","authors":["Dongwen Luo"],"pdf_url":"https://arxiv.org/pdf/2410.17696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03648v2","updated":"2024-10-23T09:11:00Z","published":"2023-08-07T14:58:53Z","title":"Generative Forests","summary":"  We focus on generative AI for a type of data that still represent one of the\nmost prevalent form of data: tabular data. Our paper introduces two key\ncontributions: a new powerful class of forest-based models fit for such tasks\nand a simple training algorithm with strong convergence guarantees in a\nboosting model that parallels that of the original weak / strong supervised\nlearning setting. This algorithm can be implemented by a few tweaks to the most\npopular induction scheme for decision tree induction (i.e. supervised learning)\nwith two classes. Experiments on the quality of generated data display\nsubstantial improvements compared to the state of the art. The losses our\nalgorithm minimize and the structure of our models make them practical for\nrelated tasks that require fast estimation of a density given a generative\nmodel and an observation (even partially specified): such tasks include missing\ndata imputation and density estimation. Additional experiments on these tasks\nreveal that our models can be notably good contenders to diverse state of the\nart methods, relying on models as diverse as (or mixing elements of) trees,\nneural nets, kernels or graphical models.\n","authors":["Richard Nock","Mathieu Guillame-Bert"],"pdf_url":"https://arxiv.org/pdf/2308.03648v2.pdf","comment":"NeurIPS'24"},{"id":"http://arxiv.org/abs/2402.04892v2","updated":"2024-10-23T09:04:57Z","published":"2024-02-07T14:24:04Z","title":"Probabilistic ML Verification via Weighted Model Integration","summary":"  In machine learning (ML) verification, the majority of procedures are\nnon-quantitative and therefore cannot be used for verifying probabilistic\nmodels, or be applied in domains where hard guarantees are practically\nunachievable. The probabilistic formal verification (PFV) of ML models is in\nits infancy, with the existing approaches limited to specific ML models,\nproperties, or both. This contrasts with standard formal methods techniques,\nwhose successful adoption in real-world scenarios is also due to their support\nfor a wide range of properties and diverse systems. We propose a unifying\nframework for the PFV of ML systems based on Weighted Model Integration (WMI),\na relatively recent formalism for probabilistic inference with algebraic and\nlogical constraints. Crucially, reducing the PFV of ML models to WMI enables\nthe verification of many properties of interest over a wide range of systems,\naddressing multiple limitations of deterministic verification and ad-hoc\nalgorithms. We substantiate the generality of the approach on prototypical\ntasks involving the verification of group fairness, monotonicity, robustness to\nnoise, probabilistic local robustness and equivalence among predictors. We\ncharacterize the challenges related to the scalability of the approach and,\nthrough our WMI-based perspective, we show how successful scaling techniques in\nthe ML verification literature can be generalized beyond their original scope.\n","authors":["Paolo Morettin","Andrea Passerini","Roberto Sebastiani"],"pdf_url":"https://arxiv.org/pdf/2402.04892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10192v3","updated":"2024-10-23T08:39:00Z","published":"2024-02-15T18:48:32Z","title":"Multi-Excitation Projective Simulation with a Many-Body Physics Inspired\n  Inductive Bias","summary":"  With the impressive progress of deep learning, applications relying on\nmachine learning are increasingly being integrated into daily life. However,\nmost deep learning models have an opaque, oracle-like nature making it\ndifficult to interpret and understand their decisions. This problem led to the\ndevelopment of the field known as eXplainable Artificial Intelligence (XAI).\nOne method in this field known as Projective Simulation (PS) models a\nchain-of-thought as a random walk of a particle on a graph with vertices that\nhave concepts attached to them. While this description has various benefits,\nincluding the possibility of quantization, it cannot be naturally used to model\nthoughts that combine several concepts simultaneously. To overcome this\nlimitation, we introduce Multi-Excitation Projective Simulation (mePS), a\ngeneralization that considers a chain-of-thought to be a random walk of several\nparticles on a hypergraph. A definition for a dynamic hypergraph is put forward\nto describe the agent's training history along with applications to AI and\nhypergraph visualization. An inductive bias inspired by the remarkably\nsuccessful few-body interaction models used in quantum many-body physics is\nformalized for our classical mePS framework and employed to tackle the\nexponential complexity associated with naive implementations of hypergraphs. We\nprove that our inductive bias reduces the complexity from exponential to\npolynomial, with the exponent representing the cutoff on how many particles can\ninteract. We numerically apply our method to two toy environments and a more\ncomplex scenario modelling the diagnosis of a broken computer. These\nenvironments demonstrate the resource savings provided by an appropriate choice\nof inductive bias, as well as showcasing aspects of interpretability. A quantum\nmodel for mePS is also briefly outlined and some future directions for it are\ndiscussed.\n","authors":["Philip A. LeMaitre","Marius Krumm","Hans J. Briegel"],"pdf_url":"https://arxiv.org/pdf/2402.10192v3.pdf","comment":"26 pages, 8 figures; Code repository at\n  https://github.com/MariusKrumm/ManyBodyMEPS. Reorganized main text for better\n  readability"},{"id":"http://arxiv.org/abs/2404.15993v4","updated":"2024-10-23T08:33:54Z","published":"2024-04-24T17:10:35Z","title":"Uncertainty Estimation and Quantification for LLMs: A Simple Supervised\n  Approach","summary":"  In this paper, we study the problem of uncertainty estimation and calibration\nfor LLMs. We begin by formulating the uncertainty estimation problem, a\nrelevant yet underexplored area in existing literature. We then propose a\nsupervised approach that leverages labeled datasets to estimate the uncertainty\nin LLMs' responses. Based on the formulation, we illustrate the difference\nbetween the uncertainty estimation for LLMs and that for standard ML models and\nexplain why the hidden neurons of the LLMs may contain uncertainty information.\nOur designed approach demonstrates the benefits of utilizing hidden activations\nto enhance uncertainty estimation across various tasks and shows robust\ntransferability in out-of-distribution settings. We distinguish the uncertainty\nestimation task from the uncertainty calibration task and show that better\nuncertainty estimation leads to better calibration performance. Furthermore,\nour method is easy to implement and adaptable to different levels of model\naccessibility including black box, grey box, and white box.\n","authors":["Linyu Liu","Yu Pan","Xiaocheng Li","Guanting Chen"],"pdf_url":"https://arxiv.org/pdf/2404.15993v4.pdf","comment":"29 pages, 14 figures"},{"id":"http://arxiv.org/abs/2405.11752v2","updated":"2024-10-23T08:29:20Z","published":"2024-05-20T03:26:58Z","title":"Towards Foundation Model for Chemical Reactor Modeling: Meta-Learning\n  with Physics-Informed Adaptation","summary":"  In this work, we present a novel application of foundation models for\nchemical reactor modeling. Accurate modeling of real-world chemical reactors\nthrough first-principles is often challenging, and the process of rebuilding\nand retraining models for each new chemical process is inefficient. This raises\na critical question: can we develop a single, universal neural network (i.e., a\nfoundation model) that can rapidly adapt to any new chemical process in a\nreactor? To address this, we propose a foundation model for chemical reactor\nmodeling that employs a meta-learning approach, followed by physics-informed\nfine-tuning on new tasks with only a few data samples. Our model is designed to\ngeneralize across three classic reactor types: continuous stirred tank\nreactors, batch reactors, and plug flow reactors. Compared to conventional\nmethods such as data-driven learning, physics-informed learning, transfer\nlearning, and meta-learning, our approach demonstrates superior performance in\nfew-shot scenarios. Specifically, it shows rapid adaptation to unseen reactions\nwith varying integer orders across different reactor set-ups, requiring minimal\ndata for fine-tuning. Source code is available at\nhttps://github.com/killingbear999/chemical-reactor-foundation-model.\n","authors":["Zihao Wang","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2405.11752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17661v1","updated":"2024-10-23T08:24:47Z","published":"2024-10-23T08:24:47Z","title":"PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a\n  resource-limited Context","summary":"  Following their success in natural language processing (NLP), there has been\na shift towards transformer models in computer vision. While transformers\nperform well and offer promising multi-tasking performance, due to their high\ncompute requirements, many resource-constrained applications still rely on\nconvolutional or hybrid models that combine the benefits of convolution and\nattention layers and achieve the best results in the sub 100M parameter range.\nSimultaneously, task adaptation techniques that allow for the use of one shared\ntransformer backbone for multiple downstream tasks, resulting in great storage\nsavings at negligible cost in performance, have not yet been adopted for hybrid\ntransformers. In this work, we investigate how to achieve the best\ntask-adaptation performance and introduce PETAH: Parameter Efficient Task\nAdaptation for Hybrid Transformers. We further combine PETAH adaptation with\npruning to achieve highly performant and storage friendly models for\nmulti-tasking. In our extensive evaluation on classification and other vision\ntasks, we demonstrate that our PETAH-adapted hybrid models outperform\nestablished task-adaptation techniques for ViTs while requiring fewer\nparameters and being more efficient on mobile hardware.\n","authors":["Maximilian Augustin","Syed Shakib Sarwar","Mostafa Elhoushi","Sai Qian Zhang","Yuecheng Li","Barbara De Salvo"],"pdf_url":"https://arxiv.org/pdf/2410.17661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17655v1","updated":"2024-10-23T08:18:26Z","published":"2024-10-23T08:18:26Z","title":"Mapping the Media Landscape: Predicting Factual Reporting and Political\n  Bias Through Web Interactions","summary":"  Bias assessment of news sources is paramount for professionals,\norganizations, and researchers who rely on truthful evidence for information\ngathering and reporting. While certain bias indicators are discernible from\ncontent analysis, descriptors like political bias and fake news pose greater\nchallenges. In this paper, we propose an extension to a recently presented news\nmedia reliability estimation method that focuses on modeling outlets and their\nlongitudinal web interactions. Concretely, we assess the classification\nperformance of four reinforcement learning strategies on a large news media\nhyperlink graph. Our experiments, targeting two challenging bias descriptors,\nfactual reporting and political bias, showed a significant performance\nimprovement at the source media level. Additionally, we validate our methods on\nthe CLEF 2023 CheckThat! Lab challenge, outperforming the reported results in\nboth, F1-score and the official MAE metric. Furthermore, we contribute by\nreleasing the largest annotated dataset of news source media, categorized with\nfactual reporting and political bias labels. Our findings suggest that\nprofiling news media sources based on their hyperlink interactions over time is\nfeasible, offering a bird's-eye view of evolving media landscapes.\n","authors":["Dairazalia Sánchez-Cortés","Sergio Burdisso","Esaú Villatoro-Tello","Petr Motlicek"],"pdf_url":"https://arxiv.org/pdf/2410.17655v1.pdf","comment":"Accepted to CLEF 2024"},{"id":"http://arxiv.org/abs/2407.13432v3","updated":"2024-10-23T08:07:05Z","published":"2024-07-18T12:01:09Z","title":"The Art of Imitation: Learning Long-Horizon Manipulation Tasks from Few\n  Demonstrations","summary":"  Task Parametrized Gaussian Mixture Models (TP-GMM) are a sample-efficient\nmethod for learning object-centric robot manipulation tasks. However, there are\nseveral open challenges to applying TP-GMMs in the wild. In this work, we\ntackle three crucial challenges synergistically. First, end-effector velocities\nare non-Euclidean and thus hard to model using standard GMMs. We thus propose\nto factorize the robot's end-effector velocity into its direction and\nmagnitude, and model them using Riemannian GMMs. Second, we leverage the\nfactorized velocities to segment and sequence skills from complex demonstration\ntrajectories. Through the segmentation, we further align skill trajectories and\nhence leverage time as a powerful inductive bias. Third, we present a method to\nautomatically detect relevant task parameters per skill from visual\nobservations. Our approach enables learning complex manipulation tasks from\njust five demonstrations while using only RGB-D observations. Extensive\nexperimental evaluations on RLBench demonstrate that our approach achieves\nstate-of-the-art performance with 20-fold improved sample efficiency. Our\npolicies generalize across different environments, object instances, and object\npositions, while the learned skills are reusable.\n","authors":["Jan Ole von Hartz","Tim Welschehold","Abhinav Valada","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2407.13432v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17648v1","updated":"2024-10-23T08:07:00Z","published":"2024-10-23T08:07:00Z","title":"Towards Active Participant-Centric Vertical Federated Learning: Some\n  Representations May Be All You Need","summary":"  Vertical Federated Learning (VFL) enables collaborative model training across\ndifferent participants with distinct features and common samples, while\npreserving data privacy. Existing VFL methodologies often struggle with\nrealistic data partitions, typically incurring high communication costs and\nsignificant operational complexity. In this work, we introduce a novel\nsimplified approach to VFL, Active Participant-Centric VFL (APC-VFL), that, to\nthe best of our knowledge, is the first to require only a single communication\nround between participants, and allows the active participant to do inference\nin a non collaborative fashion. This method integrates unsupervised\nrepresentation learning with knowledge distillation to achieve comparable\naccuracy to traditional VFL methods based on vertical split learning in\nclassical settings, reducing required communication rounds by up to\n$4200\\times$, while being more flexible. Our approach also shows improvements\ncompared to non-federated local models, as well as a comparable VFL proposal,\nVFedTrans, offering an efficient and flexible solution for collaborative\nlearning.\n","authors":["Jon Irureta","Jon Imaz","Aizea Lojo","Marco González","Iñigo Perona"],"pdf_url":"https://arxiv.org/pdf/2410.17648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15294v3","updated":"2024-10-23T08:06:25Z","published":"2024-01-27T04:42:50Z","title":"Integral Operator Approaches for Scattered Data Fitting on Spheres","summary":"  This paper focuses on scattered data fitting problems on spheres. We study\nthe approximation performance of a class of weighted spectral filter\nalgorithms, including Tikhonov regularization, Landaweber iteration, spectral\ncut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded\nrandom noise. For the analysis, we develop an integral operator approach that\ncan be regarded as an extension of the widely used sampling inequality approach\nand norming set method in the community of scattered data fitting. After\nproviding an equivalence between the operator differences and quadrature rules,\nwe succeed in deriving optimal Sobolev-type error estimates of weighted\nspectral filter algorithms. Our derived error estimates do not suffer from the\nsaturation phenomenon for Tikhonov regularization in the literature,\nnative-space-barrier for existing error analysis and adapts to different\nembedding spaces. We also propose a divide-and-conquer scheme to equip weighted\nspectral filter algorithms to reduce their computational burden and present the\noptimal approximation error bounds.\n","authors":["Shao-Bo Lin"],"pdf_url":"https://arxiv.org/pdf/2401.15294v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09079v2","updated":"2024-10-23T08:05:57Z","published":"2024-06-13T13:03:37Z","title":"Hadamard Representations: Augmenting Hyperbolic Tangents in RL","summary":"  Activation functions are one of the key components of a deep neural network.\nThe most commonly used activation functions can be classed into the category of\ncontinuously differentiable (e.g. tanh) and linear-unit functions (e.g. ReLU),\nboth having their own strengths and drawbacks with respect to downstream\nperformance and representation capacity through learning (e.g. measured by the\nnumber of dead neurons and the effective rank). In reinforcement learning, the\nperformance of continuously differentiable activations often falls short as\ncompared to linear-unit functions. We provide insights into the vanishing\ngradients associated with the former, and show that the dying neuron problem is\nnot exclusive to ReLU's. To alleviate vanishing gradients and the resulting\ndying neuron problem occurring with continuously differentiable activations, we\npropose a Hadamard representation. Using deep Q-networks and proximal policy\noptimization in the Atari domain, we show faster learning, a reduction in dead\nneurons and increased effective rank.\n","authors":["Jacob E. Kooi","Mark Hoogendoorn","Vincent François-Lavet"],"pdf_url":"https://arxiv.org/pdf/2406.09079v2.pdf","comment":"24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.17647v1","updated":"2024-10-23T08:04:12Z","published":"2024-10-23T08:04:12Z","title":"Entity-based Reinforcement Learning for Autonomous Cyber Defence","summary":"  A significant challenge for autonomous cyber defence is ensuring a defensive\nagent's ability to generalise across diverse network topologies and\nconfigurations.\n  This capability is necessary for agents to remain effective when deployed in\ndynamically changing environments, such as an enterprise network where devices\nmay frequently join and leave.\n  Standard approaches to deep reinforcement learning, where policies are\nparameterised using a fixed-input multi-layer perceptron (MLP) expect\nfixed-size observation and action spaces. In autonomous cyber defence, this\nmakes it hard to develop agents that generalise to environments with network\ntopologies different from those trained on, as the number of nodes affects the\nnatural size of the observation and action spaces. To overcome this limitation,\nwe reframe the problem of autonomous network defence using entity-based\nreinforcement learning, where the observation and action space of an agent are\ndecomposed into a collection of discrete entities. This framework enables the\nuse of policy parameterisations specialised in compositional generalisation.\nNamely, we train a Transformer-based policy on the Yawning Titan cyber-security\nsimulation environment and test its generalisation capabilities across various\nnetwork topologies. We demonstrate that this approach significantly outperforms\nan MLP-based policy on fixed networks, and has the ability for zero-shot\ngeneralisation to networks of a different size to those seen in training.\n  These findings highlight the potential for entity-based reinforcement\nlearning to advance the field of autonomous cyber defence by providing more\ngeneralisable policies capable of handling variations in real-world network\nenvironments.\n","authors":["Isaac Symes Thompson","Alberto Caron","Chris Hicks","Vasilios Mavroudis"],"pdf_url":"https://arxiv.org/pdf/2410.17647v1.pdf","comment":"Material to appear in the proceedings of the 1st International\n  Workshop on Autonomous Cybersecurity at ACM CCS 2024"},{"id":"http://arxiv.org/abs/2405.04098v2","updated":"2024-10-23T07:57:41Z","published":"2024-05-07T08:05:20Z","title":"Binarized Simplicial Convolutional Neural Networks","summary":"  Graph Neural Networks have a limitation of solely processing features on\ngraph nodes, neglecting data on high-dimensional structures such as edges and\ntriangles. Simplicial Convolutional Neural Networks (SCNN) represent\nhigher-order structures using simplicial complexes to break this limitation\nalbeit still lacking time efficiency. In this paper, we propose a novel neural\nnetwork architecture on simplicial complexes named Binarized Simplicial\nConvolutional Neural Networks (Bi-SCNN) based on the combination of simplicial\nconvolution with a binary-sign forward propagation strategy. The usage of the\nHodge Laplacian on a binary-sign forward propagation enables Bi-SCNN to\nefficiently and effectively represent simplicial features that have\nhigher-order structures than traditional graph node representations. Compared\nto the previous Simplicial Convolutional Neural Networks, the reduced model\ncomplexity of Bi-SCNN shortens the execution time without sacrificing the\nprediction performance and is less prone to the over-smoothing effect.\nExperimenting with real-world citation and ocean-drifter data confirmed that\nour proposed Bi-SCNN is efficient and accurate.\n","authors":["Yi Yan","Ercan E. Kuruoglu"],"pdf_url":"https://arxiv.org/pdf/2405.04098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17631v1","updated":"2024-10-23T07:48:35Z","published":"2024-10-23T07:48:35Z","title":"Exploring structure diversity in atomic resolution microscopy with graph\n  neural networks","summary":"  The emergence of deep learning (DL) has provided great opportunities for the\nhigh-throughput analysis of atomic-resolution micrographs. However, the DL\nmodels trained by image patches in fixed size generally lack efficiency and\nflexibility when processing micrographs containing diversified atomic\nconfigurations. Herein, inspired by the similarity between the atomic\nstructures and graphs, we describe a few-shot learning framework based on an\nequivariant graph neural network (EGNN) to analyze a library of atomic\nstructures (e.g., vacancies, phases, grain boundaries, doping, etc.), showing\nsignificantly promoted robustness and three orders of magnitude reduced\ncomputing parameters compared to the image-driven DL models, which is\nespecially evident for those aggregated vacancy lines with flexible lattice\ndistortion. Besides, the intuitiveness of graphs enables quantitative and\nstraightforward extraction of the atomic-scale structural features in batches,\nthus statistically unveiling the self-assembly dynamics of vacancy lines under\nelectron beam irradiation. A versatile model toolkit is established by\nintegrating EGNN sub-models for single structure recognition to process images\ninvolving varied configurations in the form of a task chain, leading to the\ndiscovery of novel doping configurations with superior electrocatalytic\nproperties for hydrogen evolution reactions. This work provides a powerful tool\nto explore structure diversity in a fast, accurate, and intelligent manner.\n","authors":["Zheng Luo","Ming Feng","Zijian Gao","Jinyang Yu","Liang Hu","Tao Wang","Shenao Xue","Shen Zhou","Fangping Ouyang","Dawei Feng","Kele Xu","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17628v1","updated":"2024-10-23T07:44:14Z","published":"2024-10-23T07:44:14Z","title":"Feature Learning in Attention Mechanisms Is More Compact and Stable Than\n  in Convolution","summary":"  Attention and convolution are fundamental techniques in machine learning.\nWhile they use different approaches to learn features - attention mechanisms\ncapture both global and local data relathionships, while convolutional layers\nfocus on local patterns - both methods are effective for various tasks.\nAlthough the feature learning of both models is well-studied individually,\nthere has not been a direct comparison of their feature learning dynamics. In\nthis paper, we compare their Lipschitz continuity with respect to the\nWasserstein distance and covering numbers under similar settings. We\ndemonstrate that attention processes data in a more compact and stable manner.\nCompactness refers to the lower variance and intrinsic dimensionality of the\nactivation outputs, while stability refers to the changes between inputs and\noutputs. We validate our findings through experiments using topological data\nanalysis, measuring the 1-, 2-, and infinity-Wasserstein distances between the\noutputs of each layer from both models. Furthermore, we extend our comparison\nto Vision Transformers (ViTs) and ResNets, showing that while ViTs have higher\noutput variance, their feature learning is more stable than that of ResNets.\n","authors":["Baiyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.17628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17624v1","updated":"2024-10-23T07:29:30Z","published":"2024-10-23T07:29:30Z","title":"Incremental Learning of Affordances using Markov Logic Networks","summary":"  Affordances enable robots to have a semantic understanding of their\nsurroundings. This allows them to have more acting flexibility when completing\na given task. Capturing object affordances in a machine learning model is a\ndifficult task, because of their dependence on contextual information. Markov\nLogic Networks (MLN) combine probabilistic reasoning with logic that is able to\ncapture such context. Mobile robots operate in partially known environments\nwherein unseen object affordances can be observed. This new information must be\nincorporated into the existing knowledge, without having to retrain the MLN\nfrom scratch. We introduce the MLN Cumulative Learning Algorithm (MLN-CLA).\nMLN-CLA learns new relations in various knowledge domains by retaining\nknowledge and only updating the changed knowledge, for which the MLN is\nretrained. We show that MLN-CLA is effective for accumulative learning and\nzero-shot affordance inference, outperforming strong baselines.\n","authors":["George Potter","Gertjan Burghouts","Joris Sijs"],"pdf_url":"https://arxiv.org/pdf/2410.17624v1.pdf","comment":"accepted at IEEE IRC 2024"},{"id":"http://arxiv.org/abs/2410.05623v2","updated":"2024-10-23T07:28:19Z","published":"2024-10-08T02:11:35Z","title":"Understanding Gradient Boosting Classifier: Training, Prediction, and\n  the Role of $γ_j$","summary":"  The Gradient Boosting Classifier (GBC) is a widely used machine learning\nalgorithm for binary classification, which builds decision trees iteratively to\nminimize prediction errors. This document explains the GBC's training and\nprediction processes, focusing on the computation of terminal node values\n$\\gamma_j$, which are crucial to optimizing the logistic loss function. We\nderive $\\gamma_j$ through a Taylor series approximation and provide a\nstep-by-step pseudocode for the algorithm's implementation. The guide explains\nthe theory of GBC and its practical application, demonstrating its\neffectiveness in binary classification tasks. We provide a step-by-step example\nin the appendix to help readers understand.\n","authors":["Hung-Hsuan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05811v3","updated":"2024-10-23T07:26:07Z","published":"2024-03-09T06:19:53Z","title":"Statistical Efficiency of Distributional Temporal Difference Learning","summary":"  Distributional reinforcement learning (DRL) has achieved empirical success in\nvarious domains. One core task in the field of DRL is distributional policy\nevaluation, which involves estimating the return distribution $\\eta^\\pi$ for a\ngiven policy $\\pi$. The distributional temporal difference learning has been\naccordingly proposed, which is an extension of the temporal difference learning\n(TD) in the classic RL area. In the tabular case, \\citet{rowland2018analysis}\nand \\citet{rowland2023analysis} proved the asymptotic convergence of two\ninstances of distributional TD, namely categorical temporal difference learning\n(CTD) and quantile temporal difference learning (QTD), respectively. In this\npaper, we go a step further and analyze the finite-sample performance of\ndistributional TD. To facilitate theoretical analysis, we propose\nnon-parametric distributional TD learning (NTD). For a $\\gamma$-discounted\ninfinite-horizon tabular Markov decision process, we show that for NTD we need\n$\\tilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1-\\gamma)^{2p+1}}\\right)$ iterations\nto achieve an $\\varepsilon$-optimal estimator with high probability, when the\nestimation error is measured by the $p$-Wasserstein distance. This sample\ncomplexity bound is minimax optimal up to logarithmic factors in the case of\nthe $1$-Wasserstein distance. To achieve this, we establish a novel Freedman's\ninequality in Hilbert spaces, which would be of independent interest. In\naddition, we revisit CTD, showing that the same non-asymptotic convergence\nbounds hold for CTD in the case of the $p$-Wasserstein distance for $p\\geq 1$.\n","authors":["Yang Peng","Liangyu Zhang","Zhihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05811v3.pdf","comment":"NeurIPS 2024 (oral)"},{"id":"http://arxiv.org/abs/2410.17617v1","updated":"2024-10-23T07:14:37Z","published":"2024-10-23T07:14:37Z","title":"Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in\n  Heterogeneous Information Networks","summary":"  This paper explores the applications and challenges of graph neural networks\n(GNNs) in processing complex graph data brought about by the rapid development\nof the Internet. Given the heterogeneity and redundancy problems that graph\ndata often have, traditional GNN methods may be overly dependent on the initial\nstructure and attribute information of the graph, which limits their ability to\naccurately simulate more complex relationships and patterns in the graph.\nTherefore, this study proposes a graph neural network model under a\nself-supervised learning framework, which can flexibly combine different types\nof additional information of the attribute graph and its nodes, so as to better\nmine the deep features in the graph data. By introducing a self-supervisory\nmechanism, it is expected to improve the adaptability of existing models to the\ndiversity and complexity of graph data and improve the overall performance of\nthe model.\n","authors":["Jianjun Wei","Yue Liu","Xin Huang","Xin Zhang","Wenyi Liu","Xu Yan"],"pdf_url":"https://arxiv.org/pdf/2410.17617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15899v2","updated":"2024-10-23T07:05:26Z","published":"2024-10-21T11:23:23Z","title":"On the Design and Performance of Machine Learning Based Error Correcting\n  Decoders","summary":"  This paper analyzes the design and competitiveness of four neural network\n(NN) architectures recently proposed as decoders for forward error correction\n(FEC) codes. We first consider the so-called single-label neural network (SLNN)\nand the multi-label neural network (MLNN) decoders which have been reported to\nachieve near maximum likelihood (ML) performance. Here, we show analytically\nthat SLNN and MLNN decoders can always achieve ML performance, regardless of\nthe code dimensions -- although at the cost of computational complexity -- and\nno training is in fact required. We then turn our attention to two\ntransformer-based decoders: the error correction code transformer (ECCT) and\nthe cross-attention message passing transformer (CrossMPT). We compare their\nperformance against traditional decoders, and show that ordered statistics\ndecoding outperforms these transformer-based decoders. The results in this\npaper cast serious doubts on the application of NN-based FEC decoders in the\nshort and medium block length regime.\n","authors":["Yuncheng Yuan","Péter Scheepers","Lydia Tasiou","Yunus Can Gültekin","Federico Corradi","Alex Alvarado"],"pdf_url":"https://arxiv.org/pdf/2410.15899v2.pdf","comment":"6 pages, 4 figures, submitted for possible presentation in a\n  conference (v2: Pre-FEC BER curves are corrected)"},{"id":"http://arxiv.org/abs/2311.00656v3","updated":"2024-10-23T06:53:57Z","published":"2023-11-01T17:02:41Z","title":"Adaptive Spatio-temporal Estimation on the Graph Edges via Line Graph\n  Transformation","summary":"  Spatio-temporal estimation of signals on graph edges is challenging because\nmost conventional Graph Signal Processing techniques are defined on the graph\nnodes. Leveraging the Line Graph transform, the Line Graph Least Mean Square\n(LGLMS) algorithm is proposed to conduct adaptive estimation of time-varying\nedge signals by projecting the edge signals from edge space to node space.\nLGLMS is an adaptive algorithm analogous to the classical LMS algorithm but\napplied to graph edges. Unlike edge-specific methods, LGLMS retains all GSP\nconcepts and techniques originally designed for graph nodes, without the need\nfor redefinition on the edges. Experimenting with transportation graphs and\nmeteorological graphs, with the signal observations having noisy and missing\nvalues, we confirmed that LGLMS is suitable for the online prediction of\ntime-varying edge signals.\n","authors":["Yi Yan","Ercan Engin Kuruoglu"],"pdf_url":"https://arxiv.org/pdf/2311.00656v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14759v2","updated":"2024-10-23T06:51:05Z","published":"2024-10-18T09:53:20Z","title":"Universal approximation results for neural networks with non-polynomial\n  activation function over non-compact domains","summary":"  In this paper, we generalize the universal approximation property of\nsingle-hidden-layer feed-forward neural networks beyond the classical\nformulation over compact domains. More precisely, by assuming that the\nactivation function is non-polynomial, we derive universal approximation\nresults for neural networks within function spaces over non-compact subsets of\na Euclidean space, e.g., weighted spaces, $L^p$-spaces, and (weighted) Sobolev\nspaces over unbounded domains, where the latter includes the approximation of\nthe (weak) derivatives. Furthermore, we provide some dimension-independent\nrates for approximating a function with sufficiently regular and integrable\nFourier transform by neural networks with non-polynomial activation function.\n","authors":["Ariel Neufeld","Philipp Schmocker"],"pdf_url":"https://arxiv.org/pdf/2410.14759v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.08410"},{"id":"http://arxiv.org/abs/2410.17592v1","updated":"2024-10-23T06:40:13Z","published":"2024-10-23T06:40:13Z","title":"A Kernel Perspective on Distillation-based Collaborative Learning","summary":"  Over the past decade, there is a growing interest in collaborative learning\nthat can enhance AI models of multiple parties. However, it is still\nchallenging to enhance performance them without sharing private data and models\nfrom individual parties. One recent promising approach is to develop\ndistillation-based algorithms that exploit unlabeled public data but the\nresults are still unsatisfactory in both theory and practice. To tackle this\nproblem, we rigorously analyze a representative distillation-based algorithm in\nthe view of kernel regression. This work provides the first theoretical results\nto prove the (nearly) minimax optimality of the nonparametric collaborative\nlearning algorithm that does not directly share local data or models in\nmassively distributed statistically heterogeneous environments. Inspired by our\ntheoretical results, we also propose a practical distillation-based\ncollaborative learning algorithm based on neural network architecture. Our\nalgorithm successfully bridges the gap between our theoretical assumptions and\npractical settings with neural networks through feature kernel matching. We\nsimulate various regression tasks to verify our theory and demonstrate the\npractical feasibility of our proposed algorithm.\n","authors":["Sejun Park","Kihun Hong","Ganguk Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.17592v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17589v1","updated":"2024-10-23T06:35:41Z","published":"2024-10-23T06:35:41Z","title":"Challenge on Sound Scene Synthesis: Evaluating Text-to-Audio Generation","summary":"  Despite significant advancements in neural text-to-audio generation,\nchallenges persist in controllability and evaluation. This paper addresses\nthese issues through the Sound Scene Synthesis challenge held as part of the\nDetection and Classification of Acoustic Scenes and Events 2024. We present an\nevaluation protocol combining objective metric, namely Fr\\'echet Audio\nDistance, with perceptual assessments, utilizing a structured prompt format to\nenable diverse captions and effective evaluation. Our analysis reveals varying\nperformance across sound categories and model architectures, with larger models\ngenerally excelling but innovative lightweight approaches also showing promise.\nThe strong correlation between objective metrics and human ratings validates\nour evaluation approach. We discuss outcomes in terms of audio quality,\ncontrollability, and architectural considerations for text-to-audio\nsynthesizers, providing direction for future research.\n","authors":["Junwon Lee","Modan Tailleur","Laurie M. Heller","Keunwoo Choi","Mathieu Lagrange","Brian McFee","Keisuke Imoto","Yuki Okamoto"],"pdf_url":"https://arxiv.org/pdf/2410.17589v1.pdf","comment":"accepted to NeurIPS 2024 Workshop: Audio Imagination"},{"id":"http://arxiv.org/abs/2410.17587v1","updated":"2024-10-23T06:30:20Z","published":"2024-10-23T06:30:20Z","title":"Predicting Company Growth by Econophysics informed Machine Learning","summary":"  Predicting company growth is crucial for strategic adjustment, operational\ndecision-making, risk assessment, and loan eligibility reviews. Traditional\nmodels for company growth often focus too much on theory, overlooking practical\nforecasting, or they rely solely on time series forecasting techniques,\nignoring interpretability and the inherent mechanisms of company growth. In\nthis paper, we propose a machine learning-based prediction framework that\nincorporates an econophysics model for company growth. Our model captures both\nthe intrinsic growth mechanisms of companies led by scaling laws and the\nfluctuations influenced by random factors and individual decisions,\ndemonstrating superior predictive performance compared with methods that use\ntime series techniques alone. Its advantages are more pronounced in long-range\nprediction tasks. By explicitly modeling the baseline growth and volatility\ncomponents, our model is more interpretable.\n","authors":["Ruyi Tao","Kaiwei Liu","Xu Jing","Jiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17587v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2402.12617v2","updated":"2024-10-23T06:28:19Z","published":"2024-02-20T00:51:05Z","title":"Generative AI Security: Challenges and Countermeasures","summary":"  Generative AI's expanding footprint across numerous industries has led to\nboth excitement and increased scrutiny. This paper delves into the unique\nsecurity challenges posed by Generative AI, and outlines potential research\ndirections for managing these risks.\n","authors":["Banghua Zhu","Norman Mu","Jiantao Jiao","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2402.12617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15012v2","updated":"2024-10-23T06:27:26Z","published":"2024-03-22T07:56:31Z","title":"Empirical investigation of multi-source cross-validation in clinical ECG\n  classification","summary":"  Traditionally, machine learning-based clinical prediction models have been\ntrained and evaluated on patient data from a single source, such as a hospital.\nCross-validation methods can be used to estimate the accuracy of such models on\nnew patients originating from the same source, by repeated random splitting of\nthe data. However, such estimates tend to be highly overoptimistic when\ncompared to accuracy obtained from deploying models to sources not represented\nin the dataset, such as a new hospital. The increasing availability of\nmulti-source medical datasets provides new opportunities for obtaining more\ncomprehensive and realistic evaluations of expected accuracy through\nsource-level cross-validation designs.\n  In this study, we present a systematic empirical evaluation of standard\nK-fold cross-validation and leave-source-out cross-validation methods in a\nmulti-source setting. We consider the task of electrocardiogram based\ncardiovascular disease classification, combining and harmonizing the openly\navailable PhysioNet CinC Challenge 2021 and the Shandong Provincial Hospital\ndatasets for our study.\n  Our results show that K-fold cross-validation, both on single-source and\nmulti-source data, systemically overestimates prediction performance when the\nend goal is to generalize to new sources. Leave-source-out cross-validation\nprovides more reliable performance estimates, having close to zero bias though\nlarger variability. The evaluation highlights the dangers of obtaining\nmisleading cross-validation results on medical data and demonstrates how these\nissues can be mitigated when having access to multi-source data.\n","authors":["Tuija Leinonen","David Wong","Antti Vasankari","Ali Wahab","Ramesh Nadarajah","Matti Kaisti","Antti Airola"],"pdf_url":"https://arxiv.org/pdf/2403.15012v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.17579v1","updated":"2024-10-23T06:08:45Z","published":"2024-10-23T06:08:45Z","title":"Bonsai: Gradient-free Graph Distillation for Node Classification","summary":"  Graph distillation has emerged as a promising avenue to enable scalable\ntraining of GNNs by compressing the training dataset while preserving essential\ngraph characteristics. Our study uncovers significant shortcomings in current\ngraph distillation techniques. First, the majority of the algorithms\nparadoxically require training on the full dataset to perform distillation.\nSecond, due to their gradient-emulating approach, these methods require fresh\ndistillation for any change in hyperparameters or GNN architecture, limiting\ntheir flexibility and reusability. Finally, they fail to achieve substantial\nsize reduction due to synthesizing fully-connected, edge-weighted graphs. To\naddress these challenges, we present Bonsai, a novel graph distillation method\nempowered by the observation that \\textit{computation trees} form the\nfundamental processing units of message-passing GNNs. Bonsai distills datasets\nby encoding a careful selection of \\textit{exemplar} trees that maximize the\nrepresentation of all computation trees in the training set. This unique\napproach imparts Bonsai as the first linear-time, model-agnostic graph\ndistillation algorithm for node classification that outperforms existing\nbaselines across $6$ real-world datasets on accuracy, while being $22$ times\nfaster on average. Bonsai is grounded in rigorous mathematical guarantees on\nthe adopted approximation strategies making it robust to GNN architectures,\ndatasets, and parameters.\n","authors":["Mridul Gupta","Samyak Jain","Vansh Ramani","Hariprasad Kodamana","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2410.17579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03801v2","updated":"2024-10-23T06:05:11Z","published":"2024-10-04T08:14:24Z","title":"P1-KAN an effective Kolmogorov Arnold Network for function approximation","summary":"  A new Kolmogorov-Arnold network (KAN) is proposed to approximate potentially\nirregular functions in high dimension. We show that it outperforms multilayer\nperceptrons in terms of accuracy and converges faster. We also compare it with\nseveral proposed KAN networks: the original spline-based KAN network appears to\nbe more effective for smooth functions, while the P1-KAN network is more\neffective for irregular functions.\n","authors":["Xavier Warin"],"pdf_url":"https://arxiv.org/pdf/2410.03801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17574v1","updated":"2024-10-23T05:55:21Z","published":"2024-10-23T05:55:21Z","title":"Adversarial Domain Adaptation for Metal Cutting Sound Detection:\n  Leveraging Abundant Lab Data for Scarce Industry Data","summary":"  Cutting state monitoring in the milling process is crucial for improving\nmanufacturing efficiency and tool life. Cutting sound detection using machine\nlearning (ML) models, inspired by experienced machinists, can be employed as a\ncost-effective and non-intrusive monitoring method in a complex manufacturing\nenvironment. However, labeling industry data for training is costly and\ntime-consuming. Moreover, industry data is often scarce. In this study, we\npropose a novel adversarial domain adaptation (DA) approach to leverage\nabundant lab data to learn from scarce industry data, both labeled, for\ntraining a cutting-sound detection model. Rather than adapting the features\nfrom separate domains directly, we project them first into two separate latent\nspaces that jointly work as the feature space for learning domain-independent\nrepresentations. We also analyze two different mechanisms for adversarial\nlearning where the discriminator works as an adversary and a critic in separate\nsettings, enabling our model to learn expressive domain-invariant and\ndomain-ingrained features, respectively. We collected cutting sound data from\nmultiple sensors in different locations, prepared datasets from lab and\nindustry domain, and evaluated our learning models on them. Experiments showed\nthat our models outperformed the multi-layer perceptron based vanilla domain\nadaptation models in labeling tasks on the curated datasets, achieving near\n92%, 82% and 85% accuracy respectively for three different sensors installed in\nindustry settings.\n","authors":["Mir Imtiaz Mostafiz","Eunseob Kim","Adrian Shuai Li","Elisa Bertino","Martin Byung-Guk Jun","Ali Shakouri"],"pdf_url":"https://arxiv.org/pdf/2410.17574v1.pdf","comment":"8 pages, 3 figures, 3 tables, First two named Authors have equal\n  contribution (Co-first author)"},{"id":"http://arxiv.org/abs/2410.17573v1","updated":"2024-10-23T05:54:41Z","published":"2024-10-23T05:54:41Z","title":"Securing Federated Learning Against Novel and Classic Backdoor Threats\n  During Foundation Model Integration","summary":"  Federated learning (FL) enables decentralized model training while preserving\nprivacy. Recently, integrating Foundation Models (FMs) into FL has boosted\nperformance but also introduced a novel backdoor attack mechanism. Attackers\ncan exploit the FM's capabilities to embed backdoors into synthetic data\ngenerated by FMs used for model fusion, subsequently infecting all client\nmodels through knowledge sharing without involvement in the long-lasting FL\nprocess. These novel attacks render existing FL backdoor defenses ineffective,\nas they primarily detect anomalies among client updates, which may appear\nuniformly malicious under this attack. Our work proposes a novel data-free\ndefense strategy by constraining abnormal activations in the hidden feature\nspace during model aggregation on the server. The activation constraints,\noptimized using synthetic data alongside FL training, mitigate the attack while\nbarely affecting model performance, as the parameters remain untouched.\nExtensive experiments demonstrate its effectiveness against both novel and\nclassic backdoor attacks, outperforming existing defenses while maintaining\nmodel performance.\n","authors":["Xiaohuan Bi","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17555v2","updated":"2024-10-23T05:49:00Z","published":"2024-09-26T05:57:35Z","title":"Advancing Open-Set Domain Generalization Using Evidential Bi-Level\n  Hardest Domain Scheduler","summary":"  In Open-Set Domain Generalization (OSDG), the model is exposed to both new\nvariations of data appearance (domains) and open-set conditions, where both\nknown and novel categories are present at test time. The challenges of this\ntask arise from the dual need to generalize across diverse domains and\naccurately quantify category novelty, which is critical for applications in\ndynamic environments. Recently, meta-learning techniques have demonstrated\nsuperior results in OSDG, effectively orchestrating the meta-train and -test\ntasks by employing varied random categories and predefined domain partition\nstrategies. These approaches prioritize a well-designed training schedule over\ntraditional methods that focus primarily on data augmentation and the\nenhancement of discriminative feature learning. The prevailing meta-learning\nmodels in OSDG typically utilize a predefined sequential domain scheduler to\nstructure data partitions. However, a crucial aspect that remains inadequately\nexplored is the influence brought by strategies of domain schedulers during\ntraining. In this paper, we observe that an adaptive domain scheduler benefits\nmore in OSDG compared with prefixed sequential and random domain schedulers. We\npropose the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) to achieve\nan adaptive domain scheduler. This method strategically sequences domains by\nassessing their reliabilities in utilizing a follower network, trained with\nconfidence scores learned in an evidential manner, regularized by max rebiasing\ndiscrepancy, and optimized in a bi-level manner. The results show that our\nmethod substantially improves OSDG performance and achieves more discriminative\nembeddings for both the seen and unseen categories. The source code is publicly\navailable at https://github.com/KPeng9510/EBiL-HaDS.\n","authors":["Kunyu Peng","Di Wen","Kailun Yang","Ao Luo","Yufan Chen","Jia Fu","M. Saquib Sarfraz","Alina Roitberg","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2409.17555v2.pdf","comment":"Accepted to NeurIPS 2024. The source code is publicly available at\n  https://github.com/KPeng9510/EBiL-HaDS"},{"id":"http://arxiv.org/abs/2408.04869v3","updated":"2024-10-23T05:44:21Z","published":"2024-08-09T05:15:36Z","title":"UCB Exploration for Fixed-Budget Bayesian Best Arm Identification","summary":"  We study best-arm identification (BAI) in the fixed-budget setting. Adaptive\nallocations based on upper confidence bounds (UCBs), such as UCBE, are known to\nwork well in BAI. However, it is well-known that its optimal regret is\ntheoretically dependent on instances, which we show to be an artifact in many\nfixed-budget BAI problems. In this paper we propose an UCB exploration\nalgorithm that is both theoretically and empirically efficient for the fixed\nbudget BAI problem under a Bayesian setting. The key idea is to learn prior\ninformation, which can enhance the performance of UCB-based BAI algorithm as it\nhas done in the cumulative regret minimization problem. We establish bounds on\nthe failure probability and the simple regret for the Bayesian BAI problem,\nproviding upper bounds of order $\\tilde{O}(\\sqrt{K/n})$, up to logarithmic\nfactors, where $n$ represents the budget and $K$ denotes the number of arms.\nFurthermore, we demonstrate through empirical results that our approach\nconsistently outperforms state-of-the-art baselines.\n","authors":["Rong J. B. Zhu","Yanqi Qiu"],"pdf_url":"https://arxiv.org/pdf/2408.04869v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02316v3","updated":"2024-10-23T05:26:10Z","published":"2024-02-04T02:09:18Z","title":"Diffusion Models are Certifiably Robust Classifiers","summary":"  Generative learning, recognized for its effective modeling of data\ndistributions, offers inherent advantages in handling out-of-distribution\ninstances, especially for enhancing robustness to adversarial attacks. Among\nthese, diffusion classifiers, utilizing powerful diffusion models, have\ndemonstrated superior empirical robustness. However, a comprehensive\ntheoretical understanding of their robustness is still lacking, raising\nconcerns about their vulnerability to stronger future attacks. In this study,\nwe prove that diffusion classifiers possess $O(1)$ Lipschitzness, and establish\ntheir certified robustness, demonstrating their inherent resilience. To achieve\nnon-constant Lipschitzness, thereby obtaining much tighter certified\nrobustness, we generalize diffusion classifiers to classify Gaussian-corrupted\ndata. This involves deriving the evidence lower bounds (ELBOs) for these\ndistributions, approximating the likelihood using the ELBO, and calculating\nclassification probabilities via Bayes' theorem. Experimental results show the\nsuperior certified robustness of these Noised Diffusion Classifiers (NDCs).\nNotably, we achieve over 80% and 70% certified robustness on CIFAR-10 under\nadversarial perturbations with \\(\\ell_2\\) norms less than 0.25 and 0.5,\nrespectively, using a single off-the-shelf diffusion model without any\nadditional data.\n","authors":["Huanran Chen","Yinpeng Dong","Shitong Shao","Zhongkai Hao","Xiao Yang","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.02316v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.10648v2","updated":"2024-10-23T05:24:23Z","published":"2024-10-14T15:59:16Z","title":"A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers","summary":"  Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences.\n","authors":["Alex Stein","Samuel Sharpe","Doron Bergman","Senthil Kumar","C. Bayan Bruss","John Dickerson","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2410.10648v2.pdf","comment":"10 pages, 6 pages of references+appendix"},{"id":"http://arxiv.org/abs/2410.17566v1","updated":"2024-10-23T05:19:51Z","published":"2024-10-23T05:19:51Z","title":"Differentially Private Learning Needs Better Model Initialization and\n  Self-Distillation","summary":"  Differentially private SGD (DPSGD) enables privacy-preserving training of\nlanguage models, but often reduces utility, diversity, and linguistic quality.\nWe introduce DPRefine, a three-phase method that initializes a model using data\nsynthesis from a small pre-trained LM with rigorous filtering, applies DP\nfinetuning on private data, and performs self-distillation to refine outputs.\nThis approach significantly outperforms vanilla DPSGD, with AlpacaEval\npreferring DPRefine's generations in 78.4% of cases across all datasets. Our\nanalysis reveals that DPRefine reduces linguistic errors in generated text by\n84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.\nIt also reduces inconsistencies of non-private models, such as hallucinated\ndetails and misattributed quotes. We find that small models like GPT-2 can be\neffective for initialization and distillation, highlighting their potential in\nenabling scalable and efficient deployment of privacy-preserving language.\n","authors":["Ivoline C. Ngong","Joseph P. Near","Niloofar Mireshghallah"],"pdf_url":"https://arxiv.org/pdf/2410.17566v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2405.03239v2","updated":"2024-10-23T05:18:11Z","published":"2024-05-06T07:48:34Z","title":"Deep Learning for Detecting and Early Predicting Chronic Obstructive\n  Pulmonary Disease from Spirogram Time Series","summary":"  Chronic Obstructive Pulmonary Disease (COPD) is a chronic lung disease that\ncauses airflow obstruction. Current methods can only detect COPD from prominent\nfeatures in spirogram (Volume-Flow time series) but cannot predict future COPD\nrisk from subtle data patterns. We propose a deep learning-based method,\nDeepSpiro, for early prediction of future COPD risk. DeepSpiro consists of four\nkey components: SpiroSmoother for stabilizing the Volume-Flow curve,\nSpiroEncoder for capturing volume evolution through key patches of varying\nlengths, SpiroExplainer for integrating heterogeneous data and explaining\npredictions through volume attention, and SpiroPredictor for predicting the\ndisease risk of undiagnosed high-risk patients based on key patch concavity,\nwith prediction horizons of 1, 2, 3, 4, 5 years, or even longer. Evaluated on\nthe UK Biobank dataset, DeepSpiro achieved an AUC of 0.8328 for COPD detection\nand demonstrated strong predictive performance for future COPD risk (p-value <\n0.001). DeepSpiro effectively predicts the long-term progression of the\ndisease.\n","authors":["Shuhao Mei","Xin Li","Yuxi Zhou","Jiahao Xu","Yong Zhang","Yuxuan Wan","Shan Cao","Qinghao Zhao","Shijia Geng","Junqing Xie","Shengyong Chen","Shenda Hong"],"pdf_url":"https://arxiv.org/pdf/2405.03239v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17564v1","updated":"2024-10-23T05:15:59Z","published":"2024-10-23T05:15:59Z","title":"DisenGCD: A Meta Multigraph-assisted Disentangled Graph Learning\n  Framework for Cognitive Diagnosis","summary":"  Existing graph learning-based cognitive diagnosis (CD) methods have made\nrelatively good results, but their student, exercise, and concept\nrepresentations are learned and exchanged in an implicit unified graph, which\nmakes the interaction-agnostic exercise and concept representations be learned\npoorly, failing to provide high robustness against noise in students'\ninteractions. Besides, lower-order exercise latent representations obtained in\nshallow layers are not well explored when learning the student representation.\nTo tackle the issues, this paper suggests a meta multigraph-assisted\ndisentangled graph learning framework for CD (DisenGCD), which learns three\ntypes of representations on three disentangled graphs: student-exercise-concept\ninteraction, exercise-concept relation, and concept dependency graphs,\nrespectively. Specifically, the latter two graphs are first disentangled from\nthe interaction graph. Then, the student representation is learned from the\ninteraction graph by a devised meta multigraph learning module; multiple\nlearnable propagation paths in this module enable current student latent\nrepresentation to access lower-order exercise latent representations, which can\nlead to more effective nad robust student representations learned; the exercise\nand concept representations are learned on the relation and dependency graphs\nby graph attention modules. Finally, a novel diagnostic function is devised to\nhandle three disentangled representations for prediction. Experiments show\nbetter performance and robustness of DisenGCD than state-of-the-art CD methods\nand demonstrate the effectiveness of the disentangled learning framework and\nmeta multigraph module. The source code is available at\n\\textcolor{red}{\\url{https://github.com/BIMK/Intelligent-Education/tree/main/DisenGCD}}.\n","authors":["Shangshang Yang","Mingyang Chen","Ziwen Wang","Xiaoshan Yu","Panpan Zhang","Haiping Ma","Xingyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17564v1.pdf","comment":"21 pages, Accepted by NeurIPS 2024 as a poster"},{"id":"http://arxiv.org/abs/2410.17557v1","updated":"2024-10-23T04:46:36Z","published":"2024-10-23T04:46:36Z","title":"BlurryScope: a cost-effective and compact scanning microscope for\n  automated HER2 scoring using deep learning on blurry image data","summary":"  We developed a rapid scanning optical microscope, termed \"BlurryScope\", that\nleverages continuous image acquisition and deep learning to provide a\ncost-effective and compact solution for automated inspection and analysis of\ntissue sections. BlurryScope integrates specialized hardware with a neural\nnetwork-based model to quickly process motion-blurred histological images and\nperform automated pathology classification. This device offers comparable speed\nto commercial digital pathology scanners, but at a significantly lower price\npoint and smaller size/weight, making it ideal for fast triaging in small\nclinics, as well as for resource-limited settings. To demonstrate the\nproof-of-concept of BlurryScope, we implemented automated classification of\nhuman epidermal growth factor receptor 2 (HER2) scores on immunohistochemically\n(IHC) stained breast tissue sections, achieving concordant results with those\nobtained from a high-end digital scanning microscope. We evaluated this\napproach by scanning HER2-stained tissue microarrays (TMAs) at a continuous\nspeed of 5 mm/s, which introduces bidirectional motion blur artifacts. These\ncompromised images were then used to train our network models. Using a test set\nof 284 unique patient cores, we achieved blind testing accuracies of 79.3% and\n89.7% for 4-class (0, 1+, 2+, 3+) and 2-class (0/1+ , 2+/3+) HER2 score\nclassification, respectively. BlurryScope automates the entire workflow, from\nimage scanning to stitching and cropping of regions of interest, as well as\nHER2 score classification. We believe BlurryScope has the potential to enhance\nthe current pathology infrastructure in resource-scarce environments, save\ndiagnostician time and bolster cancer identification and classification across\nvarious clinical environments.\n","authors":["Michael John Fanous","Christopher Michael Seybold","Hanlong Chen","Nir Pillar","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2410.17557v1.pdf","comment":"18 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2410.17554v1","updated":"2024-10-23T04:40:45Z","published":"2024-10-23T04:40:45Z","title":"LEADS: Lightweight Embedded Assisted Driving System","summary":"  With the rapid development of electric vehicles, formula races that face high\nschool and university students have become more popular than ever as the\nthreshold for design and manufacturing has been lowered. In many cases, we see\nteams inspired by or directly using toolkits and technologies inherited from\nstandardized commercial vehicles. These architectures are usually overly\ncomplicated for amateur applications like the races. In order to improve the\nefficiency and simplify the development of instrumentation, control, and\nanalysis systems, we propose LEADS (Lightweight Embedded Assisted Driving\nSystem), a dedicated solution for such scenarios.\n","authors":["Tianhao Fu","Querobin Mascarenhas","Andrew Forti"],"pdf_url":"https://arxiv.org/pdf/2410.17554v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2305.10015v3","updated":"2024-10-23T04:34:35Z","published":"2023-05-17T07:49:16Z","title":"Utility Theory of Synthetic Data Generation","summary":"  Synthetic data algorithms are widely employed in industries to generate\nartificial data for downstream learning tasks. While existing research\nprimarily focuses on empirically evaluating utility of synthetic data, its\ntheoretical understanding is largely lacking. This paper bridges the\npractice-theory gap by establishing relevant utility theory in a statistical\nlearning framework. It considers two utility metrics: generalization and\nranking of models trained on synthetic data. The former is defined as the\ngeneralization difference between models trained on synthetic and on real data.\nBy deriving analytical bounds for this utility metric, we demonstrate that the\nsynthetic feature distribution does not need to be similar as that of real data\nfor ensuring comparable generalization of synthetic models, provided proper\nmodel specifications in downstream learning tasks. The latter utility metric\nstudies the relative performance of models trained on synthetic data. In\nparticular, we discover that the distribution of synthetic data is not\nnecessarily similar as the real one to ensure consistent model comparison.\nInterestingly, consistent model comparison is still achievable even when\nsynthetic responses are not well generated, as long as downstream models are\nseparable by a generalization gap. Finally, extensive experiments on\nnon-parametric models and deep neural networks have been conducted to validate\nthese theoretical findings.\n","authors":["Shirong Xu","Will Wei Sun","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2305.10015v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17551v1","updated":"2024-10-23T04:32:37Z","published":"2024-10-23T04:32:37Z","title":"Multimodal Information Bottleneck for Deep Reinforcement Learning with\n  Multiple Sensors","summary":"  Reinforcement learning has achieved promising results on robotic control\ntasks but struggles to leverage information effectively from multiple sensory\nmodalities that differ in many characteristics. Recent works construct\nauxiliary losses based on reconstruction or mutual information to extract joint\nrepresentations from multiple sensory inputs to improve the sample efficiency\nand performance of reinforcement learning algorithms. However, the\nrepresentations learned by these methods could capture information irrelevant\nto learning a policy and may degrade the performance. We argue that compressing\ninformation in the learned joint representations about raw multimodal\nobservations is helpful, and propose a multimodal information bottleneck model\nto learn task-relevant joint representations from egocentric images and\nproprioception. Our model compresses and retains the predictive information in\nmultimodal observations for learning a compressed joint representation, which\nfuses complementary information from visual and proprioceptive feedback and\nmeanwhile filters out task-irrelevant information in raw multimodal\nobservations. We propose to minimize the upper bound of our multimodal\ninformation bottleneck objective for computationally tractable optimization.\nExperimental evaluations on several challenging locomotion tasks with\negocentric images and proprioception show that our method achieves better\nsample efficiency and zero-shot robustness to unseen white noise than leading\nbaselines. We also empirically demonstrate that leveraging information from\negocentric images and proprioception is more helpful for learning policies on\nlocomotion tasks than solely using one single modality.\n","authors":["Bang You","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17551v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2405.15310v3","updated":"2024-10-23T04:08:23Z","published":"2024-05-24T07:52:53Z","title":"Spectraformer: A Unified Random Feature Framework for Transformer","summary":"  Linearization of attention using various kernel approximation and kernel\nlearning techniques has shown promise. Past methods use a subset of\ncombinations of component functions and weight matrices within the random\nfeatures paradigm. We identify the need for a systematic comparison of\ndifferent combinations of weight matrices and component functions for attention\nlearning in Transformer. In this work, we introduce Spectraformer, a unified\nframework for approximating and learning the kernel function in linearized\nattention of the Transformer. We experiment with broad classes of component\nfunctions and weight matrices for three textual tasks in the LRA benchmark. Our\nempirical findings indicate that different kernels are good at different tasks\nand that kernel choice is fundamental to performant models. Our code is\navailable at: https://github.com/dukenguyenxyz/spectraformer .\n","authors":["Duke Nguyen","Aditya Joshi","Flora Salim"],"pdf_url":"https://arxiv.org/pdf/2405.15310v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09990v4","updated":"2024-10-23T04:00:40Z","published":"2022-05-20T06:53:03Z","title":"Set-based Meta-Interpolation for Few-Task Meta-Learning","summary":"  Meta-learning approaches enable machine learning systems to adapt to new\ntasks given few examples by leveraging knowledge from related tasks. However, a\nlarge number of meta-training tasks are still required for generalization to\nunseen tasks during meta-testing, which introduces a critical bottleneck for\nreal-world problems that come with only few tasks, due to various reasons\nincluding the difficulty and cost of constructing tasks. Recently, several task\naugmentation methods have been proposed to tackle this issue using\ndomain-specific knowledge to design augmentation techniques to densify the\nmeta-training task distribution. However, such reliance on domain-specific\nknowledge renders these methods inapplicable to other domains. While Manifold\nMixup based task augmentation methods are domain-agnostic, we empirically find\nthem ineffective on non-image domains. To tackle these limitations, we propose\na novel domain-agnostic task augmentation method, Meta-Interpolation, which\nutilizes expressive neural set functions to densify the meta-training task\ndistribution using bilevel optimization. We empirically validate the efficacy\nof Meta-Interpolation on eight datasets spanning across various domains such as\nimage classification, molecule property prediction, text classification and\nspeech recognition. Experimentally, we show that Meta-Interpolation\nconsistently outperforms all the relevant baselines. Theoretically, we prove\nthat task interpolation with the set function regularizes the meta-learner to\nimprove generalization.\n","authors":["Seanie Lee","Bruno Andreis","Kenji Kawaguchi","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2205.09990v4.pdf","comment":"First two authors contributed equally. Name order decided by a coin\n  toss"},{"id":"http://arxiv.org/abs/2405.15116v2","updated":"2024-10-23T03:55:34Z","published":"2024-05-24T00:14:16Z","title":"Quantifying the Gain in Weak-to-Strong Generalization","summary":"  Recent advances in large language models have shown capabilities that are\nextraordinary and near-superhuman. These models operate with such complexity\nthat reliably evaluating and aligning them proves challenging for humans. This\nleads to the natural question: can guidance from weak models (like humans)\nadequately direct the capabilities of strong models? In a recent and somewhat\nsurprising work, Burns et al. (2023) empirically demonstrated that when strong\nmodels (like GPT-4) are finetuned using labels generated by weak supervisors\n(like GPT-2), the strong models outperform their weaker counterparts -- a\nphenomenon they term weak-to-strong generalization.\n  In this work, we present a theoretical framework for understanding\nweak-to-strong generalization. Specifically, we show that the improvement in\nperformance achieved by strong models over their weaker counterparts is\nquantified by the misfit error incurred by the strong model on labels generated\nby the weaker model. Our theory reveals several curious algorithmic insights.\nFor instance, we can predict the amount by which the strong model will improve\nover the weak model, and also choose among different weak models to train the\nstrong model, based on its misfit error. We validate our theoretical findings\nthrough various empirical assessments.\n","authors":["Moses Charikar","Chirag Pabbaraju","Kirankumar Shiragur"],"pdf_url":"https://arxiv.org/pdf/2405.15116v2.pdf","comment":"19 pages; NeurIPS 2024 camera-ready version with additional\n  experiments, references and discussion"},{"id":"http://arxiv.org/abs/2410.17545v1","updated":"2024-10-23T03:50:32Z","published":"2024-10-23T03:50:32Z","title":"Predicting 30-Day Hospital Readmission in Medicare Patients: Insights\n  from an LSTM Deep Learning Model","summary":"  Readmissions among Medicare beneficiaries are a major problem for the US\nhealthcare system from a perspective of both healthcare operations and patient\ncaregiving outcomes. Our study analyzes Medicare hospital readmissions using\nLSTM networks with feature engineering to assess feature contributions. We\nselected variables from admission-level data, inpatient medical history and\npatient demography. The LSTM model is designed to capture temporal dynamics\nfrom admission-level and patient-level data. On a case study on the MIMIC\ndataset, the LSTM model outperformed the logistic regression baseline,\naccurately leveraging temporal features to predict readmission. The major\nfeatures were the Charlson Comorbidity Index, hospital length of stay, the\nhospital admissions over the past 6 months, while demographic variables were\nless impactful. This work suggests that LSTM networks offers a more promising\napproach to improve Medicare patient readmission prediction. It captures\ntemporal interactions in patient databases, enhancing current prediction models\nfor healthcare providers. Adoption of predictive models into clinical practice\nmay be more effective in identifying Medicare patients to provide early and\ntargeted interventions to improve patient outcomes.\n","authors":["Xintao Li","Sibei Liu","Dezhi Yu","Yang Zhang","Xiaoyu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17545v1.pdf","comment":"5 pages, 1 table, 5 figures, Accepted by 2024 3rd International\n  Conference on Cloud Computing, Big Data Application and Software\n  Engineering(CBASE 2024), the final version will be published on on IEEE\n  Conference proceeding"},{"id":"http://arxiv.org/abs/2402.05741v2","updated":"2024-10-23T03:39:00Z","published":"2024-02-08T15:19:50Z","title":"Real-World Robot Applications of Foundation Models: A Review","summary":"  Recent developments in foundation models, like Large Language Models (LLMs)\nand Vision-Language Models (VLMs), trained on extensive data, facilitate\nflexible application across different tasks and modalities. Their impact spans\nvarious fields, including healthcare, education, and robotics. This paper\nprovides an overview of the practical application of foundation models in\nreal-world robotics, with a primary emphasis on the replacement of specific\ncomponents within existing robot systems. The summary encompasses the\nperspective of input-output relationships in foundation models, as well as\ntheir role in perception, motion planning, and control within the field of\nrobotics. This paper concludes with a discussion of future challenges and\nimplications for practical robot applications.\n","authors":["Kento Kawaharazuka","Tatsuya Matsushima","Andrew Gambardella","Jiaxian Guo","Chris Paxton","Andy Zeng"],"pdf_url":"https://arxiv.org/pdf/2402.05741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17538v1","updated":"2024-10-23T03:38:31Z","published":"2024-10-23T03:38:31Z","title":"Primal-Dual Spectral Representation for Off-policy Evaluation","summary":"  Off-policy evaluation (OPE) is one of the most fundamental problems in\nreinforcement learning (RL) to estimate the expected long-term payoff of a\ngiven target policy with only experiences from another behavior policy that is\npotentially unknown. The distribution correction estimation (DICE) family of\nestimators have advanced the state of the art in OPE by breaking the curse of\nhorizon. However, the major bottleneck of applying DICE estimators lies in the\ndifficulty of solving the saddle-point optimization involved, especially with\nneural network implementations. In this paper, we tackle this challenge by\nestablishing a linear representation of value function and stationary\ndistribution correction ratio, i.e., primal and dual variables in the DICE\nframework, using the spectral decomposition of the transition operator. Such\nprimal-dual representation not only bypasses the non-convex non-concave\noptimization in vanilla DICE, therefore enabling an computational efficient\nalgorithm, but also paves the way for more efficient utilization of historical\ndata. We highlight that our algorithm, SpectralDICE, is the first to leverage\nthe linear representation of primal-dual variables that is both computation and\nsample efficient, the performance of which is supported by a rigorous\ntheoretical sample complexity guarantee and a thorough empirical evaluation on\nvarious benchmarks.\n","authors":["Yang Hu","Tianyi Chen","Na Li","Kai Wang","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2410.17538v1.pdf","comment":"29 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.05307v2","updated":"2024-10-23T03:38:00Z","published":"2024-08-09T19:06:38Z","title":"Audio-visual cross-modality knowledge transfer for machine\n  learning-based in-situ monitoring in laser additive manufacturing","summary":"  Various machine learning (ML)-based in-situ monitoring systems have been\ndeveloped to detect anomalies and defects in laser additive manufacturing (LAM)\nprocesses. While multimodal fusion, which integrates data from visual, audio,\nand other modalities, can improve monitoring performance, it also increases\nhardware, computational, and operational costs due to the use of multiple\nsensor types. This paper introduces a cross-modality knowledge transfer (CMKT)\nmethodology for LAM in-situ monitoring, which transfers knowledge from a source\nmodality to a target modality. CMKT enhances the representativeness of the\nfeatures extracted from the target modality, allowing the removal of source\nmodality sensors during prediction. This paper proposes three CMKT methods:\nsemantic alignment, fully supervised mapping, and semi-supervised mapping. The\nsemantic alignment method establishes a shared encoded space between modalities\nto facilitate knowledge transfer. It employs a semantic alignment loss to align\nthe distributions of identical groups (e.g., visual and audio defective groups)\nand a separation loss to distinguish different groups (e.g., visual defective\nand audio defect-free groups). The two mapping methods transfer knowledge by\nderiving features from one modality to another using fully supervised and\nsemi-supervised learning approaches. In a case study for LAM in-situ defect\ndetection, the proposed CMKT methods were compared with multimodal audio-visual\nfusion. The semantic alignment method achieved an accuracy of 98.7% while\nremoving the audio modality during the prediction phase, which is comparable to\nthe 98.2% accuracy obtained through multimodal fusion. Using explainable\nartificial intelligence, we discovered that semantic alignment CMKT can extract\nmore representative features while reducing noise by leveraging the inherent\ncorrelations between modalities.\n","authors":["Jiarui Xie","Mutahar Safdar","Lequn Chen","Seung Ki Moon","Yaoyao Fiona Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.05307v2.pdf","comment":"45 pages, 17 figures, 6 tables"},{"id":"http://arxiv.org/abs/2409.15922v2","updated":"2024-10-23T03:22:48Z","published":"2024-09-24T09:45:20Z","title":"The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM\n  Rewards","summary":"  While Vision-Language Models (VLMs) are increasingly used to generate reward\nsignals for training embodied agents to follow instructions, our research\nreveals that agents guided by VLM rewards often underperform compared to those\nemploying only intrinsic (exploration-driven) rewards, contradicting\nexpectations set by recent work. We hypothesize that false positive rewards --\ninstances where unintended trajectories are incorrectly rewarded -- are more\ndetrimental than false negatives. Our analysis confirms this hypothesis,\nrevealing that the widely used cosine similarity metric is prone to false\npositive reward estimates. To address this, we introduce BiMI ({Bi}nary\n{M}utual {I}nformation), a novel reward function designed to mitigate noise.\nBiMI significantly enhances learning efficiency across diverse and challenging\nembodied navigation environments. Our findings offer a nuanced understanding of\nhow different types of reward noise impact agent learning and highlight the\nimportance of addressing multimodal reward signal noise when training embodied\nagents\n","authors":["Sukai Huang","Nir Lipovetzky","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2409.15922v2.pdf","comment":"10 main body pages, 11 appendix pages"},{"id":"http://arxiv.org/abs/2410.14687v2","updated":"2024-10-23T03:05:37Z","published":"2024-10-03T14:17:43Z","title":"BrainTransformers: SNN-LLM","summary":"  This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments.\n","authors":["Zhengzheng Tang","Eva Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.14687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17526v1","updated":"2024-10-23T03:05:33Z","published":"2024-10-23T03:05:33Z","title":"GDDA: Semantic OOD Detection on Graphs under Covariate Shift via\n  Score-Based Diffusion Models","summary":"  Out-of-distribution (OOD) detection poses a significant challenge for Graph\nNeural Networks (GNNs), particularly in open-world scenarios with varying\ndistribution shifts. Most existing OOD detection methods on graphs primarily\nfocus on identifying instances in test data domains caused by either semantic\nshifts (changes in data classes) or covariate shifts (changes in data\nfeatures), while leaving the simultaneous occurrence of both distribution\nshifts under-explored. In this work, we address both types of shifts\nsimultaneously and introduce a novel challenge for OOD detection on graphs:\ngraph-level semantic OOD detection under covariate shift. In this scenario,\nvariations between the training and test domains result from the concurrent\npresence of both covariate and semantic shifts, where only graphs associated\nwith unknown classes are identified as OOD samples (OODs). To tackle this\nchallenge, we propose a novel two-phase framework called Graph Disentangled\nDiffusion Augmentation (GDDA). The first phase focuses on disentangling graph\nrepresentations into domain-invariant semantic factors and domain-specific\nstyle factors. In the second phase, we introduce a novel\ndistribution-shift-controlled score-based generative diffusion model that\ngenerates latent factors outside the training semantic and style spaces.\nAdditionally, auxiliary pseudo-in-distribution (InD) and pseudo-OOD graph\nrepresentations are employed to enhance the effectiveness of the energy-based\nsemantic OOD detector. Extensive empirical studies on three benchmark datasets\ndemonstrate that our approach outperforms state-of-the-art baselines.\n","authors":["Zhixia He","Chen Zhao","Minglai Shao","Yujie Lin","Dong Li","Qin Tian"],"pdf_url":"https://arxiv.org/pdf/2410.17526v1.pdf","comment":"4 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.11303v2","updated":"2024-10-23T03:00:41Z","published":"2024-10-15T05:54:17Z","title":"TSDS: Data Selection for Task-Specific Model Finetuning","summary":"  Finetuning foundation models for specific tasks is an emerging paradigm in\nmodern machine learning. The efficacy of task-specific finetuning largely\ndepends on the selection of appropriate training data. We present TSDS\n(Task-Specific Data Selection), a framework to select data for task-specific\nmodel finetuning, guided by a small but representative set of examples from the\ntarget task. To do so, we formulate data selection for task-specific finetuning\nas an optimization problem with a distribution alignment loss based on optimal\ntransport to capture the discrepancy between the selected data and the target\ndistribution. In addition, we add a regularizer to encourage the diversity of\nthe selected data and incorporate kernel density estimation into the\nregularizer to reduce the negative effects of near-duplicates among the\ncandidate data. We connect our optimization problem to nearest neighbor search\nand design efficient algorithms to compute the optimal solution based on\napproximate nearest neighbor search techniques. We evaluate our method on data\nselection for both continued pretraining and instruction tuning of language\nmodels. We show that instruction tuning using data selected by our method with\na 1% selection ratio often outperforms using the full dataset and beats the\nbaseline selection methods by 1.5 points in F1 score on average.\n","authors":["Zifan Liu","Amin Karbasi","Theodoros Rekatsinas"],"pdf_url":"https://arxiv.org/pdf/2410.11303v2.pdf","comment":"31 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.09121v2","updated":"2024-10-23T02:55:57Z","published":"2024-10-11T00:14:31Z","title":"Comparing Quantum Encoding Techniques","summary":"  As quantum computers continue to become more capable, the possibilities of\ntheir applications increase. For example, quantum techniques are being\nintegrated with classical neural networks to perform machine learning. In order\nto be used in this way, or for any other widespread use like quantum chemistry\nsimulations or cryptographic applications, classical data must be converted\ninto quantum states through quantum encoding. There are three fundamental\nencoding methods: basis, amplitude, and rotation, as well as several proposed\ncombinations. This study explores the encoding methods, specifically in the\ncontext of hybrid quantum-classical machine learning. Using the QuClassi\nquantum neural network architecture to perform binary classification of the `3'\nand `6' digits from the MNIST datasets, this study obtains several metrics such\nas accuracy, entropy, loss, and resistance to noise, while considering resource\nusage and computational complexity to compare the three main encoding methods.\n","authors":["Nidhi Munikote"],"pdf_url":"https://arxiv.org/pdf/2410.09121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16534v2","updated":"2024-10-23T02:55:14Z","published":"2024-10-21T21:48:49Z","title":"No more hard prompts: SoftSRV prompting for synthetic data generation","summary":"  We present a novel soft prompt based framework, SoftSRV, that leverages a\nfrozen pre-trained large language model (LLM) to generate targeted synthetic\ntext sequences. Given a sample from the target distribution, our proposed\nframework uses data-driven loss minimization to train a parameterized\n\"contextual\" soft prompt. This soft prompt is then used to steer the frozen LLM\nto generate synthetic sequences that are similar to the target distribution. We\nargue that SoftSRV provides a practical improvement over common hard-prompting\napproaches that rely on human-curated prompt-templates, which can be\nidiosyncratic, labor-intensive to craft, and may need to be specialized per\ndomain. We empirically evaluate SoftSRV and hard-prompting baselines by\ngenerating synthetic data to fine-tune a small Gemma model on three different\ndomains (coding, math, reasoning). To stress the generality of SoftSRV, we\nperform these evaluations without any particular specialization of the\nframework to each domain. We find that SoftSRV significantly improves upon\nhard-prompting baselines, generating data with superior fine-tuning performance\nand that better matches the target distribution according to the MAUVE\nsimilarity metric.\n","authors":["Giulia DeSalvo","Jean-Fracois Kagy","Lazaros Karydas","Afshin Rostamizadeh","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.16534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17520v1","updated":"2024-10-23T02:51:43Z","published":"2024-10-23T02:51:43Z","title":"MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control","summary":"  Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications. To\nclearly evaluate safety apart from general capabilities, we design separate\ntasks measuring safety and tasks evaluating helpfulness. The safety tasks\nchallenge agents with managing potential risks prevalent in daily life and\ninclude tests to evaluate robustness against indirect prompt injections. Our\nexperiments demonstrate that while baseline agents, based on state-of-the-art\nLLMs, perform well in executing helpful tasks, they show poor performance in\nsafety tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/.\n","authors":["Juyong Lee","Dongyoon Hahm","June Suk Choi","W. Bradley Knox","Kimin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.17520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17518v1","updated":"2024-10-23T02:50:05Z","published":"2024-10-23T02:50:05Z","title":"Univariate Conditional Variational Autoencoder for Morphogenic Patterns\n  Design in Frontal Polymerization-Based Manufacturing","summary":"  Rapid reaction-thermal diffusion during frontal polymerization (FP) with\nvariations in initial and boundary conditions destabilizes the planar mode of\nfront propagation, leading to spatially varying complex hierarchical patterns\nin polymeric materials. Although modern reaction-diffusion models can predict\nthe patterns resulting from unstable FP, the inverse design of patterns, which\naims to retrieve process conditions that produce a desired pattern, remains an\nopen challenge due to the nonunique and nonintuitive mapping between process\nconditions and patterns. In this work, we propose a novel probabilistic\ngenerative model named univariate conditional variational autoencoder (UcVAE)\nfor the inverse design of hierarchical patterns in FP-based manufacturing.\nUnlike the cVAE, which encodes both the design space and the design target, the\nUcVAE encodes only the design space. In the encoder of the UcVAE, the number of\ntraining parameters is significantly reduced compared to the cVAE, resulting in\na shorter training time while maintaining comparable performance. Given desired\npattern images, the trained UcVAE can generate multiple process condition\nsolutions that produce high-fidelity hierarchical patterns.\n","authors":["Qibang Liu","Pengfei Cai","Diab Abueidda","Seid Koric","Rafael Gomez-Bombarellig","Philippe Geubelle"],"pdf_url":"https://arxiv.org/pdf/2410.17518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07840v3","updated":"2024-10-23T02:43:03Z","published":"2023-07-15T16:16:22Z","title":"RegExplainer: Generating Explanations for Graph Neural Networks in\n  Regression Task","summary":"  Graph regression is a fundamental task and has received increasing attention\nin a wide range of graph learning tasks. However, the inference process is\noften not interpretable. Most existing explanation techniques are limited to\nunderstanding GNN behaviors in classification tasks. In this work, we seek an\nexplanation to interpret the graph regression models (XAIG-R). We show that\nexisting methods overlook the distribution shifting and continuously ordered\ndecision boundary, which hinders them away from being applied in the regression\ntasks. To address these challenges, we propose a novel objective based on the\ninformation bottleneck theory and introduce a new mix-up framework, which could\nsupport various GNNs in a model-agnostic manner. We further present a\ncontrastive learning strategy to tackle the continuously ordered labels in\nregression task. To empirically verify the effectiveness of the proposed\nmethod, we introduce three benchmark datasets and a real-life dataset for\nevaluation. Extensive experiments show the effectiveness of the proposed method\nin interpreting GNN models in regression tasks.\n","authors":["Jiaxing Zhang","Zhuomin Chen","Hao Mei","Dongsheng Luo","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2307.07840v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17511v1","updated":"2024-10-23T02:29:50Z","published":"2024-10-23T02:29:50Z","title":"Time and Frequency Synergy for Source-Free Time-Series Domain\n  Adaptations","summary":"  The issue of source-free time-series domain adaptations still gains scarce\nresearch attentions. On the other hand, existing approaches rely solely on\ntime-domain features ignoring frequency components providing complementary\ninformation. This paper proposes Time Frequency Domain Adaptation (TFDA), a\nmethod to cope with the source-free time-series domain adaptation problems.\nTFDA is developed with a dual branch network structure fully utilizing both\ntime and frequency features in delivering final predictions. It induces\npseudo-labels based on a neighborhood concept where predictions of a sample\ngroup are aggregated to generate reliable pseudo labels. The concept of\ncontrastive learning is carried out in both time and frequency domains with\npseudo label information and a negative pair exclusion strategy to make valid\nneighborhood assumptions. In addition, the time-frequency consistency technique\nis proposed using the self-distillation strategy while the uncertainty\nreduction strategy is implemented to alleviate uncertainties due to the domain\nshift problem. Last but not least, the curriculum learning strategy is\nintegrated to combat noisy pseudo labels. Our experiments demonstrate the\nadvantage of our approach over prior arts with noticeable margins in benchmark\nproblems.\n","authors":["Muhammad Tanzil Furqon","Mahardhika Pratama","Ary Mazharuddin Shiddiqi","Lin Liu","Habibullah Habibullah","Kutluyil Dogancay"],"pdf_url":"https://arxiv.org/pdf/2410.17511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14369v3","updated":"2024-10-23T02:26:20Z","published":"2024-05-23T09:45:57Z","title":"RoPINN: Region Optimized Physics-Informed Neural Networks","summary":"  Physics-informed neural networks (PINNs) have been widely applied to solve\npartial differential equations (PDEs) by enforcing outputs and gradients of\ndeep models to satisfy target equations. Due to the limitation of numerical\ncomputation, PINNs are conventionally optimized on finite selected points.\nHowever, since PDEs are usually defined on continuous domains, solely\noptimizing models on scattered points may be insufficient to obtain an accurate\nsolution for the whole domain. To mitigate this inherent deficiency of the\ndefault scatter-point optimization, this paper proposes and theoretically\nstudies a new training paradigm as region optimization. Concretely, we propose\nto extend the optimization process of PINNs from isolated points to their\ncontinuous neighborhood regions, which can theoretically decrease the\ngeneralization error, especially for hidden high-order constraints of PDEs. A\npractical training algorithm, Region Optimized PINN (RoPINN), is seamlessly\nderived from this new paradigm, which is implemented by a straightforward but\neffective Monte Carlo sampling method. By calibrating the sampling process into\ntrust regions, RoPINN finely balances optimization and generalization error.\nExperimentally, RoPINN consistently boosts the performance of diverse PINNs on\na wide range of PDEs without extra backpropagation or gradient calculation.\nCode is available at this repository: https://github.com/thuml/RoPINN.\n","authors":["Haixu Wu","Huakun Luo","Yuezhou Ma","Jianmin Wang","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2405.14369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17510v1","updated":"2024-10-23T02:25:53Z","published":"2024-10-23T02:25:53Z","title":"Congestion Forecast for Trains with Railroad-Graph-based Semi-Supervised\n  Learning using Sparse Passenger Reports","summary":"  Forecasting rail congestion is crucial for efficient mobility in transport\nsystems. We present rail congestion forecasting using reports from passengers\ncollected through a transit application. Although reports from passengers have\nreceived attention from researchers, ensuring a sufficient volume of reports is\nchallenging due to passenger's reluctance. The limited number of reports\nresults in the sparsity of the congestion label, which can be an issue in\nbuilding a stable prediction model. To address this issue, we propose a\nsemi-supervised method for congestion forecasting for trains, or SURCONFORT.\nOur key idea is twofold: firstly, we adopt semi-supervised learning to leverage\nsparsely labeled data and many unlabeled data. Secondly, in order to complement\nthe unlabeled data from nearby stations, we design a railway network-oriented\ngraph and apply the graph to semi-supervised graph regularization. Empirical\nexperiments with actual reporting data show that SURCONFORT improved the\nforecasting performance by 14.9% over state-of-the-art methods under the label\nsparsity.\n","authors":["Soto Anno","Kota Tsubouchi","Masamichi Shimosaka"],"pdf_url":"https://arxiv.org/pdf/2410.17510v1.pdf","comment":"Accepted in ACM SIGSPATIAL 2024"},{"id":"http://arxiv.org/abs/2410.17509v1","updated":"2024-10-23T02:22:07Z","published":"2024-10-23T02:22:07Z","title":"WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning\n  in Large Language Models","summary":"  The need for effective unlearning mechanisms in large language models (LLMs)\nis increasingly urgent, driven by the necessity to adhere to data regulations\nand foster ethical generative AI practices. Despite growing interest of LLM\nunlearning, much of the existing research has focused on varied unlearning\nmethod designs to boost effectiveness and efficiency. However, the inherent\nrelationship between model weights and LLM unlearning has not been extensively\nexamined. In this paper, we systematically explore how model weights interact\nwith unlearning processes in LLMs and we design the weight attribution-guided\nLLM unlearning method, WAGLE, which unveils the interconnections between\n'influence' of weights and 'influence' of data to forget and retain in LLM\ngeneration. By strategically guiding the LLM unlearning across different types\nof unlearning methods and tasks, WAGLE can erase the undesired content, while\nmaintaining the performance of the original tasks. We refer to the weight\nattribution-guided LLM unlearning method as WAGLE, which unveils the\ninterconnections between 'influence' of weights and 'influence' of data to\nforget and retain in LLM generation. Our extensive experiments show that WAGLE\nboosts unlearning performance across a range of LLM unlearning methods such as\ngradient difference and (negative) preference optimization, applications such\nas fictitious unlearning, malicious use prevention, and copyrighted information\nremoval, and models including Zephyr-7b-beta and Llama2-7b. To the best of our\nknowledge, our work offers the first principled method for attributing and\npinpointing the influential weights in enhancing LLM unlearning. It stands in\ncontrast to previous methods that lack weight attribution and simpler weight\nattribution techniques.\n","authors":["Jinghan Jia","Jiancheng Liu","Yihua Zhang","Parikshit Ram","Nathalie Baracaldo","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13334v2","updated":"2024-10-23T02:15:52Z","published":"2024-10-17T08:46:09Z","title":"Do LLMs Have Political Correctness? Analyzing Ethical Biases and\n  Jailbreak Vulnerabilities in AI Systems","summary":"  Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content. To address\nthese issues, many LLM developers have implemented various safety measures to\nalign these models. This alignment involves several techniques, including data\nfiltering during pre-training, supervised fine-tuning, reinforcement learning\nfrom human feedback, and red-teaming exercises. These methods often introduce\ndeliberate and intentional biases similar to Political Correctness (PC) to\nensure the ethical behavior of LLMs. In this paper, we delve into the\nintentional biases injected into LLMs for safety purposes and examine methods\nto circumvent these safety alignment techniques. Notably, these intentional\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20% between non-binary and cisgender keywords and by 16% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of PCJailbreak, highlighting the inherent risks posed by\nthese safety-induced biases. Additionally, we propose an efficient defense\nmethod PCDefense, which prevents jailbreak attempts by injecting defense\nprompts prior to generation. PCDefense stands as an appealing alternative to\nGuard Models, such as Llama-Guard, that require additional inference cost after\ntext generation. Our findings emphasize the urgent need for LLM developers to\nadopt a more responsible approach when designing and implementing safety\nmeasures.\n","authors":["Isack Lee","Haebin Seong"],"pdf_url":"https://arxiv.org/pdf/2410.13334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17506v1","updated":"2024-10-23T02:09:02Z","published":"2024-10-23T02:09:02Z","title":"Mitigating Graph Covariate Shift via Score-based Out-of-distribution\n  Augmentation","summary":"  Distribution shifts between training and testing datasets significantly\nimpair the model performance on graph learning. A commonly-taken causal view in\ngraph invariant learning suggests that stable predictive features of graphs are\ncausally associated with labels, whereas varying environmental features lead to\ndistribution shifts. In particular, covariate shifts caused by unseen\nenvironments in test graphs underscore the critical need for\nout-of-distribution (OOD) generalization. Existing graph augmentation methods\ndesigned to address the covariate shift often disentangle the stable and\nenvironmental features in the input space, and selectively perturb or mixup the\nenvironmental features. However, such perturbation-based methods heavily rely\non an accurate separation of stable and environmental features, and their\nexploration ability is confined to existing environmental features in the\ntraining distribution. To overcome these limitations, we introduce a novel\napproach using score-based graph generation strategies that synthesize unseen\nenvironmental features while preserving the validity and stable features of\noverall graph patterns. Our comprehensive empirical evaluations demonstrate the\nenhanced effectiveness of our method in improving graph OOD generalization.\n","authors":["Bohan Wang","Yurui Chang","Lu Lin"],"pdf_url":"https://arxiv.org/pdf/2410.17506v1.pdf","comment":"17 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.08012v3","updated":"2024-10-23T02:07:09Z","published":"2024-02-12T19:21:14Z","title":"Online Differentially Private Synthetic Data Generation","summary":"  We present a polynomial-time algorithm for online differentially private\nsynthetic data generation. For a data stream within the hypercube $[0,1]^d$ and\nan infinite time horizon, we develop an online algorithm that generates a\ndifferentially private synthetic dataset at each time $t$. This algorithm\nachieves a near-optimal accuracy bound of $O(\\log(t)t^{-1/d})$ for $d\\geq 2$\nand $O(\\log^{4.5}(t)t^{-1})$ for $d=1$ in the 1-Wasserstein distance. This\nresult extends the previous work on the continual release model for counting\nqueries to Lipschitz queries. Compared to the offline case, where the entire\ndataset is available at once, our approach requires only an extra polylog\nfactor in the accuracy bound.\n","authors":["Yiyun He","Roman Vershynin","Yizhe Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.08012v3.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2408.08470v2","updated":"2024-10-23T01:36:25Z","published":"2024-08-16T01:12:21Z","title":"Context-Aware Assistant Selection for Improved Inference Acceleration\n  with Large Language Models","summary":"  Despite their widespread adoption, large language models (LLMs) remain\nprohibitive to use under resource constraints, with their ever growing sizes\nonly increasing the barrier for use. One noted issue is the high latency\nassociated with auto-regressive generation, rendering large LLMs use dependent\non advanced computing infrastructure. Assisted decoding, where a smaller draft\nmodel guides a larger target model's generation, has helped alleviate this, but\nremains dependent on alignment between the two models. Thus if the draft model\nis insufficiently capable on some domain relative to the target model,\nperformance can degrade. Alternatively, one can leverage multiple draft models\nto better cover the expertise of the target, but when multiple black-box draft\nmodels are available, selecting an assistant without details about its\nconstruction can be difficult. To better understand this decision making\nproblem, we observe it as a contextual bandit, where a policy must choose a\ndraft model based on a context. We show that even without prior knowledge of\nthe draft models, creating an offline dataset from only outputs of independent\ndraft/target models and training a policy over the alignment of these outputs\ncan accelerate performance on multiple domains provided the candidates are\neffective. Further results show this to hold on various settings with multiple\nassisted decoding candidates, highlighting its flexibility and the advantageous\nrole that such decision making can play.\n","authors":["Jerry Huang","Prasanna Parthasarathi","Mehdi Rezagholizadeh","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2408.08470v2.pdf","comment":"2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP); 14 pages (9 pages main content + references + appendix)"},{"id":"http://arxiv.org/abs/2202.05404v7","updated":"2024-10-23T01:23:04Z","published":"2022-02-11T01:29:50Z","title":"Regularized Q-learning","summary":"  Q-learning is widely used algorithm in reinforcement learning community.\nUnder the lookup table setting, its convergence is well established. However,\nits behavior is known to be unstable with the linear function approximation\ncase. This paper develops a new Q-learning algorithm that converges when linear\nfunction approximation is used. We prove that simply adding an appropriate\nregularization term ensures convergence of the algorithm. We prove its\nstability using a recent analysis tool based on switching system models.\nMoreover, we experimentally show that it converges in environments where\nQ-learning with linear function approximation has known to diverge. We also\nprovide an error bound on the solution where the algorithm converges.\n","authors":["Han-Dong Lim","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2202.05404v7.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.17492v1","updated":"2024-10-23T01:14:54Z","published":"2024-10-23T01:14:54Z","title":"BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers","summary":"  Attacking fairness is crucial because compromised models can introduce biased\noutcomes, undermining trust and amplifying inequalities in sensitive\napplications like hiring, healthcare, and law enforcement. This highlights the\nurgent need to understand how fairness mechanisms can be exploited and to\ndevelop defenses that ensure both fairness and robustness. We introduce\nBadFair, a novel backdoored fairness attack methodology. BadFair stealthily\ncrafts a model that operates with accuracy and fairness under regular\nconditions but, when activated by certain triggers, discriminates and produces\nincorrect results for specific groups. This type of attack is particularly\nstealthy and dangerous, as it circumvents existing fairness detection methods,\nmaintaining an appearance of fairness in normal use. Our findings reveal that\nBadFair achieves a more than 85% attack success rate in attacks aimed at target\ngroups on average while only incurring a minimal accuracy loss. Moreover, it\nconsistently exhibits a significant discrimination score, distinguishing\nbetween pre-defined target and non-target attacked groups across various\ndatasets and models.\n","authors":["Jiaqi Xue","Qian Lou","Mengxin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.17492v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.10937v2","updated":"2024-10-23T01:13:24Z","published":"2024-10-14T17:59:58Z","title":"Hybrid Spatial Representations for Species Distribution Modeling","summary":"  We address an important problem in ecology called Species Distribution\nModeling (SDM), whose goal is to predict whether a species exists at a certain\nposition on Earth. In particular, we tackle a challenging version of this task,\nwhere we learn from presence-only data in a community-sourced dataset, model a\nlarge number of species simultaneously, and do not use any additional\nenvironmental information. Previous work has used neural implicit\nrepresentations to construct models that achieve promising results. However,\nimplicit representations often generate predictions of limited spatial\nprecision. We attribute this limitation to their inherently global formulation\nand inability to effectively capture local feature variations. This issue is\nespecially pronounced with presence-only data and a large number of species. To\naddress this, we propose a hybrid embedding scheme that combines both implicit\nand explicit embeddings. Specifically, the explicit embedding is implemented\nwith a multiresolution hashgrid, enabling our models to better capture local\ninformation. Experiments demonstrate that our results exceed other works by a\nlarge margin on various standard benchmarks, and that the hybrid representation\nis better than both purely implicit and explicit ones. Qualitative\nvisualizations and comprehensive ablation studies reveal that our hybrid\nrepresentation successfully addresses the two main challenges. Our code is\nopen-sourced at https://github.com/Shiran-Yuan/HSR-SDM.\n","authors":["Shiran Yuan","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.10937v2.pdf","comment":"Project codebase https://github.com/Shiran-Yuan/HSR-SDM"},{"id":"http://arxiv.org/abs/2410.14740v2","updated":"2024-10-23T01:08:59Z","published":"2024-10-17T08:33:39Z","title":"Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching","summary":"  Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.\n","authors":["Jie Peng","Zhang Cao","Huaizhi Qu","Zhengyu Zhang","Chang Guo","Yanyong Zhang","Zhichao Cao","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14740v2.pdf","comment":"24 pages, 13 figures"},{"id":"http://arxiv.org/abs/2211.03054v2","updated":"2024-10-23T01:02:38Z","published":"2022-11-06T07:56:33Z","title":"ODBAE: a high-performance model identifying complex phenotypes in\n  high-dimensional biological datasets","summary":"  Identifying complex phenotypes from high-dimensional biological data is\nchallenging due to the intricate interdependencies among different\nphysiological indicators. Traditional approaches often focus on detecting\noutliers in single variables, overlooking the broader network of interactions\nthat contribute to phenotype emergence. Here, we introduce ODBAE (Outlier\nDetection using Balanced Autoencoders), a machine learning method designed to\nuncover both subtle and extreme outliers by capturing latent relationships\namong multiple physiological parameters. ODBAE's revised loss function enhances\nits ability to detect two key types of outliers: influential points (IP), which\ndisrupt latent correlations between dimensions, and high leverage points (HLP),\nwhich deviate from the norm but go undetected by traditional autoencoder-based\nmethods. Using data from the International Mouse Phenotyping Consortium (IMPC),\nwe show that ODBAE can identify knockout mice with complex, multi-indicator\nphenotypes - normal in individual traits, but abnormal when considered\ntogether. In addition, this method reveals novel metabolism-related genes and\nuncovers coordinated abnormalities across metabolic indicators. Our results\nhighlight the utility of ODBAE in detecting joint abnormalities and advancing\nour understanding of homeostatic perturbations in biological systems.\n","authors":["Yafei Shen","Tao Zhang","Zhiwei Liu","Kalliopi Kostelidou","Ying Xu","Ling Yang"],"pdf_url":"https://arxiv.org/pdf/2211.03054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17488v1","updated":"2024-10-23T00:51:47Z","published":"2024-10-23T00:51:47Z","title":"GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion\n  Policy","summary":"  Diffusion-based policies have shown remarkable capability in executing\ncomplex robotic manipulation tasks but lack explicit characterization of\ngeometry and semantics, which often limits their ability to generalize to\nunseen objects and layouts. To enhance the generalization capabilities of\nDiffusion Policy, we introduce a novel framework that incorporates explicit\nspatial and semantic information via 3D semantic fields. We generate 3D\ndescriptor fields from multi-view RGBD observations with large foundational\nvision models, then compare these descriptor fields against reference\ndescriptors to obtain semantic fields. The proposed method explicitly considers\ngeometry and semantics, enabling strong generalization capabilities in tasks\nrequiring category-level generalization, resolving geometric ambiguities, and\nattention to subtle geometric details. We evaluate our method across eight\ntasks involving articulated objects and instances with varying shapes and\ntextures from multiple object categories. Our method demonstrates its\neffectiveness by increasing Diffusion Policy's average success rate on unseen\ninstances from 20% to 93%. Additionally, we provide a detailed analysis and\nvisualization to interpret the sources of performance gain and explain how our\nmethod can generalize to novel instances.\n","authors":["Yixuan Wang","Guang Yin","Binghao Huang","Tarik Kelestemur","Jiuguang Wang","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2410.17488v1.pdf","comment":"Accepted to Conference on Robot Learning (CoRL 2024). Project Page:\n  https://robopil.github.io/GenDP/"},{"id":"http://arxiv.org/abs/2402.01909v2","updated":"2024-10-23T00:40:23Z","published":"2024-02-02T21:21:55Z","title":"On Catastrophic Inheritance of Large Foundation Models","summary":"  Large foundation models (LFMs) are claiming incredible performances. Yet\ngreat concerns have been raised about their mythic and uninterpreted potentials\nnot only in machine learning, but also in various other disciplines. In this\nposition paper, we propose to identify a neglected issue deeply rooted in LFMs:\nCatastrophic Inheritance, describing the weaknesses and limitations inherited\nfrom biased large-scale pre-training data to behaviors of LFMs on the\ndownstream tasks, including samples that are corrupted, long-tailed, noisy,\nout-of-distributed, to name a few. Such inheritance can potentially cause\ncatastrophes to downstream applications, such as bias, lack of generalization,\ndeteriorated performance, security vulnerability, privacy leakage, and value\nmisalignment. We discuss the challenges behind this issue and propose UIM, a\nframework to Understand the catastrophic inheritance of LFMs from both\npre-training and downstream adaptation, Interpret the implications of\ncatastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims\nto unite both the machine learning and social sciences communities for more\nresponsible and promising AI development and deployment.\n","authors":["Hao Chen","Bhiksha Raj","Xing Xie","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2402.01909v2.pdf","comment":"Accepted by DMLR"},{"id":"http://arxiv.org/abs/2410.17484v1","updated":"2024-10-23T00:31:17Z","published":"2024-10-23T00:31:17Z","title":"Which Client is Reliable?: A Reliable and Personalized Prompt-based\n  Federated Learning for Medical Image Question Answering","summary":"  Conventional medical artificial intelligence (AI) models face barriers in\nclinical application and ethical issues owing to their inability to handle the\nprivacy-sensitive characteristics of medical data. We present a novel\npersonalized federated learning (pFL) method for medical visual question\nanswering (VQA) models, addressing privacy reliability challenges in the\nmedical domain. Our method introduces learnable prompts into a Transformer\narchitecture to efficiently train it on diverse medical datasets without\nmassive computational costs. Then we introduce a reliable client VQA model that\nincorporates Dempster-Shafer evidence theory to quantify uncertainty in\npredictions, enhancing the model's reliability. Furthermore, we propose a novel\ninter-client communication mechanism that uses maximum likelihood estimation to\nbalance accuracy and uncertainty, fostering efficient integration of insights\nacross clients.\n","authors":["He Zhu","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2410.17484v1.pdf","comment":null}],"Computation and Language":[{"id":"http://arxiv.org/abs/2410.17195v2","updated":"2024-10-23T07:02:09Z","published":"2024-10-22T17:13:38Z","title":"Non-myopic Generation of Language Model for Reasoning and Planning","summary":"  Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.\n","authors":["Chang Ma","Haiteng Zhao","Junlei Zhang","Junxian He","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.17195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16638v2","updated":"2024-10-23T03:41:49Z","published":"2024-10-22T02:27:57Z","title":"LLMScan: Causal Scan for LLM Misbehavior Detection","summary":"  Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.\n","authors":["Mengdi Zhang","Kai Kiat Goh","Peixin Zhang","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.16638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16451v2","updated":"2024-10-23T01:56:15Z","published":"2024-10-21T19:25:31Z","title":"Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between\n  Ghana and the U.S","summary":"  Recent work has highlighted the culturally-contingent nature of commonsense\nknowledge. We introduce AMAMMER${\\epsilon}$, a test set of 525 multiple-choice\nquestions designed to evaluate the commonsense knowledge of English LLMs,\nrelative to the cultural contexts of Ghana and the United States. To create\nAMAMMER${\\epsilon}$, we select a set of multiple-choice questions (MCQs) from\nexisting commonsense datasets and rewrite them in a multi-stage process\ninvolving surveys of Ghanaian and U.S. participants. In three rounds of\nsurveys, participants from both pools are solicited to (1) write correct and\nincorrect answer choices, (2) rate individual answer choices on a 5-point\nLikert scale, and (3) select the best answer choice from the newly-constructed\nMCQ items, in a final validation step. By engaging participants at multiple\nstages, our procedure ensures that participant perspectives are incorporated\nboth in the creation and validation of test items, resulting in high levels of\nagreement within each pool. We evaluate several off-the-shelf English LLMs on\nAMAMMER${\\epsilon}$. Uniformly, models prefer answers choices that align with\nthe preferences of U.S. annotators over Ghanaian annotators. Additionally, when\ntest items specify a cultural context (Ghana or the U.S.), models exhibit some\nability to adapt, but performance is consistently better in U.S. contexts than\nGhanaian. As large resources are devoted to the advancement of English LLMs,\nour findings underscore the need for culturally adaptable models and\nevaluations to meet the needs of diverse English-speaking populations around\nthe world.\n","authors":["Christabel Acquaye","Haozhe An","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2410.16451v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.16144v2","updated":"2024-10-23T11:17:42Z","published":"2024-10-21T16:14:57Z","title":"1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs","summary":"  Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shaoguang Mao","Shuming Ma","Hongyu Wang","Yan Xia","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.16144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03881v4","updated":"2024-10-23T01:39:31Z","published":"2024-04-05T04:04:23Z","title":"A Bi-consolidating Model for Joint Relational Triple Extraction","summary":"  Current methods to extract relational triples directly make a prediction\nbased on a possible entity pair in a raw sentence without depending on entity\nrecognition. The task suffers from a serious semantic overlapping problem, in\nwhich several relation triples may share one or two entities in a sentence. In\nthis paper, based on a two-dimensional sentence representation, a\nbi-consolidating model is proposed to address this problem by simultaneously\nreinforcing the local and global semantic features relevant to a relation\ntriple. This model consists of a local consolidation component and a global\nconsolidation component. The first component uses a pixel difference\nconvolution to enhance semantic information of a possible triple representation\nfrom adjacent regions and mitigate noise in neighbouring neighbours. The second\ncomponent strengthens the triple representation based a channel attention and a\nspatial attention, which has the advantage to learn remote semantic\ndependencies in a sentence. They are helpful to improve the performance of both\nentity identification and relation type classification in relation triple\nextraction. After evaluated on several publish datasets, the bi-consolidating\nmodel achieves competitive performance. Analytical experiments demonstrate the\neffectiveness of our model for relational triple extraction and give motivation\nfor other natural language processing tasks.\n","authors":["Xiaocheng Luo","Yanping Chen","Ruixue Tang","Caiwei Yang","Ruizhang Huang","Yongbin Qin"],"pdf_url":"https://arxiv.org/pdf/2404.03881v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18077v1","updated":"2024-10-23T17:58:49Z","published":"2024-10-23T17:58:49Z","title":"ALTA: Compiler-Based Analysis of Transformers","summary":"  We propose a new programming language called ALTA and a compiler that can map\nALTA programs to Transformer weights. ALTA is inspired by RASP, a language\nproposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler\nfrom RASP programs to Transformer weights. ALTA complements and extends this\nprior work, offering the ability to express loops and to compile programs to\nUniversal Transformers, among other advantages. ALTA allows us to\nconstructively show how Transformers can represent length-invariant algorithms\nfor computing parity and addition, as well as a solution to the SCAN benchmark\nof compositional generalization tasks, without requiring intermediate\nscratchpad decoding steps. We also propose tools to analyze cases where the\nexpressibility of an algorithm is established, but end-to-end training on a\ngiven training set fails to induce behavior consistent with the desired\nalgorithm. To this end, we explore training from ALTA execution traces as a\nmore fine-grained supervision signal. This enables additional experiments and\ntheoretical analyses relating the learnability of various algorithms to data\navailability and modeling decisions, such as positional encodings. We make the\nALTA framework -- language specification, symbolic interpreter, and weight\ncompiler -- available to the community to enable further applications and\ninsights.\n","authors":["Peter Shaw","James Cohan","Jacob Eisenstein","Kenton Lee","Jonathan Berant","Kristina Toutanova"],"pdf_url":"https://arxiv.org/pdf/2410.18077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18071v1","updated":"2024-10-23T17:54:43Z","published":"2024-10-23T17:54:43Z","title":"TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing\n  Prompts","summary":"  Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks.\n","authors":["Yuxuan Xie","Tianhua Li","Wenqi Shao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15240v2","updated":"2024-10-23T17:47:58Z","published":"2024-09-23T17:38:41Z","title":"MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue\n  Generation","summary":"  Long-term memory is important for chatbots and dialogue systems (DS) to\ncreate consistent and human-like conversations, evidenced by numerous developed\nmemory-augmented DS (MADS). To evaluate the effectiveness of such MADS,\nexisting commonly used evaluation metrics, like retrieval accuracy and\nperplexity (PPL), mainly focus on query-oriented factualness and language\nquality assessment. However, these metrics often lack practical value.\nMoreover, the evaluation dimensions are insufficient for human-like assessment\nin DS. Regarding memory-recalling paradigms, current evaluation schemes only\nconsider passive memory retrieval while ignoring diverse memory recall with\nrich triggering factors, e.g., emotions and surroundings, which can be\nessential in emotional support scenarios. To bridge the gap, we construct a\nnovel Memory-Augmented Dialogue Benchmark (MADail-Bench) covering various\nmemory-recalling paradigms based on cognitive science and psychology theories.\nThe benchmark assesses two tasks separately: memory retrieval and memory\nrecognition with the incorporation of both passive and proactive memory recall\ndata. We introduce new scoring criteria to the evaluation, including memory\ninjection, emotion support (ES) proficiency, and intimacy, to comprehensively\nassess generated responses. Results from cutting-edge embedding models and\nlarge language models on this benchmark indicate the potential for further\nadvancement. Extensive testing further reveals correlations between memory\ninjection, ES proficiency, and intimacy.\n","authors":["Junqing He","Liang Zhu","Rui Wang","Xi Wang","Reza Haffari","Jiaxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.15240v2.pdf","comment":"Submitted to NAACL 2025"},{"id":"http://arxiv.org/abs/2404.19442v2","updated":"2024-10-23T17:46:13Z","published":"2024-04-30T10:45:40Z","title":"Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs","summary":"  Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples.\n","authors":["David Ifeoluwa Adelani","A. Seza Doğruöz","Iyanuoluwa Shode","Anuoluwapo Aremu"],"pdf_url":"https://arxiv.org/pdf/2404.19442v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2407.15762v2","updated":"2024-10-23T17:42:39Z","published":"2024-07-22T16:13:38Z","title":"Conditional Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning","summary":"  Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge is to develop\nsteerable language models that trade-off multiple (conflicting) objectives in a\nflexible and efficient manner. This paper presents Conditional Language Policy\n(CLP), a general framework for finetuning language models on multiple\nobjectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through extensive experiments and ablations on two\nsummarization datasets, we show that CLP learns steerable language models that\noutperform and Pareto-dominate the existing approaches for multi-objective\nfinetuning.\n","authors":["Kaiwen Wang","Rahul Kidambi","Ryan Sullivan","Alekh Agarwal","Christoph Dann","Andrea Michi","Marco Gelmi","Yunxuan Li","Raghav Gupta","Avinava Dubey","Alexandre Ramé","Johan Ferret","Geoffrey Cideron","Le Hou","Hongkun Yu","Amr Ahmed","Aranyak Mehta","Léonard Hussenot","Olivier Bachem","Edouard Leurent"],"pdf_url":"https://arxiv.org/pdf/2407.15762v2.pdf","comment":"40 pages. Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.18057v1","updated":"2024-10-23T17:30:50Z","published":"2024-10-23T17:30:50Z","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","summary":"  Machine Unlearning (MU) is critical for enhancing privacy and security in\ndeep learning models, particularly in large multimodal language models (MLLMs),\nby removing specific private or hazardous information. While MU has made\nsignificant progress in textual and visual modalities, multimodal unlearning\n(MMU) remains significantly underexplored, partially due to the absence of a\nsuitable open-source benchmark. To address this, we introduce CLEAR, a new\nbenchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We assess 10 MU methods,\nadapting them for MMU, and highlight new challenges specific to multimodal\nforgetting. We also demonstrate that simple $\\ell_1$ regularization on LoRA\nweights significantly mitigates catastrophic forgetting, preserving model\nperformance on retained data. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR\n","authors":["Alexey Dontsov","Dmitrii Korzh","Alexey Zhavoronkin","Boris Mikheev","Denis Bobkov","Aibek Alanov","Oleg Y. Rogov","Ivan Oseledets","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2410.18057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18050v1","updated":"2024-10-23T17:24:58Z","published":"2024-10-23T17:24:58Z","title":"LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering","summary":"  Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG.\n","authors":["Qingfei Zhao","Ruobing Wang","Yukuo Cen","Daren Zha","Shicheng Tan","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2410.18050v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.18040v1","updated":"2024-10-23T17:07:32Z","published":"2024-10-23T17:07:32Z","title":"Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for\n  Russian Scientific Keyphrases","summary":"  Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts.\n","authors":["Anna Glazkova","Dmitry Morozov","Timur Garipov"],"pdf_url":"https://arxiv.org/pdf/2410.18040v1.pdf","comment":"The 12th International Conference on Analysis of Images, Social\n  Networks and Texts (AIST'2024)"},{"id":"http://arxiv.org/abs/2410.18035v1","updated":"2024-10-23T17:04:40Z","published":"2024-10-23T17:04:40Z","title":"MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language\n  Models Fine-tuning","summary":"  Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are\nhighly effective parameter-efficient fine-tuning (PEFT) methods. However, they\nintroduce significant latency in multi-tenant settings due to the LoRA modules\nand MOE routers added to multiple linear modules in the Transformer layer. To\naddress this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel\nand efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods\nby considering each LoRA module as an expert and employing a prompt-aware\nrouting mechanism. This mechanism calculates expert routing results once before\ngenerating the first new token and reuses these results for subsequent tokens,\nreducing latency. Extensive experiments and analysis on commonsense reasoning\ntasks, math reasoning tasks, and widely used LLM evaluation benchmarks\ndemonstrate that MiLoRA consistently outperforms strong PEFT baselines with\ncomparable tunable parameter budgets. Additionally, MiLoRA significantly\nreduces latency in multi-tenant settings compared to previous LoRA-based\nmethods.\n","authors":["Jingfan Zhang","Yi Zhao","Dan Chen","Xing Tian","Huanran Zheng","Wei Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.18035v1.pdf","comment":"Accepted by EMNLP 2024 Findings. arXiv admin note: substantial text\n  overlap with arXiv:2405.18203"},{"id":"http://arxiv.org/abs/2410.18032v1","updated":"2024-10-23T17:02:59Z","published":"2024-10-23T17:02:59Z","title":"GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration","summary":"  Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.\n","authors":["Xin Li","Qizhi Chu","Yubin Chen","Yang Liu","Yaoqi Liu","Zekai Yu","Weize Chen","Chen Qian","Chuan Shi","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.18032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18027v1","updated":"2024-10-23T17:00:13Z","published":"2024-10-23T17:00:13Z","title":"Cross-lingual Transfer of Reward Models in Multilingual Alignment","summary":"  Reinforcement learning with human feedback (RLHF) is shown to largely benefit\nfrom precise reward models (RMs). However, recent studies in reward modeling\nschemes are skewed towards English, limiting the applicability of RLHF in\nmultilingual alignments. In this work, we investigate the cross-lingual\ntransfer of RMs trained in diverse languages, primarily from English. Our\nexperimental results demonstrate the strong cross-lingual transfer of English\nRMs, exceeding target language RMs by 3~4% average increase in Multilingual\nRewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through\nthe representation shifts. Finally, we perform multilingual alignment to\nexemplify how cross-lingual transfer in RM propagates to enhanced multilingual\ninstruction-following capability, along with extensive analyses on\noff-the-shelf RMs. We release the code, model, and data.\n","authors":["Jiwoo Hong","Noah Lee","Rodrigo Martínez-Castaño","César Rodríguez","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2410.18027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11757v4","updated":"2024-10-23T16:41:45Z","published":"2024-06-17T17:16:45Z","title":"STAR: SocioTechnical Approach to Red Teaming Language Models","summary":"  This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.\n","authors":["Laura Weidinger","John Mellor","Bernat Guillen Pegueroles","Nahema Marchal","Ravin Kumar","Kristian Lum","Canfer Akbulut","Mark Diaz","Stevie Bergman","Mikel Rodriguez","Verena Rieser","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2406.11757v4.pdf","comment":"8 pages, 5 figures, 5 pages appendix. * denotes equal contribution"},{"id":"http://arxiv.org/abs/2409.17270v2","updated":"2024-10-23T16:27:20Z","published":"2024-09-25T18:35:45Z","title":"Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.\n","authors":["Debargha Ganguly","Srinivasan Iyengar","Vipin Chaudhary","Shivkumar Kalyanaraman"],"pdf_url":"https://arxiv.org/pdf/2409.17270v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) System 2 Reasoning At Scale Workshop"},{"id":"http://arxiv.org/abs/2407.10992v2","updated":"2024-10-23T16:19:06Z","published":"2024-06-24T09:29:14Z","title":"AlleNoise: large-scale text classification benchmark dataset with\n  real-world label noise","summary":"  Label noise remains a challenge for training robust classification models.\nMost methods for mitigating label noise have been benchmarked using primarily\ndatasets with synthetic noise. While the need for datasets with realistic noise\ndistribution has partially been addressed by web-scraped benchmarks such as\nWebVision and Clothing1M, those benchmarks are restricted to the computer\nvision domain. With the growing importance of Transformer-based models, it is\ncrucial to establish text classification benchmarks for learning with noisy\nlabels. In this paper, we present AlleNoise, a new curated text classification\nbenchmark dataset with real-world instance-dependent label noise, containing\nover 500,000 examples across approximately 5,600 classes, complemented with a\nmeaningful, hierarchical taxonomy of categories. The noise distribution comes\nfrom actual users of a major e-commerce marketplace, so it realistically\nreflects the semantics of human mistakes. In addition to the noisy labels, we\nprovide human-verified clean labels, which help to get a deeper insight into\nthe noise distribution, unlike web-scraped datasets typically used in the\nfield. We demonstrate that a representative selection of established methods\nfor learning with noisy labels is inadequate to handle such real-world noise.\nIn addition, we show evidence that these algorithms do not alleviate excessive\nmemorization. As such, with AlleNoise, we set the bar high for the development\nof label noise methods that can handle real-world label noise in text\nclassification tasks. The code and dataset are available for download at\nhttps://github.com/allegro/AlleNoise.\n","authors":["Alicja Rączkowska","Aleksandra Osowska-Kurczab","Jacek Szczerbiński","Kalina Jasinska-Kobus","Klaudia Nazarko"],"pdf_url":"https://arxiv.org/pdf/2407.10992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15720v4","updated":"2024-10-23T16:12:39Z","published":"2024-04-24T08:13:02Z","title":"Annotator-Centric Active Learning for Subjective NLP Tasks","summary":"  Active Learning (AL) addresses the high costs of collecting human annotations\nby strategically annotating the most informative samples. However, for\nsubjective NLP tasks, incorporating a wide range of perspectives in the\nannotation process is crucial to capture the variability in human judgments. We\nintroduce Annotator-Centric Active Learning (ACAL), which incorporates an\nannotator selection strategy following data sampling. Our objective is\ntwo-fold: 1) to efficiently approximate the full diversity of human judgments,\nand 2) to assess model performance using annotator-centric metrics, which value\nminority and majority perspectives equally. We experiment with multiple\nannotator selection strategies across seven subjective NLP tasks, employing\nboth traditional and novel, human-centered evaluation metrics. Our findings\nindicate that ACAL improves data efficiency and excels in annotator-centric\nperformance evaluations. However, its success depends on the availability of a\nsufficiently large and diverse pool of annotators to sample from.\n","authors":["Michiel van der Meer","Neele Falk","Pradeep K. Murukannaiah","Enrico Liscio"],"pdf_url":"https://arxiv.org/pdf/2404.15720v4.pdf","comment":"Accepted at EMNLP2024"},{"id":"http://arxiv.org/abs/2410.14979v2","updated":"2024-10-23T15:43:28Z","published":"2024-10-19T05:01:56Z","title":"Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration","summary":"  Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.\n","authors":["Wei Xie","Shuoyoucheng Ma","Zhenhua Wang","Enze Wang","Baosheng Wang","Jinshu Su"],"pdf_url":"https://arxiv.org/pdf/2410.14979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17973v1","updated":"2024-10-23T15:37:08Z","published":"2024-10-23T15:37:08Z","title":"Together We Can: Multilingual Automatic Post-Editing for Low-Resource\n  Languages","summary":"  This exploratory study investigates the potential of multilingual Automatic\nPost-Editing (APE) systems to enhance the quality of machine translations for\nlow-resource Indo-Aryan languages. Focusing on two closely related language\npairs, English-Marathi and English-Hindi, we exploit the linguistic\nsimilarities to develop a robust multilingual APE model. To facilitate\ncross-linguistic transfer, we generate synthetic Hindi-Marathi and\nMarathi-Hindi APE triplets. Additionally, we incorporate a Quality Estimation\n(QE)-APE multi-task learning framework. While the experimental results\nunderline the complementary nature of APE and QE, we also observe that QE-APE\nmultitask learning facilitates effective domain adaptation. Our experiments\ndemonstrate that the multilingual APE models outperform their corresponding\nEnglish-Hindi and English-Marathi single-pair models by $2.5$ and $2.39$ TER\npoints, respectively, with further notable improvements over the multilingual\nAPE model observed through multi-task learning ($+1.29$ and $+1.44$ TER\npoints), data augmentation ($+0.53$ and $+0.45$ TER points) and domain\nadaptation ($+0.35$ and $+0.45$ TER points). We release the synthetic data,\ncode, and models accrued during this study publicly at\nhttps://github.com/cfiltnlp/Multilingual-APE.\n","authors":["Sourabh Deoghare","Diptesh Kanojia","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2410.17973v1.pdf","comment":"Accepted at Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17972v1","updated":"2024-10-23T15:37:02Z","published":"2024-10-23T15:37:02Z","title":"Dependency Graph Parsing as Sequence Labeling","summary":"  Various linearizations have been proposed to cast syntactic dependency\nparsing as sequence labeling. However, these approaches do not support more\ncomplex graph-based representations, such as semantic dependencies or enhanced\nuniversal dependencies, as they cannot handle reentrancy or cycles. By\nextending them, we define a range of unbounded and bounded linearizations that\ncan be used to cast graph parsing as a tagging task, enlarging the toolbox of\nproblems that can be solved under this paradigm. Experimental results on\nsemantic dependency and enhanced UD parsing show that with a good choice of\nencoding, sequence-labeling dependency graph parsers combine high efficiency\nwith accuracies close to the state of the art, in spite of their simplicity.\n","authors":["Ana Ezquerro","David Vilares","Carlos Gómez-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2410.17972v1.pdf","comment":"Accepted at EMNLP-2024"},{"id":"http://arxiv.org/abs/2410.17963v1","updated":"2024-10-23T15:30:37Z","published":"2024-10-23T15:30:37Z","title":"A Time-Aware Approach to Early Detection of Anorexia: UNSL at eRisk 2024","summary":"  The eRisk laboratory aims to address issues related to early risk detection\non the Web. In this year's edition, three tasks were proposed, where Task 2 was\nabout early detection of signs of anorexia. Early risk detection is a problem\nwhere precision and speed are two crucial objectives. Our research group solved\nTask 2 by defining a CPI+DMC approach, addressing both objectives\nindependently, and a time-aware approach, where precision and speed are\nconsidered a combined single-objective. We implemented the last approach by\nexplicitly integrating time during the learning process, considering the\nERDE{\\theta} metric as the training objective. It also allowed us to\nincorporate temporal metrics to validate and select the optimal models. We\nachieved outstanding results for the ERDE50 metric and ranking-based metrics,\ndemonstrating consistency in solving ERD problems.\n","authors":["Horacio Thompson","Marcelo Errecalde"],"pdf_url":"https://arxiv.org/pdf/2410.17963v1.pdf","comment":"In Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble,\n  France"},{"id":"http://arxiv.org/abs/2410.17960v1","updated":"2024-10-23T15:28:53Z","published":"2024-10-23T15:28:53Z","title":"Zeitenwenden: Detecting changes in the German political discourse","summary":"  From a monarchy to a democracy, to a dictatorship and back to a democracy --\nthe German political landscape has been constantly changing ever since the\nfirst German national state was formed in 1871. After World War II, the Federal\nRepublic of Germany was formed in 1949. Since then every plenary session of the\nGerman Bundestag was logged and even has been digitized over the course of the\nlast few years. We analyze these texts using a time series variant of the topic\nmodel LDA to investigate which events had a lasting effect on the political\ndiscourse and how the political topics changed over time. This allows us to\ndetect changes in word frequency (and thus key discussion points) in political\ndiscourse.\n","authors":["Kai-Robin Lange","Jonas Rieger","Niklas Benner","Carsten Jentsch"],"pdf_url":"https://arxiv.org/pdf/2410.17960v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2201.12091v5","updated":"2024-10-23T15:28:38Z","published":"2022-01-28T13:00:17Z","title":"Linear Adversarial Concept Erasure","summary":"  Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem. We formulate the problem of identifying and erasing a linear subspace\nthat corresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear maximin\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, \\method, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod is highly expressive, effectively mitigating bias in deep nonlinear\nclassifiers while maintaining tractability and interpretability.\n","authors":["Shauli Ravfogel","Michael Twiton","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12091v5.pdf","comment":"Accepted in ICML 2022; a revised version"},{"id":"http://arxiv.org/abs/2410.17954v1","updated":"2024-10-23T15:24:54Z","published":"2024-10-23T15:24:54Z","title":"ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference","summary":"  Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.\n","authors":["Xin He","Shunkang Zhang","Yuxin Wang","Haiyan Yin","Zihao Zeng","Shaohuai Shi","Zhenheng Tang","Xiaowen Chu","Ivor Tsang","Ong Yew Soon"],"pdf_url":"https://arxiv.org/pdf/2410.17954v1.pdf","comment":"Mixture-of-Experts, Inference, Offloading"},{"id":"http://arxiv.org/abs/2410.17952v1","updated":"2024-10-23T15:24:16Z","published":"2024-10-23T15:24:16Z","title":"SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains","summary":"  Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.\n","authors":["Ran Xu","Hui Liu","Sreyashi Nag","Zhenwei Dai","Yaochen Xie","Xianfeng Tang","Chen Luo","Yang Li","Joyce C. Ho","Carl Yang","Qi He"],"pdf_url":"https://arxiv.org/pdf/2410.17952v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2406.12295v2","updated":"2024-10-23T15:23:00Z","published":"2024-06-18T05:59:28Z","title":"Fast and Slow Generating: An Empirical Study on Large and Small Language\n  Models Collaborative Decoding","summary":"  Large Language Models (LLMs) exhibit impressive capabilities across various\napplications but encounter substantial challenges such as high inference\nlatency, considerable training costs, and the generation of hallucinations.\nCollaborative decoding between large and small language models (SLMs) presents\na promising strategy to mitigate these issues through methods including\nspeculative decoding, contrastive decoding, and emulator or proxy fine-tuning.\nHowever, the specifics of such collaborations, particularly from a unified\nperspective, remain largely unexplored. Inspired by dual-process cognitive\ntheory, we propose a unified framework in this paper, termed Fast and Slow\nGenerating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs)\nare categorized as System 2 (slow and deliberate), while independent SLMs are\ndesignated as System 1 (fast and intuitive). We provide a comprehensive\nanalysis of these collaborative methodologies, elucidating their common\nproperties and shedding light on the differential knowledge capabilities of\nSystem 2 versus System 1 through the FS-GEN framework. Our findings indicate\nthat only a small proportion of collaborative interactions (approximately less\nthan 20\\% in most instances) are necessary across various methods. These\ninteractions between System 1 and System 2 conform to a scaling law related to\nthe parameter ratios, enabling predictable collaboration. Furthermore, we\nexplore the specific conditions under which collaboration proves most\neffective, particularly from an uncertainty perspective, offering novel\ninsights that may guide future optimization efforts. Our research underscores\nthat the fundamental distinction between System 1 and System 2 lies in the\nuncertainty of next token predictions, where interventions by System 2 are\ncrucial to support System 1. Code for Reproduction:\nhttps://github.com/TsinghuaC3I/FS-GEN\n","authors":["Kaiyan Zhang","Jianyu Wang","Ning Ding","Biqing Qi","Ermo Hua","Xingtai Lv","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.12295v2.pdf","comment":"update figures and results on Pythia Series"},{"id":"http://arxiv.org/abs/2410.12018v2","updated":"2024-10-23T15:21:54Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v2.pdf","comment":"ACCV 2024 Oral"},{"id":"http://arxiv.org/abs/2402.04957v3","updated":"2024-10-23T15:08:57Z","published":"2024-02-07T15:40:22Z","title":"Reconfidencing LLMs from the Grouping Loss Perspective","summary":"  Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to\ngenerating hallucinated answers in a confident tone. While efforts to elicit\nand calibrate confidence scores have proven useful, recent findings show that\ncontrolling uncertainty must go beyond calibration: predicted scores may\ndeviate significantly from the actual posterior probabilities due to the impact\nof grouping loss. In this work, we construct a new evaluation dataset derived\nfrom a knowledge base to assess confidence scores given to answers of Mistral\nand LLaMA. Experiments show that they tend to be overconfident. Further, we\nshow that they are more overconfident on some answers than others, \\emph{eg}\ndepending on the nationality of the person in the query. In\nuncertainty-quantification theory, this is grouping loss. To address this, we\npropose a solution to reconfidence LLMs, canceling not only calibration but\nalso grouping loss. The LLMs, after the reconfidencing process, indicate\nimproved confidence alignment with the accuracy of their responses.\n","authors":["Lihu Chen","Alexandre Perez-Lebel","Fabian M. Suchanek","Gaël Varoquaux"],"pdf_url":"https://arxiv.org/pdf/2402.04957v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2402.01622v4","updated":"2024-10-23T15:02:57Z","published":"2024-02-02T18:39:51Z","title":"TravelPlanner: A Benchmark for Real-World Planning with Language Agents","summary":"  Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.\n","authors":["Jian Xie","Kai Zhang","Jiangjie Chen","Tinghui Zhu","Renze Lou","Yuandong Tian","Yanghua Xiao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2402.01622v4.pdf","comment":"ICML 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2311.05876v3","updated":"2024-10-23T14:48:20Z","published":"2023-11-10T05:24:04Z","title":"Trends in Integration of Knowledge and Large Language Models: A Survey\n  and Taxonomy of Methods, Benchmarks, and Applications","summary":"  Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors.\n","authors":["Zhangyin Feng","Weitao Ma","Weijiang Yu","Lei Huang","Haotian Wang","Qianglong Chen","Weihua Peng","Xiaocheng Feng","Bing Qin","Ting liu"],"pdf_url":"https://arxiv.org/pdf/2311.05876v3.pdf","comment":"Work in progress; 22 pages. This work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2408.01119v2","updated":"2024-10-23T14:37:50Z","published":"2024-08-02T09:00:03Z","title":"Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer","summary":"  Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks.\n","authors":["Robert Belanec","Simon Ostermann","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2408.01119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00279v3","updated":"2024-10-23T14:33:44Z","published":"2023-07-01T09:18:24Z","title":"Let Me Teach You: Pedagogical Foundations of Feedback for Language\n  Models","summary":"  Natural Language Feedback (NLF) is an increasingly popular mechanism for\naligning Large Language Models (LLMs) to human preferences. Despite the\ndiversity of the information it can convey, NLF methods are often hand-designed\nand arbitrary, with little systematic grounding. At the same time, research in\nlearning sciences has long established several effective feedback models. In\nthis opinion piece, we compile ideas from pedagogy to introduce FELT, a\nfeedback framework for LLMs that outlines various characteristics of the\nfeedback space, and a feedback content taxonomy based on these variables,\nproviding a general mapping of the feedback space. In addition to streamlining\nNLF designs, FELT also brings out new, unexplored directions for research in\nNLF. We make our taxonomy available to the community, providing guides and\nexamples for mapping our categorizations to future research.\n","authors":["Beatriz Borges","Niket Tandon","Tanja Käser","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2307.00279v3.pdf","comment":"EMNLP 2024; 9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.17901v1","updated":"2024-10-23T14:18:25Z","published":"2024-10-23T14:18:25Z","title":"ELAICHI: Enhancing Low-resource TTS by Addressing Infrequent and\n  Low-frequency Character Bigrams","summary":"  Recent advancements in Text-to-Speech (TTS) technology have led to\nnatural-sounding speech for English, primarily due to the availability of\nlarge-scale, high-quality web data. However, many other languages lack access\nto such resources, relying instead on limited studio-quality data. This\nscarcity results in synthesized speech that often suffers from intelligibility\nissues, particularly with low-frequency character bigrams. In this paper, we\npropose three solutions to address this challenge. First, we leverage\nhigh-quality data from linguistically or geographically related languages to\nimprove TTS for the target language. Second, we utilize low-quality Automatic\nSpeech Recognition (ASR) data recorded in non-studio environments, which is\nrefined using denoising and speech enhancement models. Third, we apply\nknowledge distillation from large-scale models using synthetic data to generate\nmore robust outputs. Our experiments with Hindi demonstrate significant\nreductions in intelligibility issues, as validated by human evaluators. We\npropose this methodology as a viable alternative for languages with limited\naccess to high-quality data, enabling them to collectively benefit from shared\nresources.\n","authors":["Srija Anand","Praveen Srinivasa Varadhan","Mehak Singal","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2410.17901v1.pdf","comment":"11 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2410.17897v1","updated":"2024-10-23T14:15:07Z","published":"2024-10-23T14:15:07Z","title":"Value Residual Learning For Alleviating Attention Concentration In\n  Transformers","summary":"  Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.\n","authors":["Zhanchao Zhou","Tianyi Wu","Zhiyun Jiang","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2410.17897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15592v2","updated":"2024-10-23T14:08:10Z","published":"2024-10-21T02:21:56Z","title":"CPE-Pro: A Structure-Sensitive Deep Learning Method for Protein\n  Representation and Origin Evaluation","summary":"  Protein structures are important for understanding their functions and\ninteractions. Currently, many protein structure prediction methods are\nenriching the structure database. Discriminating the origin of structures is\ncrucial for distinguishing between experimentally resolved and computationally\npredicted structures, evaluating the reliability of prediction methods, and\nguiding downstream biological studies. Building on works in structure\nprediction, We developed a structure-sensitive supervised deep learning model,\nCrystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent\nand discriminate the origin of protein structures. CPE-Pro learns the\nstructural information of proteins and captures inter-structural differences to\nachieve accurate traceability on four data classes, and is expected to be\nextended to more. Simultaneously, we utilized Foldseek to encode protein\nstructures into \"structure-sequences\" and trained a protein Structural Sequence\nLanguage Model, SSLM. Preliminary experiments demonstrated that, compared to\nlarge-scale protein language models pre-trained on vast amounts of amino acid\nsequences, the \"structure-sequence\" enables the language model to learn more\ninformative protein features, enhancing and optimizing structural\nrepresentations. We have provided the code, model weights, and all related\nmaterials on https://github.com/GouWenrui/CPE-Pro-main.git.\n","authors":["Wenrui Gou","Wenhui Ge","Yang Tan","Mingchen Li","Guisheng Fan","Huiqun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.15592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17891v1","updated":"2024-10-23T14:04:22Z","published":"2024-10-23T14:04:22Z","title":"Scaling Diffusion Language Models via Adaptation from Autoregressive\n  Models","summary":"  Diffusion Language Models (DLMs) have emerged as a promising new paradigm for\ntext generative modeling, potentially addressing limitations of autoregressive\n(AR) models. However, current DLMs have been studied at a smaller scale\ncompared to their AR counterparts and lack fair comparison on language modeling\nbenchmarks. Additionally, training diffusion models from scratch at scale\nremains challenging. Given the prevalence of open-source AR language models, we\npropose adapting these models to build text diffusion models. We demonstrate\nconnections between AR and diffusion modeling objectives and introduce a simple\ncontinual pre-training approach for training diffusion models. Through\nsystematic evaluation on language modeling, reasoning, and commonsense\nbenchmarks, we show that we can convert AR models ranging from 127M to 7B\nparameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA,\nusing less than 200B tokens for training. Our experimental results reveal that\nthese models outperform earlier DLMs and are competitive with their AR\ncounterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters)\ncapable of generating fluent text, performing in-context learning, filling in\nthe middle without prompt re-ordering, and following instructions\n\\url{https://github.com/HKUNLP/DiffuLLaMA}.\n","authors":["Shansan Gong","Shivam Agarwal","Yizhe Zhang","Jiacheng Ye","Lin Zheng","Mukai Li","Chenxin An","Peilin Zhao","Wei Bi","Jiawei Han","Hao Peng","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.17891v1.pdf","comment":"25 pages. Code: https://github.com/HKUNLP/DiffuLLaMA"},{"id":"http://arxiv.org/abs/2406.14703v2","updated":"2024-10-23T14:01:14Z","published":"2024-06-20T19:50:56Z","title":"Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics","summary":"  Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.\n","authors":["Seungbeen Lee","Seungwon Lim","Seungju Han","Giyeong Oh","Hyungjoo Chae","Jiwan Chung","Minju Kim","Beong-woo Kwak","Yeonsoo Lee","Dongha Lee","Jinyoung Yeo","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.14703v2.pdf","comment":"Preprint; Under review"},{"id":"http://arxiv.org/abs/2410.17886v1","updated":"2024-10-23T14:00:48Z","published":"2024-10-23T14:00:48Z","title":"SpeakGer: A meta-data enriched speech corpus of German state and federal\n  parliaments","summary":"  The application of natural language processing on political texts as well as\nspeeches has become increasingly relevant in political sciences due to the\nability to analyze large text corpora which cannot be read by a single person.\nBut such text corpora often lack critical meta information, detailing for\ninstance the party, age or constituency of the speaker, that can be used to\nprovide an analysis tailored to more fine-grained research questions. To enable\nresearchers to answer such questions with quantitative approaches such as\nnatural language processing, we provide the SpeakGer data set, consisting of\nGerman parliament debates from all 16 federal states of Germany as well as the\nGerman Bundestag from 1947-2023, split into a total of 10,806,105 speeches.\nThis data set includes rich meta data in form of information on both reactions\nfrom the audience towards the speech as well as information about the speaker's\nparty, their age, their constituency and their party's political alignment,\nwhich enables a deeper analysis. We further provide three exploratory analyses,\ndetailing topic shares of different parties throughout time, a descriptive\nanalysis of the development of the age of an average speaker as well as a\nsentiment analysis of speeches of different parties with regards to the\nCOVID-19 pandemic.\n","authors":["Kai-Robin Lange","Carsten Jentsch"],"pdf_url":"https://arxiv.org/pdf/2410.17886v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.07799v2","updated":"2024-10-23T14:00:40Z","published":"2024-07-10T16:16:02Z","title":"Attribute or Abstain: Large Language Models as Long Document Assistants","summary":"  LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiments with different approaches to attribution on 5 LLMs\nof different sizes.\n  We find that citation, i.e. response generation and evidence extraction in\none step, performs best for large and fine-tuned models, while additional\nretrieval can help for small, prompted models. We investigate whether the \"Lost\nin the Middle'' phenomenon exists for attribution, but do not find this. We\nalso find that evidence quality can predict response quality on datasets with\nsimple responses, but not so for complex responses, as models struggle with\nproviding evidence for complex claims.\n","authors":["Jan Buchmann","Xiao Liu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.07799v2.pdf","comment":"Accepted at EMNLP 2024. Code and data:\n  https://github.com/UKPLab/arxiv2024-attribute-or-abstain"},{"id":"http://arxiv.org/abs/2407.12819v2","updated":"2024-10-23T14:00:36Z","published":"2024-07-01T10:33:46Z","title":"I've Got 99 Problems But FLOPS Ain't One","summary":"  Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.\n","authors":["Alexandru M. Gherghescu","Vlad-Andrei Bădoiu","Alexandru Agache","Mihai-Valentin Dumitru","Iuliu Vasilescu","Radu Mantu","Costin Raiciu"],"pdf_url":"https://arxiv.org/pdf/2407.12819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17875v1","updated":"2024-10-23T13:47:05Z","published":"2024-10-23T13:47:05Z","title":"Understanding Layer Significance in LLM Alignment","summary":"  Aligning large language models (LLMs) through fine-tuning is essential for\ntailoring them to specific applications. Therefore, understanding what LLMs\nlearn during the alignment process is crucial. Recent studies suggest that\nalignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To delve deeper into LLM alignment, we propose to\nidentify which layers within LLMs are most critical to the alignment process,\nthereby uncovering how alignment influences model behavior at a granular level.\nWe propose a novel approach to identify the important layers for LLM alignment\n(ILA). It involves learning a binary mask for each incremental weight matrix in\nthe LoRA algorithm, indicating the significance of each layer. ILA consistently\nidentifies important layers across various alignment datasets, with nearly 90%\noverlap even with substantial dataset differences, highlighting fundamental\npatterns in LLM alignment. Experimental results indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss.\n","authors":["Guangyuan Shi","Zexin Lu","Xiaoyu Dong","Wenlong Zhang","Xuanyu Zhang","Yujie Feng","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15055v2","updated":"2024-10-23T13:20:15Z","published":"2024-02-23T02:15:47Z","title":"Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions","summary":"  Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.\n","authors":["Clement Neo","Shay B. Cohen","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.15055v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.14774v2","updated":"2024-10-23T13:01:14Z","published":"2024-03-21T18:28:43Z","title":"Few-Shot Adversarial Prompt Learning on Vision-Language Models","summary":"  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data. Code is available at:\nhttps://github.com/lionel-w2/FAP.\n","authors":["Yiwei Zhou","Xiaobo Xia","Zhiwei Lin","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14774v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15956v2","updated":"2024-10-23T13:00:27Z","published":"2024-10-21T12:34:17Z","title":"Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs","summary":"  Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.\n","authors":["Yanzhu Guo","Simone Conia","Zelin Zhou","Min Li","Saloni Potdar","Henry Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.15956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16845v2","updated":"2024-10-23T12:53:00Z","published":"2024-06-24T17:49:28Z","title":"RaTEScore: A Metric for Radiology Report Generation","summary":"  This paper introduces a novel, entity-aware metric, termed as Radiological\nReport (Text) Evaluation (RaTEScore), to assess the quality of medical reports\ngenerated by AI models. RaTEScore emphasizes crucial medical entities such as\ndiagnostic outcomes and anatomical details, and is robust against complex\nmedical synonyms and sensitive to negation expressions. Technically, we\ndeveloped a comprehensive medical NER dataset, RaTE-NER, and trained an NER\nmodel specifically for this purpose. This model enables the decomposition of\ncomplex radiological reports into constituent medical entities. The metric\nitself is derived by comparing the similarity of entity embeddings, obtained\nfrom a language model, based on their types and relevance to clinical\nsignificance. Our evaluations demonstrate that RaTEScore aligns more closely\nwith human preference than existing metrics, validated both on established\npublic benchmarks and our newly proposed RaTE-Eval benchmark.\n","authors":["Weike Zhao","Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2406.16845v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06022v2","updated":"2024-10-23T12:49:32Z","published":"2024-10-08T13:23:58Z","title":"Can Language Models Induce Grammatical Knowledge from Indirect Evidence?","summary":"  What kinds of and how much data is necessary for language models to induce\ngrammatical knowledge to judge sentence acceptability? Recent language models\nstill have much room for improvement in their data efficiency compared to\nhumans. This paper investigates whether language models efficiently use\nindirect data (indirect evidence), from which they infer sentence\nacceptability. In contrast, humans use indirect evidence efficiently, which is\nconsidered one of the inductive biases contributing to efficient language\nacquisition. To explore this question, we introduce the Wug InDirect Evidence\nTest (WIDET), a dataset consisting of training instances inserted into the\npre-training data and evaluation instances. We inject synthetic instances with\nnewly coined wug words into pretraining data and explore the model's behavior\non evaluation data that assesses grammatical acceptability regarding those\nwords. We prepare the injected instances by varying their levels of\nindirectness and quantity. Our experiments surprisingly show that language\nmodels do not induce grammatical knowledge even after repeated exposure to\ninstances with the same structure but differing only in lexical items from\nevaluation instances in certain language phenomena. Our findings suggest a\npotential direction for future research: developing models that use latent\nindirect evidence to induce grammatical knowledge.\n","authors":["Miyu Oba","Yohei Oseki","Akiyo Fukatsu","Akari Haga","Hiroki Ouchi","Taro Watanabe","Saku Sugawara"],"pdf_url":"https://arxiv.org/pdf/2410.06022v2.pdf","comment":"This paper is accepted at EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.17820v1","updated":"2024-10-23T12:26:10Z","published":"2024-10-23T12:26:10Z","title":"Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination","summary":"  Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. While using even a smaller model as the discriminator, scaling\nthe generator leads to notable improvements in ToT performance, whereas scaling\nthe discriminator with a fixed generator yields only marginal gains. Our\nresults show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT.\n","authors":["Qiqi Chen","Xinpeng Wang","Philipp Mondorf","Michael A. Hedderich","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.17820v1.pdf","comment":"Code: github.com/mainlp/tot-eval"},{"id":"http://arxiv.org/abs/2410.17799v1","updated":"2024-10-23T11:58:58Z","published":"2024-10-23T11:58:58Z","title":"OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation","summary":"  Full-duplex spoken dialogue systems significantly advance over traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex communication\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text-based large language model (LLM) backbone into a speech-text\ndialogue LLM, capable of generating text and speech in real time, without\nmodifying the architecture of the backbone LLM. The training process comprises\nthree stages: modality alignment, half-duplex dialogue learning, and\nfull-duplex dialogue learning. Throughout all training stages, we standardize\nthe data using a flattening operation, which allows us to unify the training\nmethods and the model architecture across different modalities and tasks. Our\napproach offers a straightforward modeling technique and a promising research\ndirection for developing efficient and natural end-to-end full-duplex spoken\ndialogue systems. Audio samples of dialogues generated by OmniFlatten can be\nfound at this web site (https://omniflatten.github.io/).\n","authors":["Qinglin Zhang","Luyao Cheng","Chong Deng","Qian Chen","Wen Wang","Siqi Zheng","Jiaqing Liu","Hai Yu","Chaohong Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17799v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.05804v4","updated":"2024-10-23T11:36:57Z","published":"2024-06-09T14:42:55Z","title":"A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning","summary":"  Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work.\n","authors":["Xinzhe Li"],"pdf_url":"https://arxiv.org/pdf/2406.05804v4.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.17783v1","updated":"2024-10-23T11:32:46Z","published":"2024-10-23T11:32:46Z","title":"Leveraging the Domain Adaptation of Retrieval Augmented Generation\n  Models for Question Answering and Reducing Hallucination","summary":"  While ongoing advancements in Large Language Models have demonstrated\nremarkable success across various NLP tasks, Retrieval Augmented Generation\nModel stands out to be highly effective on downstream applications like\nQuestion Answering. Recently, RAG-end2end model further optimized the\narchitecture and achieved notable performance improvements on domain\nadaptation. However, the effectiveness of these RAG-based architectures remains\nrelatively unexplored when fine-tuned on specialized domains such as customer\nservice for building a reliable conversational AI system. Furthermore, a\ncritical challenge persists in reducing the occurrence of hallucinations while\nmaintaining high domain-specific accuracy. In this paper, we investigated the\nperformance of diverse RAG and RAG-like architectures through domain adaptation\nand evaluated their ability to generate accurate and relevant response grounded\nin the contextual knowledge base. To facilitate the evaluation of the models,\nwe constructed a novel dataset HotelConvQA, sourced from wide range of\nhotel-related conversations and fine-tuned all the models on our domain\nspecific dataset. We also addressed a critical research gap on determining the\nimpact of domain adaptation on reducing hallucinations across different RAG\narchitectures, an aspect that was not properly measured in prior work. Our\nevaluation shows positive results in all metrics by employing domain\nadaptation, demonstrating strong performance on QA tasks and providing insights\ninto their efficacy in reducing hallucinations. Our findings clearly indicate\nthat domain adaptation not only enhances the models' performance on QA tasks\nbut also significantly reduces hallucination across all evaluated RAG\narchitectures.\n","authors":["Salman Rakin","Md. A. R. Shibly","Zahin M. Hossain","Zeeshan Khan","Md. Mostofa Akbar"],"pdf_url":"https://arxiv.org/pdf/2410.17783v1.pdf","comment":"Initial Version fine-tuned on HotelConvQA"},{"id":"http://arxiv.org/abs/2404.19232v7","updated":"2024-10-23T11:19:02Z","published":"2024-04-30T03:29:30Z","title":"GRAMMAR: Grounded and Modular Methodology for Assessment of\n  Closed-Domain Retrieval-Augmented Language Model","summary":"  Retrieval-Augmented Generation (RAG) systems are widely used across various\nindustries for querying closed-domain and in-house knowledge bases. However,\nevaluating these systems presents significant challenges due to the private\nnature of closed-domain data and a scarcity of queries with verifiable ground\ntruths. Moreover, there is a lack of analytical methods to diagnose problematic\nmodules and identify types of failure, such as those caused by knowledge\ndeficits or issues with robustness. To address these challenges, we introduce\nGRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation\nframework comprising a grounded data generation process and an evaluation\nprotocol that effectively pinpoints defective modules. Our validation\nexperiments reveal that GRAMMAR provides a reliable approach for identifying\nvulnerable modules and supports hypothesis testing for textual form\nvulnerabilities. An open-source tool accompanying this framework is available\nin our GitHub repository (see https://github.com/xinzhel/grammar), allowing for\neasy reproduction of our results and enabling reliable and modular evaluation\nin closed-domain settings.\n","authors":["Xinzhe Li","Ming Liu","Shang Gao"],"pdf_url":"https://arxiv.org/pdf/2404.19232v7.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.17759v1","updated":"2024-10-23T10:50:40Z","published":"2024-10-23T10:50:40Z","title":"Latent Structures of Intertextuality in French Fiction","summary":"  Intertextuality is a key concept in literary theory that challenges\ntraditional notions of text, signification or authorship. It views texts as\npart of a vast intertextual network that is constantly evolving and being\nreconfigured. This paper argues that the field of computational literary\nstudies is the ideal place to conduct a study of intertextuality since we have\nnow the ability to systematically compare texts with each others. Specifically,\nwe present a work on a corpus of more than 12.000 French fictions from the\n18th, 19th and early 20th century. We focus on evaluating the underlying roles\nof two literary notions, sub-genres and the literary canon in the framing of\ntextuality. The article attempts to operationalize intertextuality using\nstate-of-the-art contextual language models to encode novels and capture\nfeatures that go beyond simple lexical or thematic approaches. Previous\nresearch (Hughes, 2012) supports the existence of a literary \"style of a time\",\nand our findings further reinforce this concept. Our findings also suggest that\nboth subgenres and canonicity play a significant role in shaping textual\nsimilarities within French fiction. These discoveries point to the importance\nof considering genre and canon as dynamic forces that influence the evolution\nand intertextual connections of literary works within specific historical\ncontexts.\n","authors":["Jean Barré"],"pdf_url":"https://arxiv.org/pdf/2410.17759v1.pdf","comment":"13 pages, 6 figures. Computational Humanities Research Conference\n  2024"},{"id":"http://arxiv.org/abs/2410.17739v1","updated":"2024-10-23T10:12:35Z","published":"2024-10-23T10:12:35Z","title":"Local Contrastive Editing of Gender Stereotypes","summary":"  Stereotypical bias encoded in language models (LMs) poses a threat to safe\nlanguage technology, yet our understanding of how bias manifests in the\nparameters of LMs remains incomplete. We introduce local contrastive editing\nthat enables the localization and editing of a subset of weights in a target\nmodel in relation to a reference model. We deploy this approach to identify and\nmodify subsets of weights that are associated with gender stereotypes in LMs.\nThrough a series of experiments, we demonstrate that local contrastive editing\ncan precisely localize and control a small subset (< 0.5%) of weights that\nencode gender bias. Our work (i) advances our understanding of how\nstereotypical biases can manifest in the parameter space of LMs and (ii) opens\nup new avenues for developing parameter-efficient strategies for controlling\nmodel properties in a contrastive manner.\n","authors":["Marlene Lutz","Rochelle Choenni","Markus Strohmaier","Anne Lauscher"],"pdf_url":"https://arxiv.org/pdf/2410.17739v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17736v1","updated":"2024-10-23T10:11:40Z","published":"2024-10-23T10:11:40Z","title":"MojoBench: Language Modeling and Benchmarks for Mojo","summary":"  The recently introduced Mojo programming language (PL) by Modular, has\nreceived significant attention in the scientific community due to its claimed\nsignificant speed boost over Python. Despite advancements in code Large\nLanguage Models (LLMs) across various PLs, Mojo remains unexplored in this\ncontext. To address this gap, we introduce MojoBench, the first framework for\nMojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset\ndesigned for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM\npretrained and finetuned for Mojo code generation, which supports instructions\nin 5 natural languages (NLs). Our results show that Mojo-Coder achieves a\n30-35% performance improvement over leading models like GPT-4o and\nClaude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with\nunderrepresented and unseen PLs, offering potential strategies for enhancing\nmodel adaptability. MojoBench contributes to our understanding of LLM\ncapabilities and limitations in emerging programming paradigms fostering more\nrobust code generation systems.\n","authors":["Nishat Raihan","Joanna C. S. Santos","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2410.17736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14622v2","updated":"2024-10-23T10:06:10Z","published":"2024-02-22T15:10:45Z","title":"From Keywords to Structured Summaries: Streamlining Scholarly\n  Information Access","summary":"  This paper highlights the growing importance of information retrieval (IR)\nengines in the scientific community, addressing the inefficiency of traditional\nkeyword-based search engines due to the rising volume of publications. The\nproposed solution involves structured records, underpinning advanced\ninformation technology (IT) tools, including visualization dashboards, to\nrevolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the \"reproductive number estimate of infectious diseases\"\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation information access system as\nan IR method accessible at https://orkg.org/usecases/r0-estimates.\n","authors":["Mahsa Shamsabadi","Jennifer D'Souza"],"pdf_url":"https://arxiv.org/pdf/2402.14622v2.pdf","comment":"8 pages, 3 figures | Accepted for publication as a poster paper at\n  the International Semantic Web Conference (ISWC 2024)"},{"id":"http://arxiv.org/abs/2410.17728v1","updated":"2024-10-23T10:00:23Z","published":"2024-10-23T10:00:23Z","title":"Dialectal and Low Resource Machine Translation for Aromanian","summary":"  We present a neural machine translation system that can translate between\nRomanian, English, and Aromanian (an endangered Eastern Romance language); the\nfirst of its kind. BLEU scores range from 17 to 32 depending on the direction\nand genre of the text. Alongside, we release the biggest known\nAromanian-Romanian bilingual corpus, consisting of 79k cleaned sentence pairs.\nAdditional tools such as an agnostic sentence embedder (used for both text\nmining and automatic evaluation) and a diacritics converter are also presented.\nWe publicly release our findings and models. Finally, we describe the\ndeployment of our quantized model at https://arotranslate.com.\n","authors":["Alexandru-Iulius Jerpelea","Alina-Ştefania Rădoi","Sergiu Nisioi"],"pdf_url":"https://arxiv.org/pdf/2410.17728v1.pdf","comment":"16 pages, 3 figures, 6 tables, submitted to COLING 2025"},{"id":"http://arxiv.org/abs/2406.14282v3","updated":"2024-10-23T09:42:59Z","published":"2024-06-20T13:07:38Z","title":"Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs","summary":"  Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.\n","authors":["Junjie Wang","Mingyang Chen","Binbin Hu","Dan Yang","Ziqi Liu","Yue Shen","Peng Wei","Zhiqiang Zhang","Jinjie Gu","Jun Zhou","Jeff Z. Pan","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14282v3.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.17714v1","updated":"2024-10-23T09:40:15Z","published":"2024-10-23T09:40:15Z","title":"CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient\n  Semantic Steering in Large Language Models","summary":"  Despite their impressive capabilities, large language models (LLMs) often\nlack interpretability and can generate toxic content. While using LLMs as\nfoundation models and applying semantic steering methods are widely practiced,\nwe believe that efficient methods should be based on a thorough understanding\nof LLM behavior. To this end, we propose using eye movement measures to\ninterpret LLM behavior across layers. We find that LLMs exhibit patterns\nsimilar to human gaze across layers and different layers function differently.\nInspired by these findings, we introduce a heuristic steering layer selection\nand apply it to layer intervention methods via fine-tuning and inference. Using\nlanguage toxification and detoxification as test beds, we demonstrate that our\nproposed CogSteer methods achieve better results in terms of toxicity scores\nwhile efficiently saving 97% of the computational resources and 60% of the\ntraining time. Our model-agnostic approach can be adopted into various LLMs,\ncontributing to their interpretability and promoting trustworthiness for safe\ndeployment.\n","authors":["Xintong Wang","Jingheng Pan","Longqin Jiang","Liang Ding","Xingshan Li","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2410.17714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17711v1","updated":"2024-10-23T09:36:21Z","published":"2024-10-23T09:36:21Z","title":"Beware of Calibration Data for Pruning Large Language Models","summary":"  As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL).\n","authors":["Yixin Ji","Yang Xiang","Juntao Li","Qingrong Xia","Ping Li","Xinyu Duan","Zhefeng Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17711v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.17694v1","updated":"2024-10-23T09:14:57Z","published":"2024-10-23T09:14:57Z","title":"An Adaptive Framework for Generating Systematic Explanatory Answer in\n  Online Q&A Platforms","summary":"  Question Answering (QA) systems face challenges in handling complex questions\nthat require multi-domain knowledge synthesis. The naive RAG models, although\neffective in information retrieval, struggle with complex questions that\nrequire comprehensive and in-depth answers. The pioneering task is defined as\nexplanatory answer generation, which entails handling identified challenges\nsuch as the requirement for comprehensive information and logical coherence\nwithin the generated context. To address these issues, we refer to systematic\nthinking theory and propose SynthRAG, an innovative framework designed to\nenhance QA performance. SynthRAG improves on conventional models by employing\nadaptive outlines for dynamic content structuring, generating systematic\ninformation to ensure detailed coverage, and producing customized answers\ntailored to specific user inquiries. This structured approach guarantees\nlogical coherence and thorough integration of information, yielding responses\nthat are both insightful and methodically organized. Empirical evaluations\nunderscore SynthRAG's effectiveness, demonstrating its superiority in handling\ncomplex questions, overcoming the limitations of naive RAG models, and\nsignificantly improving answer quality and depth. Furthermore, an online\ndeployment on the Zhihu platform revealed that SynthRAG's answers achieved\nnotable user engagement, with each response averaging 5.73 upvotes and\nsurpassing the performance of 79.8% of human contributors, highlighting the\npractical relevance and impact of the proposed framework. Our code is available\nat https://github.com/czy1999/SynthRAG .\n","authors":["Ziyang Chen","Xiaobin Wang","Yong Jiang","Jinzhi Liao","Pengjun Xie","Fei Huang","Xiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.17694v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17676v1","updated":"2024-10-23T08:49:51Z","published":"2024-10-23T08:49:51Z","title":"Towards a Similarity-adjusted Surprisal Theory","summary":"  Surprisal theory posits that the cognitive effort required to comprehend a\nword is determined by its contextual predictability, quantified as surprisal.\nTraditionally, surprisal theory treats words as distinct entities, overlooking\nany potential similarity between them. Giulianelli et al. (2023) address this\nlimitation by introducing information value, a measure of predictability\ndesigned to account for similarities between communicative units. Our work\nleverages Ricotta and Szeidl's (2006) diversity index to extend surprisal into\na metric that we term similarity-adjusted surprisal, exposing a mathematical\nrelationship between surprisal and information value. Similarity-adjusted\nsurprisal aligns with information value when considering graded similarities\nand reduces to standard surprisal when words are treated as distinct.\nExperimental results with reading time data indicate that similarity-adjusted\nsurprisal adds predictive power beyond standard surprisal for certain datasets,\nsuggesting it serves as a complementary measure of comprehension effort.\n","authors":["Clara Meister","Mario Giulianelli","Tiago Pimentel"],"pdf_url":"https://arxiv.org/pdf/2410.17676v1.pdf","comment":"EMNLP 2024 main conference proceedings"},{"id":"http://arxiv.org/abs/2410.17670v1","updated":"2024-10-23T08:42:36Z","published":"2024-10-23T08:42:36Z","title":"Quantifying the Risks of Tool-assisted Rephrasing to Linguistic\n  Diversity","summary":"  Writing assistants and large language models see widespread use in the\ncreation of text content. While their effectiveness for individual users has\nbeen evaluated in the literature, little is known about their proclivity to\nchange language or reduce its richness when adopted by a large user base. In\nthis paper, we take a first step towards quantifying this risk by measuring the\nsemantic and vocabulary change enacted by the use of rephrasing tools on a\nmulti-domain corpus of human-generated text.\n","authors":["Mengying Wang","Andreas Spitz"],"pdf_url":"https://arxiv.org/pdf/2410.17670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15993v4","updated":"2024-10-23T08:33:54Z","published":"2024-04-24T17:10:35Z","title":"Uncertainty Estimation and Quantification for LLMs: A Simple Supervised\n  Approach","summary":"  In this paper, we study the problem of uncertainty estimation and calibration\nfor LLMs. We begin by formulating the uncertainty estimation problem, a\nrelevant yet underexplored area in existing literature. We then propose a\nsupervised approach that leverages labeled datasets to estimate the uncertainty\nin LLMs' responses. Based on the formulation, we illustrate the difference\nbetween the uncertainty estimation for LLMs and that for standard ML models and\nexplain why the hidden neurons of the LLMs may contain uncertainty information.\nOur designed approach demonstrates the benefits of utilizing hidden activations\nto enhance uncertainty estimation across various tasks and shows robust\ntransferability in out-of-distribution settings. We distinguish the uncertainty\nestimation task from the uncertainty calibration task and show that better\nuncertainty estimation leads to better calibration performance. Furthermore,\nour method is easy to implement and adaptable to different levels of model\naccessibility including black box, grey box, and white box.\n","authors":["Linyu Liu","Yu Pan","Xiaocheng Li","Guanting Chen"],"pdf_url":"https://arxiv.org/pdf/2404.15993v4.pdf","comment":"29 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.10216v2","updated":"2024-10-23T08:22:44Z","published":"2024-06-14T17:49:59Z","title":"Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs","summary":"  Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm.\n","authors":["Rui Yang","Ruomeng Ding","Yong Lin","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10216v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17657v1","updated":"2024-10-23T08:19:18Z","published":"2024-10-23T08:19:18Z","title":"ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents","summary":"  Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks.\n","authors":["Yusheng Liao","Shuyang Jiang","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17657v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2410.17635v1","updated":"2024-10-23T07:53:29Z","published":"2024-10-23T07:53:29Z","title":"Markov Chain of Thought for Efficient Mathematical Reasoning","summary":"  Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.\n","authors":["Wen Yang","Kai Fan","Minpeng Liao"],"pdf_url":"https://arxiv.org/pdf/2410.17635v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.17632v1","updated":"2024-10-23T07:48:51Z","published":"2024-10-23T07:48:51Z","title":"LMLPA: Language Model Linguistic Personality Assessment","summary":"  Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing.\n","authors":["Jingyao Zheng","Xian Wang","Simo Hosio","Xiaoxian Xu","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2410.17632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03622v3","updated":"2024-10-23T07:20:26Z","published":"2024-04-04T17:45:08Z","title":"Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning\n  in Large Language Models","summary":"  Large language models (LLMs) have exhibited impressive performance in\nlanguage comprehension and various reasoning tasks. However, their abilities in\nspatial reasoning, a crucial aspect of human cognition, remain relatively\nunexplored. Human possess a remarkable ability to create mental images of\nunseen objects and actions through a process known as the Mind's Eye, enabling\nthe imagination of the unseen world. Inspired by this cognitive capacity, we\npropose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial\nreasoning of LLMs by visualizing their reasoning traces, thereby guiding\nsubsequent reasoning steps. We employed VoT for multi-hop spatial reasoning\ntasks, including natural language navigation, visual navigation, and visual\ntiling in 2D grid worlds. Experimental results demonstrated that VoT\nsignificantly enhances the spatial reasoning abilities of LLMs. Notably, VoT\noutperformed existing multimodal large language models (MLLMs) in these tasks.\nWhile VoT works surprisingly well on LLMs, the ability to generate mental\nimages to facilitate spatial reasoning resembles the mind's eye process,\nsuggesting its potential viability in MLLMs. Please find the dataset and codes\nat https://microsoft.github.io/visualization-of-thought\n","authors":["Wenshan Wu","Shaoguang Mao","Yadong Zhang","Yan Xia","Li Dong","Lei Cui","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2404.03622v3.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2305.12987v3","updated":"2024-10-23T07:13:49Z","published":"2023-05-22T12:47:48Z","title":"GPT-SW3: An Autoregressive Language Model for the Nordic Languages","summary":"  This paper details the process of developing the first native large\ngenerative language model for the Nordic languages, GPT-SW3. We cover all parts\nof the development process, from data collection and processing, training\nconfiguration and instruction finetuning, to evaluation and considerations for\nrelease strategies. We hope that this paper can serve as a guide and reference\nfor other researchers that undertake the development of large generative models\nfor smaller languages.\n","authors":["Ariel Ekgren","Amaru Cuba Gyllensten","Felix Stollenwerk","Joey Öhman","Tim Isbister","Evangelia Gogoulou","Fredrik Carlsson","Alice Heiman","Judit Casademont","Magnus Sahlgren"],"pdf_url":"https://arxiv.org/pdf/2305.12987v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17600v1","updated":"2024-10-23T06:54:03Z","published":"2024-10-23T06:54:03Z","title":"Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective","summary":"  Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion.\n","authors":["Rui Yang","Boming Yang","Aosong Feng","Sixun Ouyang","Moritz Blum","Tianwei She","Yuang Jiang","Freddy Lecue","Jinghui Lu","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2410.17600v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.10794"},{"id":"http://arxiv.org/abs/2410.17599v1","updated":"2024-10-23T06:52:09Z","published":"2024-10-23T06:52:09Z","title":"Cross-model Control: Improving Multiple Large Language Models in\n  One-time Training","summary":"  The number of large language models (LLMs) with varying parameter scales and\nvocabularies is increasing. While they deliver powerful performance, they also\nface a set of common optimization needs to meet specific requirements or\nstandards, such as instruction following or avoiding the output of sensitive\ninformation from the real world. However, how to reuse the fine-tuning outcomes\nof one model to other models to reduce training costs remains a challenge. To\nbridge this gap, we introduce Cross-model Control (CMC), a method that improves\nmultiple LLMs in one-time training with a portable tiny language model.\nSpecifically, we have observed that the logit shift before and after\nfine-tuning is remarkably similar across different models. Based on this\ninsight, we incorporate a tiny language model with a minimal number of\nparameters. By training alongside a frozen template LLM, the tiny model gains\nthe capability to alter the logits output by the LLMs. To make this tiny\nlanguage model applicable to models with different vocabularies, we propose a\nnovel token mapping strategy named PM-MinED. We have conducted extensive\nexperiments on instruction tuning and unlearning tasks, demonstrating the\neffectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC.\n","authors":["Jiayi Wu","Hao Sun","Hengyi Cai","Lixin Su","Shuaiqiang Wang","Dawei Yin","Xiang Li","Ming Gao"],"pdf_url":"https://arxiv.org/pdf/2410.17599v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.12617v2","updated":"2024-10-23T06:28:19Z","published":"2024-02-20T00:51:05Z","title":"Generative AI Security: Challenges and Countermeasures","summary":"  Generative AI's expanding footprint across numerous industries has led to\nboth excitement and increased scrutiny. This paper delves into the unique\nsecurity challenges posed by Generative AI, and outlines potential research\ndirections for managing these risks.\n","authors":["Banghua Zhu","Norman Mu","Jiantao Jiao","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2402.12617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15573v2","updated":"2024-10-23T06:21:09Z","published":"2024-10-21T01:36:42Z","title":"OpenMU: Your Swiss Army Knife for Music Understanding","summary":"  We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.\n","authors":["Mengjie Zhao","Zhi Zhong","Zhuoyuan Mao","Shiqi Yang","Wei-Hsiang Liao","Shusuke Takahashi","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.15573v2.pdf","comment":"Resources: https://github.com/mzhaojp22/openmu"},{"id":"http://arxiv.org/abs/2410.17578v1","updated":"2024-10-23T06:04:55Z","published":"2024-10-23T06:04:55Z","title":"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and\n  Reward Models","summary":"  Large language models (LLMs) are commonly used as evaluators in tasks (e.g.,\nreward modeling, LLM-as-a-judge), where they act as proxies for human\npreferences or judgments. This leads to the need for meta-evaluation:\nevaluating the credibility of LLMs as evaluators. However, existing benchmarks\nprimarily focus on English, offering limited insight into LLMs' effectiveness\nas evaluators in non-English contexts. To address this, we introduce MM-Eval, a\nmultilingual meta-evaluation benchmark that covers 18 languages across six\ncategories. MM-Eval evaluates various dimensions, including language-specific\nchallenges like linguistics and language hallucinations. Evaluation results\nshow that both proprietary and open-source language models have considerable\nroom for improvement. Further analysis reveals a tendency for these models to\nassign middle-ground scores to low-resource languages. We publicly release our\nbenchmark and code.\n","authors":["Guijin Son","Dongkeun Yoon","Juyoung Suk","Javier Aula-Blasco","Mano Aslan","Vu Trong Kim","Shayekh Bin Islam","Jaume Prats-Cristià","Lucía Tormo-Bañuelos","Seungone Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17578v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.17566v1","updated":"2024-10-23T05:19:51Z","published":"2024-10-23T05:19:51Z","title":"Differentially Private Learning Needs Better Model Initialization and\n  Self-Distillation","summary":"  Differentially private SGD (DPSGD) enables privacy-preserving training of\nlanguage models, but often reduces utility, diversity, and linguistic quality.\nWe introduce DPRefine, a three-phase method that initializes a model using data\nsynthesis from a small pre-trained LM with rigorous filtering, applies DP\nfinetuning on private data, and performs self-distillation to refine outputs.\nThis approach significantly outperforms vanilla DPSGD, with AlpacaEval\npreferring DPRefine's generations in 78.4% of cases across all datasets. Our\nanalysis reveals that DPRefine reduces linguistic errors in generated text by\n84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.\nIt also reduces inconsistencies of non-private models, such as hallucinated\ndetails and misattributed quotes. We find that small models like GPT-2 can be\neffective for initialization and distillation, highlighting their potential in\nenabling scalable and efficient deployment of privacy-preserving language.\n","authors":["Ivoline C. Ngong","Joseph P. Near","Niloofar Mireshghallah"],"pdf_url":"https://arxiv.org/pdf/2410.17566v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2402.14146v2","updated":"2024-10-23T04:39:25Z","published":"2024-02-21T22:02:37Z","title":"Reinforcement Learning with Dynamic Multi-Reward Weighting for\n  Multi-Style Controllable Generation","summary":"  Textual style expresses a diverse set of information, including interpersonal\ndynamics (e.g., formality) and the author's emotions or attitudes (e.g.,\ndisgust). An open question is how language models can be explicitly controlled\nso that they weave together target styles when generating text: for example, to\nproduce text that is both negative and non-toxic. One approach to such\ncontrolled generation is multi-objective reinforcement learning (RL), but how\nbest to combine multiple objectives in a reward function is an open question.\nIn this paper, we investigate various formulations of multi-style rewards,\nincluding calibrated outputs from discriminators and dynamic weighting by\ndiscriminator gradient magnitudes. We find that our proposed dynamic weighting\noutperforms static weighting approaches with respect to style control while\nmaintaining linguistic quality, and we explore its effectiveness in 2- and\n3-style control.\n","authors":["Karin de Langis","Ryan Koo","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2402.14146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17552v1","updated":"2024-10-23T04:34:49Z","published":"2024-10-23T04:34:49Z","title":"ESpeW: Robust Copyright Protection for LLM-based EaaS via\n  Embedding-Specific Watermark","summary":"  Embeddings as a Service (EaaS) is emerging as a crucial role in AI\napplications. Unfortunately, EaaS is vulnerable to model extraction attacks,\nhighlighting the urgent need for copyright protection.Although some preliminary\nworks propose applying embedding watermarks to protect EaaS, recent research\nreveals that these watermarks can be easily removed. Hence, it is crucial to\ninject robust watermarks resistant to watermark removal attacks.Existing\nwatermarking methods typically inject a target embedding into embeddings\nthrough linear interpolation when the text contains triggers. However, this\nmechanism results in each watermarked embedding having the same component,\nwhich makes the watermark easy to identify and eliminate.Motivated by this, in\nthis paper, we propose a novel embedding-specific watermarking (ESpeW)\nmechanism to offer robust copyright protection for EaaS. Our approach involves\ninjecting unique, yet readily identifiable watermarks into each embedding.\nWatermarks inserted by ESpeW are designed to maintain a significant distance\nfrom one another and to avoid sharing common components, thus making it\nsignificantly more challenging to remove the watermarks.Extensive experiments\non four popular datasets demonstrate that ESpeW can even watermark successfully\nagainst a highly aggressive removal strategy without sacrificing the quality of\nembeddings.\n","authors":["Zongqi Wang","Baoyuan Wu","Jingyuan Deng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2410.17552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17546v1","updated":"2024-10-23T03:53:46Z","published":"2024-10-23T03:53:46Z","title":"ProtoLens: Advancing Prototype Learning for Fine-Grained\n  Interpretability in Text Classification","summary":"  Deep neural networks have achieved remarkable performance in various\ntext-based tasks but often lack interpretability, making them less suitable for\napplications where transparency is critical. To address this, we propose\nProtoLens, a novel prototype-based model that provides fine-grained,\nsub-sentence level interpretability for text classification. ProtoLens uses a\nPrototype-aware Span Extraction module to identify relevant text spans\nassociated with learned prototypes and a Prototype Alignment mechanism to\nensure prototypes are semantically meaningful throughout training. By aligning\nthe prototype embeddings with human-understandable examples, ProtoLens provides\ninterpretable predictions while maintaining competitive accuracy. Extensive\nexperiments demonstrate that ProtoLens outperforms both prototype-based and\nnon-interpretable baselines on multiple text classification benchmarks. Code\nand data are available at\n\\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}.\n","authors":["Bowen Wei","Ziwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.17546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17532v1","updated":"2024-10-23T03:19:15Z","published":"2024-10-23T03:19:15Z","title":"Responsible Multilingual Large Language Models: A Survey of Development,\n  Applications, and Societal Impact","summary":"  Multilingual Large Language Models (MLLMs) represent a pivotal advancement in\ndemocratizing artificial intelligence across linguistic boundaries. While\ntheoretical foundations are well-established, practical implementation\nguidelines remain scattered. This work bridges this gap by providing a\ncomprehensive end-to-end framework for developing and deploying MLLMs in\nproduction environments. We make three distinctive contributions: First, we\npresent an actionable pipeline from data pre-processing through deployment,\nintegrating insights from academic research and industrial applications.\nSecond, using Llama2 as a case study, we provide detailed optimization\nstrategies for enhancing multilingual capabilities, including curriculum\nlearning approaches for balancing high-resource and low-resource languages,\ntokenization strategies, and effective sampling methods. Third, we offer an\ninterdisciplinary analysis that considers technical, linguistic, and cultural\nperspectives in MLLM development. Our findings reveal critical challenges in\nsupporting linguistic diversity, with 88.38% of world languages categorized as\nlow-resource, affecting over a billion speakers. We examine practical solutions\nthrough real-world applications in customer service, search engines, and\nmachine translation. By synthesizing theoretical frameworks with\nproduction-ready implementation strategies, this survey provides essential\nguidance for practitioners and researchers working to develop more inclusive\nand effective multilingual AI systems.\n","authors":["Junhua Liu","Bin Fu"],"pdf_url":"https://arxiv.org/pdf/2410.17532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17529v1","updated":"2024-10-23T03:14:07Z","published":"2024-10-23T03:14:07Z","title":"Navigate Complex Physical Worlds via Geometrically Constrained LLM","summary":"  This study investigates the potential of Large Language Models (LLMs) for\nreconstructing and constructing the physical world solely based on textual\nknowledge. It explores the impact of model performance on spatial understanding\nabilities. To enhance the comprehension of geometric and spatial relationships\nin the complex physical world, the study introduces a set of geometric\nconventions and develops a workflow based on multi-layer graphs and multi-agent\nsystem frameworks. It examines how LLMs achieve multi-step and multi-objective\ngeometric inference in a spatial environment using multi-layer graphs under\nunified geometric conventions. Additionally, the study employs a genetic\nalgorithm, inspired by large-scale model knowledge, to solve geometric\nconstraint problems. In summary, this work innovatively explores the\nfeasibility of using text-based LLMs as physical world builders and designs a\nworkflow to enhance their capabilities.\n","authors":["Yongqiang Huang","Wentao Ye","Liyao Li","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.17529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14687v2","updated":"2024-10-23T03:05:37Z","published":"2024-10-03T14:17:43Z","title":"BrainTransformers: SNN-LLM","summary":"  This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments.\n","authors":["Zhengzheng Tang","Eva Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.14687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11303v2","updated":"2024-10-23T03:00:41Z","published":"2024-10-15T05:54:17Z","title":"TSDS: Data Selection for Task-Specific Model Finetuning","summary":"  Finetuning foundation models for specific tasks is an emerging paradigm in\nmodern machine learning. The efficacy of task-specific finetuning largely\ndepends on the selection of appropriate training data. We present TSDS\n(Task-Specific Data Selection), a framework to select data for task-specific\nmodel finetuning, guided by a small but representative set of examples from the\ntarget task. To do so, we formulate data selection for task-specific finetuning\nas an optimization problem with a distribution alignment loss based on optimal\ntransport to capture the discrepancy between the selected data and the target\ndistribution. In addition, we add a regularizer to encourage the diversity of\nthe selected data and incorporate kernel density estimation into the\nregularizer to reduce the negative effects of near-duplicates among the\ncandidate data. We connect our optimization problem to nearest neighbor search\nand design efficient algorithms to compute the optimal solution based on\napproximate nearest neighbor search techniques. We evaluate our method on data\nselection for both continued pretraining and instruction tuning of language\nmodels. We show that instruction tuning using data selected by our method with\na 1% selection ratio often outperforms using the full dataset and beats the\nbaseline selection methods by 1.5 points in F1 score on average.\n","authors":["Zifan Liu","Amin Karbasi","Theodoros Rekatsinas"],"pdf_url":"https://arxiv.org/pdf/2410.11303v2.pdf","comment":"31 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.18925v3","updated":"2024-10-23T02:57:31Z","published":"2024-06-27T06:32:56Z","title":"Selective Vision is the Challenge for Visual Reasoning: A Benchmark for\n  Visual Argument Understanding","summary":"  Visual arguments, often used in advertising or social causes, rely on images\nto persuade viewers to do or believe something. Understanding these arguments\nrequires selective vision: only specific visual stimuli within an image are\nrelevant to the argument, and relevance can only be understood within the\ncontext of a broader argumentative structure. While visual arguments are\nreadily appreciated by human audiences, we ask: are today's AI capable of\nsimilar understanding? We present VisArgs, a dataset of 1,611 images annotated\nwith 5,112 visual premises (with regions), 5,574 commonsense premises, and\nreasoning trees connecting them into structured arguments. We propose three\ntasks for evaluating visual argument understanding: premise localization,\npremise identification, and conclusion deduction. Experiments show that 1)\nmachines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy,\nwhile humans reached 98.0%. Models also performed 19.5% worse when\ndistinguishing between irrelevant objects within the image compared to external\nobjects. 2) Providing relevant visual premises improved model performance\nsignificantly.\n","authors":["Jiwan Chung","Sungjae Lee","Minseo Kim","Seungju Han","Ashkan Yousefpour","Jack Hessel","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.18925v3.pdf","comment":"12 pages, 6 figures. Accepted as main paper in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17520v1","updated":"2024-10-23T02:51:43Z","published":"2024-10-23T02:51:43Z","title":"MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control","summary":"  Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications. To\nclearly evaluate safety apart from general capabilities, we design separate\ntasks measuring safety and tasks evaluating helpfulness. The safety tasks\nchallenge agents with managing potential risks prevalent in daily life and\ninclude tests to evaluate robustness against indirect prompt injections. Our\nexperiments demonstrate that while baseline agents, based on state-of-the-art\nLLMs, perform well in executing helpful tasks, they show poor performance in\nsafety tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/.\n","authors":["Juyong Lee","Dongyoon Hahm","June Suk Choi","W. Bradley Knox","Kimin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.17520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17519v1","updated":"2024-10-23T02:51:33Z","published":"2024-10-23T02:51:33Z","title":"Large Language Models Still Exhibit Bias in Long Text","summary":"  Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks.\n","authors":["Wonje Jeung","Dongjae Jeon","Ashkan Yousefpour","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2410.17519v1.pdf","comment":"22 page, 38 figures, Neurips (SoLaR Workshop)"},{"id":"http://arxiv.org/abs/2410.13334v2","updated":"2024-10-23T02:15:52Z","published":"2024-10-17T08:46:09Z","title":"Do LLMs Have Political Correctness? Analyzing Ethical Biases and\n  Jailbreak Vulnerabilities in AI Systems","summary":"  Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content. To address\nthese issues, many LLM developers have implemented various safety measures to\nalign these models. This alignment involves several techniques, including data\nfiltering during pre-training, supervised fine-tuning, reinforcement learning\nfrom human feedback, and red-teaming exercises. These methods often introduce\ndeliberate and intentional biases similar to Political Correctness (PC) to\nensure the ethical behavior of LLMs. In this paper, we delve into the\nintentional biases injected into LLMs for safety purposes and examine methods\nto circumvent these safety alignment techniques. Notably, these intentional\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20% between non-binary and cisgender keywords and by 16% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of PCJailbreak, highlighting the inherent risks posed by\nthese safety-induced biases. Additionally, we propose an efficient defense\nmethod PCDefense, which prevents jailbreak attempts by injecting defense\nprompts prior to generation. PCDefense stands as an appealing alternative to\nGuard Models, such as Llama-Guard, that require additional inference cost after\ntext generation. Our findings emphasize the urgent need for LLM developers to\nadopt a more responsible approach when designing and implementing safety\nmeasures.\n","authors":["Isack Lee","Haebin Seong"],"pdf_url":"https://arxiv.org/pdf/2410.13334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17498v1","updated":"2024-10-23T01:38:10Z","published":"2024-10-23T01:38:10Z","title":"Mechanisms of Symbol Processing for In-Context Learning in Transformer\n  Networks","summary":"  Large Language Models (LLMs) have demonstrated impressive abilities in symbol\nprocessing through in-context learning (ICL). This success flies in the face of\ndecades of predictions that artificial neural networks cannot master abstract\nsymbol manipulation. We seek to understand the mechanisms that can enable\nrobust symbol processing in transformer networks, illuminating both the\nunanticipated success, and the significant limitations, of transformers in\nsymbol processing. Borrowing insights from symbolic AI on the power of\nProduction System architectures, we develop a high-level language, PSL, that\nallows us to write symbolic programs to do complex, abstract symbol processing,\nand create compilers that precisely implement PSL programs in transformer\nnetworks which are, by construction, 100% mechanistically interpretable. We\ndemonstrate that PSL is Turing Universal, so the work can inform the\nunderstanding of transformer ICL in general. The type of transformer\narchitecture that we compile from PSL programs suggests a number of paths for\nenhancing transformers' capabilities at symbol processing. (Note: The first\nsection of the paper gives an extended synopsis of the entire paper.)\n","authors":["Paul Smolensky","Roland Fernandez","Zhenghao Herbert Zhou","Mattia Opper","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2410.17498v1.pdf","comment":"101 pages (including 30 pages of Appendices), 18 figures"},{"id":"http://arxiv.org/abs/2410.17492v1","updated":"2024-10-23T01:14:54Z","published":"2024-10-23T01:14:54Z","title":"BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers","summary":"  Attacking fairness is crucial because compromised models can introduce biased\noutcomes, undermining trust and amplifying inequalities in sensitive\napplications like hiring, healthcare, and law enforcement. This highlights the\nurgent need to understand how fairness mechanisms can be exploited and to\ndevelop defenses that ensure both fairness and robustness. We introduce\nBadFair, a novel backdoored fairness attack methodology. BadFair stealthily\ncrafts a model that operates with accuracy and fairness under regular\nconditions but, when activated by certain triggers, discriminates and produces\nincorrect results for specific groups. This type of attack is particularly\nstealthy and dangerous, as it circumvents existing fairness detection methods,\nmaintaining an appearance of fairness in normal use. Our findings reveal that\nBadFair achieves a more than 85% attack success rate in attacks aimed at target\ngroups on average while only incurring a minimal accuracy loss. Moreover, it\nconsistently exhibits a significant discrimination score, distinguishing\nbetween pre-defined target and non-target attacked groups across various\ndatasets and models.\n","authors":["Jiaqi Xue","Qian Lou","Mengxin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.17492v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.10601v2","updated":"2024-10-23T00:38:14Z","published":"2024-02-16T11:37:05Z","title":"When \"Competency\" in Reasoning Opens the Door to Vulnerability:\n  Jailbreaking LLMs via Novel Complex Ciphers","summary":"  Recent advancements in the safety of Large Language Models (LLMs) have\nprimarily focused on mitigating attacks crafted in natural language or in\ncommon encryption techniques like Base64. However, new models which often\npossess better reasoning capabilities, open the door to new attack vectors that\nwere previously non-existent in older models. This seems counter-intuitive at\nfirst glance, but these advanced models can decipher more complex cryptic\nqueries that previous models could not, making them susceptible to attacks\nusing such prompts. To exploit this vulnerability, we propose Attacks using\nCustom Encryptions (ACE), a novel method to jailbreak LLMs by leveraging custom\nencryption schemes. We evaluate the effectiveness of ACE on four\nstate-of-the-art LLMs, achieving Attack Success Rates (ASR) of up to 66% on\nclose-source models and 88% on open-source models. Building upon this, we\nintroduce Layered Attacks using Custom Encryptions (LACE), which employs\nmultiple layers of encryption through our custom ciphers to further enhance the\nASR. Our findings demonstrate that LACE significantly enhances the ability to\njailbreak LLMs, increasing the ASR of GPT-4o from 40% to 78%, a 38%\nimprovement. Our results highlight that the advanced capabilities of LLMs\nintroduce unforeseen vulnerabilities to complex attacks. Specifically complex\nand layered ciphers increase the chance of jailbreaking.\n","authors":["Divij Handa","Zehua Zhang","Amir Saeidi","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2402.10601v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17485v1","updated":"2024-10-23T00:36:06Z","published":"2024-10-23T00:36:06Z","title":"VoiceTextBlender: Augmenting Large Language Models with Speech\n  Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning","summary":"  Recent studies have augmented large language models (LLMs) with speech\ncapabilities, leading to the development of speech language models (SpeechLMs).\nEarlier SpeechLMs focused on single-turn speech-based question answering (QA),\nwhere user input comprised a speech context and a text question. More recent\nstudies have extended this to multi-turn conversations, though they often\nrequire complex, multi-stage supervised fine-tuning (SFT) with diverse data.\nAnother critical challenge with SpeechLMs is catastrophic forgetting-where\nmodels optimized for speech tasks suffer significant degradation in text-only\nperformance. To mitigate these issues, we propose a novel single-stage joint\nspeech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone.\nOur joint SFT combines text-only SFT data with three types of speech-related\ndata: speech recognition and translation, speech-based QA, and mixed-modal SFT.\nCompared to previous SpeechLMs with 7B or 13B parameters, our 3B model\ndemonstrates superior performance across various speech benchmarks while\npreserving the original capabilities on text-only tasks. Furthermore, our model\nshows emergent abilities of effectively handling previously unseen prompts and\ntasks, including multi-turn, mixed-modal inputs.\n","authors":["Yifan Peng","Krishna C. Puvvada","Zhehuai Chen","Piotr Zelasko","He Huang","Kunal Dhawan","Ke Hu","Shinji Watanabe","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2410.17485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17484v1","updated":"2024-10-23T00:31:17Z","published":"2024-10-23T00:31:17Z","title":"Which Client is Reliable?: A Reliable and Personalized Prompt-based\n  Federated Learning for Medical Image Question Answering","summary":"  Conventional medical artificial intelligence (AI) models face barriers in\nclinical application and ethical issues owing to their inability to handle the\nprivacy-sensitive characteristics of medical data. We present a novel\npersonalized federated learning (pFL) method for medical visual question\nanswering (VQA) models, addressing privacy reliability challenges in the\nmedical domain. Our method introduces learnable prompts into a Transformer\narchitecture to efficiently train it on diverse medical datasets without\nmassive computational costs. Then we introduce a reliable client VQA model that\nincorporates Dempster-Shafer evidence theory to quantify uncertainty in\npredictions, enhancing the model's reliability. Furthermore, we propose a novel\ninter-client communication mechanism that uses maximum likelihood estimation to\nbalance accuracy and uncertainty, fostering efficient integration of insights\nacross clients.\n","authors":["He Zhu","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2410.17484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17482v1","updated":"2024-10-23T00:05:57Z","published":"2024-10-23T00:05:57Z","title":"Is artificial intelligence still intelligence? LLMs generalize to novel\n  adjective-noun pairs, but don't mimic the full human distribution","summary":"  Inferences from adjective-noun combinations like \"Is artificial intelligence\nstill intelligence?\" provide a good test bed for LLMs' understanding of meaning\nand compositional generalization capability, since there are many combinations\nwhich are novel to both humans and LLMs but nevertheless elicit convergent\nhuman judgments. We study a range of LLMs and find that the largest models we\ntested are able to draw human-like inferences when the inference is determined\nby context and can generalize to unseen adjective-noun combinations. We also\npropose three methods to evaluate LLMs on these inferences out of context,\nwhere there is a distribution of human-like answers rather than a single\ncorrect answer. We find that LLMs show a human-like distribution on at most\n75\\% of our dataset, which is promising but still leaves room for improvement.\n","authors":["Hayley Ross","Kathryn Davidson","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17482v1.pdf","comment":"9 pages (23 pages with appendix). Accepted to GenBench 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.16592v3","updated":"2024-10-23T06:45:20Z","published":"2024-06-24T12:33:21Z","title":"Toward Fairer Face Recognition Datasets","summary":"  Face recognition and verification are two computer vision tasks whose\nperformance has progressed with the introduction of deep representations.\nHowever, ethical, legal, and technical challenges due to the sensitive\ncharacter of face data and biases in real training datasets hinder their\ndevelopment. Generative AI addresses privacy by creating fictitious identities,\nbut fairness problems persist. We promote fairness by introducing a demographic\nattributes balancing mechanism in generated training datasets. We experiment\nwith an existing real dataset, three generated training datasets, and the\nbalanced versions of a diffusion-based dataset. We propose a comprehensive\nevaluation that considers accuracy and fairness equally and includes a rigorous\nregression-based statistical analysis of attributes. The analysis shows that\nbalancing reduces demographic unfairness. Also, a performance gap persists\ndespite generation becoming more accurate with time. The proposed balancing\nmethod and comprehensive verification evaluation promote fairer and transparent\nface recognition and verification.\n","authors":["Alexandre Fournier-Montgieux","Michael Soumm","Adrian Popescu","Bertrand Luvison","Hervé Le Borgne"],"pdf_url":"https://arxiv.org/pdf/2406.16592v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15574v4","updated":"2024-10-23T10:54:16Z","published":"2024-05-24T14:04:03Z","title":"Meteor: Mamba-based Traversal of Rationale for Large Language and Vision\n  Models","summary":"  The rapid development of large language and vision models (LLVMs) has been\ndriven by advances in visual instruction tuning. Recently, open-source LLVMs\nhave curated high-quality visual instruction tuning datasets and utilized\nadditional vision encoders or multiple computer vision models in order to\nnarrow the performance gap with powerful closed-source LLVMs. These\nadvancements are attributed to multifaceted information required for diverse\ncapabilities, including fundamental image understanding, real-world knowledge\nabout common-sense and non-object concepts (e.g., charts, diagrams, symbols,\nsigns, and math problems), and step-by-step procedures for solving complex\nquestions. Drawing from the multifaceted information, we present a new\nefficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages\nmultifaceted rationale to enhance understanding and answering capabilities. To\nembed lengthy rationales containing abundant information, we employ the Mamba\narchitecture, capable of processing sequential data with linear time\ncomplexity. We introduce a new concept of traversal of rationale that\nfacilitates efficient embedding of rationale. Subsequently, the backbone\nmultimodal language model (MLM) is trained to generate answers with the aid of\nrationale. Through these steps, Meteor achieves significant improvements in\nvision language performances across multiple evaluation benchmarks requiring\ndiverse capabilities, without scaling up the model size or employing additional\nvision encoders and computer vision models.\n","authors":["Byung-Kwan Lee","Chae Won Kim","Beomchan Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2405.15574v4.pdf","comment":"Code is available in https://github.com/ByungKwanLee/Meteor"},{"id":"http://arxiv.org/abs/2410.18084v1","updated":"2024-10-23T17:59:58Z","published":"2024-10-23T17:59:58Z","title":"DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes","summary":"  LiDAR scene generation has been developing rapidly recently. However,\nexisting methods primarily focus on generating static and single-frame scenes,\noverlooking the inherently dynamic nature of real-world driving environments.\nIn this work, we introduce DynamicCity, a novel 4D LiDAR generation framework\ncapable of generating large-scale, high-quality LiDAR scenes that capture the\ntemporal evolution of dynamic environments. DynamicCity mainly consists of two\nkey models. 1) A VAE model for learning HexPlane as the compact 4D\nrepresentation. Instead of using naive averaging operations, DynamicCity\nemploys a novel Projection Module to effectively compress 4D LiDAR features\ninto six 2D feature maps for HexPlane construction, which significantly\nenhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we\nutilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in\nparallel, which improves both network training efficiency and reconstruction\naccuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x\ntraining speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model\nfor HexPlane generation. To make HexPlane feasible for DiT generation, a Padded\nRollout Operation is proposed to reorganize all six feature planes of the\nHexPlane as a squared 2D feature map. In particular, various conditions could\nbe introduced in the diffusion or sampling process, supporting versatile 4D\ngeneration applications, such as trajectory- and command-driven generation,\ninpainting, and layout-conditioned generation. Extensive experiments on the\nCarlaSC and Waymo datasets demonstrate that DynamicCity significantly\noutperforms existing state-of-the-art 4D LiDAR generation methods across\nmultiple metrics. The code will be released to facilitate future research.\n","authors":["Hengwei Bian","Lingdong Kong","Haozhe Xie","Liang Pan","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.18084v1.pdf","comment":"Preprint; 29 pages, 15 figures, 7 tables; Project Page at\n  https://dynamic-city.github.io/"},{"id":"http://arxiv.org/abs/2410.18083v1","updated":"2024-10-23T17:59:57Z","published":"2024-10-23T17:59:57Z","title":"FIPER: Generalizable Factorized Fields for Joint Image Compression and\n  Super-Resolution","summary":"  In this work, we propose a unified representation for Super-Resolution (SR)\nand Image Compression, termed **Factorized Fields**, motivated by the shared\nprinciples between these two tasks. Both SISR and Image Compression require\nrecovering and preserving fine image details--whether by enhancing resolution\nor reconstructing compressed data. Unlike previous methods that mainly focus on\nnetwork architecture, our proposed approach utilizes a basis-coefficient\ndecomposition to explicitly capture multi-scale visual features and structural\ncomponents in images, addressing the core challenges of both tasks. We first\nderive our SR model, which includes a Coefficient Backbone and Basis Swin\nTransformer for generalizable Factorized Fields. Then, to further unify these\ntwo tasks, we leverage the strong information-recovery capabilities of the\ntrained SR modules as priors in the compression pipeline, improving both\ncompression efficiency and detail reconstruction. Additionally, we introduce a\nmerged-basis compression branch that consolidates shared structures, further\noptimizing the compression process. Extensive experiments show that our unified\nrepresentation delivers state-of-the-art performance, achieving an average\nrelative improvement of 204.4% in PSNR over the baseline in Super-Resolution\n(SR) and 9.35% BD-rate reduction in Image Compression compared to the previous\nSOTA.\n","authors":["Yang-Che Sun","Cheng Yu Yeo","Ernie Chu","Jun-Cheng Chen","Yu-Lun Liu"],"pdf_url":"https://arxiv.org/pdf/2410.18083v1.pdf","comment":"Project page: https://jayisaking.github.io/FIPER/"},{"id":"http://arxiv.org/abs/2410.18079v1","updated":"2024-10-23T17:59:11Z","published":"2024-10-23T17:59:11Z","title":"FreeVS: Generative View Synthesis on Free Driving Trajectory","summary":"  Existing reconstruction-based novel view synthesis methods for driving scenes\nfocus on synthesizing camera views along the recorded trajectory of the ego\nvehicle. Their image rendering performance will severely degrade on viewpoints\nfalling out of the recorded trajectory, where camera rays are untrained. We\npropose FreeVS, a novel fully generative approach that can synthesize camera\nviews on free new trajectories in real driving scenes. To control the\ngeneration results to be 3D consistent with the real scenes and accurate in\nviewpoint pose, we propose the pseudo-image representation of view priors to\ncontrol the generation process. Viewpoint transformation simulation is applied\non pseudo-images to simulate camera movement in each direction. Once trained,\nFreeVS can be applied to any validation sequences without reconstruction\nprocess and synthesis views on novel trajectories. Moreover, we propose two new\nchallenging benchmarks tailored to driving scenes, which are novel camera\nsynthesis and novel trajectory synthesis, emphasizing the freedom of\nviewpoints. Given that no ground truth images are available on novel\ntrajectories, we also propose to evaluate the consistency of images synthesized\non novel trajectories with 3D perception models. Experiments on the Waymo Open\nDataset show that FreeVS has a strong image synthesis performance on both the\nrecorded trajectories and novel trajectories. Project Page:\nhttps://freevs24.github.io/\n","authors":["Qitai Wang","Lue Fan","Yuqi Wang","Yuntao Chen","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18079v1.pdf","comment":"Project Page: https://freevs24.github.io/"},{"id":"http://arxiv.org/abs/2410.18074v1","updated":"2024-10-23T17:56:33Z","published":"2024-10-23T17:56:33Z","title":"UnCLe: Unsupervised Continual Learning of Depth Completion","summary":"  We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform.\n","authors":["Suchisrit Gangopadhyay","Xien Chen","Michael Chu","Patrick Rim","Hyoungseob Park","Alex Wong"],"pdf_url":"https://arxiv.org/pdf/2410.18074v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.18072v1","updated":"2024-10-23T17:56:11Z","published":"2024-10-23T17:56:11Z","title":"WorldSimBench: Towards Video Generation Models as World Simulators","summary":"  Recent advancements in predictive models have demonstrated exceptional\ncapabilities in predicting the future state of objects and scenes. However, the\nlack of categorization based on inherent characteristics continues to hinder\nthe progress of predictive model development. Additionally, existing benchmarks\nare unable to effectively evaluate higher-capability, highly embodied\npredictive models from an embodied perspective. In this work, we classify the\nfunctionalities of predictive models into a hierarchy and take the first step\nin evaluating World Simulators by proposing a dual evaluation framework called\nWorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and\nImplicit Manipulative Evaluation, encompassing human preference assessments\nfrom the visual perspective and action-level evaluations in embodied tasks,\ncovering three representative embodied scenarios: Open-Ended Embodied\nEnvironment, Autonomous, Driving, and Robot Manipulation. In the Explicit\nPerceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment\ndataset based on fine-grained human feedback, which we use to train a Human\nPreference Evaluator that aligns with human perception and explicitly assesses\nthe visual fidelity of World Simulators. In the Implicit Manipulative\nEvaluation, we assess the video-action consistency of World Simulators by\nevaluating whether the generated situation-aware video can be accurately\ntranslated into the correct control signals in dynamic environments. Our\ncomprehensive evaluation offers key insights that can drive further innovation\nin video generation models, positioning World Simulators as a pivotal\nadvancement toward embodied artificial intelligence.\n","authors":["Yiran Qin","Zhelun Shi","Jiwen Yu","Xijun Wang","Enshen Zhou","Lijun Li","Zhenfei Yin","Xihui Liu","Lu Sheng","Jing Shao","Lei Bai","Wanli Ouyang","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18071v1","updated":"2024-10-23T17:54:43Z","published":"2024-10-23T17:54:43Z","title":"TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing\n  Prompts","summary":"  Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks.\n","authors":["Yuxuan Xie","Tianhua Li","Wenqi Shao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12568v2","updated":"2024-10-23T17:53:24Z","published":"2024-08-22T17:35:18Z","title":"Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune\n  CNNs and Transformers","summary":"  To solve ever more complex problems, Deep Neural Networks are scaled to\nbillions of parameters, leading to huge computational costs. An effective\napproach to reduce computational requirements and increase efficiency is to\nprune unnecessary components of these often over-parameterized networks.\nPrevious work has shown that attribution methods from the field of eXplainable\nAI serve as effective means to extract and prune the least relevant network\ncomponents in a few-shot fashion. We extend the current state by proposing to\nexplicitly optimize hyperparameters of attribution methods for the task of\npruning, and further include transformer-based networks in our analysis. Our\napproach yields higher model compression rates of large transformer- and\nconvolutional architectures (VGG, ResNet, ViT) compared to previous works,\nwhile still attaining high performance on ImageNet classification tasks. Here,\nour experiments indicate that transformers have a higher degree of\nover-parameterization compared to convolutional neural networks. Code is\navailable at https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.\n","authors":["Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2408.12568v2.pdf","comment":"Accepted as a workshop paper at ECCV 2024, 26 pages (11 pages\n  manuscript, 3 pages references, 12 pages appendix)"},{"id":"http://arxiv.org/abs/2410.18065v1","updated":"2024-10-23T17:42:07Z","published":"2024-10-23T17:42:07Z","title":"SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for\n  Long-Horizon Manipulation","summary":"  Robot learning has proven to be a general and effective technique for\nprogramming manipulators. Imitation learning is able to teach robots solely\nfrom human demonstrations but is bottlenecked by the capabilities of the\ndemonstrations. Reinforcement learning uses exploration to discover better\nbehaviors; however, the space of possible improvements can be too large to\nstart from scratch. And for both techniques, the learning difficulty increases\nproportional to the length of the manipulation task. Accounting for this, we\npropose SPIRE, a system that first uses Task and Motion Planning (TAMP) to\ndecompose tasks into smaller learning subproblems and second combines imitation\nand reinforcement learning to maximize their strengths. We develop novel\nstrategies to train learning agents when deployed in the context of a planning\nsystem. We evaluate SPIRE on a suite of long-horizon and contact-rich robot\nmanipulation problems. We find that SPIRE outperforms prior approaches that\nintegrate imitation learning, reinforcement learning, and planning by 35% to\n50% in average task performance, is 6 times more data efficient in the number\nof human demonstrations needed to train proficient agents, and learns to\ncomplete tasks nearly twice as efficiently. View\nhttps://sites.google.com/view/spire-corl-2024 for more details.\n","authors":["Zihan Zhou","Animesh Garg","Dieter Fox","Caelan Garrett","Ajay Mandlekar"],"pdf_url":"https://arxiv.org/pdf/2410.18065v1.pdf","comment":"Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2410.18057v1","updated":"2024-10-23T17:30:50Z","published":"2024-10-23T17:30:50Z","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","summary":"  Machine Unlearning (MU) is critical for enhancing privacy and security in\ndeep learning models, particularly in large multimodal language models (MLLMs),\nby removing specific private or hazardous information. While MU has made\nsignificant progress in textual and visual modalities, multimodal unlearning\n(MMU) remains significantly underexplored, partially due to the absence of a\nsuitable open-source benchmark. To address this, we introduce CLEAR, a new\nbenchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We assess 10 MU methods,\nadapting them for MMU, and highlight new challenges specific to multimodal\nforgetting. We also demonstrate that simple $\\ell_1$ regularization on LoRA\nweights significantly mitigates catastrophic forgetting, preserving model\nperformance on retained data. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR\n","authors":["Alexey Dontsov","Dmitrii Korzh","Alexey Zhavoronkin","Boris Mikheev","Denis Bobkov","Aibek Alanov","Oleg Y. Rogov","Ivan Oseledets","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2410.18057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18052v1","updated":"2024-10-23T17:26:22Z","published":"2024-10-23T17:26:22Z","title":"In-Pixel Foreground and Contrast Enhancement Circuits with Customizable\n  Mapping","summary":"  This paper presents an innovative in-pixel contrast enhancement circuit that\nperforms image processing directly within the pixel circuit. The circuit can be\ntuned for different modes of operation. In foreground enhancement mode, it\nsuppresses low-intensity background pixels to nearly zero, isolating the\nforeground for better object visibility. In contrast enhancement mode, it\nimproves overall image contrast. The contrast enhancement function is\ncustomizable both during the design phase and in real-time, allowing the\ncircuit to adapt to specific applications and varying lighting conditions. A\nmodel of the designed pixel circuit is developed and applied to a full pixel\narray, demonstrating significant improvements in image quality. Simulations\nperformed in HSPICE show a nearly 6x increase in Michelson Contrast Ratio (CR)\nin the foreground enhancement mode. The simulation results indicate its\npotential for real-time, adaptive contrast enhancement across various imaging\nenvironments.\n","authors":["Md Rahatul Islam Udoy","Md Mazharul Islam","Elijah Johnson","Ahmedullah Aziz"],"pdf_url":"https://arxiv.org/pdf/2410.18052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18051v1","updated":"2024-10-23T17:25:26Z","published":"2024-10-23T17:25:26Z","title":"Real time anomalies detection on video","summary":"  Nowadays, many places use security cameras. Unfortunately, when an incident\noccurs, these technologies are used to show past events. So it can be\nconsidered as a deterrence tool than a detection tool. In this article, we will\npropose a deep learning approach trying to solve this problematic. This\napproach uses convolutional models (CNN) to extract relevant characteristics\nlinked to the video images, theses characteristics will form times series to be\nanalyzed by LSTM / GRU models.\n","authors":["Fabien Poirier"],"pdf_url":"https://arxiv.org/pdf/2410.18051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18013v1","updated":"2024-10-23T16:42:56Z","published":"2024-10-23T16:42:56Z","title":"Scalable Ranked Preference Optimization for Text-to-Image Generation","summary":"  Direct Preference Optimization (DPO) has emerged as a powerful approach to\nalign text-to-image (T2I) models with human feedback. Unfortunately, successful\napplication of DPO to T2I models requires a huge amount of resources to collect\nand label large-scale datasets, e.g., millions of generated paired images\nannotated with human preferences. In addition, these human preference datasets\ncan get outdated quickly as the rapid improvements of T2I models lead to higher\nquality images. In this work, we investigate a scalable approach for collecting\nlarge-scale and fully synthetic datasets for DPO training. Specifically, the\npreferences for paired images are generated using a pre-trained reward\nfunction, eliminating the need for involving humans in the annotation process,\ngreatly improving the dataset collection efficiency. Moreover, we demonstrate\nthat such datasets allow averaging predictions across multiple models and\ncollecting ranked preferences as opposed to pairwise preferences. Furthermore,\nwe introduce RankDPO to enhance DPO-based methods using the ranking feedback.\nApplying RankDPO on SDXL and SD3-Medium models with our synthetically generated\npreference dataset ``Syn-Pic'' improves both prompt-following (on benchmarks\nlike T2I-Compbench, GenEval, and DPG-Bench) and visual quality (through user\nstudies). This pipeline presents a practical and scalable solution to develop\nbetter preference datasets to enhance the performance of text-to-image models.\n","authors":["Shyamgopal Karthik","Huseyin Coskun","Zeynep Akata","Sergey Tulyakov","Jian Ren","Anil Kag"],"pdf_url":"https://arxiv.org/pdf/2410.18013v1.pdf","comment":"Project Page: https://snap-research.github.io/RankDPO/"},{"id":"http://arxiv.org/abs/2409.04429v2","updated":"2024-10-23T16:42:06Z","published":"2024-09-06T17:49:56Z","title":"VILA-U: a Unified Foundation Model Integrating Visual Understanding and\n  Generation","summary":"  VILA-U is a Unified foundation model that integrates Video, Image, Language\nunderstanding and generation. Traditional visual language models (VLMs) use\nseparate modules for understanding and generating visual content, which can\nlead to misalignment and increased complexity. In contrast, VILA-U employs a\nsingle autoregressive next-token prediction framework for both tasks,\neliminating the need for additional components like diffusion models. This\napproach not only simplifies the model but also achieves near state-of-the-art\nperformance in visual language understanding and generation. The success of\nVILA-U is attributed to two main factors: the unified vision tower that aligns\ndiscrete visual tokens with textual inputs during pretraining, which enhances\nvisual perception, and autoregressive image generation can achieve similar\nquality as diffusion models with high-quality dataset. This allows VILA-U to\nperform comparably to more complex models using a fully token-based\nautoregressive framework.\n","authors":["Yecheng Wu","Zhuoyang Zhang","Junyu Chen","Haotian Tang","Dacheng Li","Yunhao Fang","Ligeng Zhu","Enze Xie","Hongxu Yin","Li Yi","Song Han","Yao Lu"],"pdf_url":"https://arxiv.org/pdf/2409.04429v2.pdf","comment":"Code: https://github.com/mit-han-lab/vila-u. The first two authors\n  contributed equally to this work"},{"id":"http://arxiv.org/abs/2403.05489v2","updated":"2024-10-23T16:39:15Z","published":"2024-03-08T17:54:38Z","title":"JointMotion: Joint Self-Supervision for Joint Motion Prediction","summary":"  We present JointMotion, a self-supervised pre-training method for joint\nmotion prediction in self-driving vehicles. Our method jointly optimizes a\nscene-level objective connecting motion and environments, and an instance-level\nobjective to refine learned representations. Scene-level representations are\nlearned via non-contrastive similarity learning of past motion sequences and\nenvironment context. At the instance level, we use masked autoencoding to\nrefine multimodal polyline representations. We complement this with an adaptive\npre-training decoder that enables JointMotion to generalize across different\nenvironment representations, fusion mechanisms, and dataset characteristics.\nNotably, our method reduces the joint final displacement error of Wayformer,\nHPTR, and Scene Transformer models by 3\\%, 8\\%, and 12\\%, respectively; and\nenables transfer learning between the Waymo Open Motion and the Argoverse 2\nMotion Forecasting datasets. Code: https://github.com/kit-mrt/future-motion\n","authors":["Royden Wagner","Omer Sahin Tas","Marvin Klemp","Carlos Fernandez"],"pdf_url":"https://arxiv.org/pdf/2403.05489v2.pdf","comment":"CoRL'24 camera-ready"},{"id":"http://arxiv.org/abs/2309.17327v2","updated":"2024-10-23T16:25:17Z","published":"2023-09-29T15:34:39Z","title":"Telling Stories for Common Sense Zero-Shot Action Recognition","summary":"  Video understanding has long suffered from reliance on large labeled\ndatasets, motivating research into zero-shot learning. Recent progress in\nlanguage modeling presents opportunities to advance zero-shot video analysis,\nbut constructing an effective semantic space relating action classes remains\nchallenging. We address this by introducing a novel dataset, Stories, which\ncontains rich textual descriptions for diverse action classes extracted from\nWikiHow articles. For each class, we extract multi-sentence narratives\ndetailing the necessary steps, scenes, objects, and verbs that characterize the\naction. This contextual data enables modeling of nuanced relationships between\nactions, paving the way for zero-shot transfer. We also propose an approach\nthat harnesses Stories to improve feature generation for training zero-shot\nclassification. Without any target dataset fine-tuning, our method achieves new\nstate-of-the-art on multiple benchmarks, improving top-1 accuracy by up to\n6.1%. We believe Stories provides a valuable resource that can catalyze\nprogress in zero-shot action recognition. The textual narratives forge\nconnections between seen and unseen classes, overcoming the bottleneck of\nlabeled data that has long impeded advancements in this exciting domain. The\ndata can be found here: https://github.com/kini5gowda/Stories .\n","authors":["Shreyank N Gowda","Laura Sevilla-Lara"],"pdf_url":"https://arxiv.org/pdf/2309.17327v2.pdf","comment":"Accepted in ACCV 2024!"},{"id":"http://arxiv.org/abs/2410.03189v2","updated":"2024-10-23T16:22:59Z","published":"2024-10-04T07:02:13Z","title":"Generalizable Prompt Tuning for Vision-Language Models","summary":"  Prompt tuning for vision-language models such as CLIP involves optimizing the\ntext prompts used to generate image-text pairs for specific downstream tasks.\nWhile hand-crafted or template-based prompts are generally applicable to a\nwider range of unseen classes, they tend to perform poorly in downstream tasks\n(i.e., seen classes). Learnable soft prompts, on the other hand, often perform\nwell in downstream tasks but lack generalizability. Additionally, prior\nresearch has predominantly concentrated on the textual modality, with very few\nstudies attempting to explore the prompt's generalization potential from the\nvisual modality. Keeping these limitations in mind, we investigate how to\nprompt tuning to obtain both a competitive downstream performance and\ngeneralization. The study shows that by treating soft and hand-crafted prompts\nas dual views of the textual modality, and maximizing their mutual information,\nwe can better ensemble task-specific and general semantic information.\nMoreover, to generate more expressive prompts, the study introduces a\nclass-wise augmentation from the visual modality, resulting in significant\nrobustness to a wider range of unseen classes. Extensive evaluations on several\nbenchmarks report that the proposed approach achieves competitive results in\nterms of both task-specific performance and general abilities.\n","authors":["Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03189v2.pdf","comment":"in progress"},{"id":"http://arxiv.org/abs/2410.17997v1","updated":"2024-10-23T16:12:03Z","published":"2024-10-23T16:12:03Z","title":"Characterization of the multiplicity of solutions for camera pose given\n  two vertically-aligned landmarks and accelerometer","summary":"  We consider the problem of recovering the position and orientation of a\ncamera equipped with an accelerometer from sensor images of two labeled\nlandmarks whose positions in a coordinate system aligned in a known way with\ngravity are known. This a variant on the much studied P$n$P problem of\nrecovering camera position and orientation from $n$ points without any\ngravitational data. It is proved that in three types of singular cases there\nare infinitely many solutions, in another type of case there is one, and in a\nfinal type of case there are two. A precise characterization of each type of\ncase. In particular, there is always a unique solution in the practically\ninteresting case where the two landmarks are at the same altitude and the\ncamera is at a different altitude. This case is studied by numerical simulation\nand an implementation on a consumer cellphone. It is also proved that if the\ntwo landmarks are unlabeled, then apart from the same singular cases, there are\nstill always one or two solutions.\n","authors":["Alexander R. Pruss"],"pdf_url":"https://arxiv.org/pdf/2410.17997v1.pdf","comment":"32 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.19553v2","updated":"2024-10-23T16:06:32Z","published":"2024-07-28T18:20:08Z","title":"Exploring the Adversarial Robustness of CLIP for AI-generated Image\n  Detection","summary":"  In recent years, many forensic detectors have been proposed to detect\nAI-generated images and prevent their use for malicious purposes. Convolutional\nneural networks (CNNs) have long been the dominant architecture in this field\nand have been the subject of intense study. However, recently proposed\nTransformer-based detectors have been shown to match or even outperform\nCNN-based detectors, especially in terms of generalization. In this paper, we\nstudy the adversarial robustness of AI-generated image detectors, focusing on\nContrastive Language-Image Pretraining (CLIP)-based methods that rely on Visual\nTransformer (ViT) backbones and comparing their performance with CNN-based\nmethods. We study the robustness to different adversarial attacks under a\nvariety of conditions and analyze both numerical results and frequency-domain\npatterns. CLIP-based detectors are found to be vulnerable to white-box attacks\njust like CNN-based detectors. However, attacks do not easily transfer between\nCNN-based and CLIP-based methods. This is also confirmed by the different\ndistribution of the adversarial noise patterns in the frequency domain.\nOverall, this analysis provides new insights into the properties of forensic\ndetectors that can help to develop more effective strategies.\n","authors":["Vincenzo De Rosa","Fabrizio Guillaro","Giovanni Poggi","Davide Cozzolino","Luisa Verdoliva"],"pdf_url":"https://arxiv.org/pdf/2407.19553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17988v1","updated":"2024-10-23T16:01:31Z","published":"2024-10-23T16:01:31Z","title":"A Pipeline for Segmenting and Structuring RGB-D Data for Robotics\n  Applications","summary":"  We introduce a novel pipeline for segmenting and structuring color and depth\n(RGB-D) data. Existing processing pipelines for RGB-D data have focused on\nextracting geometric information alone. This approach precludes the development\nof more advanced robotic navigation and manipulation algorithms, which benefit\nfrom a semantic understanding of their environment. Our pipeline can segment\nRGB-D data into accurate semantic masks. These masks are then used to fuse raw\ncaptured point clouds into semantically separated point clouds. We store this\ninformation using the Universal Scene Description (USD) file format, a format\nsuitable for easy querying by downstream robotics algorithms, human-friendly\nvisualization, and robotics simulation.\n","authors":["Zhiwu Zheng","Lauren Mentzer","Berk Iskender","Michael Price","Colm Prendergast","Audren Cloitre"],"pdf_url":"https://arxiv.org/pdf/2410.17988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17983v1","updated":"2024-10-23T15:51:33Z","published":"2024-10-23T15:51:33Z","title":"Robust Two-View Geometry Estimation with Implicit Differentiation","summary":"  We present a novel two-view geometry estimation framework which is based on a\ndifferentiable robust loss function fitting. We propose to treat the robust\nfundamental matrix estimation as an implicit layer, which allows us to avoid\nbackpropagation through time and significantly improves the numerical\nstability. To take full advantage of the information from the feature matching\nstage we incorporate learnable weights that depend on the matching confidences.\nIn this way our solution brings together feature extraction, matching and\ntwo-view geometry estimation in a unified end-to-end trainable pipeline. We\nevaluate our approach on the camera pose estimation task in both outdoor and\nindoor scenarios. The experiments on several datasets show that the proposed\nmethod outperforms both classic and learning-based state-of-the-art methods by\na large margin. The project webpage is available at:\nhttps://github.com/VladPyatov/ihls\n","authors":["Vladislav Pyatov","Iaroslav Koshelev","Stamatis Lefkimmiatis"],"pdf_url":"https://arxiv.org/pdf/2410.17983v1.pdf","comment":"IROS 2024 Accepted"},{"id":"http://arxiv.org/abs/2410.17966v1","updated":"2024-10-23T15:34:06Z","published":"2024-10-23T15:34:06Z","title":"A Wavelet Diffusion GAN for Image Super-Resolution","summary":"  In recent years, diffusion models have emerged as a superior alternative to\ngenerative adversarial networks (GANs) for high-fidelity image generation, with\nwide applications in text-to-image generation, image-to-image translation, and\nsuper-resolution. However, their real-time feasibility is hindered by slow\ntraining and inference speeds. This study addresses this challenge by proposing\na wavelet-based conditional Diffusion GAN scheme for Single-Image\nSuper-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm to\nreduce the timesteps required by the reverse diffusion process and the Discrete\nWavelet Transform (DWT) to achieve dimensionality reduction, decreasing\ntraining and inference times significantly. The results of an experimental\nvalidation on the CelebA-HQ dataset confirm the effectiveness of our proposed\nscheme. Our approach outperforms other state-of-the-art methodologies\nsuccessfully ensuring high-fidelity output while overcoming inherent drawbacks\nassociated with diffusion models in time-sensitive applications.\n","authors":["Lorenzo Aloisi","Luigi Sigillo","Aurelio Uncini","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2410.17966v1.pdf","comment":"The paper has been accepted at Italian Workshop on Neural Networks\n  (WIRN) 2024"},{"id":"http://arxiv.org/abs/2408.15205v2","updated":"2024-10-23T15:33:47Z","published":"2024-08-27T17:06:22Z","title":"Leveraging Hallucinations to Reduce Manual Prompt Dependency in\n  Promptable Segmentation","summary":"  Promptable segmentation typically requires instance-specific manual prompts\nto guide the segmentation of each desired object. To minimize such a need,\ntask-generic promptable segmentation has been introduced, which employs a\nsingle task-generic prompt to segment various images of different objects in\nthe same task. Current methods use Multimodal Large Language Models (MLLMs) to\nreason detailed instance-specific prompts from a task-generic prompt for\nimproving segmentation accuracy. The effectiveness of this segmentation heavily\ndepends on the precision of these derived prompts. However, MLLMs often suffer\nhallucinations during reasoning, resulting in inaccurate prompting. While\nexisting methods focus on eliminating hallucinations to improve a model, we\nargue that MLLM hallucinations can reveal valuable contextual insights when\nleveraged correctly, as they represent pre-trained large-scale knowledge beyond\nindividual images. In this paper, we utilize hallucinations to mine\ntask-related information from images and verify its accuracy for enhancing\nprecision of the generated prompts. Specifically, we introduce an iterative\nPrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a\nmask generator.The prompt generator uses a multi-scale chain of thought\nprompting, initially exploring hallucinations for extracting extended\ncontextual knowledge on a test image.These hallucinations are then reduced to\nformulate precise instance-specific prompts, directing the mask generator to\nproduce masks that are consistent with task semantics by mask semantic\nalignment. The generated masks iteratively induce the prompt generator to focus\nmore on task-relevant image areas and reduce irrelevant hallucinations,\nresulting jointly in better prompts and masks. Experiments on 5 benchmarks\ndemonstrate the effectiveness of ProMaC. Code given in\nhttps://lwpyh.github.io/ProMaC/.\n","authors":["Jian Hu","Jiayi Lin","Junchi Yan","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2408.15205v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17959v1","updated":"2024-10-23T15:28:25Z","published":"2024-10-23T15:28:25Z","title":"Medical Imaging Complexity and its Effects on GAN Performance","summary":"  The proliferation of machine learning models in diverse clinical applications\nhas led to a growing need for high-fidelity, medical image training data. Such\ndata is often scarce due to cost constraints and privacy concerns. Alleviating\nthis burden, medical image synthesis via generative adversarial networks (GANs)\nemerged as a powerful method for synthetically generating photo-realistic\nimages based on existing sets of real medical images. However, the exact image\nset size required to efficiently train such a GAN is unclear. In this work, we\nexperimentally establish benchmarks that measure the relationship between a\nsample dataset size and the fidelity of the generated images, given the\ndataset's distribution of image complexities. We analyze statistical metrics\nbased on delentropy, an image complexity measure rooted in Shannon's entropy in\ninformation theory. For our pipeline, we conduct experiments with two\nstate-of-the-art GANs, StyleGAN 3 and SPADE-GAN, trained on multiple medical\nimaging datasets with variable sample sizes. Across both GANs, general\nperformance improved with increasing training set size but suffered with\nincreasing complexity.\n","authors":["William Cagas","Chan Ko","Blake Hsiao","Shryuk Grandhi","Rishi Bhattacharya","Kevin Zhu","Michael Lam"],"pdf_url":"https://arxiv.org/pdf/2410.17959v1.pdf","comment":"Accepted to ACCV, Workshop on Generative AI for Synthetic Medical\n  Data"},{"id":"http://arxiv.org/abs/2410.12018v2","updated":"2024-10-23T15:21:54Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v2.pdf","comment":"ACCV 2024 Oral"},{"id":"http://arxiv.org/abs/2406.14856v2","updated":"2024-10-23T15:08:59Z","published":"2024-06-21T04:02:19Z","title":"Accessible, At-Home Detection of Parkinson's Disease via Multi-task\n  Video Analysis","summary":"  Limited accessibility to neurological care leads to underdiagnosed\nParkinson's Disease (PD), preventing early intervention. Existing AI-based PD\ndetection methods primarily focus on unimodal analysis of motor or speech\ntasks, overlooking the multifaceted nature of the disease. To address this, we\nintroduce a large-scale, multi-task video dataset consisting of 1102 sessions\n(each containing videos of finger tapping, facial expression, and speech tasks\ncaptured via webcam) from 845 participants (272 with PD). We propose a novel\nUncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal\ndata to enhance diagnostic accuracy. UFNet employs independent task-specific\nnetworks, trained with Monte Carlo Dropout for uncertainty quantification,\nfollowed by self-attended fusion of features, with attention weights\ndynamically adjusted based on task-specific uncertainties. To ensure\npatient-centered evaluation, the participants were randomly split into three\nsets: 60% for training, 20% for model selection, and 20% for final performance\nevaluation. UFNet significantly outperformed single-task models in terms of\naccuracy, area under the ROC curve (AUROC), and sensitivity while maintaining\nnon-inferior specificity. Withholding uncertain predictions further boosted the\nperformance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9%\nsensitivity, and 92.6+-0.3% specificity, at the expense of not being able to\npredict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further\nanalysis suggests that the trained model does not exhibit any detectable bias\nacross sex and ethnic subgroups and is most effective for individuals aged\nbetween 50 and 80. Requiring only a webcam and microphone, our approach\nfacilitates accessible home-based PD screening, especially in regions with\nlimited healthcare resources.\n","authors":["Md Saiful Islam","Tariq Adnan","Jan Freyberg","Sangwu Lee","Abdelrahman Abdelkader","Meghan Pawlik","Cathe Schwartz","Karen Jaffe","Ruth B. Schneider","E Ray Dorsey","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2406.14856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17932v1","updated":"2024-10-23T14:54:48Z","published":"2024-10-23T14:54:48Z","title":"VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian\n  Splatting and Neural Points","summary":"  Recent advances in novel view synthesis (NVS), particularly neural radiance\nfields (NeRF) and Gaussian splatting (3DGS), have demonstrated impressive\nresults in photorealistic scene rendering. These techniques hold great\npotential for applications in virtual tourism and teleportation, where\nimmersive realism is crucial. However, the high-performance demands of virtual\nreality (VR) systems present challenges in directly utilizing even such\nfast-to-render scene representations like 3DGS due to latency and computational\nconstraints.\n  In this paper, we propose foveated rendering as a promising solution to these\nobstacles. We analyze state-of-the-art NVS methods with respect to their\nrendering performance and compatibility with the human visual system. Our\napproach introduces a novel foveated rendering approach for Virtual Reality,\nthat leverages the sharp, detailed output of neural point rendering for the\nfoveal region, fused with a smooth rendering of 3DGS for the peripheral vision.\n  Our evaluation confirms that perceived sharpness and detail-richness are\nincreased by our approach compared to a standard VR-ready 3DGS configuration.\nOur system meets the necessary performance requirements for real-time VR\ninteractions, ultimately enhancing the user's immersive experience.\n  Project page: https://lfranke.github.io/vr_splatting\n","authors":["Linus Franke","Laura Fink","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2410.17932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02240v4","updated":"2024-10-23T14:53:38Z","published":"2024-10-03T06:25:53Z","title":"SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial\n  Attack","summary":"  Deep neural network based systems deployed in sensitive environments are\nvulnerable to adversarial attacks. Unrestricted adversarial attacks typically\nmanipulate the semantic content of an image (e.g., color or texture) to create\nadversarial examples that are both effective and photorealistic. Recent works\nhave utilized the diffusion inversion process to map images into a latent\nspace, where high-level semantics are manipulated by introducing perturbations.\nHowever, they often results in substantial semantic distortions in the denoised\noutput and suffers from low efficiency. In this study, we propose a novel\nframework called Semantic-Consistent Unrestricted Adversarial Attacks (SCA),\nwhich employs an inversion method to extract edit-friendly noise maps and\nutilizes Multimodal Large Language Model (MLLM) to provide semantic guidance\nthroughout the process. Under the condition of rich semantic information\nprovided by MLLM, we perform the DDPM denoising process of each step using a\nseries of edit-friendly noise maps, and leverage DPM Solver++ to accelerate\nthis process, enabling efficient sampling with semantic consistency. Compared\nto existing methods, our framework enables the efficient generation of\nadversarial examples that exhibit minimal discernible semantic changes.\nConsequently, we for the first time introduce Semantic-Consistent Adversarial\nExamples (SCAE). Extensive experiments and visualizations have demonstrated the\nhigh efficiency of SCA, particularly in being on average 12 times faster than\nthe state-of-the-art attacks. Our research can further draw attention to the\nsecurity of multimedia information.\n","authors":["Zihao Pan","Weibin Wu","Yuhang Cao","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.02240v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08401v3","updated":"2024-10-23T14:48:44Z","published":"2024-04-12T11:15:15Z","title":"PnLCalib: Sports Field Registration via Points and Lines Optimization","summary":"  Camera calibration in broadcast sports videos presents numerous challenges\nfor accurate sports field registration due to multiple camera angles, varying\ncamera parameters, and frequent occlusions of the field. Traditional\nsearch-based methods depend on initial camera pose estimates, which can\nstruggle in non-standard positions and dynamic environments. In response, we\npropose an optimization-based calibration pipeline that leverages a 3D soccer\nfield model and a predefined set of keypoints to overcome these limitations.\nOur method also introduces a novel refinement module that improves initial\ncalibration by using detected field lines in a non-linear optimization process.\nThis approach outperforms existing techniques in both multi-view and\nsingle-view 3D camera calibration tasks, while maintaining competitive\nperformance in homography estimation. Extensive experimentation on real-world\nsoccer datasets, including SoccerNet-Calibration, WorldCup 2014, and\nTS-WorldCup, highlights the robustness and accuracy of our method across\ndiverse broadcast scenarios. Our approach offers significant improvements in\ncamera calibration precision and reliability.\n","authors":["Marc Gutiérrez-Pérez","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2404.08401v3.pdf","comment":"Extended version of \"No Bells, Just Whistles: Sports Field\n  Registration Leveraging Geometric Properties\""},{"id":"http://arxiv.org/abs/2312.05349v3","updated":"2024-10-23T14:47:10Z","published":"2023-12-08T20:12:26Z","title":"PixLore: A Dataset-driven Approach to Rich Image Captioning","summary":"  In the domain of vision-language integration, generating detailed image\ncaptions poses a significant challenge due to the lack of curated and rich\ndatasets. This study introduces PixLore, a novel method that leverages Querying\nTransformers through the fine-tuning of the BLIP-2 model using the LoRa method\non a standard commercial GPU. The followed approach, which involves training on\na carefully assembled dataset from state-of-the-art Computer Vision models\ncombined and augmented by ChatGPT, addresses the question of whether intricate\nimage understanding can be achieved with an ensemble of smaller-scale models,\nreferred to as Knowledge Stitching. Comparative evaluations against major\nmodels such as GPT-4 and Google Bard demonstrate that PixLore-2.7B, despite\nhaving considerably fewer parameters, is rated higher than the existing\nState-of-the-Art models in over half of the assessments. Precisely, PixLore\noutperform Bard and BLIP-2, which score approximately 35.18% and 27.98% lower\nthan PixLore in the task of image captioning. This research not only presents a\ngroundbreaking approach but also highlights the importance of well-curated\ndatasets in enhancing the performance of smaller models.\n","authors":["Diego Bonilla-Salvador","Marcelino Martínez-Sober","Joan Vila-Francés","Antonio José Serrano-López","Pablo Rodríguez-Belenguer","Fernando Mateo"],"pdf_url":"https://arxiv.org/pdf/2312.05349v3.pdf","comment":"Paper in preprint pending of publication"},{"id":"http://arxiv.org/abs/2402.17307v2","updated":"2024-10-23T14:42:07Z","published":"2024-02-27T08:31:39Z","title":"Denoising Diffusion Models for Inpainting of Healthy Brain Tissue","summary":"  This paper is a contribution to the \"BraTS 2023 Local Synthesis of Healthy\nBrain Tissue via Inpainting Challenge\". The task of this challenge is to\ntransform tumor tissue into healthy tissue in brain magnetic resonance (MR)\nimages. This idea originates from the problem that MR images can be evaluated\nusing automatic processing tools, however, many of these tools are optimized\nfor the analysis of healthy tissue. By solving the given inpainting task, we\nenable the automatic analysis of images featuring lesions, and further\ndownstream tasks. Our approach builds on denoising diffusion probabilistic\nmodels. We use a 2D model that is trained using slices in which healthy tissue\nwas cropped out and is learned to be inpainted again. This allows us to use the\nground truth healthy tissue during training. In the sampling stage, we replace\nthe slices containing diseased tissue in the original 3D volume with the slices\ncontaining the healthy tissue inpainting. With our approach, we achieve\ncomparable results to the competing methods. On the validation set our model\nachieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113. In\nfuture we plan to extend our 2D model to a 3D model, allowing to inpaint the\nregion of interest as a whole without losing context information of neighboring\nslices.\n","authors":["Alicia Durrer","Philippe C. Cattin","Julia Wolleb"],"pdf_url":"https://arxiv.org/pdf/2402.17307v2.pdf","comment":"12 pages, 5 figures, MICCAI challenge submission"},{"id":"http://arxiv.org/abs/2410.17920v1","updated":"2024-10-23T14:38:57Z","published":"2024-10-23T14:38:57Z","title":"Gaze-Assisted Medical Image Segmentation","summary":"  The annotation of patient organs is a crucial part of various diagnostic and\ntreatment procedures, such as radiotherapy planning. Manual annotation is\nextremely time-consuming, while its automation using modern image analysis\ntechniques has not yet reached levels sufficient for clinical adoption. This\npaper investigates the idea of semi-supervised medical image segmentation using\nhuman gaze as interactive input for segmentation correction. In particular, we\nfine-tuned the Segment Anything Model in Medical Images (MedSAM), a public\nsolution that uses various prompt types as additional input for semi-automated\nsegmentation correction. We used human gaze data from reading abdominal images\nas a prompt for fine-tuning MedSAM. The model was validated on a public WORD\ndatabase, which consists of 120 CT scans of 16 abdominal organs. The results of\nthe gaze-assisted MedSAM were shown to be superior to the results of the\nstate-of-the-art segmentation models. In particular, the average Dice\ncoefficient for 16 abdominal organs was 85.8%, 86.7%, 81.7%, and 90.5% for\nnnUNetV2, ResUNet, original MedSAM, and our gaze-assisted MedSAM model,\nrespectively.\n","authors":["Leila Khaertdinova","Ilya Pershin","Tatiana Shmykova","Bulat Ibragimov"],"pdf_url":"https://arxiv.org/pdf/2410.17920v1.pdf","comment":"16 pages, 4 figures, Accepted to AIM-FM Workshop @ NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.17918v1","updated":"2024-10-23T14:34:39Z","published":"2024-10-23T14:34:39Z","title":"Addressing Asynchronicity in Clinical Multimodal Fusion via\n  Individualized Chest X-ray Generation","summary":"  Integrating multi-modal clinical data, such as electronic health records\n(EHR) and chest X-ray images (CXR), is particularly beneficial for clinical\nprediction tasks. However, in a temporal setting, multi-modal data are often\ninherently asynchronous. EHR can be continuously collected but CXR is generally\ntaken with a much longer interval due to its high cost and radiation dose. When\nclinical prediction is needed, the last available CXR image might have been\noutdated, leading to suboptimal predictions. To address this challenge, we\npropose DDL-CXR, a method that dynamically generates an up-to-date latent\nrepresentation of the individualized CXR images. Our approach leverages latent\ndiffusion models for patient-specific generation strategically conditioned on a\nprevious CXR image and EHR time series, providing information regarding\nanatomical structures and disease progressions, respectively. In this way, the\ninteraction across modalities could be better captured by the latent CXR\ngeneration process, ultimately improving the prediction performance.\nExperiments using MIMIC datasets show that the proposed model could effectively\naddress asynchronicity in multimodal fusion and consistently outperform\nexisting methods.\n","authors":["Wenfang Yao","Chen Liu","Kejing Yin","William K. Cheung","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2410.17918v1.pdf","comment":"Accepted by NeurIPS-24"},{"id":"http://arxiv.org/abs/2403.08511v3","updated":"2024-10-23T14:21:40Z","published":"2024-03-13T13:16:26Z","title":"A Multimodal Fusion Network For Student Emotion Recognition Based on\n  Transformer and Tensor Product","summary":"  This paper introduces a new multi-modal model based on the Transformer\narchitecture and tensor product fusion strategy, combining BERT's text vectors\nand ViT's image vectors to classify students' psychological conditions, with an\naccuracy of 93.65%. The purpose of the study is to accurately analyze the\nmental health status of students from various data sources. This paper\ndiscusses modal fusion methods, including early, late and intermediate fusion,\nto overcome the challenges of integrating multi-modal information. Ablation\nstudies compare the performance of different models and fusion techniques,\nshowing that the proposed model outperforms existing methods such as CLIP and\nViLBERT in terms of accuracy and inference speed. Conclusions indicate that\nwhile this model has significant advantages in emotion recognition, its\npotential to incorporate other data modalities provides areas for future\nresearch.\n","authors":["Ao Xiang","Zongqing Qi","Han Wang","Qin Yang","Danqing Ma"],"pdf_url":"https://arxiv.org/pdf/2403.08511v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19472v2","updated":"2024-10-23T14:02:12Z","published":"2024-09-28T22:41:49Z","title":"Towards Croppable Implicit Neural Representations","summary":"  Implicit Neural Representations (INRs) have peaked interest in recent years\ndue to their ability to encode natural signals using neural networks. While\nINRs allow for useful applications such as interpolating new coordinates and\nsignal compression, their black-box nature makes it difficult to modify them\npost-training. In this paper we explore the idea of editable INRs, and\nspecifically focus on the widely used cropping operation. To this end, we\npresent Local-Global SIRENs -- a novel INR architecture that supports cropping\nby design. Local-Global SIRENs are based on combining local and global feature\nextraction for signal encoding. What makes their design unique is the ability\nto effortlessly remove specific portions of an encoded signal, with a\nproportional weight decrease. This is achieved by eliminating the corresponding\nweights from the network, without the need for retraining. We further show how\nthis architecture can be used to support the straightforward extension of\npreviously encoded signals. Beyond signal editing, we examine how the\nLocal-Global approach can accelerate training, enhance encoding of various\nsignals, improve downstream performance, and be applied to modern INRs such as\nINCODE, highlighting its potential and flexibility. Code is available at\nhttps://github.com/maorash/Local-Global-INRs.\n","authors":["Maor Ashkenazi","Eran Treister"],"pdf_url":"https://arxiv.org/pdf/2409.19472v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.06927v2","updated":"2024-10-23T14:01:27Z","published":"2024-08-13T14:29:00Z","title":"Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class\n  Feature Compensator","summary":"  Dataset distillation has emerged as a technique aiming to condense\ninformative features from large, natural datasets into a compact and synthetic\nform. While recent advancements have refined this technique, its performance is\nbottlenecked by the prevailing class-specific synthesis paradigm. Under this\nparadigm, synthetic data is optimized exclusively for a pre-assigned one-hot\nlabel, creating an implicit class barrier in feature condensation. This leads\nto inefficient utilization of the distillation budget and oversight of\ninter-class feature distributions, which ultimately limits the effectiveness\nand efficiency, as demonstrated in our analysis. To overcome these constraints,\nthis paper presents the Inter-class Feature Compensator (INFER), an innovative\ndistillation approach that transcends the class-specific data-label framework\nwidely utilized in current dataset distillation methods. Specifically, INFER\nleverages a Universal Feature Compensator (UFC) to enhance feature integration\nacross classes, enabling the generation of multiple additional synthetic\ninstances from a single UFC input. This significantly improves the efficiency\nof the distillation budget. Moreover, INFER enriches inter-class interactions\nduring the distillation, thereby enhancing the effectiveness and\ngeneralizability of the distilled data. By allowing for the linear\ninterpolation of labels similar to those in the original dataset, INFER\nmeticulously optimizes the synthetic data and dramatically reduces the size of\nsoft labels in the synthetic dataset to almost zero, establishing a new\nbenchmark for efficiency and effectiveness in dataset distillation.\n","authors":["Xin Zhang","Jiawei Du","Ping Liu","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.06927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17885v1","updated":"2024-10-23T13:58:39Z","published":"2024-10-23T13:58:39Z","title":"R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric\n  Reasoning in Large Multimodal Models","summary":"  Existing Large Multimodal Models (LMMs) struggle with mathematical geometric\nreasoning due to a lack of high-quality image-text paired data. Current\ngeometric data generation approaches, which apply preset templates to generate\ngeometric data or use Large Language Models (LLMs) to rephrase questions and\nanswers (Q&A), unavoidably limit data accuracy and diversity. To synthesize\nhigher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT)\ngeometry problem generation pipeline. First, we introduce GeoChain to produce\nhigh-fidelity geometric images and corresponding descriptions highlighting\nrelations among geometric elements. We then design a Reverse A&Q method that\nreasons step-by-step based on the descriptions and generates questions in\nreverse from the reasoning results. Experiments demonstrate that the proposed\nmethod brings significant and consistent improvements on multiple LMM\nbaselines, achieving new performance records in the 2B, 7B, and 8B settings.\nNotably, R-CoT-8B significantly outperforms previous state-of-the-art\nopen-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while\nalso surpassing the closed-source model GPT-4o by an average of 13% across both\ndatasets. The code is available at https://github.com/dle666/R-CoT.\n","authors":["Linger Deng","Yuliang Liu","Bohan Li","Dongliang Luo","Liang Wu","Chengquan Zhang","Pengyuan Lyu","Ziyang Zhang","Gang Zhang","Errui Ding","Yingying Zhu","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2410.17885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17880v1","updated":"2024-10-23T13:52:22Z","published":"2024-10-23T13:52:22Z","title":"A utility-based spatial analysis of residential street-level conditions;\n  A case study of Rotterdam","summary":"  Residential location choices are traditionally modelled using factors related\nto accessibility and socioeconomic environments, neglecting the importance of\nlocal street-level conditions. Arguably, this neglect is due to data practices.\nToday, however, street-level images -- which are highly effective at encoding\nstreet-level conditions -- are widely available. Additionally, recent advances\nin discrete choice models incorporating computer vision capabilities offer\nopportunities to integrate street-level conditions into residential location\nchoice analysis. This study leverages these developments to investigate the\nspatial distribution of utility derived from street-level conditions in\nresidential location choices on a city-wide scale. In our case study of\nRotterdam, the Netherlands, we find that the utility derived from street-level\nconditions varies significantly on a highly localised scale, with conditions\nrapidly changing even within neighbourhoods. Our results also reveal that the\nhigh real-estate prices in the city centre cannot be attributed to attractive\nstreet-level conditions. Furthermore, whereas the city centre is characterised\nby relatively unattractive residential street-level conditions, neighbourhoods\nin the southern part of the city -- often perceived as problematic -- exhibit\nsurprisingly appealing street-level environments. The methodological\ncontribution of this paper is that it advances the discrete choice models\nincorporating computer vision capabilities by introducing a semantic\nregularisation layer to the model. Thereby, it adds explainability and\neliminates the need for a separate pipeline to extract information from images,\nstreamlining the analysis. As such, this paper's findings and methodological\nadvancements pave the way for further studies to explore integrating\nstreet-level conditions in urban planning.\n","authors":["Sander van Cranenburgh","Francisco Garrido-Valenzuela"],"pdf_url":"https://arxiv.org/pdf/2410.17880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17863v1","updated":"2024-10-23T13:35:18Z","published":"2024-10-23T13:35:18Z","title":"CASCRNet: An Atrous Spatial Pyramid Pooling and Shared Channel Residual\n  based Network for Capsule Endoscopy","summary":"  This manuscript summarizes work on the Capsule Vision Challenge 2024 by\nMISAHUB. To address the multi-class disease classification task, which is\nchallenging due to the complexity and imbalance in the Capsule Vision challenge\ndataset, this paper proposes CASCRNet (Capsule endoscopy-Aspp-SCR-Network), a\nparameter-efficient and novel model that uses Shared Channel Residual (SCR)\nblocks and Atrous Spatial Pyramid Pooling (ASPP) blocks. Further, the\nperformance of the proposed model is compared with other well-known approaches.\nThe experimental results yield that proposed model provides better disease\nclassification results. The proposed model was successful in classifying\ndiseases with an F1 Score of 78.5% and a Mean AUC of 98.3%, which is promising\ngiven its compact architecture.\n","authors":["K V Srinanda","M Manvith Prabhu","Shyam Lal"],"pdf_url":"https://arxiv.org/pdf/2410.17863v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.17858v1","updated":"2024-10-23T13:29:02Z","published":"2024-10-23T13:29:02Z","title":"Blendify -- Python rendering framework for Blender","summary":"  With the rapid growth of the volume of research fields like computer vision\nand computer graphics, researchers require effective and user-friendly\nrendering tools to visualize results. While advanced tools like Blender offer\npowerful capabilities, they also require a significant effort to master. This\ntechnical report introduces Blendify, a lightweight Python-based framework that\nseamlessly integrates with Blender, providing a high-level API for scene\ncreation and rendering. Blendify reduces the complexity of working with\nBlender's native API by automating object creation, handling the colors and\nmaterial linking, and implementing features such as shadow-catcher objects\nwhile maintaining support for high-quality ray-tracing rendering output. With a\nfocus on usability Blendify enables efficient and flexible rendering workflow\nfor rendering in common computer vision and computer graphics use cases. The\ncode is available at https://github.com/ptrvilya/blendify\n","authors":["Vladimir Guzov","Ilya A. Petrov","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2410.17858v1.pdf","comment":"Project page: https://virtualhumans.mpi-inf.mpg.de/blendify/"},{"id":"http://arxiv.org/abs/2410.17856v1","updated":"2024-10-23T13:26:59Z","published":"2024-10-23T13:26:59Z","title":"ROCKET-1: Master Open-World Interaction with Visual-Temporal Context\n  Prompting","summary":"  Vision-language models (VLMs) have excelled in multimodal tasks, but adapting\nthem to embodied decision-making in open-world environments presents\nchallenges. A key issue is the difficulty in smoothly connecting individual\nentities in low-level observations with abstract concepts required for\nplanning. A common approach to address this problem is through the use of\nhierarchical agents, where VLMs serve as high-level reasoners that break down\ntasks into executable sub-tasks, typically specified using language and\nimagined observations. However, language often fails to effectively convey\nspatial information, while generating future images with sufficient accuracy\nremains challenging. To address these limitations, we propose visual-temporal\ncontext prompting, a novel communication protocol between VLMs and policy\nmodels. This protocol leverages object segmentation from both past and present\nobservations to guide policy-environment interactions. Using this approach, we\ntrain ROCKET-1, a low-level policy that predicts actions based on concatenated\nvisual observations and segmentation masks, with real-time object tracking\nprovided by SAM-2. Our method unlocks the full potential of VLMs\nvisual-language reasoning abilities, enabling them to solve complex creative\ntasks, especially those heavily reliant on spatial understanding. Experiments\nin Minecraft demonstrate that our approach allows agents to accomplish\npreviously unattainable tasks, highlighting the effectiveness of\nvisual-temporal context prompting in embodied decision-making. Codes and demos\nwill be available on the project page: https://craftjarvis.github.io/ROCKET-1.\n","authors":["Shaofei Cai","Zihao Wang","Kewei Lian","Zhancun Mu","Xiaojian Ma","Anji Liu","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2410.17856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17855v1","updated":"2024-10-23T13:26:19Z","published":"2024-10-23T13:26:19Z","title":"TAGE: Trustworthy Attribute Group Editing for Stable Few-shot Image\n  Generation","summary":"  Generative Adversarial Networks (GANs) have emerged as a prominent research\nfocus for image editing tasks, leveraging the powerful image generation\ncapabilities of the GAN framework to produce remarkable results.However,\nprevailing approaches are contingent upon extensive training datasets and\nexplicit supervision, presenting a significant challenge in manipulating the\ndiverse attributes of new image classes with limited sample availability. To\nsurmount this hurdle, we introduce TAGE, an innovative image generation network\ncomprising three integral modules: the Codebook Learning Module (CLM), the Code\nPrediction Module (CPM) and the Prompt-driven Semantic Module (PSM). The CPM\nmodule delves into the semantic dimensions of category-agnostic attributes,\nencapsulating them within a discrete codebook. This module is predicated on the\nconcept that images are assemblages of attributes, and thus, by editing these\ncategory-independent attributes, it is theoretically possible to generate\nimages from unseen categories. Subsequently, the CPM module facilitates\nnaturalistic image editing by predicting indices of category-independent\nattribute vectors within the codebook. Additionally, the PSM module generates\nsemantic cues that are seamlessly integrated into the Transformer architecture\nof the CPM, enhancing the model's comprehension of the targeted attributes for\nediting. With these semantic cues, the model can generate images that\naccentuate desired attributes more prominently while maintaining the integrity\nof the original category, even with a limited number of samples. We have\nconducted extensive experiments utilizing the Animal Faces, Flowers, and\nVGGFaces datasets. The results of these experiments demonstrate that our\nproposed method not only achieves superior performance but also exhibits a high\ndegree of stability when compared to other few-shot image generation\ntechniques.\n","authors":["Ruicheng Zhang","Guoheng Huang","Yejing Huo","Xiaochen Yuan","Zhizhen Zhou","Xuhang Chen","Guo Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.17855v1.pdf","comment":"Accepted by International Conference on Signal Processing Systems\n  Conference"},{"id":"http://arxiv.org/abs/2410.15613v2","updated":"2024-10-23T13:07:41Z","published":"2024-10-21T03:17:25Z","title":"Exploring Stronger Transformer Representation Learning for Occluded\n  Person Re-Identification","summary":"  Due to some complex factors (e.g., occlusion, pose variation and diverse\ncamera perspectives), extracting stronger feature representation in person\nre-identification remains a challenging task. In this paper, we proposed a\nnovel self-supervision and supervision combining transformer-based person\nre-identification framework, namely SSSC-TransReID. Different from the general\ntransformer-based person re-identification models, we designed a\nself-supervised contrastive learning branch, which can enhance the feature\nrepresentation for person re-identification without negative samples or\nadditional pre-training. In order to train the contrastive learning branch, we\nalso proposed a novel random rectangle mask strategy to simulate the occlusion\nin real scenes, so as to enhance the feature representation for occlusion.\nFinally, we utilized the joint-training loss function to integrate the\nadvantages of supervised learning with ID tags and self-supervised contrastive\nlearning without negative samples, which can reinforce the ability of our model\nto excavate stronger discriminative features, especially for occlusion.\nExtensive experimental results on several benchmark datasets show our proposed\nmodel obtains superior Re-ID performance consistently and outperforms the\nstate-of-the-art ReID methods by large margins on the mean average accuracy\n(mAP) and Rank-1 accuracy.\n","authors":["Zhangjian Ji","Donglin Cheng","Kai Feng"],"pdf_url":"https://arxiv.org/pdf/2410.15613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17839v1","updated":"2024-10-23T13:05:26Z","published":"2024-10-23T13:05:26Z","title":"Few-shot NeRF by Adaptive Rendering Loss Regularization","summary":"  Novel view synthesis with sparse inputs poses great challenges to Neural\nRadiance Field (NeRF). Recent works demonstrate that the frequency\nregularization of Positional Encoding (PE) can achieve promising results for\nfew-shot NeRF. In this work, we reveal that there exists an inconsistency\nbetween the frequency regularization of PE and rendering loss. This prevents\nfew-shot NeRF from synthesizing higher-quality novel views. To mitigate this\ninconsistency, we propose Adaptive Rendering loss regularization for few-shot\nNeRF, dubbed AR-NeRF. Specifically, we present a two-phase rendering\nsupervision and an adaptive rendering loss weight learning strategy to align\nthe frequency relationship between PE and 2D-pixel supervision. In this way,\nAR-NeRF can learn global structures better in the early training phase and\nadaptively learn local details throughout the training process. Extensive\nexperiments show that our AR-NeRF achieves state-of-the-art performance on\ndifferent datasets, including object-level and complex scenes.\n","authors":["Qingshan Xu","Xuanyu Yi","Jianyao Xu","Wenbing Tao","Yew-Soon Ong","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17839v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2410.13896v2","updated":"2024-10-23T13:01:22Z","published":"2024-10-15T02:41:52Z","title":"From Real Artifacts to Virtual Reference: A Robust Framework for\n  Translating Endoscopic Images","summary":"  Domain adaptation, which bridges the distributions across different\nmodalities, plays a crucial role in multimodal medical image analysis. In\nendoscopic imaging, combining pre-operative data with intra-operative imaging\nis important for surgical planning and navigation. However, existing domain\nadaptation methods are hampered by distribution shift caused by in vivo\nartifacts, necessitating robust techniques for aligning noisy and artifact\nabundant patient endoscopic videos with clean virtual images reconstructed from\npre-operative tomographic data for pose estimation during intraoperative\nguidance. This paper presents an artifact-resilient image translation method\nand an associated benchmark for this purpose. The method incorporates a novel\n``local-global'' translation framework and a noise-resilient feature extraction\nstrategy. For the former, it decouples the image translation process into a\nlocal step for feature denoising, and a global step for global style transfer.\nFor feature extraction, a new contrastive learning strategy is proposed, which\ncan extract noise-resilient features for establishing robust correspondence\nacross domains. Detailed validation on both public and in-house clinical\ndatasets has been conducted, demonstrating significantly improved performance\ncompared to the current state-of-the-art.\n","authors":["Junyang Wu","Fangfang Xie","Jiayuan Sun","Yun Gu","Guang-Zhong Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14774v2","updated":"2024-10-23T13:01:14Z","published":"2024-03-21T18:28:43Z","title":"Few-Shot Adversarial Prompt Learning on Vision-Language Models","summary":"  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data. Code is available at:\nhttps://github.com/lionel-w2/FAP.\n","authors":["Yiwei Zhou","Xiaobo Xia","Zhiwei Lin","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14774v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.13152v3","updated":"2024-10-23T12:56:05Z","published":"2024-05-21T18:45:18Z","title":"Enhancing Interaction Modeling with Agent Selection and Physical\n  Coefficient for Trajectory Prediction","summary":"  A thorough understanding of the interaction between the target agent and\nsurrounding agents is a prerequisite for accurate trajectory prediction.\nAlthough many methods have been explored, they all assign correlation\ncoefficients to surrounding agents in a purely learning-based manner. In this\nstudy, we present ASPILin, which manually selects interacting agents and\ncalculates their correlations instead of attention scores. Surprisingly, these\nsimple modifications can significantly improve prediction performance and\nsubstantially reduce computational costs. Additionally, ASPILin models the\ninteracting agents at each past time step separately, rather than only modeling\nthe interacting agents at the current time step. This clarifies the causal\nchain of the target agent's historical trajectory and helps the model better\nunderstand dynamic interactions. We intentionally simplified our model in other\naspects, such as map encoding. Remarkably, experiments conducted on the\nINTERACTION, highD, and CitySim datasets demonstrate that our method is\nefficient and straightforward, outperforming other state-of-the-art methods.\n","authors":["Shiji Huang","Lei Ye","Min Chen","Wenhai Luo","Dihong Wang","Chenqi Xu","Deyuan Liang"],"pdf_url":"https://arxiv.org/pdf/2405.13152v3.pdf","comment":"code:https://github.com/kkk00714/ASPILin"},{"id":"http://arxiv.org/abs/2410.17832v1","updated":"2024-10-23T12:51:07Z","published":"2024-10-23T12:51:07Z","title":"Exploiting Text-Image Latent Spaces for the Description of Visual\n  Concepts","summary":"  Concept Activation Vectors (CAVs) offer insights into neural network\ndecision-making by linking human friendly concepts to the model's internal\nfeature extraction process. However, when a new set of CAVs is discovered, they\nmust still be translated into a human understandable description. For\nimage-based neural networks, this is typically done by visualizing the most\nrelevant images of a CAV, while the determination of the concept is left to\nhumans. In this work, we introduce an approach to aid the interpretation of\nnewly discovered concept sets by suggesting textual descriptions for each CAV.\nThis is done by mapping the most relevant images representing a CAV into a\ntext-image embedding where a joint description of these relevant images can be\ncomputed. We propose utilizing the most relevant receptive fields instead of\nfull images encoded. We demonstrate the capabilities of this approach in\nmultiple experiments with and without given CAV labels, showing that the\nproposed approach provides accurate descriptions for the CAVs and reduces the\nchallenge of concept interpretation.\n","authors":["Laines Schmalwasser","Jakob Gawlikowski","Joachim Denzler","Julia Niebling"],"pdf_url":"https://arxiv.org/pdf/2410.17832v1.pdf","comment":"19 pages, 7 figures, to be published in ICPR"},{"id":"http://arxiv.org/abs/2210.01708v4","updated":"2024-10-23T12:50:18Z","published":"2022-10-04T16:08:54Z","title":"Conquering the Communication Constraints to Enable Large Pre-Trained\n  Models in Federated Learning","summary":"  Federated learning (FL) has emerged as a promising paradigm for enabling the\ncollaborative training of models without centralized access to the raw data on\nlocal devices. In the typical FL paradigm (e.g., FedAvg), model weights are\nsent to and from the server each round to participating clients. Recently, the\nuse of small pre-trained models has been shown effective in federated learning\noptimization and improving convergence. However, recent state-of-the-art\npre-trained models are getting more capable but also have more parameters. In\nconventional FL, sharing the enormous model weights can quickly put a massive\ncommunication burden on the system, especially if more capable models are\nemployed. Can we find a solution to enable those strong and readily-available\npre-trained models in FL to achieve excellent performance while simultaneously\nreducing the communication burden? To this end, we investigate the use of\nparameter-efficient fine-tuning in federated learning and thus introduce a new\nframework: FedPEFT. Specifically, we systemically evaluate the performance of\nFedPEFT across a variety of client stability, data distribution, and\ndifferential privacy settings. By only locally tuning and globally sharing a\nsmall portion of the model weights, significant reductions in the total\ncommunication overhead can be achieved while maintaining competitive or even\nbetter performance in a wide range of federated learning scenarios, providing\ninsight into a new paradigm for practical and effective federated systems.\n","authors":["Guangyu Sun","Umar Khalid","Matias Mendieta","Taojiannan Yang","Pu Wang","Minwoo Lee","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2210.01708v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17823v1","updated":"2024-10-23T12:32:21Z","published":"2024-10-23T12:32:21Z","title":"Att2CPC: Attention-Guided Lossy Attribute Compression of Point Clouds","summary":"  With the great progress of 3D sensing and acquisition technology, the volume\nof point cloud data has grown dramatically, which urges the development of\nefficient point cloud compression methods. In this paper, we focus on the task\nof learned lossy point cloud attribute compression (PCAC). We propose an\nefficient attention-based method for lossy compression of point cloud\nattributes leveraging on an autoencoder architecture. Specifically, at the\nencoding side, we conduct multiple downsampling to best exploit the local\nattribute patterns, in which effective External Cross Attention (ECA) is\ndevised to hierarchically aggregate features by intergrating attributes and\ngeometry contexts. At the decoding side, the attributes of the point cloud are\nprogressively reconstructed based on the multi-scale representation and the\nzero-padding upsampling tactic. To the best of our knowledge, this is the first\napproach to introduce attention mechanism to point-based lossy PCAC task. We\nverify the compression efficiency of our model on various sequences, including\nhuman body frames, sparse objects, and large-scale point cloud scenes.\nExperiments show that our method achieves an average improvement of 1.15 dB and\n2.13 dB in BD-PSNR of Y channel and YUV channel, respectively, when comparing\nwith the state-of-the-art point-based method Deep-PCAC. Codes of this paper are\navailable at https://github.com/I2-Multimedia-Lab/Att2CPC.\n","authors":["Kai Liu","Kang You","Pan Gao","Manoranjan Paul"],"pdf_url":"https://arxiv.org/pdf/2410.17823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17822v1","updated":"2024-10-23T12:32:20Z","published":"2024-10-23T12:32:20Z","title":"DREB-Net: Dual-stream Restoration Embedding Blur-feature Fusion Network\n  for High-mobility UAV Object Detection","summary":"  Object detection algorithms are pivotal components of unmanned aerial vehicle\n(UAV) imaging systems, extensively employed in complex fields. However, images\ncaptured by high-mobility UAVs often suffer from motion blur cases, which\nsignificantly impedes the performance of advanced object detection algorithms.\nTo address these challenges, we propose an innovative object detection\nalgorithm specifically designed for blurry images, named DREB-Net (Dual-stream\nRestoration Embedding Blur-feature Fusion Network). First, DREB-Net addresses\nthe particularities of blurry image object detection problem by incorporating a\nBlurry image Restoration Auxiliary Branch (BRAB) during the training phase.\nSecond, it fuses the extracted shallow features via Multi-level\nAttention-Guided Feature Fusion (MAGFF) module, to extract richer features.\nHere, the MAGFF module comprises local attention modules and global attention\nmodules, which assign different weights to the branches. Then, during the\ninference phase, the deep feature extraction of the BRAB can be removed to\nreduce computational complexity and improve detection speed. In loss function,\na combined loss of MSE and SSIM is added to the BRAB to restore blurry images.\nFinally, DREB-Net introduces Fast Fourier Transform in the early stages of\nfeature extraction, via a Learnable Frequency domain Amplitude Modulation\nModule (LFAMM), to adjust feature amplitude and enhance feature processing\ncapability. Experimental results indicate that DREB-Net can still effectively\nperform object detection tasks under motion blur in captured images, showcasing\nexcellent performance and broad application prospects. Our source code will be\navailable at https://github.com/EEIC-Lab/DREB-Net.git.\n","authors":["Qingpeng Li","Yuxin Zhang","Leyuan Fang","Yuhan Kang","Shutao Li","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.17822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17816v1","updated":"2024-10-23T12:19:12Z","published":"2024-10-23T12:19:12Z","title":"Deep Learning for Active Region Classification: A Systematic Study from\n  Convolutional Neural Networks to Vision Transformers","summary":"  A solar active region can significantly disrupt the Sun Earth space\nenvironment, often leading to severe space weather events such as solar flares\nand coronal mass ejections. As a consequence, the automatic classification of\nactive region groups is the crucial starting point for accurately and promptly\npredicting solar activity. This study presents our results concerned with the\napplication of deep learning techniques to the classification of active region\ncutouts based on the Mount Wilson classification scheme. Specifically, we have\nexplored the latest advancements in image classification architectures, from\nConvolutional Neural Networks to Vision Transformers, and reported on their\nperformances for the active region classification task, showing that the\ncrucial point for their effectiveness consists in a robust training process\nbased on the latest advances in the field.\n","authors":["Edoardo Legnaro","Sabrina Guastavino","Michele Piana","Anna Maria Massone"],"pdf_url":"https://arxiv.org/pdf/2410.17816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17814v1","updated":"2024-10-23T12:18:36Z","published":"2024-10-23T12:18:36Z","title":"Learning Lossless Compression for High Bit-Depth Volumetric Medical\n  Image","summary":"  Recent advances in learning-based methods have markedly enhanced the\ncapabilities of image compression. However, these methods struggle with high\nbit-depth volumetric medical images, facing issues such as degraded\nperformance, increased memory demand, and reduced processing speed. To address\nthese challenges, this paper presents the Bit-Division based Lossless\nVolumetric Image Compression (BD-LVIC) framework, which is tailored for high\nbit-depth medical volume compression. The BD-LVIC framework skillfully divides\nthe high bit-depth volume into two lower bit-depth segments: the Most\nSignificant Bit-Volume (MSBV) and the Least Significant Bit-Volume (LSBV). The\nMSBV concentrates on the most significant bits of the volumetric medical image,\ncapturing vital structural details in a compact manner. This reduction in\ncomplexity greatly improves compression efficiency using traditional codecs.\nConversely, the LSBV deals with the least significant bits, which encapsulate\nintricate texture details. To compress this detailed information effectively,\nwe introduce an effective learning-based compression model equipped with a\nTransformer-Based Feature Alignment Module, which exploits both intra-slice and\ninter-slice redundancies to accurately align features. Subsequently, a Parallel\nAutoregressive Coding Module merges these features to precisely estimate the\nprobability distribution of the least significant bit-planes. Our extensive\ntesting demonstrates that the BD-LVIC framework not only sets new performance\nbenchmarks across various datasets but also maintains a competitive coding\nspeed, highlighting its significant potential and practical utility in the\nrealm of volumetric medical image compression.\n","authors":["Kai Wang","Yuanchao Bai","Daxin Li","Deming Zhai","Junjun Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17814v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2410.17812v1","updated":"2024-10-23T12:17:03Z","published":"2024-10-23T12:17:03Z","title":"PGDiffSeg: Prior-Guided Denoising Diffusion Model with Parameter-Shared\n  Attention for Breast Cancer Segmentation","summary":"  Early detection through imaging and accurate diagnosis is crucial in\nmitigating the high mortality rate associated with breast cancer. However,\nlocating tumors from low-resolution and high-noise medical images is extremely\nchallenging. Therefore, this paper proposes a novel PGDiffSeg (Prior-Guided\nDiffusion Denoising Model with Parameter-Shared Attention) that applies\ndiffusion denoising methods to breast cancer medical image segmentation,\naccurately recovering the affected areas from Gaussian noise. Firstly, we\ndesign a parallel pipeline for noise processing and semantic information\nprocessing and propose a parameter-shared attention module (PSA) in multi-layer\nthat seamlessly integrates these two pipelines. This integration empowers\nPGDiffSeg to incorporate semantic details at multiple levels during the\ndenoising process, producing highly accurate segmentation maps. Secondly, we\nintroduce a guided strategy that leverages prior knowledge to simulate the\ndecision-making process of medical professionals, thereby enhancing the model's\nability to locate tumor positions precisely. Finally, we provide the first-ever\ndiscussion on the interpretability of the generative diffusion model in the\ncontext of breast cancer segmentation. Extensive experiments have demonstrated\nthe superiority of our model over the current state-of-the-art approaches,\nconfirming its effectiveness as a flexible diffusion denoising method suitable\nfor medical image research. Our code will be publicly available later.\n","authors":["Feiyan Feng","Tianyu Liu","Hong Wang","Jun Zhao","Wei Li","Yanshen Sun"],"pdf_url":"https://arxiv.org/pdf/2410.17812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17810v1","updated":"2024-10-23T12:12:56Z","published":"2024-10-23T12:12:56Z","title":"EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning","summary":"  Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin.\n","authors":["Yaxiong Wang","Yaxiong Wang","Lianwei Wu","Lechao Cheng","Zhun Zhong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17809v1","updated":"2024-10-23T12:11:26Z","published":"2024-10-23T12:11:26Z","title":"An Intelligent Agentic System for Complex Image Restoration Problems","summary":"  Real-world image restoration (IR) is inherently complex and often requires\ncombining multiple specialized models to address diverse degradations. Inspired\nby human problem-solving, we propose AgenticIR, an agentic system that mimics\nthe human approach to image processing by following five key stages:\nPerception, Scheduling, Execution, Reflection, and Rescheduling. AgenticIR\nleverages large language models (LLMs) and vision-language models (VLMs) that\ninteract via text generation to dynamically operate a toolbox of IR models. We\nfine-tune VLMs for image quality analysis and employ LLMs for reasoning,\nguiding the system step by step. To compensate for LLMs' lack of specific IR\nknowledge and experience, we introduce a self-exploration method, allowing the\nLLM to observe and summarize restoration results into referenceable documents.\nExperiments demonstrate AgenticIR's potential in handling complex IR tasks,\nrepresenting a promising path toward achieving general intelligence in visual\nprocessing.\n","authors":["Kaiwen Zhu","Jinjin Gu","Zhiyuan You","Yu Qiao","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2410.17809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17802v1","updated":"2024-10-23T11:59:49Z","published":"2024-10-23T11:59:49Z","title":"GenUDC: High Quality 3D Mesh Generation with Unsigned Dual Contouring\n  Representation","summary":"  Generating high-quality meshes with complex structures and realistic surfaces\nis the primary goal of 3D generative models. Existing methods typically employ\nsequence data or deformable tetrahedral grids for mesh generation. However,\nsequence-based methods have difficulty producing complex structures with many\nfaces due to memory limits. The deformable tetrahedral grid-based method\nMeshDiffusion fails to recover realistic surfaces due to the inherent ambiguity\nin deformable grids. We propose the GenUDC framework to address these\nchallenges by leveraging the Unsigned Dual Contouring (UDC) as the mesh\nrepresentation. UDC discretizes a mesh in a regular grid and divides it into\nthe face and vertex parts, recovering both complex structures and fine details.\nAs a result, the one-to-one mapping between UDC and mesh resolves the ambiguity\nproblem. In addition, GenUDC adopts a two-stage, coarse-to-fine generative\nprocess for 3D mesh generation. It first generates the face part as a rough\nshape and then the vertex part to craft a detailed shape. Extensive evaluations\ndemonstrate the superiority of UDC as a mesh representation and the favorable\nperformance of GenUDC in mesh generation. The code and trained models are\navailable at https://github.com/TrepangCat/GenUDC.\n","authors":["Ruowei Wang","Jiaqi Li","Dan Zeng","Xueqi Ma","Zixiang Xu","Jianwei Zhang","Qijun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.17802v1.pdf","comment":"ACMMM 2024, code:https://github.com/TrepangCat/GenUDC"},{"id":"http://arxiv.org/abs/2309.16515v3","updated":"2024-10-23T11:56:02Z","published":"2023-09-28T15:22:02Z","title":"Latent Noise Segmentation: How Neural Noise Leads to the Emergence of\n  Segmentation and Grouping","summary":"  Humans are able to segment images effortlessly without supervision using\nperceptual grouping. Here, we propose a counter-intuitive computational\napproach to solving unsupervised perceptual grouping and segmentation: that\nthey arise because of neural noise, rather than in spite of it. We (1)\nmathematically demonstrate that under realistic assumptions, neural noise can\nbe used to separate objects from each other; (2) that adding noise in a DNN\nenables the network to segment images even though it was never trained on any\nsegmentation labels; and (3) that segmenting objects using noise results in\nsegmentation performance that aligns with the perceptual grouping phenomena\nobserved in humans, and is sample-efficient. We introduce the Good Gestalt (GG)\ndatasets -- six datasets designed to specifically test perceptual grouping, and\nshow that our DNN models reproduce many important phenomena in human\nperception, such as illusory contours, closure, continuity, proximity, and\nocclusion. Finally, we (4) show that our model improves performance on our GG\ndatasets compared to other tested unsupervised models by $24.9\\%$. Together,\nour results suggest a novel unsupervised segmentation method requiring few\nassumptions, a new explanation for the formation of perceptual grouping, and a\nnovel potential benefit of neural noise.\n","authors":["Ben Lonnqvist","Zhengqing Wu","Michael H. Herzog"],"pdf_url":"https://arxiv.org/pdf/2309.16515v3.pdf","comment":"ICML 2024 camera ready version"},{"id":"http://arxiv.org/abs/2410.17785v1","updated":"2024-10-23T11:35:44Z","published":"2024-10-23T11:35:44Z","title":"TranSPORTmer: A Holistic Approach to Trajectory Understanding in\n  Multi-Agent Sports","summary":"  Understanding trajectories in multi-agent scenarios requires addressing\nvarious tasks, including predicting future movements, imputing missing\nobservations, inferring the status of unseen agents, and classifying different\nglobal states. Traditional data-driven approaches often handle these tasks\nseparately with specialized models. We introduce TranSPORTmer, a unified\ntransformer-based framework capable of addressing all these tasks, showcasing\nits application to the intricate dynamics of multi-agent sports scenarios like\nsoccer and basketball. Using Set Attention Blocks, TranSPORTmer effectively\ncaptures temporal dynamics and social interactions in an equivariant manner.\nThe model's tasks are guided by an input mask that conceals missing or\nyet-to-be-predicted observations. Additionally, we introduce a CLS extra agent\nto classify states along soccer trajectories, including passes, possessions,\nuncontrolled states, and out-of-play intervals, contributing to an enhancement\nin modeling trajectories. Evaluations on soccer and basketball datasets show\nthat TranSPORTmer outperforms state-of-the-art task-specific models in player\nforecasting, player forecasting-imputation, ball inference, and ball\nimputation. https://youtu.be/8VtSRm8oGoE\n","authors":["Guillem Capellera","Luis Ferraz","Antonio Rubio","Antonio Agudo","Francesc Moreno-Noguer"],"pdf_url":"https://arxiv.org/pdf/2410.17785v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.17779v1","updated":"2024-10-23T11:31:06Z","published":"2024-10-23T11:31:06Z","title":"ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language\n  Tuning","summary":"  Recent advancements in multimodal fusion have witnessed the remarkable\nsuccess of vision-language (VL) models, which excel in various multimodal\napplications such as image captioning and visual question answering. However,\nbuilding VL models requires substantial hardware resources, where efficiency is\nrestricted by two key factors: the extended input sequence of the language\nmodel with vision features demands more computational operations, and a large\nnumber of additional learnable parameters increase memory complexity. These\nchallenges significantly restrict the broader applicability of such models. To\nbridge this gap, we propose ADEM-VL, an efficient vision-language method that\ntunes VL models based on pretrained large language models (LLMs) by adopting a\nparameter-free cross-attention mechanism for similarity measurements in\nmultimodal fusion. This approach only requires embedding vision features into\nthe language space, significantly reducing the number of trainable parameters\nand accelerating both training and inference speeds. To enhance representation\nlearning in fusion module, we introduce an efficient multiscale feature\ngeneration scheme that requires only a single forward pass through the vision\nencoder. Moreover, we propose an adaptive fusion scheme that dynamically\ndiscards less relevant visual information for each text token based on its\nattention score. This ensures that the fusion process prioritizes the most\npertinent visual features. With experiments on various tasks including visual\nquestion answering, image captioning, and instruction-following, we demonstrate\nthat our framework outperforms existing approaches. Specifically, our method\nsurpasses existing methods by an average accuracy of 0.77% on ScienceQA\ndataset, with reduced training and inference latency, demonstrating the\nsuperiority of our framework. The code is available at\nhttps://github.com/Hao840/ADEM-VL.\n","authors":["Zhiwei Hao","Jianyuan Guo","Li Shen","Yong Luo","Han Hu","Yonggang Wen"],"pdf_url":"https://arxiv.org/pdf/2410.17779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17774v1","updated":"2024-10-23T11:23:05Z","published":"2024-10-23T11:23:05Z","title":"Quasi-Medial Distance Field (Q-MDF): A Robust Method for Approximating\n  and Discretizing Neural Medial Axis","summary":"  The medial axis, a lower-dimensional shape descriptor, plays an important\nrole in the field of digital geometry processing. Despite its importance,\nrobust computation of the medial axis transform from diverse inputs, especially\npoint clouds with defects, remains a significant challenge. In this paper, we\ntackle the challenge by proposing a new implicit method that diverges from\nmainstream explicit medial axis computation techniques. Our key technical\ninsight is the difference between the signed distance field (SDF) and the\nmedial field (MF) of a solid shape is the unsigned distance field (UDF) of the\nshape's medial axis. This allows for formulating medial axis computation as an\nimplicit reconstruction problem. Utilizing a modified double covering method,\nwe extract the medial axis as the zero level-set of the UDF. Extensive\nexperiments show that our method has enhanced accuracy and robustness in\nlearning compact medial axis transform from thorny meshes and point clouds\ncompared to existing methods.\n","authors":["Jiayi Kong","Chen Zong","Jun Luo","Shiqing Xin","Fei Hou","Hanqing Jiang","Chen Qian","Ying He"],"pdf_url":"https://arxiv.org/pdf/2410.17774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17772v1","updated":"2024-10-23T11:19:48Z","published":"2024-10-23T11:19:48Z","title":"Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation\n  Models","summary":"  A central challenge towards developing robots that can relate human language\nto their perception and actions is the scarcity of natural language annotations\nin diverse robot datasets. Moreover, robot policies that follow natural\nlanguage instructions are typically trained on either templated language or\nexpensive human-labeled instructions, hindering their scalability. To this end,\nwe introduce NILS: Natural language Instruction Labeling for Scalability. NILS\nautomatically labels uncurated, long-horizon robot data at scale in a zero-shot\nmanner without any human intervention. NILS combines pretrained vision-language\nfoundation models in order to detect objects in a scene, detect object-centric\nchanges, segment tasks from large datasets of unlabelled interaction data and\nultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a\nkitchen play dataset show that NILS can autonomously annotate diverse robot\ndemonstrations of unlabeled and unstructured datasets while alleviating several\nshortcomings of crowdsourced human annotations, such as low data quality and\ndiversity. We use NILS to label over 115k trajectories obtained from over 430\nhours of robot data. We open-source our auto-labeling code and generated\nannotations on our website: http://robottasklabeling.github.io.\n","authors":["Nils Blank","Moritz Reuss","Marcel Rühle","Ömer Erdinç Yağmurlu","Fabian Wenzel","Oier Mees","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.17772v1.pdf","comment":"Project Website at https://robottasklabeling.github.io/"},{"id":"http://arxiv.org/abs/2404.00362v2","updated":"2024-10-23T11:06:02Z","published":"2024-03-30T13:28:53Z","title":"STBA: Towards Evaluating the Robustness of DNNs for Query-Limited\n  Black-box Scenario","summary":"  Many attack techniques have been proposed to explore the vulnerability of\nDNNs and further help to improve their robustness. Despite the significant\nprogress made recently, existing black-box attack methods still suffer from\nunsatisfactory performance due to the vast number of queries needed to optimize\ndesired perturbations. Besides, the other critical challenge is that\nadversarial examples built in a noise-adding manner are abnormal and struggle\nto successfully attack robust models, whose robustness is enhanced by\nadversarial training against small perturbations. There is no doubt that these\ntwo issues mentioned above will significantly increase the risk of exposure and\nresult in a failure to dig deeply into the vulnerability of DNNs. Hence, it is\nnecessary to evaluate DNNs' fragility sufficiently under query-limited settings\nin a non-additional way. In this paper, we propose the Spatial Transform\nBlack-box Attack (STBA), a novel framework to craft formidable adversarial\nexamples in the query-limited scenario. Specifically, STBA introduces a flow\nfield to the high-frequency part of clean images to generate adversarial\nexamples and adopts the following two processes to enhance their naturalness\nand significantly improve the query efficiency: a) we apply an estimated flow\nfield to the high-frequency part of clean images to generate adversarial\nexamples instead of introducing external noise to the benign image, and b) we\nleverage an efficient gradient estimation method based on a batch of samples to\noptimize such an ideal flow field under query-limited settings. Compared to\nexisting score-based black-box baselines, extensive experiments indicated that\nSTBA could effectively improve the imperceptibility of the adversarial examples\nand remarkably boost the attack success rate under query-limited settings.\n","authors":["Renyang Liu","Kwok-Yan Lam","Wei Zhou","Sixing Wu","Jun Zhao","Dongting Hu","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2404.00362v2.pdf","comment":"Accepted by T-MM"},{"id":"http://arxiv.org/abs/2403.14626v2","updated":"2024-10-23T11:05:01Z","published":"2024-03-21T17:59:55Z","title":"ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras\n  Based on Transformer","summary":"  Obstacle detection and tracking represent a critical component in robot\nautonomous navigation. In this paper, we propose ODTFormer, a Transformer-based\nmodel to address both obstacle detection and tracking problems. For the\ndetection task, our approach leverages deformable attention to construct a 3D\ncost volume, which is decoded progressively in the form of voxel occupancy\ngrids. We further track the obstacles by matching the voxels between\nconsecutive frames. The entire model can be optimized in an end-to-end manner.\nThrough extensive experiments on DrivingStereo and KITTI benchmarks, our model\nachieves state-of-the-art performance in the obstacle detection task. We also\nreport comparable accuracy to state-of-the-art obstacle tracking models while\nrequiring only a fraction of their computation cost, typically ten-fold to\ntwenty-fold less. The code and model weights will be publicly released.\n","authors":["Tianye Ding","Hongyu Li","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14626v2.pdf","comment":"8 pages. Accepted by IROS 2024"},{"id":"http://arxiv.org/abs/2410.17752v1","updated":"2024-10-23T10:29:18Z","published":"2024-10-23T10:29:18Z","title":"AdaDiffSR: Adaptive Region-aware Dynamic Acceleration Diffusion Model\n  for Real-World Image Super-Resolution","summary":"  Diffusion models (DMs) have shown promising results on single-image\nsuper-resolution and other image-to-image translation tasks. Benefiting from\nmore computational resources and longer inference times, they are able to yield\nmore realistic images. Existing DMs-based super-resolution methods try to\nachieve an overall average recovery over all regions via iterative refinement,\nignoring the consideration that different input image regions require different\ntimesteps to reconstruct. In this work, we notice that previous DMs-based\nsuper-resolution methods suffer from wasting computational resources to\nreconstruct invisible details. To further improve the utilization of\ncomputational resources, we propose AdaDiffSR, a DMs-based SR pipeline with\ndynamic timesteps sampling strategy (DTSS). Specifically, by introducing the\nmulti-metrics latent entropy module (MMLE), we can achieve dynamic perception\nof the latent spatial information gain during the denoising process, thereby\nguiding the dynamic selection of the timesteps. In addition, we adopt a\nprogressive feature injection module (PFJ), which dynamically injects the\noriginal image features into the denoising process based on the current\ninformation gain, so as to generate images with both fidelity and realism.\nExperiments show that our AdaDiffSR achieves comparable performance over\ncurrent state-of-the-art DMs-based SR methods while consuming less\ncomputational resources and inference time on both synthetic and real-world\ndatasets.\n","authors":["Yuanting Fan","Chengxu Liu","Nengzhong Yin","Changlong Gao","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2410.17752v1.pdf","comment":"18 pages, 6 figures, ECCV2024 accepted"},{"id":"http://arxiv.org/abs/2410.17751v1","updated":"2024-10-23T10:28:17Z","published":"2024-10-23T10:28:17Z","title":"VISAGE: Video Synthesis using Action Graphs for Surgery","summary":"  Surgical data science (SDS) is a field that analyzes patient data before,\nduring, and after surgery to improve surgical outcomes and skills. However,\nsurgical data is scarce, heterogeneous, and complex, which limits the\napplicability of existing machine learning methods. In this work, we introduce\nthe novel task of future video generation in laparoscopic surgery. This task\ncan augment and enrich the existing surgical data and enable various\napplications, such as simulation, analysis, and robot-aided surgery.\nUltimately, it involves not only understanding the current state of the\noperation but also accurately predicting the dynamic and often unpredictable\nnature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis\nusing Action Graphs for Surgery), leverages the power of action scene graphs to\ncapture the sequential nature of laparoscopic procedures and utilizes diffusion\nmodels to synthesize temporally coherent video sequences. VISAGE predicts the\nfuture frames given only a single initial frame, and the action graph triplets.\nBy incorporating domain-specific knowledge through the action graph, VISAGE\nensures the generated videos adhere to the expected visual and motion patterns\nobserved in real laparoscopic procedures. The results of our experiments\ndemonstrate high-fidelity video generation for laparoscopy procedures, which\nenables various applications in SDS.\n","authors":["Yousef Yeganeh","Rachmadio Lazuardi","Amir Shamseddin","Emine Dari","Yash Thirani","Nassir Navab Azade Farshad"],"pdf_url":"https://arxiv.org/pdf/2410.17751v1.pdf","comment":"Accepted at MICCAI 2024 Embodied AI and Robotics for HealTHcare\n  (EARTH) Workshop"},{"id":"http://arxiv.org/abs/2410.17741v1","updated":"2024-10-23T10:16:01Z","published":"2024-10-23T10:16:01Z","title":"Efficient Neural Implicit Representation for 3D Human Reconstruction","summary":"  High-fidelity digital human representations are increasingly in demand in the\ndigital world, particularly for interactive telepresence, AR/VR, 3D graphics,\nand the rapidly evolving metaverse. Even though they work well in small spaces,\nconventional methods for reconstructing 3D human motion frequently require the\nuse of expensive hardware and have high processing costs. This study presents\nHumanAvatar, an innovative approach that efficiently reconstructs precise human\navatars from monocular video sources. At the core of our methodology, we\nintegrate the pre-trained HuMoR, a model celebrated for its proficiency in\nhuman motion estimation. This is adeptly fused with the cutting-edge neural\nradiance field technology, Instant-NGP, and the state-of-the-art articulated\nmodel, Fast-SNARF, to enhance the reconstruction fidelity and speed. By\ncombining these two technologies, a system is created that can render quickly\nand effectively while also providing estimation of human pose parameters that\nare unmatched in accuracy. We have enhanced our system with an advanced\nposture-sensitive space reduction technique, which optimally balances rendering\nquality with computational efficiency. In our detailed experimental analysis\nusing both artificial and real-world monocular videos, we establish the\nadvanced performance of our approach. HumanAvatar consistently equals or\nsurpasses contemporary leading-edge reconstruction techniques in quality.\nFurthermore, it achieves these complex reconstructions in minutes, a fraction\nof the time typically required by existing methods. Our models achieve a\ntraining speed that is 110X faster than that of State-of-The-Art (SoTA)\nNeRF-based models. Our technique performs noticeably better than SoTA dynamic\nhuman NeRF methods if given an identical runtime limit. HumanAvatar can provide\neffective visuals after only 30 seconds of training.\n","authors":["Zexu Huang","Sarah Monazam Erfani","Siying Lu","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2410.17741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17740v1","updated":"2024-10-23T10:14:37Z","published":"2024-10-23T10:14:37Z","title":"Emotion Recognition with Facial Attention and Objective Activation\n  Functions","summary":"  In this paper, we study the effect of introducing channel and spatial\nattention mechanisms, namely SEN-Net, ECA-Net, and CBAM, to existing CNN\nvision-based models such as VGGNet, ResNet, and ResNetV2 to perform the Facial\nEmotion Recognition task. We show that not only attention can significantly\nimprove the performance of these models but also that combining them with a\ndifferent activation function can further help increase the performance of\nthese models.\n","authors":["Andrzej Miskow","Abdulrahman Altahhan"],"pdf_url":"https://arxiv.org/pdf/2410.17740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17735v1","updated":"2024-10-23T10:11:39Z","published":"2024-10-23T10:11:39Z","title":"New Insight in Cervical Cancer Diagnosis Using Convolution Neural\n  Network Architecture","summary":"  The Pap smear is a screening method for early cervical cancer diagnosis. The\nselection of the right optimizer in the convolutional neural network (CNN)\nmodel is key to the success of the CNN in image classification, including the\nclassification of cervical cancer Pap smear images. In this study, stochastic\ngradient descent (SGD), RMSprop, Adam, AdaGrad, AdaDelta, Adamax, and Nadam\noptimizers were used to classify cervical cancer Pap smear images from the\nSipakMed dataset. Resnet-18, Resnet-34, and VGG-16 are the CNN architectures\nused in this study, and each architecture uses a transfer-learning model. Based\non the test results, we conclude that the transfer learning model performs\nbetter on all CNNs and optimization techniques and that in the transfer\nlearning model, the optimization has little influence on the training of the\nmodel. Adamax, with accuracy values of 72.8% and 66.8%, had the best accuracy\nfor the VGG-16 and Resnet-18 architectures, respectively. Resnet-34 had 54.0%.\nThis is 0.034% lower than Nadam. Overall, Adamax is a suitable optimizer for\nCNN in cervical cancer classification on Resnet-18, Resnet-34, and VGG-16\narchitectures. This study provides new insights into the configuration of CNN\nmodels for Pap smear image analysis.\n","authors":["Ach. Khozaimi","Wayan Firdaus Mahmudy"],"pdf_url":"https://arxiv.org/pdf/2410.17735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17734v1","updated":"2024-10-23T10:07:13Z","published":"2024-10-23T10:07:13Z","title":"YOLO-Vehicle-Pro: A Cloud-Edge Collaborative Framework for Object\n  Detection in Autonomous Driving under Adverse Weather Conditions","summary":"  With the rapid advancement of autonomous driving technology, efficient and\naccurate object detection capabilities have become crucial factors in ensuring\nthe safety and reliability of autonomous driving systems. However, in\nlow-visibility environments such as hazy conditions, the performance of\ntraditional object detection algorithms often degrades significantly, failing\nto meet the demands of autonomous driving. To address this challenge, this\npaper proposes two innovative deep learning models: YOLO-Vehicle and\nYOLO-Vehicle-Pro. YOLO-Vehicle is an object detection model tailored\nspecifically for autonomous driving scenarios, employing multimodal fusion\ntechniques to combine image and textual information for object detection.\nYOLO-Vehicle-Pro builds upon this foundation by introducing an improved image\ndehazing algorithm, enhancing detection performance in low-visibility\nenvironments. In addition to model innovation, this paper also designs and\nimplements a cloud-edge collaborative object detection system, deploying models\non edge devices and offloading partial computational tasks to the cloud in\ncomplex situations. Experimental results demonstrate that on the KITTI dataset,\nthe YOLO-Vehicle-v1s model achieved 92.1% accuracy while maintaining a\ndetection speed of 226 FPS and an inference time of 12ms, meeting the real-time\nrequirements of autonomous driving. When processing hazy images, the\nYOLO-Vehicle-Pro model achieved a high accuracy of 82.3% mAP@50 on the Foggy\nCityscapes dataset while maintaining a detection speed of 43 FPS.\n","authors":["Xiguang Li","Jiafu Chen","Yunhe Sun","Na Lin","Ammar Hawbani","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.17734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17725v1","updated":"2024-10-23T09:55:22Z","published":"2024-10-23T09:55:22Z","title":"YOLOv11: An Overview of the Key Architectural Enhancements","summary":"  This study presents an architectural analysis of YOLOv11, the latest\niteration in the YOLO (You Only Look Once) series of object detection models.\nWe examine the models architectural innovations, including the introduction of\nthe C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid\nPooling - Fast), and C2PSA (Convolutional block with Parallel Spatial\nAttention) components, which contribute in improving the models performance in\nseveral ways such as enhanced feature extraction. The paper explores YOLOv11's\nexpanded capabilities across various computer vision tasks, including object\ndetection, instance segmentation, pose estimation, and oriented object\ndetection (OBB). We review the model's performance improvements in terms of\nmean Average Precision (mAP) and computational efficiency compared to its\npredecessors, with a focus on the trade-off between parameter count and\naccuracy. Additionally, the study discusses YOLOv11's versatility across\ndifferent model sizes, from nano to extra-large, catering to diverse\napplication needs from edge devices to high-performance computing environments.\nOur research provides insights into YOLOv11's position within the broader\nlandscape of object detection and its potential impact on real-time computer\nvision applications.\n","authors":["Rahima Khanam","Muhammad Hussain"],"pdf_url":"https://arxiv.org/pdf/2410.17725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15881v3","updated":"2024-10-23T09:52:23Z","published":"2024-08-28T15:52:23Z","title":"LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation","summary":"  We introduce LLaVA-MoD, a novel framework designed to enable the efficient\ntraining of small-scale Multimodal Language Models (s-MLLM) by distilling\nknowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamental\nchallenges in MLLM distillation. First, we optimize the network structure of\ns-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the\nlanguage model, striking a balance between computational efficiency and model\nexpressiveness. Second, we propose a progressive knowledge transfer strategy to\nensure comprehensive knowledge migration. This strategy begins with mimic\ndistillation, where we minimize the Kullback-Leibler (KL) divergence between\noutput distributions to enable the student model to emulate the teacher\nnetwork's understanding. Following this, we introduce preference distillation\nvia Direct Preference Optimization (DPO), where the key lies in treating l-MLLM\nas the reference model. During this phase, the s-MLLM's ability to discriminate\nbetween superior and inferior examples is significantly enhanced beyond l-MLLM,\nleading to a better student that surpasses its teacher, particularly in\nhallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoD\noutperforms existing models across various multimodal benchmarks while\nmaintaining a minimal number of activated parameters and low computational\ncosts. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpasses\nQwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% of\nthe training data and 23% trainable parameters. These results underscore\nLLaVA-MoD's ability to effectively distill comprehensive knowledge from its\nteacher model, paving the way for the development of more efficient MLLMs. The\ncode will be available on: https://github.com/shufangxun/LLaVA-MoD.\n","authors":["Fangxun Shu","Yue Liao","Le Zhuo","Chenning Xu","Lei Zhang","Guanghao Zhang","Haonan Shi","Long Chen","Tao Zhong","Wanggui He","Siming Fu","Haoyuan Li","Bolin Li","Zhelun Yu","Si Liu","Hongsheng Li","Hao Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.15881v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14693v2","updated":"2024-10-23T09:42:29Z","published":"2024-04-23T02:50:38Z","title":"DIP-Watermark: A Double Identity Protection Method Based on Robust\n  Adversarial Watermark","summary":"  The wide deployment of Face Recognition (FR) systems poses privacy risks. One\ncountermeasure is adversarial attack, deceiving unauthorized malicious FR, but\nit also disrupts regular identity verification of trusted authorizers,\nexacerbating the potential threat of identity impersonation. To address this,\nwe propose the first double identity protection scheme based on traceable\nadversarial watermarking, termed DIP-Watermark. DIP-Watermark employs a\none-time watermark embedding to deceive unauthorized FR models and allows\nauthorizers to perform identity verification by extracting the watermark.\nSpecifically, we propose an information-guided adversarial attack against FR\nmodels. The encoder embeds an identity-specific watermark into the deep feature\nspace of the carrier, guiding recognizable features of the image to deviate\nfrom the source identity. We further adopt a collaborative meta-optimization\nstrategy compatible with sub-tasks, which regularizes the joint optimization\ndirection of the encoder and decoder. This strategy enhances the representation\nof universal carrier features, mitigating multi-objective optimization\nconflicts in watermarking. Experiments confirm that DIP-Watermark achieves\nsignificant attack success rates and traceability accuracy on state-of-the-art\nFR models, exhibiting remarkable robustness that outperforms the existing\nprivacy protection methods using adversarial attacks and deep watermarking, or\nsimple combinations of the two. Our work potentially opens up new insights into\nproactive protection for FR privacy.\n","authors":["Yunming Zhang","Dengpan Ye","Caiyun Xie","Sipeng Shen","Ziyi Liu","Jiacheng Deng","Long Tang"],"pdf_url":"https://arxiv.org/pdf/2404.14693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17715v1","updated":"2024-10-23T09:42:17Z","published":"2024-10-23T09:42:17Z","title":"Continual Learning on a Data Diet","summary":"  Continual Learning (CL) methods usually learn from all available data.\nHowever, this is not the case in human cognition which efficiently focuses on\nkey experiences while disregarding the redundant information. Similarly, not\nall data points in a dataset have equal potential; some can be more informative\nthan others. This disparity may significantly impact the performance, as both\nthe quality and quantity of samples directly influence the model's\ngeneralizability and efficiency. Drawing inspiration from this, we explore the\npotential of learning from important samples and present an empirical study for\nevaluating coreset selection techniques in the context of CL to stimulate\nresearch in this unexplored area. We train different continual learners on\nincreasing amounts of selected samples and investigate the learning-forgetting\ndynamics by shedding light on the underlying mechanisms driving their improved\nstability-plasticity balance. We present several significant observations:\nlearning from selectively chosen samples (i) enhances incremental accuracy,\n(ii) improves knowledge retention of previous tasks, and (iii) refines learned\nrepresentations. This analysis contributes to a deeper understanding of\nselective learning strategies in CL scenarios.\n","authors":["Elif Ceren Gok Yildirim","Murat Onur Yildirim","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2410.17715v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.17678v2","updated":"2024-10-23T09:15:32Z","published":"2024-03-26T13:05:49Z","title":"Hierarchical Light Transformer Ensembles for Multimodal Trajectory\n  Forecasting","summary":"  Accurate trajectory forecasting is crucial for the performance of various\nsystems, such as advanced driver-assistance systems and self-driving vehicles.\nThese forecasts allow to anticipate events leading to collisions and,\ntherefore, to mitigate them. Deep Neural Networks have excelled in motion\nforecasting, but issues like overconfidence and uncertainty quantification\npersist. Deep Ensembles address these concerns, yet applying them to multimodal\ndistributions remains challenging. In this paper, we propose a novel approach\nnamed Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently\ntraining an ensemble of Transformer architectures using a novel hierarchical\nloss function. HLT-Ens leverages grouped fully connected layers, inspired by\ngrouped convolution techniques, to capture multimodal distributions,\neffectively. Through extensive experimentation, we demonstrate that HLT-Ens\nachieves state-of-the-art performance levels, offering a promising avenue for\nimproving trajectory forecasting techniques.\n","authors":["Adrien Lafage","Mathieu Barbier","Gianni Franchi","David Filliat"],"pdf_url":"https://arxiv.org/pdf/2403.17678v2.pdf","comment":"acknowledgement added"},{"id":"http://arxiv.org/abs/2410.17691v1","updated":"2024-10-23T09:13:11Z","published":"2024-10-23T09:13:11Z","title":"Longitudinal Causal Image Synthesis","summary":"  Clinical decision-making relies heavily on causal reasoning and longitudinal\nanalysis. For example, for a patient with Alzheimer's disease (AD), how will\nthe brain grey matter atrophy in a year if intervened on the A-beta level in\ncerebrospinal fluid? The answer is fundamental to diagnosis and follow-up\ntreatment. However, this kind of inquiry involves counterfactual medical images\nwhich can not be acquired by instrumental or correlation-based image synthesis\nmodels. Yet, such queries require counterfactual medical images, not obtainable\nthrough standard image synthesis models. Hence, a causal longitudinal image\nsynthesis (CLIS) method, enabling the synthesis of such images, is highly\nvaluable. However, building a CLIS model confronts three primary yet unmet\nchallenges: mismatched dimensionality between high-dimensional images and\nlow-dimensional tabular variables, inconsistent collection intervals of\nfollow-up data, and inadequate causal modeling capability of existing causal\ngraph methods for image data. In this paper, we established a tabular-visual\ncausal graph (TVCG) for CLIS overcoming these challenges through a novel\nintegration of generative imaging, continuous-time modeling, and structural\ncausal models combined with a neural network. We train our CLIS based on the\nADNI dataset and evaluate it on two other AD datasets, which illustrate the\noutstanding yet controllable quality of the synthesized images and the\ncontributions of synthesized MRI to the characterization of AD progression,\nsubstantiating the reliability and utility in clinics.\n","authors":["Yujia Li","Han Li","ans S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.17691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13752v2","updated":"2024-10-23T08:53:05Z","published":"2023-05-23T07:09:09Z","title":"Pulling Target to Source: A New Perspective on Domain Adaptive Semantic\n  Segmentation","summary":"  Domain adaptive semantic segmentation aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. However, existing methods\nprimarily focus on directly learning qualified target features, making it\nchallenging to guarantee their discrimination in the absence of target labels.\nThis work provides a new perspective. We observe that the features learned with\nsource data manage to keep categorically discriminative during training,\nthereby enabling us to implicitly learn adequate target representations by\nsimply \\textbf{pulling target features close to source features for each\ncategory}. To this end, we propose T2S-DA, which we interpret as a form of\npulling Target to Source for Domain Adaptation, encouraging the model in\nlearning similar cross-domain features. Also, considering the pixel categories\nare heavily imbalanced for segmentation datasets, we come up with a dynamic\nre-weighting strategy to help the model concentrate on those underperforming\nclasses. Extensive experiments confirm that T2S-DA learns a more discriminative\nand generalizable representation, significantly surpassing the\nstate-of-the-art. We further show that our method is quite qualified for the\ndomain generalization task, verifying its domain-invariant property.\n","authors":["Haochen Wang","Yujun Shen","Jingjing Fei","Wei Li","Liwei Wu","Yuxi Wang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13752v2.pdf","comment":"Accepted by IJCV"},{"id":"http://arxiv.org/abs/2410.17664v1","updated":"2024-10-23T08:33:23Z","published":"2024-10-23T08:33:23Z","title":"Deep Generative Models for 3D Medical Image Synthesis","summary":"  Deep generative modeling has emerged as a powerful tool for synthesizing\nrealistic medical images, driving advances in medical image analysis, disease\ndiagnosis, and treatment planning. This chapter explores various deep\ngenerative models for 3D medical image synthesis, with a focus on Variational\nAutoencoders (VAEs), Generative Adversarial Networks (GANs), and Denoising\nDiffusion Models (DDMs). We discuss the fundamental principles, recent\nadvances, as well as strengths and weaknesses of these models and examine their\napplications in clinically relevant problems, including unconditional and\nconditional generation tasks like image-to-image translation and image\nreconstruction. We additionally review commonly used evaluation metrics for\nassessing image fidelity, diversity, utility, and privacy and provide an\noverview of current challenges in the field.\n","authors":["Paul Friedrich","Yannik Frisch","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2410.17664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00505v2","updated":"2024-10-23T08:28:53Z","published":"2024-06-01T17:27:34Z","title":"Improving Text Generation on Images with Synthetic Captions","summary":"  The recent emergence of latent diffusion models such as SDXL and SD 1.5 has\nshown significant capability in generating highly detailed and realistic\nimages. Despite their remarkable ability to produce images, generating accurate\ntext within images still remains a challenging task. In this paper, we examine\nthe validity of fine-tuning approaches in generating legible text within the\nimage. We propose a low-cost approach by leveraging SDXL without any\ntime-consuming training on large-scale datasets. The proposed strategy employs\na fine-tuning technique that examines the effects of data refinement levels and\nsynthetic captions. Moreover, our results demonstrate how our small scale\nfine-tuning approach can improve the accuracy of text generation in different\nscenarios without the need of additional multimodal encoders. Our experiments\nshow that with the addition of random letters to our raw dataset, our model's\nperformance improves in producing well-formed visual text.\n","authors":["Jun Young Koh","Sang Hyun Park","Joy Song"],"pdf_url":"https://arxiv.org/pdf/2406.00505v2.pdf","comment":"2024 16th IIAI International Congress on Advanced Applied Informatics\n  (IIAI-AAI)"},{"id":"http://arxiv.org/abs/2407.16364v2","updated":"2024-10-23T08:27:23Z","published":"2024-07-23T10:11:56Z","title":"Harmonizing Visual Text Comprehension and Generation","summary":"  In this work, we present TextHarmony, a unified and versatile multimodal\ngenerative model proficient in comprehending and generating visual text.\nSimultaneously generating images and texts typically results in performance\ndegradation due to the inherent inconsistency between vision and language\nmodalities. To overcome this challenge, existing approaches resort to\nmodality-specific data for supervised fine-tuning, necessitating distinct model\ninstances. We propose Slide-LoRA, which dynamically aggregates\nmodality-specific and modality-agnostic LoRA experts, partially decoupling the\nmultimodal generation space. Slide-LoRA harmonizes the generation of vision and\nlanguage within a singular model instance, thereby facilitating a more unified\ngenerative process. Additionally, we develop a high-quality image caption\ndataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source\nMLLM to enhance visual text generation capabilities further. Comprehensive\nexperiments across various benchmarks demonstrate the effectiveness of the\nproposed approach. Empowered by Slide-LoRA, TextHarmony achieves comparable\nperformance to modality-specific fine-tuning results with only a 2% increase in\nparameters and shows an average improvement of 2.5% in visual text\ncomprehension tasks and 4.0% in visual text generation tasks. Our work\ndelineates the viability of an integrated approach to multimodal generation\nwithin the visual text domain, setting a foundation for subsequent inquiries.\nCode is available at https://github.com/bytedance/TextHarmony.\n","authors":["Zhen Zhao","Jingqun Tang","Binghong Wu","Chunhui Lin","Shu Wei","Hao Liu","Xin Tan","Zhizhong Zhang","Can Huang","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2407.16364v2.pdf","comment":"accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2312.12540v4","updated":"2024-10-23T08:20:12Z","published":"2023-12-19T19:19:19Z","title":"Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion\n  Models","summary":"  Diffusion inversion is the problem of taking an image and a text prompt that\ndescribes it and finding a noise latent that would generate the exact same\nimage. Most current deterministic inversion techniques operate by approximately\nsolving an implicit equation and may converge slowly or yield poor\nreconstructed images. We formulate the problem by finding the roots of an\nimplicit equation and devlop a method to solve it efficiently. Our solution is\nbased on Newton-Raphson (NR), a well-known technique in numerical analysis. We\nshow that a vanilla application of NR is computationally infeasible while\nnaively transforming it to a computationally tractable alternative tends to\nconverge to out-of-distribution solutions, resulting in poor reconstruction and\nediting. We therefore derive an efficient guided formulation that fastly\nconverges and provides high-quality reconstructions and editing. We showcase\nour method on real image editing with three popular open-sourced diffusion\nmodels: Stable Diffusion, SDXL-Turbo, and Flux with different deterministic\nschedulers. Our solution, Guided Newton-Raphson Inversion, inverts an image\nwithin 0.4 sec (on an A100 GPU) for few-step models (SDXL-Turbo and Flux.1),\nopening the door for interactive image editing. We further show improved\nresults in image interpolation and generation of rare objects.\n","authors":["Dvir Samuel","Barak Meiri","Haggai Maron","Yoad Tewel","Nir Darshan","Shai Avidan","Gal Chechik","Rami Ben-Ari"],"pdf_url":"https://arxiv.org/pdf/2312.12540v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17642v1","updated":"2024-10-23T07:58:47Z","published":"2024-10-23T07:58:47Z","title":"Surgical Scene Segmentation by Transformer With Asymmetric Feature\n  Enhancement","summary":"  Surgical scene segmentation is a fundamental task for robotic-assisted\nlaparoscopic surgery understanding. It often contains various anatomical\nstructures and surgical instruments, where similar local textures and\nfine-grained structures make the segmentation a difficult task. Vision-specific\ntransformer method is a promising way for surgical scene understanding.\nHowever, there are still two main challenges. Firstly, the absence of\ninner-patch information fusion leads to poor segmentation performance.\nSecondly, the specific characteristics of anatomy and instruments are not\nspecifically modeled. To tackle the above challenges, we propose a novel\nTransformer-based framework with an Asymmetric Feature Enhancement module\n(TAFE), which enhances local information and then actively fuses the improved\nfeature pyramid into the embeddings from transformer encoders by a multi-scale\ninteraction attention strategy. The proposed method outperforms the SOTA\nmethods in several different surgical segmentation tasks and additionally\nproves its ability of fine-grained structure recognition. Code is available at\nhttps://github.com/cyuan-sjtu/ViT-asym.\n","authors":["Cheng Yuan","Yutong Ban"],"pdf_url":"https://arxiv.org/pdf/2410.17642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17637v1","updated":"2024-10-23T07:56:48Z","published":"2024-10-23T07:56:48Z","title":"MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large\n  Vision-Language Models","summary":"  Visual preference alignment involves training Large Vision-Language Models\n(LVLMs) to predict human preferences between visual inputs. This is typically\nachieved by using labeled datasets of chosen/rejected pairs and employing\noptimization algorithms like direct preference optimization (DPO). Existing\nvisual alignment methods, primarily designed for single-image scenarios,\nstruggle to effectively handle the complexity of multi-image tasks due to the\nscarcity of diverse training data and the high cost of annotating\nchosen/rejected pairs. We present Multi-Image Augmented Direct Preference\nOptimization (MIA-DPO), a visual preference alignment approach that effectively\nhandles multi-image inputs. MIA-DPO mitigates the scarcity of diverse\nmulti-image training data by extending single-image data with unrelated images\narranged in grid collages or pic-in-pic formats, significantly reducing the\ncosts associated with multi-image data annotations. Our observation reveals\nthat attention values of LVLMs vary considerably across different images. We\nuse attention values to identify and filter out rejected responses the model\nmay have mistakenly focused on. Our attention-aware selection for constructing\nthe chosen/rejected pairs without relying on (i) human annotation, (ii) extra\ndata, and (iii) external models or APIs. MIA-DPO is compatible with various\narchitectures and outperforms existing methods on five multi-image benchmarks,\nachieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the\nrecent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's\nability to understand single images.\n","authors":["Ziyu Liu","Yuhang Zang","Xiaoyi Dong","Pan Zhang","Yuhang Cao","Haodong Duan","Conghui He","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17637v1.pdf","comment":"Project URL: https://github.com/Liuziyu77/MIA-DPO"},{"id":"http://arxiv.org/abs/2410.15767v2","updated":"2024-10-23T07:55:10Z","published":"2024-10-21T08:27:13Z","title":"Improving Instance Optimization in Deformable Image Registration with\n  Gradient Projection","summary":"  Deformable image registration is inherently a multi-objective optimization\n(MOO) problem, requiring a delicate balance between image similarity and\ndeformation regularity. These conflicting objectives often lead to poor\noptimization outcomes, such as being trapped in unsatisfactory local minima or\nexperiencing slow convergence. Deep learning methods have recently gained\npopularity in this domain due to their efficiency in processing large datasets\nand achieving high accuracy. However, they often underperform during test time\ncompared to traditional optimization techniques, which further explore\niterative, instance-specific gradient-based optimization. This performance gap\nis more pronounced when a distribution shift between training and test data\nexists. To address this issue, we focus on the instance optimization (IO)\nparadigm, which involves additional optimization for test-time instances based\non a pre-trained model. IO effectively combines the generalization capabilities\nof deep learning with the fine-tuning advantages of instance-specific\noptimization. Within this framework, we emphasize the use of gradient\nprojection to mitigate conflicting updates in MOO. This technique projects\nconflicting gradients into a common space, better aligning the dual objectives\nand enhancing optimization stability. We validate our method using a\nstate-of-the-art foundation model on the 3D Brain inter-subject registration\ntask (LUMIR) from the Learn2Reg 2024 Challenge. Our results show significant\nimprovements over standard gradient descent, leading to more accurate and\nreliable registration results.\n","authors":["Yi Zhang","Yidong Zhao","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2410.15767v2.pdf","comment":"Learn2Reg Challenge at MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.17622v1","updated":"2024-10-23T07:26:19Z","published":"2024-10-23T07:26:19Z","title":"Bridging the Gaps: Utilizing Unlabeled Face Recognition Datasets to\n  Boost Semi-Supervised Facial Expression Recognition","summary":"  In recent years, Facial Expression Recognition (FER) has gained increasing\nattention. Most current work focuses on supervised learning, which requires a\nlarge amount of labeled and diverse images, while FER suffers from the scarcity\nof large, diverse datasets and annotation difficulty. To address these\nproblems, we focus on utilizing large unlabeled Face Recognition (FR) datasets\nto boost semi-supervised FER. Specifically, we first perform face\nreconstruction pre-training on large-scale facial images without annotations to\nlearn features of facial geometry and expression regions, followed by two-stage\nfine-tuning on FER datasets with limited labels. In addition, to further\nalleviate the scarcity of labeled and diverse images, we propose a Mixup-based\ndata augmentation strategy tailored for facial images, and the loss weights of\nreal and virtual images are determined according to the intersection-over-union\n(IoU) of the faces in the two images. Experiments on RAF-DB, AffectNet, and\nFERPlus show that our method outperforms existing semi-supervised FER methods\nand achieves new state-of-the-art performance. Remarkably, with only 5%, 25%\ntraining sets,our method achieves 64.02% on AffectNet,and 88.23% on RAF-DB,\nwhich is comparable to fully supervised state-of-the-art methods. Codes will be\nmade publicly available at https://github.com/zhelishisongjie/SSFER.\n","authors":["Jie Song","Mengqiao He","Jinhua Feng","Bairong Shen"],"pdf_url":"https://arxiv.org/pdf/2410.17622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14947v2","updated":"2024-10-23T07:18:51Z","published":"2024-08-27T10:44:34Z","title":"ERX: A Fast Real-Time Anomaly Detection Algorithm for Hyperspectral Line\n  Scanning","summary":"  Detecting unexpected objects (anomalies) in real time has great potential for\nmonitoring, managing, and protecting the environment. Hyperspectral line-scan\ncameras are a low-cost solution that enhance confidence in anomaly detection\nover RGB and multispectral imagery. However, existing line-scan algorithms are\ntoo slow when using small computers (e.g. those onboard a drone or small\nsatellite), do not adapt to changing scenery, or lack robustness against\ngeometric distortions. This paper introduces the Exponentially moving RX\nalgorithm (ERX) to address these issues, and compares it with existing RX-based\nanomaly detection methods for hyperspectral line scanning. Three large and more\ncomplex datasets are also introduced to better assess the practical challenges\nwhen using line-scan cameras (two hyperspectral and one multispectral). ERX is\nevaluated using a Jetson Xavier NX compute module, achieving the best\ncombination of speed and detection performance. This research paves the way for\nfuture studies in grouping and locating anomalous objects, adaptive and\nautomatic threshold selection, and real-time field tests. The datasets and the\nPython code are available at: https://github.com/WiseGamgee/HyperAD.\n","authors":["Samuel Garske","Bradley Evans","Christopher Artlett","KC Wong"],"pdf_url":"https://arxiv.org/pdf/2408.14947v2.pdf","comment":"17 pages, 13 figures, 4 tables, code and datasets accessible at\n  https://github.com/WiseGamgee/HyperAD"},{"id":"http://arxiv.org/abs/2404.07554v2","updated":"2024-10-23T07:16:42Z","published":"2024-04-11T08:36:13Z","title":"CAT: Contrastive Adapter Training for Personalized Image Generation","summary":"  The emergence of various adapters, including Low-Rank Adaptation (LoRA)\napplied from the field of natural language processing, has allowed diffusion\nmodels to personalize image generation at a low cost. However, due to the\nvarious challenges including limited datasets and shortage of regularization\nand computation resources, adapter training often results in unsatisfactory\noutcomes, leading to the corruption of the backbone model's prior knowledge.\nOne of the well known phenomena is the loss of diversity in object generation,\nespecially within the same class which leads to generating almost identical\nobjects with minor variations. This poses challenges in generation\ncapabilities. To solve this issue, we present Contrastive Adapter Training\n(CAT), a simple yet effective strategy to enhance adapter training through the\napplication of CAT loss. Our approach facilitates the preservation of the base\nmodel's original knowledge when the model initiates adapters. Furthermore, we\nintroduce the Knowledge Preservation Score (KPS) to evaluate CAT's ability to\nkeep the former information. We qualitatively and quantitatively compare CAT's\nimprovement. Finally, we mention the possibility of CAT in the aspects of\nmulti-concept adapter and optimization.\n","authors":["Jae Wan Park","Sang Hyun Park","Jun Young Koh","Junha Lee","Min Song"],"pdf_url":"https://arxiv.org/pdf/2404.07554v2.pdf","comment":"CVPRW 2024"},{"id":"http://arxiv.org/abs/2410.17610v1","updated":"2024-10-23T07:06:08Z","published":"2024-10-23T07:06:08Z","title":"ImDy: Human Inverse Dynamics from Imitated Observations","summary":"  Inverse dynamics (ID), which aims at reproducing the driven torques from\nhuman kinematic observations, has been a critical tool for gait analysis.\nHowever, it is hindered from wider application to general motion due to its\nlimited scalability. Conventional optimization-based ID requires expensive\nlaboratory setups, restricting its availability. To alleviate this problem, we\npropose to exploit the recently progressive human motion imitation algorithms\nto learn human inverse dynamics in a data-driven manner. The key insight is\nthat the human ID knowledge is implicitly possessed by motion imitators, though\nnot directly applicable. In light of this, we devise an efficient data\ncollection pipeline with state-of-the-art motion imitation algorithms and\nphysics simulators, resulting in a large-scale human inverse dynamics benchmark\nas Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint\ntorque and full-body ground reaction force data. With ImDy, we train a\ndata-driven human inverse dynamics solver ImDyS(olver) in a fully supervised\nmanner, which conducts ID and ground reaction force estimation simultaneously.\nExperiments on ImDy and real-world data demonstrate the impressive competency\nof ImDyS in human inverse dynamics and ground reaction force estimation.\nMoreover, the potential of ImDy(-S) as a fundamental motion analysis tool is\nexhibited with downstream applications. The project page is\nhttps://foruck.github.io/ImDy/.\n","authors":["Xinpeng Liu","Junxuan Liang","Zili Lin","Haowen Hou","Yong-Lu Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2410.17610v1.pdf","comment":"Yong-Lu Li and Cewu Lu are the corresponding authors"},{"id":"http://arxiv.org/abs/2406.14927v2","updated":"2024-10-23T07:01:34Z","published":"2024-06-21T07:37:17Z","title":"Gaussian-Informed Continuum for Physical Property Identification and\n  Simulation","summary":"  This paper studies the problem of estimating physical properties (system\nidentification) through visual observations. To facilitate geometry-aware\nguidance in physical property estimation, we introduce a novel hybrid framework\nthat leverages 3D Gaussian representation to not only capture explicit shapes\nbut also enable the simulated continuum to render object masks as 2D shape\nsurrogates during training.\n  We propose a new dynamic 3D Gaussian framework based on motion factorization\nto recover the object as 3D Gaussian point sets across different time states.\n  Furthermore, we develop a coarse-to-fine filling strategy to generate the\ndensity fields of the object from the Gaussian reconstruction, allowing for the\nextraction of object continuums along with their surfaces and the integration\nof Gaussian attributes into these continuums.\n  In addition to the extracted object surfaces, the Gaussian-informed continuum\nalso enables the rendering of object masks during simulations, serving as\n2D-shape guidance for physical property estimation.\n  Extensive experimental evaluations demonstrate that our pipeline achieves\nstate-of-the-art performance across multiple benchmarks and metrics.\nAdditionally, we illustrate the effectiveness of the proposed method through\nreal-world demonstrations, showcasing its practical utility.\n  Our project page is at https://jukgei.github.io/project/gic.\n","authors":["Junhao Cai","Yuji Yang","Weihao Yuan","Yisheng He","Zilong Dong","Liefeng Bo","Hui Cheng","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14927v2.pdf","comment":"21 pages, 8 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17606v1","updated":"2024-10-23T07:01:16Z","published":"2024-10-23T07:01:16Z","title":"Towards Effective Data-Free Knowledge Distillation via Diverse Diffusion\n  Augmentation","summary":"  Data-free knowledge distillation (DFKD) has emerged as a pivotal technique in\nthe domain of model compression, substantially reducing the dependency on the\noriginal training data. Nonetheless, conventional DFKD methods that employ\nsynthesized training data are prone to the limitations of inadequate diversity\nand discrepancies in distribution between the synthesized and original\ndatasets. To address these challenges, this paper introduces an innovative\napproach to DFKD through diverse diffusion augmentation (DDA). Specifically, we\nrevise the paradigm of common data synthesis in DFKD to a composite process\nthrough leveraging diffusion models subsequent to data synthesis for\nself-supervised augmentation, which generates a spectrum of data samples with\nsimilar distributions while retaining controlled variations. Furthermore, to\nmitigate excessive deviation in the embedding space, we introduce an image\nfiltering technique grounded in cosine similarity to maintain fidelity during\nthe knowledge distillation process. Comprehensive experiments conducted on\nCIFAR-10, CIFAR-100, and Tiny-ImageNet datasets showcase the superior\nperformance of our method across various teacher-student network\nconfigurations, outperforming the contemporary state-of-the-art DFKD methods.\nCode will be available at:https://github.com/SLGSP/DDA.\n","authors":["Muquan Li","Dongyang Zhang","Tao He","Xiurui Xie","Yuan-Fang Li","Ke Qin"],"pdf_url":"https://arxiv.org/pdf/2410.17606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17598v1","updated":"2024-10-23T06:51:59Z","published":"2024-10-23T06:51:59Z","title":"PlantCamo: Plant Camouflage Detection","summary":"  Camouflaged Object Detection (COD) aims to detect objects with camouflaged\nproperties. Although previous studies have focused on natural (animals and\ninsects) and unnatural (artistic and synthetic) camouflage detection, plant\ncamouflage has been neglected. However, plant camouflage plays a vital role in\nnatural camouflage. Therefore, this paper introduces a new challenging problem\nof Plant Camouflage Detection (PCD). To address this problem, we introduce the\nPlantCamo dataset, which comprises 1,250 images with camouflaged plants\nrepresenting 58 object categories in various natural scenes. To investigate the\ncurrent status of plant camouflage detection, we conduct a large-scale\nbenchmark study using 20+ cutting-edge COD models on the proposed dataset. Due\nto the unique characteristics of plant camouflage, including holes and\nirregular borders, we developed a new framework, named PCNet, dedicated to PCD.\nOur PCNet surpasses performance thanks to its multi-scale global feature\nenhancement and refinement. Finally, we discuss the potential applications and\ninsights, hoping this work fills the gap in fine-grained COD research and\nfacilitates further intelligent ecology research. All resources will be\navailable on https://github.com/yjybuaa/PlantCamo.\n","authors":["Jinyu Yang","Qingwei Wang","Feng Zheng","Peng Chen","Aleš Leonardis","Deng-Ping Fan"],"pdf_url":"https://arxiv.org/pdf/2410.17598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17594v1","updated":"2024-10-23T06:47:29Z","published":"2024-10-23T06:47:29Z","title":"How to Continually Adapt Text-to-Image Diffusion Models for Flexible\n  Customization?","summary":"  Custom diffusion models (CDMs) have attracted widespread attention due to\ntheir astonishing generative ability for personalized concepts. However, most\nexisting CDMs unreasonably assume that personalized concepts are fixed and\ncannot change over time. Moreover, they heavily suffer from catastrophic\nforgetting and concept neglect on old personalized concepts when continually\nlearning a series of new concepts. To address these challenges, we propose a\nnovel Concept-Incremental text-to-image Diffusion Model (CIDM), which can\nresolve catastrophic forgetting and concept neglect to learn new customization\ntasks in a concept-incremental manner. Specifically, to surmount the\ncatastrophic forgetting of old concepts, we develop a concept consolidation\nloss and an elastic weight aggregation module. They can explore task-specific\nand task-shared knowledge during training, and aggregate all low-rank weights\nof old concepts based on their contributions during inference. Moreover, in\norder to address concept neglect, we devise a context-controllable synthesis\nstrategy that leverages expressive region features and noise estimation to\ncontrol the contexts of generated images according to user conditions.\nExperiments validate that our CIDM surpasses existing custom diffusion models.\nThe source codes are available at https://github.com/JiahuaDong/CIFC.\n","authors":["Jiahua Dong","Wenqi Liang","Hongliu Li","Duzhen Zhang","Meng Cao","Henghui Ding","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2410.17594v1.pdf","comment":"Accepted to NeurIPS2024"},{"id":"http://arxiv.org/abs/2406.08035v2","updated":"2024-10-23T06:37:01Z","published":"2024-06-12T09:36:52Z","title":"LVBench: An Extreme Long Video Understanding Benchmark","summary":"  Recent progress in multimodal large language models has markedly enhanced the\nunderstanding of short videos (typically under one minute), and several\nevaluation datasets have emerged accordingly. However, these advancements fall\nshort of meeting the demands of real-world applications such as embodied\nintelligence for long-term decision-making, in-depth movie reviews and\ndiscussions, and live sports commentary, all of which require comprehension of\nlong videos spanning several hours. To address this gap, we introduce LVBench,\na benchmark specifically designed for long video understanding. Our dataset\ncomprises publicly sourced videos and encompasses a diverse set of tasks aimed\nat long video comprehension and information extraction. LVBench is designed to\nchallenge multimodal models to demonstrate long-term memory and extended\ncomprehension capabilities. Our extensive evaluations reveal that current\nmultimodal models still underperform on these demanding long video\nunderstanding tasks. Through LVBench, we aim to spur the development of more\nadvanced models capable of tackling the complexities of long video\ncomprehension. Our data and code are publicly available at:\nhttps://lvbench.github.io.\n","authors":["Weihan Wang","Zehai He","Wenyi Hong","Yean Cheng","Xiaohan Zhang","Ji Qi","Xiaotao Gu","Shiyu Huang","Bin Xu","Yuxiao Dong","Ming Ding","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2406.08035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17555v2","updated":"2024-10-23T05:49:00Z","published":"2024-09-26T05:57:35Z","title":"Advancing Open-Set Domain Generalization Using Evidential Bi-Level\n  Hardest Domain Scheduler","summary":"  In Open-Set Domain Generalization (OSDG), the model is exposed to both new\nvariations of data appearance (domains) and open-set conditions, where both\nknown and novel categories are present at test time. The challenges of this\ntask arise from the dual need to generalize across diverse domains and\naccurately quantify category novelty, which is critical for applications in\ndynamic environments. Recently, meta-learning techniques have demonstrated\nsuperior results in OSDG, effectively orchestrating the meta-train and -test\ntasks by employing varied random categories and predefined domain partition\nstrategies. These approaches prioritize a well-designed training schedule over\ntraditional methods that focus primarily on data augmentation and the\nenhancement of discriminative feature learning. The prevailing meta-learning\nmodels in OSDG typically utilize a predefined sequential domain scheduler to\nstructure data partitions. However, a crucial aspect that remains inadequately\nexplored is the influence brought by strategies of domain schedulers during\ntraining. In this paper, we observe that an adaptive domain scheduler benefits\nmore in OSDG compared with prefixed sequential and random domain schedulers. We\npropose the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) to achieve\nan adaptive domain scheduler. This method strategically sequences domains by\nassessing their reliabilities in utilizing a follower network, trained with\nconfidence scores learned in an evidential manner, regularized by max rebiasing\ndiscrepancy, and optimized in a bi-level manner. The results show that our\nmethod substantially improves OSDG performance and achieves more discriminative\nembeddings for both the seen and unseen categories. The source code is publicly\navailable at https://github.com/KPeng9510/EBiL-HaDS.\n","authors":["Kunyu Peng","Di Wen","Kailun Yang","Ao Luo","Yufan Chen","Jia Fu","M. Saquib Sarfraz","Alina Roitberg","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2409.17555v2.pdf","comment":"Accepted to NeurIPS 2024. The source code is publicly available at\n  https://github.com/KPeng9510/EBiL-HaDS"},{"id":"http://arxiv.org/abs/2407.15815v2","updated":"2024-10-23T05:32:34Z","published":"2024-07-22T17:29:02Z","title":"Learning to Manipulate Anywhere: A Visual Generalizable Framework For\n  Reinforcement Learning","summary":"  Can we endow visuomotor robots with generalization capabilities to operate in\ndiverse open-world scenarios? In this paper, we propose \\textbf{Maniwhere}, a\ngeneralizable framework tailored for visual reinforcement learning, enabling\nthe trained robot policies to generalize across a combination of multiple\nvisual disturbance types. Specifically, we introduce a multi-view\nrepresentation learning approach fused with Spatial Transformer Network (STN)\nmodule to capture shared semantic information and correspondences among\ndifferent viewpoints. In addition, we employ a curriculum-based randomization\nand augmentation approach to stabilize the RL training process and strengthen\nthe visual generalization ability. To exhibit the effectiveness of Maniwhere,\nwe meticulously design 8 tasks encompassing articulate objects, bi-manual, and\ndexterous hand manipulation tasks, demonstrating Maniwhere's strong visual\ngeneralization and sim2real transfer abilities across 3 hardware platforms. Our\nexperiments show that Maniwhere significantly outperforms existing\nstate-of-the-art methods. Videos are provided at\nhttps://gemcollector.github.io/maniwhere/.\n","authors":["Zhecheng Yuan","Tianming Wei","Shuiqi Cheng","Gu Zhang","Yuanpei Chen","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2407.15815v2.pdf","comment":"Webpage: https://gemcollector.github.io/maniwhere/"},{"id":"http://arxiv.org/abs/2312.11309v2","updated":"2024-10-23T05:30:24Z","published":"2023-12-18T16:02:43Z","title":"The Ultimate Combo: Boosting Adversarial Example Transferability by\n  Composing Data Augmentations","summary":"  To help adversarial examples generalize from surrogate machine-learning (ML)\nmodels to targets, certain transferability-based black-box evasion attacks\nincorporate data augmentations (e.g., random resizing). Yet, prior work has\nexplored limited augmentations and their composition. To fill the gap, we\nsystematically studied how data augmentation affects transferability.\nSpecifically, we explored 46 augmentation techniques originally proposed to\nhelp ML models generalize to unseen benign samples, and assessed how they\nimpact transferability, when applied individually or composed. Performing\nexhaustive search on a small subset of augmentation techniques and genetic\nsearch on all techniques, we identified augmentation combinations that help\npromote transferability. Extensive experiments with the ImageNet and CIFAR-10\ndatasets and 18 models showed that simple color-space augmentations (e.g.,\ncolor to greyscale) attain high transferability when combined with standard\naugmentations. Furthermore, we discovered that composing augmentations impacts\ntransferability mostly monotonically (i.e., more augmentations $\\rightarrow$\n$\\ge$transferability). We also found that the best composition significantly\noutperformed the state of the art (e.g., 91.8% vs. $\\le$82.5% average\ntransferability to adversarially trained targets on ImageNet). Lastly, our\ntheoretical analysis, backed by empirical evidence, intuitively explains why\ncertain augmentations promote transferability.\n","authors":["Zebin Yun","Achi-Or Weingarten","Eyal Ronen","Mahmood Sharif"],"pdf_url":"https://arxiv.org/pdf/2312.11309v2.pdf","comment":"Accepted by AISec'24"},{"id":"http://arxiv.org/abs/2402.02316v3","updated":"2024-10-23T05:26:10Z","published":"2024-02-04T02:09:18Z","title":"Diffusion Models are Certifiably Robust Classifiers","summary":"  Generative learning, recognized for its effective modeling of data\ndistributions, offers inherent advantages in handling out-of-distribution\ninstances, especially for enhancing robustness to adversarial attacks. Among\nthese, diffusion classifiers, utilizing powerful diffusion models, have\ndemonstrated superior empirical robustness. However, a comprehensive\ntheoretical understanding of their robustness is still lacking, raising\nconcerns about their vulnerability to stronger future attacks. In this study,\nwe prove that diffusion classifiers possess $O(1)$ Lipschitzness, and establish\ntheir certified robustness, demonstrating their inherent resilience. To achieve\nnon-constant Lipschitzness, thereby obtaining much tighter certified\nrobustness, we generalize diffusion classifiers to classify Gaussian-corrupted\ndata. This involves deriving the evidence lower bounds (ELBOs) for these\ndistributions, approximating the likelihood using the ELBO, and calculating\nclassification probabilities via Bayes' theorem. Experimental results show the\nsuperior certified robustness of these Noised Diffusion Classifiers (NDCs).\nNotably, we achieve over 80% and 70% certified robustness on CIFAR-10 under\nadversarial perturbations with \\(\\ell_2\\) norms less than 0.25 and 0.5,\nrespectively, using a single off-the-shelf diffusion model without any\nadditional data.\n","authors":["Huanran Chen","Yinpeng Dong","Shitong Shao","Zhongkai Hao","Xiao Yang","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.02316v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17565v1","updated":"2024-10-23T05:19:20Z","published":"2024-10-23T05:19:20Z","title":"Double Banking on Knowledge: Customized Modulation and Prototypes for\n  Multi-Modality Semi-supervised Medical Image Segmentation","summary":"  Multi-modality (MM) semi-supervised learning (SSL) based medical image\nsegmentation has recently gained increasing attention for its ability to\nutilize MM data and reduce reliance on labeled images. However, current methods\nface several challenges: (1) Complex network designs hinder scalability to\nscenarios with more than two modalities. (2) Focusing solely on\nmodality-invariant representation while neglecting modality-specific features,\nleads to incomplete MM learning. (3) Leveraging unlabeled data with generative\nmethods can be unreliable for SSL. To address these problems, we propose Double\nBank Dual Consistency (DBDC), a novel MM-SSL approach for medical image\nsegmentation. To address challenge (1), we propose a modality all-in-one\nsegmentation network that accommodates data from any number of modalities,\nremoving the limitation on modality count. To address challenge (2), we design\ntwo learnable plug-in banks, Modality-Level Modulation bank (MLMB) and\nModality-Level Prototype (MLPB) bank, to capture both modality-invariant and\nmodality-specific knowledge. These banks are updated using our proposed\nModality Prototype Contrastive Learning (MPCL). Additionally, we design\nModality Adaptive Weighting (MAW) to dynamically adjust learning weights for\neach modality, ensuring balanced MM learning as different modalities learn at\ndifferent rates. Finally, to address challenge (3), we introduce a Dual\nConsistency (DC) strategy that enforces consistency at both the image and\nfeature levels without relying on generative methods. We evaluate our method on\na 2-to-4 modality segmentation task using three open-source datasets, and\nextensive experiments show that our method outperforms state-of-the-art\napproaches.\n","authors":["Yingyu Chen","Ziyuan Yang","Ming Yan","Zhongzhou Zhang","Hui Yu","Yan Liu","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06316v2","updated":"2024-10-23T05:14:19Z","published":"2023-12-11T12:03:30Z","title":"SemiSAM: Enhancing Semi-Supervised Medical Image Segmentation via\n  SAM-Assisted Consistency Regularization","summary":"  Semi-supervised learning has attracted much attention due to its less\ndependence on acquiring abundant annotations from experts compared to fully\nsupervised methods, which is especially important for medical image\nsegmentation which typically requires intensive pixel/voxel-wise labeling by\ndomain experts. Although semi-supervised methods can improve the performance by\nutilizing unlabeled data, there are still gaps between fully supervised methods\nunder extremely limited annotation scenarios. In this paper, we propose a\nsimple yet efficient strategy to explore the usage of the Segment Anything\nModel (SAM) for enhancing semi-supervised medical image segmentation.\nConcretely, the segmentation model trained with domain knowledge provides\ninformation for localization and generating input prompts to the SAM. Then the\ngenerated pseudo-labels of SAM are utilized as additional supervision to assist\nin the learning procedure of the semi-supervised framework. Extensive\nexperiments demonstrate that SemiSAM significantly improves the performance of\nexisting semi-supervised frameworks when only one or a few labeled images are\navailable and shows strong efficiency as a plug-and-play strategy for\nsemi-supervised medical image segmentation.\n","authors":["Yichi Zhang","Jin Yang","Yuchen Liu","Yuan Cheng","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2312.06316v2.pdf","comment":"Accept for BIBM 2024"},{"id":"http://arxiv.org/abs/2410.17557v1","updated":"2024-10-23T04:46:36Z","published":"2024-10-23T04:46:36Z","title":"BlurryScope: a cost-effective and compact scanning microscope for\n  automated HER2 scoring using deep learning on blurry image data","summary":"  We developed a rapid scanning optical microscope, termed \"BlurryScope\", that\nleverages continuous image acquisition and deep learning to provide a\ncost-effective and compact solution for automated inspection and analysis of\ntissue sections. BlurryScope integrates specialized hardware with a neural\nnetwork-based model to quickly process motion-blurred histological images and\nperform automated pathology classification. This device offers comparable speed\nto commercial digital pathology scanners, but at a significantly lower price\npoint and smaller size/weight, making it ideal for fast triaging in small\nclinics, as well as for resource-limited settings. To demonstrate the\nproof-of-concept of BlurryScope, we implemented automated classification of\nhuman epidermal growth factor receptor 2 (HER2) scores on immunohistochemically\n(IHC) stained breast tissue sections, achieving concordant results with those\nobtained from a high-end digital scanning microscope. We evaluated this\napproach by scanning HER2-stained tissue microarrays (TMAs) at a continuous\nspeed of 5 mm/s, which introduces bidirectional motion blur artifacts. These\ncompromised images were then used to train our network models. Using a test set\nof 284 unique patient cores, we achieved blind testing accuracies of 79.3% and\n89.7% for 4-class (0, 1+, 2+, 3+) and 2-class (0/1+ , 2+/3+) HER2 score\nclassification, respectively. BlurryScope automates the entire workflow, from\nimage scanning to stitching and cropping of regions of interest, as well as\nHER2 score classification. We believe BlurryScope has the potential to enhance\nthe current pathology infrastructure in resource-scarce environments, save\ndiagnostician time and bolster cancer identification and classification across\nvarious clinical environments.\n","authors":["Michael John Fanous","Christopher Michael Seybold","Hanlong Chen","Nir Pillar","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2410.17557v1.pdf","comment":"18 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2409.05280v2","updated":"2024-10-23T04:41:51Z","published":"2024-09-09T02:18:50Z","title":"RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac\n  Segmentation","summary":"  Cardiovascular disease remains a predominant global health concern,\nresponsible for a significant portion of mortality worldwide. Accurate\nsegmentation of cardiac medical imaging data is pivotal in mitigating fatality\nrates associated with cardiovascular conditions. However, existing\nstate-of-the-art (SOTA) neural networks, including both CNN-based and\nTransformer-based approaches, exhibit limitations in practical applicability\ndue to their inability to effectively capture inter-slice connections alongside\nintra-slice information. This deficiency is particularly evident in datasets\nfeaturing intricate, long-range details along the z-axis, such as coronary\narteries in axial views. Additionally, SOTA methods fail to differentiate\nnon-cardiac components from myocardium in segmentation, leading to the\n\"spraying\" phenomenon. To address these challenges, we present\nRotCAtt-TransUNet++, a novel architecture tailored for robust segmentation of\ncomplex cardiac structures. Our approach emphasizes modeling global contexts by\naggregating multiscale features with nested skip connections in the encoder. It\nintegrates transformer layers to capture interactions between patches and\nemploys a rotatory attention mechanism to capture connectivity between multiple\nslices (inter-slice information). Additionally, a channel-wise cross-attention\ngate guides the fused multi-scale channel-wise information and features from\ndecoder stages to bridge semantic gaps. Experimental results demonstrate that\nour proposed model outperforms existing SOTA approaches across four cardiac\ndatasets and one abdominal dataset. Importantly, coronary arteries and\nmyocardium are annotated with near-perfect accuracy during inference. An\nablation study shows that the rotatory attention mechanism effectively\ntransforms embedded vectorized patches in the semantic dimensional space,\nenhancing segmentation accuracy.\n","authors":["Quoc-Bao Nguyen-Le","Tuan-Hy Le","Anh-Triet Do","Quoc-Huy Trinh"],"pdf_url":"https://arxiv.org/pdf/2409.05280v2.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.12029v2","updated":"2024-10-23T04:36:26Z","published":"2023-09-21T12:51:11Z","title":"Exploring Self-Supervised Skeleton-Based Human Action Recognition under\n  Occlusions","summary":"  To integrate self-supervised skeleton-based action recognition methods into\nautonomous robotic systems, it is crucial to consider adverse situations\ninvolving target occlusions. Such a scenario, despite its practical relevance,\nis rarely addressed in existing self-supervised skeleton-based action\nrecognition methods. To empower models with the capacity to address occlusion,\nwe propose a simple and effective method. We first pre-train using occluded\nskeleton sequences, then use k-means clustering (KMeans) on sequence embeddings\nto group semantically similar samples. Next, we propose KNN-Imputation to fill\nin missing skeleton data based on the closest sample neighbors. Imputing\nincomplete skeleton sequences to create relatively complete sequences as input\nprovides significant benefits to existing skeleton-based self-supervised\nmethods. Meanwhile, building on the state-of-the-art Partial Spatio-Temporal\nLearning (PSTL), we introduce an Occluded Partial Spatio-Temporal Learning\n(OPSTL) framework. This enhancement utilizes Adaptive Spatial Masking (ASM) for\nbetter use of high-quality, intact skeletons. The new proposed method is\nverified on the challenging occluded versions of the NTURGB+D 60 and NTURGB+D\n120. The source code is publicly available at https://github.com/cyfml/OPSTL.\n","authors":["Yifei Chen","Kunyu Peng","Alina Roitberg","David Schneider","Jiaming Zhang","Junwei Zheng","Ruiping Liu","Yufan Chen","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2309.12029v2.pdf","comment":"The source code is publicly available at\n  https://github.com/cyfml/OPSTL"},{"id":"http://arxiv.org/abs/2305.19599v4","updated":"2024-10-23T03:59:05Z","published":"2023-05-31T06:59:21Z","title":"RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine\n  Semantic Re-alignment","summary":"  Recent advances in text-to-image diffusion models have achieved remarkable\nsuccess in generating high-quality, realistic images from textual descriptions.\nHowever, these approaches have faced challenges in precisely aligning the\ngenerated visual content with the textual concepts described in the prompts. In\nthis paper, we propose a two-stage coarse-to-fine semantic re-alignment method,\nnamed RealignDiff, aimed at improving the alignment between text and images in\ntext-to-image diffusion models. In the coarse semantic re-alignment phase, a\nnovel caption reward, leveraging the BLIP-2 model, is proposed to evaluate the\nsemantic discrepancy between the generated image caption and the given text\nprompt. Subsequently, the fine semantic re-alignment stage employs a local\ndense caption generation module and a re-weighting attention modulation module\nto refine the previously generated images from a local semantic view.\nExperimental results on the MS-COCO and ViLG-300 datasets demonstrate that the\nproposed two-stage coarse-to-fine semantic re-alignment method outperforms\nother baseline re-alignment techniques by a substantial margin in both visual\nquality and semantic similarity with the input prompt.\n","authors":["Guian Fang","Zutao Jiang","Jianhua Han","Guansong Lu","Hang Xu","Shengcai Liao","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2305.19599v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17543v1","updated":"2024-10-23T03:47:29Z","published":"2024-10-23T03:47:29Z","title":"Unsupervised Low-dose CT Reconstruction with One-way Conditional\n  Normalizing Flows","summary":"  Deep-learning methods have shown promising performance for low-dose computed\ntomography (LDCT) reconstruction. However, supervised methods face the problem\nof lacking labeled data in clinical scenarios, and the CNN-based unsupervised\ndenoising methods would cause excessive smoothing in the reconstructed image.\nRecently, the normalizing flows (NFs) based methods have shown advantages in\nproducing detail-rich images and avoiding over-smoothing, however, there are\nstill issues: (1) Although the alternating optimization in the data and latent\nspace can well utilize the regularization and generation capabilities of NFs,\nthe current two-way transformation strategy of noisy images and latent\nvariables would cause detail loss and secondary artifacts; and (2) Training NFs\non high-resolution CT images is hard due to huge computation. Though using\nconditional normalizing flows (CNFs) to learn conditional probability can\nreduce the computational burden, current methods require labeled data for\nconditionalization, and the unsupervised CNFs-based LDCT reconstruction remains\na problem. To tackle these problems, we propose a novel CNFs-based unsupervised\nLDCT iterative reconstruction algorithm. It employs strict one-way\ntransformation when performing alternating optimization in the dual spaces,\nthus effectively avoiding the problems of detail loss and secondary artifacts.\nBy proposing a novel unsupervised conditionalization strategy, we train CNFs on\nhigh-resolution CT images, thus achieving fast and high-quality unsupervised\nreconstruction. Experiments on different datasets suggest that the performance\nof the proposed algorithm could surpass some state-of-the-art unsupervised and\neven supervised methods.\n","authors":["Ran An","Ke Chen","Hongwei Li"],"pdf_url":"https://arxiv.org/pdf/2410.17543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05741v2","updated":"2024-10-23T03:39:00Z","published":"2024-02-08T15:19:50Z","title":"Real-World Robot Applications of Foundation Models: A Review","summary":"  Recent developments in foundation models, like Large Language Models (LLMs)\nand Vision-Language Models (VLMs), trained on extensive data, facilitate\nflexible application across different tasks and modalities. Their impact spans\nvarious fields, including healthcare, education, and robotics. This paper\nprovides an overview of the practical application of foundation models in\nreal-world robotics, with a primary emphasis on the replacement of specific\ncomponents within existing robot systems. The summary encompasses the\nperspective of input-output relationships in foundation models, as well as\ntheir role in perception, motion planning, and control within the field of\nrobotics. This paper concludes with a discussion of future challenges and\nimplications for practical robot applications.\n","authors":["Kento Kawaharazuka","Tatsuya Matsushima","Andrew Gambardella","Jiaxian Guo","Chris Paxton","Andy Zeng"],"pdf_url":"https://arxiv.org/pdf/2402.05741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17534v1","updated":"2024-10-23T03:28:46Z","published":"2024-10-23T03:28:46Z","title":"OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object\n  Tracking","summary":"  Open-vocabulary object perception has become an important topic in artificial\nintelligence, which aims to identify objects with novel classes that have not\nbeen seen during training. Under this setting, open-vocabulary object detection\n(OVD) in a single image has been studied in many literature. However,\nopen-vocabulary object tracking (OVT) from a video has been studied less, and\none reason is the shortage of benchmarks. In this work, we have built a new\nlarge-scale benchmark for open-vocabulary multi-object tracking namely OVT-B.\nOVT-B contains 1,048 categories of objects and 1,973 videos with 637,608\nbounding box annotations, which is much larger than the sole open-vocabulary\ntracking dataset, i.e., OVTAO-val dataset (200+ categories, 900+ videos). The\nproposed OVT-B can be used as a new benchmark to pave the way for OVT research.\nWe also develop a simple yet effective baseline method for OVT. It integrates\nthe motion features for object tracking, which is an important feature for MOT\nbut is ignored in previous OVT methods. Experimental results have verified the\nusefulness of the proposed benchmark and the effectiveness of our method. We\nhave released the benchmark to the public at\nhttps://github.com/Coo1Sea/OVT-B-Dataset.\n","authors":["Haiji Liang","Ruize Han"],"pdf_url":"https://arxiv.org/pdf/2410.17534v1.pdf","comment":"15 pages, 6 figures, accepted at NeurIPS 2024 Dataset and Benchmark\n  Track"},{"id":"http://arxiv.org/abs/2406.14515v2","updated":"2024-10-23T03:09:54Z","published":"2024-06-20T17:26:01Z","title":"MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding","summary":"  The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.\n","authors":["Xinyu Fang","Kangrui Mao","Haodong Duan","Xiangyu Zhao","Yining Li","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14515v2.pdf","comment":"Accepted in NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2406.18925v3","updated":"2024-10-23T02:57:31Z","published":"2024-06-27T06:32:56Z","title":"Selective Vision is the Challenge for Visual Reasoning: A Benchmark for\n  Visual Argument Understanding","summary":"  Visual arguments, often used in advertising or social causes, rely on images\nto persuade viewers to do or believe something. Understanding these arguments\nrequires selective vision: only specific visual stimuli within an image are\nrelevant to the argument, and relevance can only be understood within the\ncontext of a broader argumentative structure. While visual arguments are\nreadily appreciated by human audiences, we ask: are today's AI capable of\nsimilar understanding? We present VisArgs, a dataset of 1,611 images annotated\nwith 5,112 visual premises (with regions), 5,574 commonsense premises, and\nreasoning trees connecting them into structured arguments. We propose three\ntasks for evaluating visual argument understanding: premise localization,\npremise identification, and conclusion deduction. Experiments show that 1)\nmachines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy,\nwhile humans reached 98.0%. Models also performed 19.5% worse when\ndistinguishing between irrelevant objects within the image compared to external\nobjects. 2) Providing relevant visual premises improved model performance\nsignificantly.\n","authors":["Jiwan Chung","Sungjae Lee","Minseo Kim","Seungju Han","Ashkan Yousefpour","Jack Hessel","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.18925v3.pdf","comment":"12 pages, 6 figures. Accepted as main paper in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01023v2","updated":"2024-10-23T02:55:20Z","published":"2024-10-01T19:32:57Z","title":"Can visual language models resolve textual ambiguity with visual cues?\n  Let visual puns tell you!","summary":"  Humans possess multimodal literacy, allowing them to actively integrate\ninformation from various modalities to form reasoning. Faced with challenges\nlike lexical ambiguity in text, we supplement this with other modalities, such\nas thumbnail images or textbook illustrations. Is it possible for machines to\nachieve a similar multimodal understanding capability? In response, we present\nUnderstanding Pun with Image Explanations (UNPIE), a novel benchmark designed\nto assess the impact of multimodal inputs in resolving lexical ambiguities.\nPuns serve as the ideal subject for this evaluation due to their intrinsic\nambiguity. Our dataset includes 1,000 puns, each accompanied by an image that\nexplains both meanings. We pose three multimodal challenges with the\nannotations to assess different aspects of multimodal literacy; Pun Grounding,\nDisambiguation, and Reconstruction. The results indicate that various Socratic\nModels and Visual-Language Models improve over the text-only models when given\nvisual context, particularly as the complexity of the tasks increases.\n","authors":["Jiwan Chung","Seungwon Lim","Jaehyun Jeon","Seungbeen Lee","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01023v2.pdf","comment":"Accepted as main paper in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17521v1","updated":"2024-10-23T02:52:53Z","published":"2024-10-23T02:52:53Z","title":"Diffusion Priors for Variational Likelihood Estimation and Image\n  Denoising","summary":"  Real-world noise removal is crucial in low-level computer vision. Due to the\nremarkable generation capabilities of diffusion models, recent attention has\nshifted towards leveraging diffusion priors for image restoration tasks.\nHowever, existing diffusion priors-based methods either consider simple noise\ntypes or rely on approximate posterior estimation, limiting their effectiveness\nin addressing structured and signal-dependent noise commonly found in\nreal-world images. In this paper, we build upon diffusion priors and propose\nadaptive likelihood estimation and MAP inference during the reverse diffusion\nprocess to tackle real-world noise. We introduce an independent,\nnon-identically distributed likelihood combined with the noise precision\n(inverse variance) prior and dynamically infer the precision posterior using\nvariational Bayes during the generation process. Meanwhile, we rectify the\nestimated noise variance through local Gaussian convolution. The final denoised\nimage is obtained by propagating intermediate MAP solutions that balance the\nupdated likelihood and diffusion prior. Additionally, we explore the local\ndiffusion prior inherent in low-resolution diffusion models, enabling direct\nhandling of high-resolution noisy images. Extensive experiments and analyses on\ndiverse real-world datasets demonstrate the effectiveness of our method. Code\nis available at https://github.com/HUST-Tan/DiffusionVI.\n","authors":["Jun Cheng","Shan Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17521v1.pdf","comment":"Accepted by NeurIPS2024 as Spotlight"},{"id":"http://arxiv.org/abs/2405.20279v2","updated":"2024-10-23T02:38:44Z","published":"2024-05-30T17:33:10Z","title":"CV-VAE: A Compatible Video VAE for Latent Generative Video Models","summary":"  Spatio-temporal compression of videos, utilizing networks such as Variational\nAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other\nvideo generative models. For instance, many LLM-like video models learn the\ndistribution of discrete tokens derived from 3D VAEs within the VQVAE\nframework, while most diffusion-based video models capture the distribution of\ncontinuous latent extracted by 2D VAEs without quantization. The temporal\ncompression is simply realized by uniform frame sampling which results in\nunsmooth motion between consecutive frames. Currently, there lacks of a\ncommonly used continuous video (3D) VAE for latent diffusion-based video models\nin the research community. Moreover, since current diffusion-based approaches\nare often implemented using pre-trained text-to-image (T2I) models, directly\ntraining a video VAE without considering the compatibility with existing T2I\nmodels will result in a latent space gap between them, which will take huge\ncomputational resources for training to bridge the gap even with the T2I models\nas initialization. To address this issue, we propose a method for training a\nvideo VAE of latent video models, namely CV-VAE, whose latent space is\ncompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion\n(SD). The compatibility is achieved by the proposed novel latent space\nregularization, which involves formulating a regularization loss using the\nimage VAE. Benefiting from the latent space compatibility, video models can be\ntrained seamlessly from pre-trained T2I or video models in a truly\nspatio-temporally compressed latent space, rather than simply sampling video\nframes at equal intervals. With our CV-VAE, existing video models can generate\nfour times more frames with minimal finetuning. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed video VAE.\n","authors":["Sijie Zhao","Yong Zhang","Xiaodong Cun","Shaoshu Yang","Muyao Niu","Xiaoyu Li","Wenbo Hu","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2405.20279v2.pdf","comment":"Project Page: https://ailab-cvc.github.io/cvvae/index.html"},{"id":"http://arxiv.org/abs/2410.17514v1","updated":"2024-10-23T02:38:12Z","published":"2024-10-23T02:38:12Z","title":"PathMoCo: A Novel Framework to Improve Feature Embedding in\n  Self-supervised Contrastive Learning for Histopathological Images","summary":"  Self-supervised contrastive learning has become a cornerstone in various\nareas, particularly histopathological image analysis. Image augmentation plays\na crucial role in self-supervised contrastive learning, as it generates\nvariations in image samples. However, traditional image augmentation techniques\noften overlook the unique characteristics of histopathological images. In this\npaper, we propose a new histopathology-specific image augmentation method\ncalled stain reconstruction augmentation (SRA). We integrate our SRA with MoCo\nv3, a leading model in self-supervised contrastive learning, along with our\nadditional contrastive loss terms, and call the new model PathMoCo. We\ndemonstrate that our PathMoCo always outperforms the standard MoCo v3 across\nvarious downstream tasks and achieves comparable or superior performance to\nother foundation models pre-trained on significantly larger histopathology\ndatasets.\n","authors":["Hamid Manoochehri","Bodong Zhang","Beatrice S. Knudsen","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2410.17514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17513v1","updated":"2024-10-23T02:36:47Z","published":"2024-10-23T02:36:47Z","title":"HCDN: A Change Detection Network for Construction Housekeeping Using\n  Feature Fusion and Large Vision Models","summary":"  Workplace safety has received increasing attention as millions of workers\nworldwide suffer from work-related accidents. Despite poor housekeeping is a\nsignificant contributor to construction accidents, there remains a significant\nlack of technological research focused on improving housekeeping practices in\nconstruction sites. Recognizing and locating poor housekeeping in a dynamic\nconstruction site is an important task that can be improved through computer\nvision approaches. Despite advances in AI and computer vision, existing methods\nfor detecting poor housekeeping conditions face many challenges, including\nlimited explanations, lack of locating of poor housekeeping, and lack of\nannotated datasets. On the other hand, change detection which aims to detect\nthe changed environmental conditions (e.g., changing from good to poor\nhousekeeping) and 'where' the change has occurred (e.g., location of objects\ncausing poor housekeeping), has not been explored to the problem of\nhousekeeping management. To address these challenges, we propose the\nHousekeeping Change Detection Network (HCDN), an advanced change detection\nneural network that integrates a feature fusion module and a large vision\nmodel, achieving state-of-the-art performance. Additionally, we introduce the\napproach to establish a novel change detection dataset (named Housekeeping-CCD)\nfocused on housekeeping in construction sites, along with a housekeeping\nsegmentation dataset. Our contributions include significant performance\nimprovements compared to existing methods, providing an effective tool for\nenhancing construction housekeeping and safety. To promote further development,\nwe share our source code and trained models for global researchers:\nhttps://github.com/NUS-DBE/Housekeeping-CD.\n","authors":["Kailai Sun","Zherui Shao","Yang Miang Goh","Jing Tian","Vincent J. L. Gan"],"pdf_url":"https://arxiv.org/pdf/2410.17513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15070v2","updated":"2024-10-23T02:35:58Z","published":"2024-07-21T06:03:11Z","title":"GPHM: Gaussian Parametric Head Model for Monocular Head Avatar\n  Reconstruction","summary":"  Creating high-fidelity 3D human head avatars is crucial for applications in\nVR/AR, digital human, and film production. Recent advances have leveraged\nmorphable face models to generate animated head avatars from easily accessible\ndata, representing varying identities and expressions within a low-dimensional\nparametric space. However, existing methods often struggle with modeling\ncomplex appearance details, e.g., hairstyles, and suffer from low rendering\nquality and efficiency. In this paper we introduce a novel approach, 3D\nGaussian Parametric Head Model, which employs 3D Gaussians to accurately\nrepresent the complexities of the human head, allowing precise control over\nboth identity and expression. The Gaussian model can handle intricate details,\nenabling realistic representations of varying appearances and complex\nexpressions. Furthermore, we presents a well-designed training framework to\nensure smooth convergence, providing a robust guarantee for learning the rich\ncontent. Our method achieves high-quality, photo-realistic rendering with\nreal-time efficiency, making it a valuable contribution to the field of\nparametric head models. Finally, we apply the 3D Gaussian Parametric Head Model\nto monocular video or few-shot head avatar reconstruction tasks, which enables\ninstant reconstruction of high-quality 3D head avatars even when input data is\nextremely limited, surpassing previous methods in terms of reconstruction\nquality and training speed.\n","authors":["Yuelang Xu","Zhaoqi Su","Qingyao Wu","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15070v2.pdf","comment":"Project page: https://yuelangx.github.io/gphmv2/"},{"id":"http://arxiv.org/abs/2403.08350v2","updated":"2024-10-23T02:16:37Z","published":"2024-03-13T08:54:31Z","title":"CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large\n  Language Model","summary":"  Instruction tuning represents a prevalent strategy employed by Multimodal\nLarge Language Models (MLLMs) to align with human instructions and adapt to new\ntasks. Nevertheless, MLLMs encounter the challenge of adapting to users'\nevolving knowledge and demands. Therefore, how to retain existing skills while\nacquiring new knowledge needs to be investigated. In this paper, we present a\ncomprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess\nexisting MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10\ncommonly used datasets spanning 8 task categories, ensuring a diverse range of\ninstructions and tasks. Besides, the trained model is evaluated from two\naspects: Instruction Following and General Knowledge, which assess the\nalignment with human intention and knowledge preserved for reasoning,\nrespectively. Experiments on CoIN demonstrate that current powerful MLLMs still\nsuffer catastrophic forgetting, and the failure in intention alignment assumes\nthe main responsibility, instead of the knowledge forgetting. To this end, we\nintroduce MoELoRA to MLLMs which is effective to retain the previous\ninstruction alignment. Experimental results consistently illustrate the\nforgetting decreased from this method on CoIN.\n","authors":["Cheng Chen","Junchen Zhu","Xu Luo","Hengtao Shen","Lianli Gao","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2403.08350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05166v4","updated":"2024-10-23T02:10:29Z","published":"2024-09-08T17:35:48Z","title":"CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes","summary":"  Current methodologies for novel view synthesis (NVS) in dynamic scenes\nencounter significant challenges in harmonizing memory consumption, model\ncomplexity, training efficiency, and rendering fidelity. Existing offline\ntechniques, while delivering high-quality results, are often characterized by\nsubstantial memory demands and limited scalability. In contrast, online methods\ngrapple with the challenge of balancing rapid convergence with model\ncompactness. To address these issues, we propose continual dynamic neural\ngraphics primitives (CD-NGP). Our approach synergizes features from both\ntemporal and spatial hash encodings to achieve high rendering quality, employs\nparameter reuse to enhance scalability, and leverages a continual learning\nframework to mitigate memory overhead. Furthermore, we introduce a novel\ndataset comprising multi-view, exceptionally long video sequences with\nsubstantial rigid and non-rigid motion, thereby substantiating the scalability\nof our method.\n","authors":["Zhenhuan Liu","Shuai Liu","Zhiwei Ning","Jie Yang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.05166v4.pdf","comment":"new template, editing"},{"id":"http://arxiv.org/abs/2410.17505v1","updated":"2024-10-23T02:05:05Z","published":"2024-10-23T02:05:05Z","title":"PLGS: Robust Panoptic Lifting with 3D Gaussian Splatting","summary":"  Previous methods utilize the Neural Radiance Field (NeRF) for panoptic\nlifting, while their training and rendering speed are unsatisfactory. In\ncontrast, 3D Gaussian Splatting (3DGS) has emerged as a prominent technique due\nto its rapid training and rendering speed. However, unlike NeRF, the\nconventional 3DGS may not satisfy the basic smoothness assumption as it does\nnot rely on any parameterized structures to render (e.g., MLPs). Consequently,\nthe conventional 3DGS is, in nature, more susceptible to noisy 2D mask\nsupervision. In this paper, we propose a new method called PLGS that enables\n3DGS to generate consistent panoptic segmentation masks from noisy 2D\nsegmentation masks while maintaining superior efficiency compared to NeRF-based\nmethods. Specifically, we build a panoptic-aware structured 3D Gaussian model\nto introduce smoothness and design effective noise reduction strategies. For\nthe semantic field, instead of initialization with structure from motion, we\nconstruct reliable semantic anchor points to initialize the 3D Gaussians. We\nthen use these anchor points as smooth regularization during training.\nAdditionally, we present a self-training approach using pseudo labels generated\nby merging the rendered masks with the noisy masks to enhance the robustness of\nPLGS. For the instance field, we project the 2D instance masks into 3D space\nand match them with oriented bounding boxes to generate cross-view consistent\ninstance masks for supervision. Experiments on various benchmarks demonstrate\nthat our method outperforms previous state-of-the-art methods in terms of both\nsegmentation quality and speed.\n","authors":["Yu Wang","Xiaobao Wei","Ming Lu","Guoliang Kang"],"pdf_url":"https://arxiv.org/pdf/2410.17505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17502v1","updated":"2024-10-23T02:00:07Z","published":"2024-10-23T02:00:07Z","title":"Bilateral Hippocampi Segmentation in Low Field MRIs Using Mutual Feature\n  Learning via Dual-Views","summary":"  Accurate hippocampus segmentation in brain MRI is critical for studying\ncognitive and memory functions and diagnosing neurodevelopmental disorders.\nWhile high-field MRIs provide detailed imaging, low-field MRIs are more\naccessible and cost-effective, which eliminates the need for sedation in\nchildren, though they often suffer from lower image quality. In this paper, we\npresent a novel deep-learning approach for the automatic segmentation of\nbilateral hippocampi in low-field MRIs. Extending recent advancements in infant\nbrain segmentation to underserved communities through the use of low-field MRIs\nensures broader access to essential diagnostic tools, thereby supporting better\nhealthcare outcomes for all children. Inspired by our previous work, Co-BioNet,\nthe proposed model employs a dual-view structure to enable mutual feature\nlearning via high-frequency masking, enhancing segmentation accuracy by\nleveraging complementary information from different perspectives. Extensive\nexperiments demonstrate that our method provides reliable segmentation outcomes\nfor hippocampal analysis in low-resource settings. The code is publicly\navailable at: https://github.com/himashi92/LoFiHippSeg.\n","authors":["Himashi Peiris","Zhaolin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.17502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17494v1","updated":"2024-10-23T01:25:25Z","published":"2024-10-23T01:25:25Z","title":"Enhancing Multimodal Medical Image Classification using Cross-Graph\n  Modal Contrastive Learning","summary":"  The classification of medical images is a pivotal aspect of disease\ndiagnosis, often enhanced by deep learning techniques. However, traditional\napproaches typically focus on unimodal medical image data, neglecting the\nintegration of diverse non-image patient data. This paper proposes a novel\nCross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal medical\nimage classification. The model effectively integrates both image and non-image\ndata by constructing cross-modality graphs and leveraging contrastive learning\nto align multimodal features in a shared latent space. An inter-modality\nfeature scaling module further optimizes the representation learning process by\nreducing the gap between heterogeneous modalities. The proposed approach is\nevaluated on two datasets: a Parkinson's disease (PD) dataset and a public\nmelanoma dataset. Results demonstrate that CGMCL outperforms conventional\nunimodal methods in accuracy, interpretability, and early disease prediction.\nAdditionally, the method shows superior performance in multi-class melanoma\nclassification. The CGMCL framework provides valuable insights into medical\nimage classification while offering improved disease interpretability and\npredictive capabilities.\n","authors":["Jun-En Ding","Chien-Chin Hsu","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10937v2","updated":"2024-10-23T01:13:24Z","published":"2024-10-14T17:59:58Z","title":"Hybrid Spatial Representations for Species Distribution Modeling","summary":"  We address an important problem in ecology called Species Distribution\nModeling (SDM), whose goal is to predict whether a species exists at a certain\nposition on Earth. In particular, we tackle a challenging version of this task,\nwhere we learn from presence-only data in a community-sourced dataset, model a\nlarge number of species simultaneously, and do not use any additional\nenvironmental information. Previous work has used neural implicit\nrepresentations to construct models that achieve promising results. However,\nimplicit representations often generate predictions of limited spatial\nprecision. We attribute this limitation to their inherently global formulation\nand inability to effectively capture local feature variations. This issue is\nespecially pronounced with presence-only data and a large number of species. To\naddress this, we propose a hybrid embedding scheme that combines both implicit\nand explicit embeddings. Specifically, the explicit embedding is implemented\nwith a multiresolution hashgrid, enabling our models to better capture local\ninformation. Experiments demonstrate that our results exceed other works by a\nlarge margin on various standard benchmarks, and that the hybrid representation\nis better than both purely implicit and explicit ones. Qualitative\nvisualizations and comprehensive ablation studies reveal that our hybrid\nrepresentation successfully addresses the two main challenges. Our code is\nopen-sourced at https://github.com/Shiran-Yuan/HSR-SDM.\n","authors":["Shiran Yuan","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.10937v2.pdf","comment":"Project codebase https://github.com/Shiran-Yuan/HSR-SDM"},{"id":"http://arxiv.org/abs/2410.17489v1","updated":"2024-10-23T00:59:27Z","published":"2024-10-23T00:59:27Z","title":"Unsupervised Domain Adaptation for Action Recognition via\n  Self-Ensembling and Conditional Embedding Alignment","summary":"  Recent advancements in deep learning-based wearable human action recognition\n(wHAR) have improved the capture and classification of complex motions, but\nadoption remains limited due to the lack of expert annotations and domain\ndiscrepancies from user variations. Limited annotations hinder the model's\nability to generalize to out-of-distribution samples. While data augmentation\ncan improve generalizability, unsupervised augmentation techniques must be\napplied carefully to avoid introducing noise. Unsupervised domain adaptation\n(UDA) addresses domain discrepancies by aligning conditional distributions with\nlabeled target samples, but vanilla pseudo-labeling can lead to error\npropagation. To address these challenges, we propose $\\mu$DAR, a novel joint\noptimization architecture comprised of three functions: (i) consistency\nregularizer between augmented samples to improve model classification\ngeneralizability, (ii) temporal ensemble for robust pseudo-label generation and\n(iii) conditional distribution alignment to improve domain generalizability.\nThe temporal ensemble works by aggregating predictions from past epochs to\nsmooth out noisy pseudo-label predictions, which are then used in the\nconditional distribution alignment module to minimize kernel-based class-wise\nconditional maximum mean discrepancy ($k$CMMD) between the source and target\nfeature space to learn a domain invariant embedding. The\nconsistency-regularized augmentations ensure that multiple augmentations of the\nsame sample share the same labels; this results in (a) strong generalization\nwith limited source domain samples and (b) consistent pseudo-label generation\nin target samples. The novel integration of these three modules in $\\mu$DAR\nresults in a range of $\\approx$ 4-12% average macro-F1 score improvement over\nsix state-of-the-art UDA methods in four benchmark wHAR datasets\n","authors":["Indrajeet Ghosh","Garvit Chugh","Abu Zaher Md Faridee","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2410.17489v1.pdf","comment":"This work has been accepted to the Proceedings of the IEEE\n  International Conference on Data Mining, 2024"},{"id":"http://arxiv.org/abs/2410.17488v1","updated":"2024-10-23T00:51:47Z","published":"2024-10-23T00:51:47Z","title":"GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion\n  Policy","summary":"  Diffusion-based policies have shown remarkable capability in executing\ncomplex robotic manipulation tasks but lack explicit characterization of\ngeometry and semantics, which often limits their ability to generalize to\nunseen objects and layouts. To enhance the generalization capabilities of\nDiffusion Policy, we introduce a novel framework that incorporates explicit\nspatial and semantic information via 3D semantic fields. We generate 3D\ndescriptor fields from multi-view RGBD observations with large foundational\nvision models, then compare these descriptor fields against reference\ndescriptors to obtain semantic fields. The proposed method explicitly considers\ngeometry and semantics, enabling strong generalization capabilities in tasks\nrequiring category-level generalization, resolving geometric ambiguities, and\nattention to subtle geometric details. We evaluate our method across eight\ntasks involving articulated objects and instances with varying shapes and\ntextures from multiple object categories. Our method demonstrates its\neffectiveness by increasing Diffusion Policy's average success rate on unseen\ninstances from 20% to 93%. Additionally, we provide a detailed analysis and\nvisualization to interpret the sources of performance gain and explain how our\nmethod can generalize to novel instances.\n","authors":["Yixuan Wang","Guang Yin","Binghao Huang","Tarik Kelestemur","Jiuguang Wang","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2410.17488v1.pdf","comment":"Accepted to Conference on Robot Learning (CoRL 2024). Project Page:\n  https://robopil.github.io/GenDP/"},{"id":"http://arxiv.org/abs/2410.17484v1","updated":"2024-10-23T00:31:17Z","published":"2024-10-23T00:31:17Z","title":"Which Client is Reliable?: A Reliable and Personalized Prompt-based\n  Federated Learning for Medical Image Question Answering","summary":"  Conventional medical artificial intelligence (AI) models face barriers in\nclinical application and ethical issues owing to their inability to handle the\nprivacy-sensitive characteristics of medical data. We present a novel\npersonalized federated learning (pFL) method for medical visual question\nanswering (VQA) models, addressing privacy reliability challenges in the\nmedical domain. Our method introduces learnable prompts into a Transformer\narchitecture to efficiently train it on diverse medical datasets without\nmassive computational costs. Then we introduce a reliable client VQA model that\nincorporates Dempster-Shafer evidence theory to quantify uncertainty in\npredictions, enhancing the model's reliability. Furthermore, we propose a novel\ninter-client communication mechanism that uses maximum likelihood estimation to\nbalance accuracy and uncertainty, fostering efficient integration of insights\nacross clients.\n","authors":["He Zhu","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2410.17484v1.pdf","comment":null}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2410.17106v2","updated":"2024-10-23T02:52:58Z","published":"2024-10-22T15:30:24Z","title":"Feature Homomorphism -- A Cryptographic Scheme For Data Verification\n  Under Ciphertext-Only Conditions","summary":"  Privacy computing involves the extensive exchange and processing of encrypted\ndata. For the parties involved in these interactions, how to determine the\nconsistency of exchanged data without accessing the original data, ensuring\ntamper resistance, non-repudiation, quality traceability, indexing, and\nretrieval during the use of encrypted data, which is a key topic of achieving\n\"Data Availability versus Visibility\". This paper proposes a new type of\nhomomorphism: Feature Homomorphism, and based on this feature, introduces a\ncryptographic scheme for data verification under ciphertext-only conditions.\nThe proposed scheme involves designing a group of algorithms that meet the\nrequirements outlined in this paper, including encryption/decryption algorithms\nand Feature Homomorphic Algorithm. This group of algorithms not only allows for\nthe encryption and decryption of data but also ensures that the plaintext and\nits corresponding ciphertext, encrypted using the specified encryption\nalgorithm, satisfy the following property: the eigenvalue of the plaintext\nobtained using the Feature Homomorphic Algorithm is equal to the eigenvalue of\nthe ciphertext obtained using the same algorithm. With this group of\nalgorithms, it is possible to verify data consistency directly by comparing the\neigenvalues of the plaintext and ciphertext without accessing the original data\n(i.e., under ciphertext-only conditions). This can be used for tamper\nresistance, non-repudiation, and quality traceability. Additionally, the\neigenvalue can serve as a ciphertext index, enabling searchable encryption.\nThis scheme completes a piece of the puzzle in homomorphic encryption.\n  Keywords: Privacy Computing, Data Consistency, Searchable Encryption,\nZero-Knowledge Proof, Feature Homomorphism\n","authors":["Huang Neng"],"pdf_url":"https://arxiv.org/pdf/2410.17106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10963v2","updated":"2024-10-23T17:47:20Z","published":"2024-08-20T16:00:17Z","title":"KeySpace: Public Key Infrastructure Considerations in Interplanetary\n  Networks","summary":"  As satellite networks grow larger and begin to incorporate interplanetary\ncommunication, there is an increasing interest in the unsolved problem of how\nto approach PKI in these conditions. In this paper we explore the goals and\nrequirements for implementing key management systems in satellite networks,\nfocusing on megaconstellations and interplanetary networks. We design a set of\nstandardized experiments which can be used to compare systems against one\nanother for particular network topologies. Using these, we demonstrate that\nterrestrial PKI techniques are feasible in highly distributed interplanetary\nnetworks, showing that it is possible to configure PKI systems to achieve\nefficient low-latency connection establishment, and minimize the impact of\nattacks through effective revocations.\n  We evaluate this by building the Deep Space Network Simulator (DSNS), a novel\nnetwork simulator aimed at efficient simulation of large space networks. We run\nsimulations evaluating connection establishment and key revocation under a wide\nrange of PKI configurations. Finally, we propose and evaluate two additional\nconfiguration options: OCSP Hybrid, and the use of relay nodes as a firewall.\nTogether these minimize the extent of the network an attacker can reach with a\ncompromised key, and reduce the attacker's load on interplanetary relay links.\n","authors":["Joshua Smailes","Sebastian Köhler","Simon Birnbach","Martin Strohmeier","Ivan Martinovic"],"pdf_url":"https://arxiv.org/pdf/2408.10963v2.pdf","comment":"14 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.18053v1","updated":"2024-10-23T17:26:52Z","published":"2024-10-23T17:26:52Z","title":"B-Side: Binary-Level Static System Call Identification","summary":"  System call filtering is widely used to secure programs in multi-tenant\nenvironments, and to sandbox applications in modern desktop software deployment\nand package management systems. Filtering rules are hard to write and maintain\nmanually, hence generating them automatically is essential. To that aim,\nanalysis tools able to identify every system call that can legitimately be\ninvoked by a program are needed. Existing static analysis works lack precision\nbecause of a high number of false positives, and/or assume the availability of\nprogram/libraries source code -- something unrealistic in many scenarios such\nas cloud production environments.\n  We present B-Side, a static binary analysis tool able to identify a superset\nof the system calls that an x86-64 static/dynamic executable may invoke at\nruntime. B-Side assumes no access to program/libraries sources, and shows a\ngood degree of precision by leveraging symbolic execution, combined with a\nheuristic to detect system call wrappers, which represent an important source\nof precision loss in existing works. B-Side also allows to statically detect\nphases of execution in a program in which different filtering rules can be\napplied. We validate B-Side and demonstrate its higher precision compared to\nstate-of-the-art works: over a set of popular applications, B-Side's average\n$F_1$ score is 0.81, vs. 0.31 and 0.53 for competitors. Over 557 static and\ndynamically-compiled binaries taken from the Debian repositories, B-Side\nidentifies an average of 43 system calls, vs. 271 and 95 for two state-of-the\nart competitors. We further evaluate the strictness of the phase-based\nfiltering policies that can be obtained with B-Side.\n","authors":["Gaspard Thévenon","Kevin Nguetchouang","Kahina Lazri","Alain Tchana","Pierre Olivier"],"pdf_url":"https://arxiv.org/pdf/2410.18053v1.pdf","comment":"Accepted to appear in the 25th ACM/IFIP International Middleware\n  Conference (Middleware'24)"},{"id":"http://arxiv.org/abs/2410.02916v2","updated":"2024-10-23T17:26:06Z","published":"2024-10-03T19:07:53Z","title":"Safeguard is a Double-edged Sword: Denial-of-service Attack on Large\n  Language Models","summary":"  Safety is a paramount concern of large language models (LLMs) in their open\ndeployment. To this end, safeguard methods aim to enforce the ethical and\nresponsible use of LLMs through safety alignment or guardrail mechanisms.\nHowever, we found that the malicious attackers could exploit false positives of\nsafeguards, i.e., fooling the safeguard model to block safe content mistakenly,\nleading to a new denial-of-service (DoS) attack on LLMs. Specifically, by\nsoftware or phishing attacks on user client software, attackers insert a short,\nseemingly innocuous adversarial prompt into to user prompt templates in\nconfiguration files; thus, this prompt appears in final user requests without\nvisibility in the user interface and is not trivial to identify. By designing\nan optimization process that utilizes gradient and attention information, our\nattack can automatically generate seemingly safe adversarial prompts,\napproximately only 30 characters long, that universally block over 97\\% of user\nrequests on Llama Guard 3. The attack presents a new dimension of evaluating\nLLM safeguards focusing on false positives, fundamentally different from the\nclassic jailbreak.\n","authors":["Qingzhao Zhang","Ziyang Xiong","Z. Morley Mao"],"pdf_url":"https://arxiv.org/pdf/2410.02916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14008v3","updated":"2024-10-23T16:22:34Z","published":"2024-09-21T04:03:26Z","title":"Cyber-Physical Authentication Scheme for Secure V2G Transactions","summary":"  The rapid adoption of electric vehicles (EVs) globally has catalyzed the need\nfor robust cybersecurity measures within vehicle-to-grid (V2G) networks. As\nthese networks are increasingly being integrated into smart charging\ninfrastructures, they also introduce new vulnerabilities that threaten grid\nstability and user privacy This paper proposes a cyber-physical authentication\nprotocol and trading smart contract tailored to plug and charge (PnC)\noperations within blockchain-based V2G systems. The protocol leverages advanced\ncryptographic techniques and blockchain to ensure secure, transparent, and\ntamper-proof energy transactions between EVs and charging stations. Key\ncontributions include the development of a cyber-physical authentication\nmethod, the implementation of a smart contract framework for secure energy\ntrading, and a detailed security and privacy analysis. The proposed protocol\neffectively mitigates risks such as man-in-the-middle (MitM) attacks and replay\nattacks while preserving user anonymity and data integrity.\n","authors":["Yunwang Chen","Yanmin Zhao","Siuming Yiu"],"pdf_url":"https://arxiv.org/pdf/2409.14008v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17986v1","updated":"2024-10-23T16:00:14Z","published":"2024-10-23T16:00:14Z","title":"Federated Transformer: Multi-Party Vertical Federated Learning on\n  Practical Fuzzily Linked Data","summary":"  Federated Learning (FL) is an evolving paradigm that enables multiple parties\nto collaboratively train models without sharing raw data. Among its variants,\nVertical Federated Learning (VFL) is particularly relevant in real-world,\ncross-organizational collaborations, where distinct features of a shared\ninstance group are contributed by different parties. In these scenarios,\nparties are often linked using fuzzy identifiers, leading to a common practice\ntermed as multi-party fuzzy VFL. Existing models generally address either\nmulti-party VFL or fuzzy VFL between two parties. Extending these models to\npractical multi-party fuzzy VFL typically results in significant performance\ndegradation and increased costs for maintaining privacy. To overcome these\nlimitations, we introduce the Federated Transformer (FeT), a novel framework\nthat supports multi-party VFL with fuzzy identifiers. FeT innovatively encodes\nthese identifiers into data representations and employs a transformer\narchitecture distributed across different parties, incorporating three new\ntechniques to enhance performance. Furthermore, we have developed a multi-party\nprivacy framework for VFL that integrates differential privacy with secure\nmulti-party computation, effectively protecting local representations while\nminimizing associated utility costs. Our experiments demonstrate that the FeT\nsurpasses the baseline models by up to 46\\% in terms of accuracy when scaled to\n50 parties. Additionally, in two-party fuzzy VFL settings, FeT also shows\nimproved performance and privacy over cutting-edge VFL models.\n","authors":["Zhaomin Wu","Junyi Hou","Yiqun Diao","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2410.17986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17933v1","updated":"2024-10-23T14:55:53Z","published":"2024-10-23T14:55:53Z","title":"Multi-Continental Healthcare Modelling Using Blockchain-Enabled\n  Federated Learning","summary":"  One of the biggest challenges of building artificial intelligence (AI) model\nin healthcare area is the data sharing. Since healthcare data is private,\nsensitive, and heterogeneous, collecting sufficient data for modelling is\nexhausted, costly, and sometimes impossible. In this paper, we propose a\nframework for global healthcare modelling using datasets from multi-continents\n(Europe, North America and Asia) while without sharing the local datasets, and\nchoose glucose management as a study model to verify its effectiveness.\nTechnically, blockchain-enabled federated learning is implemented with adaption\nto make it meet with the privacy and safety requirements of healthcare data,\nmeanwhile rewards honest participation and penalize malicious activities using\nits on-chain incentive mechanism. Experimental results show that the proposed\nframework is effective, efficient, and privacy preserved. Its prediction\naccuracy is much better than the models trained from limited personal data and\nis similar to, and even slightly better than, the results from a centralized\ndataset. This work paves the way for international collaborations on healthcare\nprojects, where additional data is crucial for reducing bias and providing\nbenefits to humanity.\n","authors":["Rui Sun","Zhipeng Wang","Hengrui Zhang","Ming Jiang","Yizhe Wen","Jiqun Zhang","Jiahao Sun","Shuoying Zhang","Erwu Liu","Kezhi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17933v1.pdf","comment":"Accepted by IEEE Global Blockchain Conference"},{"id":"http://arxiv.org/abs/2410.17928v1","updated":"2024-10-23T14:47:12Z","published":"2024-10-23T14:47:12Z","title":"SJMalloc: the security-conscious, fast, thread-safe and memory-efficient\n  heap allocator","summary":"  Heap-based exploits that leverage memory management errors continue to pose a\nsignificant threat to application security. The root cause of these\nvulnerabilities are the memory management errors within the applications,\nhowever various hardened allocator designs have been proposed as mitigation. A\ncommon feature of these designs is the strategic decision to store heap\nmetadata separately from the application data in use, thereby reducing the risk\nof metadata corruption leading to security breaches. Despite their potential\nbenefits, hardened allocators have not been widely adopted in real-world\napplications. The primary barrier to their adoption is the performance\noverheads they introduce. These overheads can negatively impact the efficiency\nand speed of applications, which is a critical consideration for developers and\nsystem administrators. Having learned from previous implementations, we\ndeveloped SJMalloc, a general-purpose, high-performance allocator that\naddresses these concerns. SJMalloc stores its metadata out-of-band, away from\nthe application's data on the heap. This design choice not only enhances\nsecurity but also improves performance. Across a variety of real-world\nworkloads, SJMalloc demonstrates a ~6% performance improvement compared to\nGLibcs allocator, while using only ~5% more memory. Furthermore, SJMalloc\nsuccessfully passes the generic elements of the GLibc malloc testsuite and can\nthus be used as a drop-in replacement for the standard allocator, offering an\neasy upgrade path for enhanced security and performance without requiring\nchanges to existing applications.\n","authors":["Stephan Bauroth"],"pdf_url":"https://arxiv.org/pdf/2410.17928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17925v1","updated":"2024-10-23T14:41:59Z","published":"2024-10-23T14:41:59Z","title":"Securing Stack Smashing Protection in WebAssembly Applications","summary":"  WebAssembly is an instruction set architecture and binary format standard,\ndesigned for secure execution by an interpreter. Previous work has shown that\nWebAssembly is vulnerable to buffer overflow due to the lack of effective\nprotection mechanisms. In this paper, we evaluate the implementation of Stack\nSmashing Protection (SSP) in WebAssembly standalone runtimes, and uncover two\nweaknesses in their current implementation. The first one is the possibility to\noverwrite the SSP reference value because of the contiguous memory zones inside\na WebAssembly process. The second comes from the reliance of WebAssembly on the\nruntime to provide randomness in order to initialize the SSP reference value,\nwhich impacts the robustness of the solution. We address these two flaws by\nhardening the SSP implementation in terms of storage and random generator\nfailure, in a way that is generalizable to all of WebAssembly. We evaluate our\nnew, more robust, solution to prove that the implemented improvements do not\nreduce the efficiency of SSP.\n","authors":["Quentin Michaud","Yohan Pipereau","Olivier Levillain","Dhouha Ayed"],"pdf_url":"https://arxiv.org/pdf/2410.17925v1.pdf","comment":"Presented at PLAS 24 (ACM CCS workshop) https://plas24.github.io/"},{"id":"http://arxiv.org/abs/2410.17910v1","updated":"2024-10-23T14:28:32Z","published":"2024-10-23T14:28:32Z","title":"Slot: Provenance-Driven APT Detection through Graph Reinforcement\n  Learning","summary":"  Advanced Persistent Threats (APTs) represent sophisticated cyberattacks\ncharacterized by their ability to remain undetected within the victim system\nfor extended periods, aiming to exfiltrate sensitive data or disrupt\noperations. Existing detection approaches often struggle to effectively\nidentify these complex threats, construct the attack chain for defense\nfacilitation, or resist adversarial attacks. To overcome these challenges, we\npropose Slot, an advanced APT detection approach based on provenance graphs and\ngraph reinforcement learning. Slot excels in uncovering multi-level hidden\nrelationships, such as causal, contextual, and indirect connections, among\nsystem behaviors through provenance graph mining. By pioneering the integration\nof graph reinforcement learning, Slot dynamically adapts to new user activities\nand evolving attack strategies, enhancing its resilience against adversarial\nattacks. Additionally, Slot automatically constructs the attack chain according\nto detected attacks with clustering algorithms, providing precise\nidentification of attack paths and facilitating the development of defense\nstrategies. Evaluations with real-world datasets demonstrate Slot's outstanding\naccuracy, efficiency, adaptability, and robustness in APT detection, with most\nmetrics surpassing state-of-the-art methods. Additionally, case studies\nconducted to assess Slot's effectiveness in supporting APT defense further\nestablish it as a practical and reliable tool for cybersecurity protection.\n","authors":["Wei Qiao","Yebo Feng","Teng Li","Zijian Zhang","Zhengzi Xu","Zhuo Ma","Yulong Shen","JianFeng Ma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03489v2","updated":"2024-10-23T13:38:29Z","published":"2024-10-04T14:59:39Z","title":"Gradient-based Jailbreak Images for Multimodal Fusion Models","summary":"  Augmenting language models with image inputs may enable more effective\njailbreak attacks through continuous optimization, unlike text inputs that\nrequire discrete optimization. However, new multimodal fusion models tokenize\nall input modalities using non-differentiable functions, which hinders\nstraightforward attacks. In this work, we introduce the notion of a tokenizer\nshortcut that approximates tokenization with a continuous function and enables\ncontinuous optimization. We use tokenizer shortcuts to create the first\nend-to-end gradient image attacks against multimodal fusion models. We evaluate\nour attacks on Chameleon models and obtain jailbreak images that elicit harmful\ninformation for 72.5% of prompts. Jailbreak images outperform text jailbreaks\noptimized with the same objective and require 3x lower compute budget to\noptimize 50x more input tokens. Finally, we find that representation\nengineering defenses, like Circuit Breakers, trained only on text attacks can\neffectively transfer to adversarial image inputs.\n","authors":["Javier Rando","Hannah Korevaar","Erik Brinkman","Ivan Evtimov","Florian Tramèr"],"pdf_url":"https://arxiv.org/pdf/2410.03489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14774v2","updated":"2024-10-23T13:01:14Z","published":"2024-03-21T18:28:43Z","title":"Few-Shot Adversarial Prompt Learning on Vision-Language Models","summary":"  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data. Code is available at:\nhttps://github.com/lionel-w2/FAP.\n","authors":["Yiwei Zhou","Xiaobo Xia","Zhiwei Lin","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14774v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17792v1","updated":"2024-10-23T11:47:04Z","published":"2024-10-23T11:47:04Z","title":"Enhancing Federated Learning Convergence with Dynamic Data Queue and\n  Data Entropy-driven Participant Selection","summary":"  Federated Learning (FL) is a decentralized approach for collaborative model\ntraining on edge devices. This distributed method of model training offers\nadvantages in privacy, security, regulatory compliance, and cost-efficiency.\nOur emphasis in this research lies in addressing statistical complexity in FL,\nespecially when the data stored locally across devices is not identically and\nindependently distributed (non-IID). We have observed an accuracy reduction of\nup to approximately 10\\% to 30\\%, particularly in skewed scenarios where each\nedge device trains with only 1 class of data. This reduction is attributed to\nweight divergence, quantified using the Euclidean distance between device-level\nclass distributions and the population distribution, resulting in a bias term\n(\\(\\delta_k\\)). As a solution, we present a method to improve convergence in FL\nby creating a global subset of data on the server and dynamically distributing\nit across devices using a Dynamic Data queue-driven Federated Learning (DDFL).\nNext, we leverage Data Entropy metrics to observe the process during each\ntraining round and enable reasonable device selection for aggregation.\nFurthermore, we provide a convergence analysis of our proposed DDFL to justify\ntheir viability in practical FL scenarios, aiming for better device selection,\na non-sub-optimal global model, and faster convergence. We observe that our\napproach results in a substantial accuracy boost of approximately 5\\% for the\nMNIST dataset, around 18\\% for CIFAR-10, and 20\\% for CIFAR-100 with a 10\\%\nglobal subset of data, outperforming the state-of-the-art (SOTA) aggregation\nalgorithms.\n","authors":["Charuka Herath","Xiaolan Liu","Sangarapillai Lambotharan","Yogachandran Rahulamathavan"],"pdf_url":"https://arxiv.org/pdf/2410.17792v1.pdf","comment":"The Journal is submitted to IEEE Transactions in the Internet of\n  Things"},{"id":"http://arxiv.org/abs/2404.18814v2","updated":"2024-10-23T11:17:12Z","published":"2024-04-29T15:51:49Z","title":"Belt and Braces: When Federated Learning Meets Differential Privacy","summary":"  Federated learning (FL) has great potential for large-scale machine learning\n(ML) without exposing raw data.Differential privacy (DP) is the de facto\nstandard of privacy protection with provable guarantees.Advances in ML suggest\nthat DP would be a perfect fit for FL with comprehensive privacy preservation.\nHence, extensive efforts have been devoted to achieving practically usable FL\nwith DP, which however is still challenging.Practitioners often not only are\nnot fully aware of its development and categorization, but also face a hard\nchoice between privacy and utility. Therefore, it calls for a holistic review\nof current advances and an investigation on the challenges and opportunities\nfor highly usable FL systems with a DP guarantee. In this article, we first\nintroduce the primary concepts of FL and DP, and highlight the benefits of\nintegration. We then review the current developments by categorizing different\nparadigms and notions. Aiming at usable FL with DP, we present the optimization\nprinciples to seek a better tradeoff between model utility and privacy loss.\nFinally, we discuss future challenges in the emergent areas and relevant\nresearch topics.\n","authors":["Xuebin Ren","Shusen Yang","Cong Zhao","Julie McCann","Zongben Xu"],"pdf_url":"https://arxiv.org/pdf/2404.18814v2.pdf","comment":"10 pages, 4 figures, accepted by and to appear in Communications of\n  the ACM (CACM)"},{"id":"http://arxiv.org/abs/2410.17731v1","updated":"2024-10-23T10:06:02Z","published":"2024-10-23T10:06:02Z","title":"Time-to-Lie: Identifying Industrial Control System Honeypots Using the\n  Internet Control Message Protocol","summary":"  The convergence of information and operational technology networks has\ncreated previously unforeseen security issues. To address these issues, both\nresearchers and practitioners have integrated threat intelligence methods into\nthe security operations of converged networks, with some of the most valuable\ntools being honeypots that imitate industrial control systems (ICS). However,\nthe development and deployment of such honeypots is a process rich with\npitfalls, which can lead to undiagnosed weaknesses in the threat intelligence\nbeing gathered. This paper presents a side-channel method of covertly\nidentifying ICS honeypots using the time-to-live (TTL) values of target\ndevices. We show that many ICS honeypots can be readily identified, via minimal\ninteractions, using only basic networking tools. In a study of over 8,000\ndevices presenting as ICS systems, we detail how our method compares to an\nexisting honeypot detection approach, and outline what our methodology reveals\nabout the current population of live ICS honeypots. In demonstrating our\nmethod, this study aims to raise awareness of the viability of the TTL\nheuristic and the prevalence of its misconfiguration despite its presence in\nliterature.\n","authors":["Jacob Williams","Matthew Edwards","Joseph Gardiner"],"pdf_url":"https://arxiv.org/pdf/2410.17731v1.pdf","comment":"11 pages, 2 listings, 5 tables, 6 figures"},{"id":"http://arxiv.org/abs/2404.14693v2","updated":"2024-10-23T09:42:29Z","published":"2024-04-23T02:50:38Z","title":"DIP-Watermark: A Double Identity Protection Method Based on Robust\n  Adversarial Watermark","summary":"  The wide deployment of Face Recognition (FR) systems poses privacy risks. One\ncountermeasure is adversarial attack, deceiving unauthorized malicious FR, but\nit also disrupts regular identity verification of trusted authorizers,\nexacerbating the potential threat of identity impersonation. To address this,\nwe propose the first double identity protection scheme based on traceable\nadversarial watermarking, termed DIP-Watermark. DIP-Watermark employs a\none-time watermark embedding to deceive unauthorized FR models and allows\nauthorizers to perform identity verification by extracting the watermark.\nSpecifically, we propose an information-guided adversarial attack against FR\nmodels. The encoder embeds an identity-specific watermark into the deep feature\nspace of the carrier, guiding recognizable features of the image to deviate\nfrom the source identity. We further adopt a collaborative meta-optimization\nstrategy compatible with sub-tasks, which regularizes the joint optimization\ndirection of the encoder and decoder. This strategy enhances the representation\nof universal carrier features, mitigating multi-objective optimization\nconflicts in watermarking. Experiments confirm that DIP-Watermark achieves\nsignificant attack success rates and traceability accuracy on state-of-the-art\nFR models, exhibiting remarkable robustness that outperforms the existing\nprivacy protection methods using adversarial attacks and deep watermarking, or\nsimple combinations of the two. Our work potentially opens up new insights into\nproactive protection for FR privacy.\n","authors":["Yunming Zhang","Dengpan Ye","Caiyun Xie","Sipeng Shen","Ziyi Liu","Jiacheng Deng","Long Tang"],"pdf_url":"https://arxiv.org/pdf/2404.14693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17647v1","updated":"2024-10-23T08:04:12Z","published":"2024-10-23T08:04:12Z","title":"Entity-based Reinforcement Learning for Autonomous Cyber Defence","summary":"  A significant challenge for autonomous cyber defence is ensuring a defensive\nagent's ability to generalise across diverse network topologies and\nconfigurations.\n  This capability is necessary for agents to remain effective when deployed in\ndynamically changing environments, such as an enterprise network where devices\nmay frequently join and leave.\n  Standard approaches to deep reinforcement learning, where policies are\nparameterised using a fixed-input multi-layer perceptron (MLP) expect\nfixed-size observation and action spaces. In autonomous cyber defence, this\nmakes it hard to develop agents that generalise to environments with network\ntopologies different from those trained on, as the number of nodes affects the\nnatural size of the observation and action spaces. To overcome this limitation,\nwe reframe the problem of autonomous network defence using entity-based\nreinforcement learning, where the observation and action space of an agent are\ndecomposed into a collection of discrete entities. This framework enables the\nuse of policy parameterisations specialised in compositional generalisation.\nNamely, we train a Transformer-based policy on the Yawning Titan cyber-security\nsimulation environment and test its generalisation capabilities across various\nnetwork topologies. We demonstrate that this approach significantly outperforms\nan MLP-based policy on fixed networks, and has the ability for zero-shot\ngeneralisation to networks of a different size to those seen in training.\n  These findings highlight the potential for entity-based reinforcement\nlearning to advance the field of autonomous cyber defence by providing more\ngeneralisable policies capable of handling variations in real-world network\nenvironments.\n","authors":["Isaac Symes Thompson","Alberto Caron","Chris Hicks","Vasilios Mavroudis"],"pdf_url":"https://arxiv.org/pdf/2410.17647v1.pdf","comment":"Material to appear in the proceedings of the 1st International\n  Workshop on Autonomous Cybersecurity at ACM CCS 2024"},{"id":"http://arxiv.org/abs/2311.16169v3","updated":"2024-10-23T07:32:15Z","published":"2023-11-16T13:17:20Z","title":"Understanding the Effectiveness of Large Language Models in Detecting\n  Security Vulnerabilities","summary":"  While automated vulnerability detection techniques have made promising\nprogress in detecting security vulnerabilities, their scalability and\napplicability remain challenging. The remarkable performance of Large Language\nModels (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted\nrecent works to explore if LLMs can be used to detect vulnerabilities. In this\npaper, we perform a more comprehensive study by concurrently examining a higher\nnumber of datasets, languages and LLMs, and qualitatively evaluating\nperformance across prompts and vulnerability classes while addressing the\nshortcomings of existing tools. Concretely, we evaluate the effectiveness of 16\npre-trained LLMs on 5,000 code samples from five diverse security datasets.\nThese balanced datasets encompass both synthetic and real-world projects in\nJava and C/C++ and cover 25 distinct vulnerability classes.\n  Overall, LLMs across all scales and families show modest effectiveness in\ndetecting vulnerabilities, obtaining an average accuracy of 62.8% and F1 score\nof 0.71 across datasets. They are significantly better at detecting\nvulnerabilities only requiring intra-procedural analysis, such as OS Command\nInjection and NULL Pointer Dereference. Moreover, they report higher accuracies\non these vulnerabilities than popular static analysis tools, such as CodeQL.\n  We find that advanced prompting strategies that involve step-by-step analysis\nsignificantly improve performance of LLMs on real-world datasets in terms of F1\nscore (by upto 0.18 on average). Interestingly, we observe that LLMs show\npromising abilities at performing parts of the analysis correctly, such as\nidentifying vulnerability-related specifications and leveraging natural\nlanguage information to understand code behavior (e.g., to check if code is\nsanitized). We expect our insights to guide future work on LLM-augmented\nvulnerability detection systems.\n","authors":["Avishree Khare","Saikat Dutta","Ziyang Li","Alaia Solko-Breslin","Rajeev Alur","Mayur Naik"],"pdf_url":"https://arxiv.org/pdf/2311.16169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12617v2","updated":"2024-10-23T06:28:19Z","published":"2024-02-20T00:51:05Z","title":"Generative AI Security: Challenges and Countermeasures","summary":"  Generative AI's expanding footprint across numerous industries has led to\nboth excitement and increased scrutiny. This paper delves into the unique\nsecurity challenges posed by Generative AI, and outlines potential research\ndirections for managing these risks.\n","authors":["Banghua Zhu","Norman Mu","Jiantao Jiao","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2402.12617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04870v5","updated":"2024-10-23T05:55:31Z","published":"2024-08-09T05:20:05Z","title":"ConfusedPilot: Confused Deputy Risks in RAG-based LLMs","summary":"  Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.\n","authors":["Ayush RoyChowdhury","Mulong Luo","Prateek Sahu","Sarbartha Banerjee","Mohit Tiwari"],"pdf_url":"https://arxiv.org/pdf/2408.04870v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17573v1","updated":"2024-10-23T05:54:41Z","published":"2024-10-23T05:54:41Z","title":"Securing Federated Learning Against Novel and Classic Backdoor Threats\n  During Foundation Model Integration","summary":"  Federated learning (FL) enables decentralized model training while preserving\nprivacy. Recently, integrating Foundation Models (FMs) into FL has boosted\nperformance but also introduced a novel backdoor attack mechanism. Attackers\ncan exploit the FM's capabilities to embed backdoors into synthetic data\ngenerated by FMs used for model fusion, subsequently infecting all client\nmodels through knowledge sharing without involvement in the long-lasting FL\nprocess. These novel attacks render existing FL backdoor defenses ineffective,\nas they primarily detect anomalies among client updates, which may appear\nuniformly malicious under this attack. Our work proposes a novel data-free\ndefense strategy by constraining abnormal activations in the hidden feature\nspace during model aggregation on the server. The activation constraints,\noptimized using synthetic data alongside FL training, mitigate the attack while\nbarely affecting model performance, as the parameters remain untouched.\nExtensive experiments demonstrate its effectiveness against both novel and\nclassic backdoor attacks, outperforming existing defenses while maintaining\nmodel performance.\n","authors":["Xiaohuan Bi","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17566v1","updated":"2024-10-23T05:19:51Z","published":"2024-10-23T05:19:51Z","title":"Differentially Private Learning Needs Better Model Initialization and\n  Self-Distillation","summary":"  Differentially private SGD (DPSGD) enables privacy-preserving training of\nlanguage models, but often reduces utility, diversity, and linguistic quality.\nWe introduce DPRefine, a three-phase method that initializes a model using data\nsynthesis from a small pre-trained LM with rigorous filtering, applies DP\nfinetuning on private data, and performs self-distillation to refine outputs.\nThis approach significantly outperforms vanilla DPSGD, with AlpacaEval\npreferring DPRefine's generations in 78.4% of cases across all datasets. Our\nanalysis reveals that DPRefine reduces linguistic errors in generated text by\n84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.\nIt also reduces inconsistencies of non-private models, such as hallucinated\ndetails and misattributed quotes. We find that small models like GPT-2 can be\neffective for initialization and distillation, highlighting their potential in\nenabling scalable and efficient deployment of privacy-preserving language.\n","authors":["Ivoline C. Ngong","Joseph P. Near","Niloofar Mireshghallah"],"pdf_url":"https://arxiv.org/pdf/2410.17566v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2410.17533v1","updated":"2024-10-23T03:25:55Z","published":"2024-10-23T03:25:55Z","title":"FedGMark: Certifiably Robust Watermarking for Federated Graph Learning","summary":"  Federated graph learning (FedGL) is an emerging learning paradigm to\ncollaboratively train graph data from various clients. However, during the\ndevelopment and deployment of FedGL models, they are susceptible to illegal\ncopying and model theft. Backdoor-based watermarking is a well-known method for\nmitigating these attacks, as it offers ownership verification to the model\nowner. We take the first step to protect the ownership of FedGL models via\nbackdoor-based watermarking. Existing techniques have challenges in achieving\nthe goal: 1) they either cannot be directly applied or yield unsatisfactory\nperformance; 2) they are vulnerable to watermark removal attacks; and 3) they\nlack of formal guarantees. To address all the challenges, we propose FedGMark,\nthe first certified robust backdoor-based watermarking for FedGL. FedGMark\nleverages the unique graph structure and client information in FedGL to learn\ncustomized and diverse watermarks. It also designs a novel GL architecture that\nfacilitates defending against both the empirical and theoretically worst-case\nwatermark removal attacks. Extensive experiments validate the promising\nempirical and provable watermarking performance of FedGMark. Source code is\navailable at: https://github.com/Yuxin104/FedGMark.\n","authors":["Yuxin Yang","Qiang Li","Yuan Hong","Binghui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17533v1.pdf","comment":"This paper is accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17492v1","updated":"2024-10-23T01:14:54Z","published":"2024-10-23T01:14:54Z","title":"BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers","summary":"  Attacking fairness is crucial because compromised models can introduce biased\noutcomes, undermining trust and amplifying inequalities in sensitive\napplications like hiring, healthcare, and law enforcement. This highlights the\nurgent need to understand how fairness mechanisms can be exploited and to\ndevelop defenses that ensure both fairness and robustness. We introduce\nBadFair, a novel backdoored fairness attack methodology. BadFair stealthily\ncrafts a model that operates with accuracy and fairness under regular\nconditions but, when activated by certain triggers, discriminates and produces\nincorrect results for specific groups. This type of attack is particularly\nstealthy and dangerous, as it circumvents existing fairness detection methods,\nmaintaining an appearance of fairness in normal use. Our findings reveal that\nBadFair achieves a more than 85% attack success rate in attacks aimed at target\ngroups on average while only incurring a minimal accuracy loss. Moreover, it\nconsistently exhibits a significant discrimination score, distinguishing\nbetween pre-defined target and non-target attacked groups across various\ndatasets and models.\n","authors":["Jiaqi Xue","Qian Lou","Mengxin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.17492v1.pdf","comment":"Accepted by EMNLP 2024"}],"Neural and Evolutionary Computing":[{"id":"http://arxiv.org/abs/2410.14759v2","updated":"2024-10-23T06:51:05Z","published":"2024-10-18T09:53:20Z","title":"Universal approximation results for neural networks with non-polynomial\n  activation function over non-compact domains","summary":"  In this paper, we generalize the universal approximation property of\nsingle-hidden-layer feed-forward neural networks beyond the classical\nformulation over compact domains. More precisely, by assuming that the\nactivation function is non-polynomial, we derive universal approximation\nresults for neural networks within function spaces over non-compact subsets of\na Euclidean space, e.g., weighted spaces, $L^p$-spaces, and (weighted) Sobolev\nspaces over unbounded domains, where the latter includes the approximation of\nthe (weak) derivatives. Furthermore, we provide some dimension-independent\nrates for approximating a function with sufficiently regular and integrable\nFourier transform by neural networks with non-polynomial activation function.\n","authors":["Ariel Neufeld","Philipp Schmocker"],"pdf_url":"https://arxiv.org/pdf/2410.14759v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.08410"},{"id":"http://arxiv.org/abs/2410.03801v2","updated":"2024-10-23T06:05:11Z","published":"2024-10-04T08:14:24Z","title":"P1-KAN an effective Kolmogorov Arnold Network for function approximation","summary":"  A new Kolmogorov-Arnold network (KAN) is proposed to approximate potentially\nirregular functions in high dimension. We show that it outperforms multilayer\nperceptrons in terms of accuracy and converges faster. We also compare it with\nseveral proposed KAN networks: the original spline-based KAN network appears to\nbe more effective for smooth functions, while the P1-KAN network is more\neffective for irregular functions.\n","authors":["Xavier Warin"],"pdf_url":"https://arxiv.org/pdf/2410.03801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14687v2","updated":"2024-10-23T03:05:37Z","published":"2024-10-03T14:17:43Z","title":"BrainTransformers: SNN-LLM","summary":"  This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments.\n","authors":["Zhengzheng Tang","Eva Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.14687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19841v2","updated":"2024-10-23T16:27:27Z","published":"2024-09-30T00:47:13Z","title":"Counter-Current Learning: A Biologically Plausible Dual Network Approach\n  for Deep Learning","summary":"  Despite its widespread use in neural networks, error backpropagation has\nfaced criticism for its lack of biological plausibility, suffering from issues\nsuch as the backward locking problem and the weight transport problem. These\nlimitations have motivated researchers to explore more biologically plausible\nlearning algorithms that could potentially shed light on how biological neural\nsystems adapt and learn. Inspired by the counter-current exchange mechanisms\nobserved in biological systems, we propose counter-current learning (CCL), a\nbiologically plausible framework for credit assignment in neural networks. This\nframework employs a feedforward network to process input data and a feedback\nnetwork to process targets, with each network enhancing the other through\nanti-parallel signal propagation. By leveraging the more informative signals\nfrom the bottom layer of the feedback network to guide the updates of the top\nlayer of the feedforward network and vice versa, CCL enables the simultaneous\ntransformation of source inputs to target outputs and the dynamic mutual\ninfluence of these transformations. Experimental results on MNIST,\nFashionMNIST, CIFAR10, and CIFAR100 datasets using multi-layer perceptrons and\nconvolutional neural networks demonstrate that CCL achieves comparable\nperformance to other biologically plausible algorithms while offering a more\nbiologically realistic learning mechanism. Furthermore, we showcase the\napplicability of our approach to an autoencoder task, underscoring its\npotential for unsupervised representation learning. Our work presents a\ndirection for biologically inspired and plausible learning algorithms, offering\nan alternative mechanism of learning and adaptation in neural networks.\n","authors":["Chia-Hsiang Kao","Bharath Hariharan"],"pdf_url":"https://arxiv.org/pdf/2409.19841v2.pdf","comment":"Accepted at NeurIPS 2024. Code available at\n  https://github.com/IandRover/CCL-NeurIPS24"},{"id":"http://arxiv.org/abs/2409.17270v2","updated":"2024-10-23T16:27:20Z","published":"2024-09-25T18:35:45Z","title":"Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.\n","authors":["Debargha Ganguly","Srinivasan Iyengar","Vipin Chaudhary","Shivkumar Kalyanaraman"],"pdf_url":"https://arxiv.org/pdf/2409.17270v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) System 2 Reasoning At Scale Workshop"},{"id":"http://arxiv.org/abs/2410.17970v1","updated":"2024-10-23T15:36:08Z","published":"2024-10-23T15:36:08Z","title":"Optical Generative Models","summary":"  Generative models cover various application areas, including image, video and\nmusic synthesis, natural language processing, and molecular design, among many\nothers. As digital generative models become larger, scalable inference in a\nfast and energy-efficient manner becomes a challenge. Here, we present optical\ngenerative models inspired by diffusion models, where a shallow and fast\ndigital encoder first maps random noise into phase patterns that serve as\noptical generative seeds for a desired data distribution; a jointly-trained\nfree-space-based reconfigurable decoder all-optically processes these\ngenerative seeds to create novel images (never seen before) following the\ntarget data distribution. Except for the illumination power and the random seed\ngeneration through a shallow encoder, these optical generative models do not\nconsume computing power during the synthesis of novel images. We report the\noptical generation of monochrome and multi-color novel images of handwritten\ndigits, fashion products, butterflies, and human faces, following the data\ndistributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets,\nrespectively, achieving an overall performance comparable to digital neural\nnetwork-based generative models. To experimentally demonstrate optical\ngenerative models, we used visible light to generate, in a snapshot, novel\nimages of handwritten digits and fashion products. These optical generative\nmodels might pave the way for energy-efficient, scalable and rapid inference\ntasks, further exploiting the potentials of optics and photonics for artificial\nintelligence-generated content.\n","authors":["Shiqi Chen","Yuhang Li","Hanlong Chen","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2410.17970v1.pdf","comment":"24 Pages, 9 Figures"},{"id":"http://arxiv.org/abs/2410.17498v1","updated":"2024-10-23T01:38:10Z","published":"2024-10-23T01:38:10Z","title":"Mechanisms of Symbol Processing for In-Context Learning in Transformer\n  Networks","summary":"  Large Language Models (LLMs) have demonstrated impressive abilities in symbol\nprocessing through in-context learning (ICL). This success flies in the face of\ndecades of predictions that artificial neural networks cannot master abstract\nsymbol manipulation. We seek to understand the mechanisms that can enable\nrobust symbol processing in transformer networks, illuminating both the\nunanticipated success, and the significant limitations, of transformers in\nsymbol processing. Borrowing insights from symbolic AI on the power of\nProduction System architectures, we develop a high-level language, PSL, that\nallows us to write symbolic programs to do complex, abstract symbol processing,\nand create compilers that precisely implement PSL programs in transformer\nnetworks which are, by construction, 100% mechanistically interpretable. We\ndemonstrate that PSL is Turing Universal, so the work can inform the\nunderstanding of transformer ICL in general. The type of transformer\narchitecture that we compile from PSL programs suggests a number of paths for\nenhancing transformers' capabilities at symbol processing. (Note: The first\nsection of the paper gives an extended synopsis of the entire paper.)\n","authors":["Paul Smolensky","Roland Fernandez","Zhenghao Herbert Zhou","Mattia Opper","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2410.17498v1.pdf","comment":"101 pages (including 30 pages of Appendices), 18 figures"}],"Operating Systems":[{"id":"http://arxiv.org/abs/2410.18053v1","updated":"2024-10-23T17:26:52Z","published":"2024-10-23T17:26:52Z","title":"B-Side: Binary-Level Static System Call Identification","summary":"  System call filtering is widely used to secure programs in multi-tenant\nenvironments, and to sandbox applications in modern desktop software deployment\nand package management systems. Filtering rules are hard to write and maintain\nmanually, hence generating them automatically is essential. To that aim,\nanalysis tools able to identify every system call that can legitimately be\ninvoked by a program are needed. Existing static analysis works lack precision\nbecause of a high number of false positives, and/or assume the availability of\nprogram/libraries source code -- something unrealistic in many scenarios such\nas cloud production environments.\n  We present B-Side, a static binary analysis tool able to identify a superset\nof the system calls that an x86-64 static/dynamic executable may invoke at\nruntime. B-Side assumes no access to program/libraries sources, and shows a\ngood degree of precision by leveraging symbolic execution, combined with a\nheuristic to detect system call wrappers, which represent an important source\nof precision loss in existing works. B-Side also allows to statically detect\nphases of execution in a program in which different filtering rules can be\napplied. We validate B-Side and demonstrate its higher precision compared to\nstate-of-the-art works: over a set of popular applications, B-Side's average\n$F_1$ score is 0.81, vs. 0.31 and 0.53 for competitors. Over 557 static and\ndynamically-compiled binaries taken from the Debian repositories, B-Side\nidentifies an average of 43 system calls, vs. 271 and 95 for two state-of-the\nart competitors. We further evaluate the strictness of the phase-based\nfiltering policies that can be obtained with B-Side.\n","authors":["Gaspard Thévenon","Kevin Nguetchouang","Kahina Lazri","Alain Tchana","Pierre Olivier"],"pdf_url":"https://arxiv.org/pdf/2410.18053v1.pdf","comment":"Accepted to appear in the 25th ACM/IFIP International Middleware\n  Conference (Middleware'24)"},{"id":"http://arxiv.org/abs/2410.17928v1","updated":"2024-10-23T14:47:12Z","published":"2024-10-23T14:47:12Z","title":"SJMalloc: the security-conscious, fast, thread-safe and memory-efficient\n  heap allocator","summary":"  Heap-based exploits that leverage memory management errors continue to pose a\nsignificant threat to application security. The root cause of these\nvulnerabilities are the memory management errors within the applications,\nhowever various hardened allocator designs have been proposed as mitigation. A\ncommon feature of these designs is the strategic decision to store heap\nmetadata separately from the application data in use, thereby reducing the risk\nof metadata corruption leading to security breaches. Despite their potential\nbenefits, hardened allocators have not been widely adopted in real-world\napplications. The primary barrier to their adoption is the performance\noverheads they introduce. These overheads can negatively impact the efficiency\nand speed of applications, which is a critical consideration for developers and\nsystem administrators. Having learned from previous implementations, we\ndeveloped SJMalloc, a general-purpose, high-performance allocator that\naddresses these concerns. SJMalloc stores its metadata out-of-band, away from\nthe application's data on the heap. This design choice not only enhances\nsecurity but also improves performance. Across a variety of real-world\nworkloads, SJMalloc demonstrates a ~6% performance improvement compared to\nGLibcs allocator, while using only ~5% more memory. Furthermore, SJMalloc\nsuccessfully passes the generic elements of the GLibc malloc testsuite and can\nthus be used as a drop-in replacement for the standard allocator, offering an\neasy upgrade path for enhanced security and performance without requiring\nchanges to existing applications.\n","authors":["Stephan Bauroth"],"pdf_url":"https://arxiv.org/pdf/2410.17928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17577v1","updated":"2024-10-23T06:04:04Z","published":"2024-10-23T06:04:04Z","title":"Arcus: SLO Management for Accelerators in the Cloud with Traffic Shaping","summary":"  Cloud servers use accelerators for common tasks (e.g., encryption,\ncompression, hashing) to improve CPU/GPU efficiency and overall performance.\nHowever, users' Service-level Objectives (SLOs) can be violated due to\naccelerator-related contention. The root cause is that existing solutions for\naccelerators only focus on isolation or fair allocation of compute and memory\nresources; they overlook the contention for communication-related resources.\nSpecifically, three communication-induced challenges drive us to re-think the\nproblem: (1) Accelerator traffic patterns are diverse, hard to predict, and\nmixed across users, (2) communication-related components lack effective\nlow-level isolation mechanism to configure, and (3) computational heterogeneity\nof accelerators lead to unique relationships between the traffic mixture and\nthe corresponding accelerator performance. The focus of this work is meeting\nSLOs in accelerator-rich systems. We present \\design{}, treating accelerator\nSLO management as traffic management with proactive traffic shaping. We develop\nan SLO-aware protocol coupled with an offloaded interface on an architecture\nthat supports precise and scalable traffic shaping. We guarantee accelerator\nSLO for various circumstances, with up to 45% tail latency reduction and less\nthan 1% throughput variance.\n","authors":["Jiechen Zhao","Ran Shu","Katie Lim","Zewen Fan","Thomas Anderson","Mingyu Gao","Natalie Enright Jerger"],"pdf_url":"https://arxiv.org/pdf/2410.17577v1.pdf","comment":null}],"Networking and Internet Architecture":[{"id":"http://arxiv.org/abs/2408.10963v2","updated":"2024-10-23T17:47:20Z","published":"2024-08-20T16:00:17Z","title":"KeySpace: Public Key Infrastructure Considerations in Interplanetary\n  Networks","summary":"  As satellite networks grow larger and begin to incorporate interplanetary\ncommunication, there is an increasing interest in the unsolved problem of how\nto approach PKI in these conditions. In this paper we explore the goals and\nrequirements for implementing key management systems in satellite networks,\nfocusing on megaconstellations and interplanetary networks. We design a set of\nstandardized experiments which can be used to compare systems against one\nanother for particular network topologies. Using these, we demonstrate that\nterrestrial PKI techniques are feasible in highly distributed interplanetary\nnetworks, showing that it is possible to configure PKI systems to achieve\nefficient low-latency connection establishment, and minimize the impact of\nattacks through effective revocations.\n  We evaluate this by building the Deep Space Network Simulator (DSNS), a novel\nnetwork simulator aimed at efficient simulation of large space networks. We run\nsimulations evaluating connection establishment and key revocation under a wide\nrange of PKI configurations. Finally, we propose and evaluate two additional\nconfiguration options: OCSP Hybrid, and the use of relay nodes as a firewall.\nTogether these minimize the extent of the network an attacker can reach with a\ncompromised key, and reduce the attacker's load on interplanetary relay links.\n","authors":["Joshua Smailes","Sebastian Köhler","Simon Birnbach","Martin Strohmeier","Ivan Martinovic"],"pdf_url":"https://arxiv.org/pdf/2408.10963v2.pdf","comment":"14 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2312.01204v3","updated":"2024-10-23T17:22:02Z","published":"2023-12-02T18:56:20Z","title":"A Comprehensive Survey of Wireless Time-Sensitive Networking (TSN):\n  Architecture, Technologies, Applications, and Open Issues","summary":"  Time-sensitive networking (TSN) is expected to be a key component of critical\nmachine-type communication networks in areas such as Industry 4.0, robotics and\nautonomous vehicles. With rising mobility requirements in industrial\napplications and the prevalence of wireless networks, wireless network\nintegration into TSN is becoming increasingly important. This survey article\npresents a comprehensive review of the current literature on wireless TSN,\nincluding an overview of the architecture of a wireless TSN network and an\nexamination of the various wireless technologies and protocols that can be or\nare used in such networks. In addition, the article discusses industrial\napplications of wireless TSN, among them industrial automation, robotics, and\nautonomous vehicles. The article concludes by summarizing the challenges and\nopen issues related to the integration of TSN into wireless networks, and by\noffering suggestions for future research directions.\n","authors":["Kouros Zanbouri","Md. Noor-A-Rahim","Jobish John","Cormac J. Sreenan","H. Vincent Poor","Dirk Pesch"],"pdf_url":"https://arxiv.org/pdf/2312.01204v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18002v1","updated":"2024-10-23T16:25:22Z","published":"2024-10-23T16:25:22Z","title":"Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges","summary":"  Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.\n","authors":["Yuchen Liu","Zhiyuan Peng","Zifan Zhang","Hanzhi Yu","Mingzhe Chen"],"pdf_url":"https://arxiv.org/pdf/2410.18002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17971v1","updated":"2024-10-23T15:36:43Z","published":"2024-10-23T15:36:43Z","title":"Dynamic Spectrum Access for Ambient Backscatter Communication-assisted\n  D2D Systems with Quantum Reinforcement Learning","summary":"  Spectrum access is an essential problem in device-to-device (D2D)\ncommunications. However, with the recent growth in the number of mobile\ndevices, the wireless spectrum is becoming scarce, resulting in low spectral\nefficiency for D2D communications. To address this problem, this paper aims to\nintegrate the ambient backscatter communication technology into D2D devices to\nallow them to backscatter ambient RF signals to transmit their data when the\nshared spectrum is occupied by mobile users. To obtain the optimal spectrum\naccess policy, i.e., stay idle or access the shared spectrum and perform active\ntransmissions or backscattering ambient RF signals for transmissions, to\nmaximize the average throughput for D2D users, deep reinforcement learning\n(DRL) can be adopted. However, DRL-based solutions may require long training\ntime due to the curse of dimensionality issue as well as complex deep neural\nnetwork architectures. For that, we develop a novel quantum reinforcement\nlearning (RL) algorithm that can achieve a faster convergence rate with fewer\ntraining parameters compared to DRL thanks to the quantum superposition and\nquantum entanglement principles. Specifically, instead of using conventional\ndeep neural networks, the proposed quantum RL algorithm uses a parametrized\nquantum circuit to approximate an optimal policy. Extensive simulations then\ndemonstrate that the proposed solution not only can significantly improve the\naverage throughput of D2D devices when the shared spectrum is busy but also can\nachieve much better performance in terms of convergence rate and learning\ncomplexity compared to existing DRL-based methods.\n","authors":["Nguyen Van Huynh","Bolun Zhang","Dinh-Hieu Tran","Dinh Thai Hoang","Diep N. Nguyen","Gan Zheng","Dusit Niyato","Quoc-Viet Pham"],"pdf_url":"https://arxiv.org/pdf/2410.17971v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.12819v2","updated":"2024-10-23T14:00:36Z","published":"2024-07-01T10:33:46Z","title":"I've Got 99 Problems But FLOPS Ain't One","summary":"  Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.\n","authors":["Alexandru M. Gherghescu","Vlad-Andrei Bădoiu","Alexandru Agache","Mihai-Valentin Dumitru","Iuliu Vasilescu","Radu Mantu","Costin Raiciu"],"pdf_url":"https://arxiv.org/pdf/2407.12819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17618v1","updated":"2024-10-23T07:15:52Z","published":"2024-10-23T07:15:52Z","title":"V2V Path Loss Modeling at 26 GHz Based on Real-Traffic Measurements","summary":"  In this letter, we investigate single-slope path loss models complemented\nwith shadowing effects in the context of vehicular communications. We present\nseveral models obtained based on extensive measurement campaigns with\ninter-vehicle transmission conducted at 26.555 GHz in real-traffic experiments,\nmainly along high-speed roads. Particular attention has been put on the impact\nof aerial characteristics (omnidirectional versus directional), surrounding\nenvironment (e.g., urban versus rural), and their mounting point on cars (at\nthe rooftop, on the bumper, and below the car chassis). Finally, the effect of\nsignal ducting and of the number of blocking cars has been analyzed and the\ndecorrelation time has been discussed\n","authors":["Pawel Kryszkiewicz","Adrian Kliks","Pawel Sroka","Michal Sybis"],"pdf_url":"https://arxiv.org/pdf/2410.17618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17572v1","updated":"2024-10-23T05:49:42Z","published":"2024-10-23T05:49:42Z","title":"Ultra-reliable urban air mobility networks","summary":"  Recently, urban air mobility (UAM) has attracted attention as an emerging\ntechnology that will bring innovation to urban transportation and aviation\nsystems. Since the UAM systems pursue fully autonomous flight without a pilot,\nwireless communication is a key function not only for flight control signals,\nbut also for navigation and safety information. The essential information is\ncalled a command and control (C2) message, and the UAM networks must be\nconfigured so that the UAM can receive the C2 message by securing a continuous\nlink stability without any interruptions. Nevertheless, a lot of prior works\nhave focused only on improving the average performance without solving the\nlow-reliability in the cell edges and coverage holes of urban areas. In this\ndissertation, we identify the factors that hinder the communication link\nreliability in considering three-dimensional (3D) urban environments, and\npropose a antenna configuration, resource utilization, and transmission\nstrategy to enable UAM receiving C2 messages regardless of time and space.\nFirst, through stochastic geometry modeling, we analyze the signal blockage\neffects caused by the urban buildings. The blockage probability is calculated\naccording to the shape, height, and density of the buildings, and the coverage\nprobability of the received signal is derived by reflecting the blockage\nevents. Furthermore, the low-reliability area is identified by analyzing the\ncoverage performance according to the positions of the UAMs. To overcome the\nlow-reliability region, we propose three methods for UAM network operation: i)\noptimization of antennas elevation tilting, ii) frequency reuse with\nmulti-layered narrow beam, and iii) assistive transmissions by the master UAM.\n","authors":["Hyunsoo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17572v1.pdf","comment":"PhD thesis, 64 pages, 24 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.17525v1","updated":"2024-10-23T03:04:24Z","published":"2024-10-23T03:04:24Z","title":"Physics-driven AI for Channel Estimation in Cellular Network","summary":"  In cellular mobile networks, wireless channel quality (CQ) is a crucial\nfactor in determining communication performance and user's network experience.\nAccurately predicting CQ based on real environmental characteristics, specific\nbase station configurations and user trajectories can help network operators\noptimize base station deployment, improving coverage and capacity. The Received\nSignal Reference Power (RSRP) and Signal-to-Interference-plus-Noise Ratio\n(SINR) of user equipment (UE) are key indicators of CQ in wireless\ncommunication. However, existing researches have limitations in terms of\ngeneration accuracy. Regression methods such as statistical inference and\nrandom forests fail to effectively capture the unique characteristics of\nwireless environments; theoretical derivations relying on specific\ncommunication protocols lack generalization capability; data-driven machine\nlearning (ML) methods like Long Short-Term Memory (LSTM) Network often suffer\nfrom a lack of interpretability. To overcome these limitations, we propose\nphysics-informed diffusion models, which accurately generate RSRP and SINR at\nUE based on the wireless environment, base station configurations, and user\ntrajectories. The model adopts a modular and end-to-end design, employing a\nteacher-student framework to achieve knowledge distillation. This method\nintegrates expert knowledge into the training of diffusion models, enhancing\nboth the interpretability and accuracy, while also facilitating faster\nconvergence of the model parameters. Furthermore, it allows for self-adaptation\nin various scenarios through few-shot learning. This approach provides valuable\nguidance for optimizing base station deployment, predicting user network\nexperience, and building real-world simulators.\n","authors":["Xiaoqian Qi","Haoye Chai","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.17525v1.pdf","comment":"7 pages, 6 figures"}]},"2024-10-24T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2410.17078v2","updated":"2024-10-24T17:12:19Z","published":"2024-10-22T14:56:50Z","title":"FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters","summary":"  The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce.\n","authors":["Hasibul Jamil","Abdul Alim","Laurent Schares","Pavlos Maniotis","Liran Schour","Ali Sydney","Abdullah Kayi","Tevfik Kosar","Bengi Karacali"],"pdf_url":"https://arxiv.org/pdf/2410.17078v2.pdf","comment":"Submitted for peer reviewing in IEEE ICC 2025"},{"id":"http://arxiv.org/abs/2410.16724v2","updated":"2024-10-24T11:57:28Z","published":"2024-10-22T06:16:00Z","title":"Efficient Scheduling of Vehicular Tasks on Edge Systems with Green\n  Energy and Battery Storage","summary":"  The autonomous vehicle industry is rapidly expanding, requiring significant\ncomputational resources for tasks like perception and decision-making.\nVehicular edge computing has emerged to meet this need, utilizing roadside\ncomputational units (roadside edge servers) to support autonomous vehicles.\nAligning with the trend of green cloud computing, these roadside edge servers\noften get energy from solar power. Additionally, each roadside computational\nunit is equipped with a battery for storing solar power, ensuring continuous\ncomputational operation during periods of low solar energy availability.\n  In our research, we address the scheduling of computational tasks generated\nby autonomous vehicles to roadside units with power consumption proportional to\nthe cube of the computational load of the server. Each computational task is\nassociated with a revenue, dependent on its computational needs and deadline.\nOur objective is to maximize the total revenue of the system of roadside\ncomputational units.\n  We propose an offline heuristics approach based on predicted solar energy and\nincoming task patterns for different time slots. Additionally, we present\nheuristics for real-time adaptation to varying solar energy and task patterns\nfrom predicted values for different time slots. Our comparative analysis shows\nthat our methods outperform state-of-the-art approaches upto 40\\% for real-life\ndatasets.\n","authors":["Suvarthi Sarkar","Abinash Kumar Ray","Aryabartta Sahu"],"pdf_url":"https://arxiv.org/pdf/2410.16724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16731v4","updated":"2024-10-24T00:44:49Z","published":"2024-02-26T16:52:35Z","title":"PyGim: An Efficient Graph Neural Network Library for Real\n  Processing-In-Memory Architectures","summary":"  Graph Neural Networks (GNNs) are emerging ML models to analyze\ngraph-structure data. Graph Neural Network (GNN) execution involves both\ncompute-intensive and memory-intensive kernels, the latter dominates the total\ntime, being significantly bottlenecked by data movement between memory and\nprocessors. Processing-In-Memory (PIM) systems can alleviate this data movement\nbottleneck by placing simple processors near or inside to memory arrays. In\nthis work, we introduce PyGim, an efficient ML library that accelerates GNNs on\nreal PIM systems. We propose intelligent parallelization techniques for\nmemory-intensive kernels of GNNs tailored for real PIM systems, and develop\nhandy Python API for them. We provide hybrid GNN execution, in which the\ncompute-intensive and memory-intensive kernels are executed in\nprocessor-centric and memory-centric computing systems, respectively. We\nextensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using\nemerging GNN models, and demonstrate that it outperforms its state-of-the-art\nCPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource\nutilization than CPU and GPU systems. Our work provides useful recommendations\nfor software, system and hardware designers. PyGim is publicly available at\nhttps://github.com/CMU-SAFARI/PyGim.\n","authors":["Christina Giannoula","Peiming Yang","Ivan Fernandez Vega","Jiacheng Yang","Sankeerth Durvasula","Yu Xin Li","Mohammad Sadrosadati","Juan Gomez Luna","Onur Mutlu","Gennady Pekhimenko"],"pdf_url":"https://arxiv.org/pdf/2402.16731v4.pdf","comment":null}],"Hardware Architecture":[{"id":"http://arxiv.org/abs/2410.17078v2","updated":"2024-10-24T17:12:19Z","published":"2024-10-22T14:56:50Z","title":"FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters","summary":"  The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce.\n","authors":["Hasibul Jamil","Abdul Alim","Laurent Schares","Pavlos Maniotis","Liran Schour","Ali Sydney","Abdullah Kayi","Tevfik Kosar","Bengi Karacali"],"pdf_url":"https://arxiv.org/pdf/2410.17078v2.pdf","comment":"Submitted for peer reviewing in IEEE ICC 2025"},{"id":"http://arxiv.org/abs/2410.16724v2","updated":"2024-10-24T11:57:28Z","published":"2024-10-22T06:16:00Z","title":"Efficient Scheduling of Vehicular Tasks on Edge Systems with Green\n  Energy and Battery Storage","summary":"  The autonomous vehicle industry is rapidly expanding, requiring significant\ncomputational resources for tasks like perception and decision-making.\nVehicular edge computing has emerged to meet this need, utilizing roadside\ncomputational units (roadside edge servers) to support autonomous vehicles.\nAligning with the trend of green cloud computing, these roadside edge servers\noften get energy from solar power. Additionally, each roadside computational\nunit is equipped with a battery for storing solar power, ensuring continuous\ncomputational operation during periods of low solar energy availability.\n  In our research, we address the scheduling of computational tasks generated\nby autonomous vehicles to roadside units with power consumption proportional to\nthe cube of the computational load of the server. Each computational task is\nassociated with a revenue, dependent on its computational needs and deadline.\nOur objective is to maximize the total revenue of the system of roadside\ncomputational units.\n  We propose an offline heuristics approach based on predicted solar energy and\nincoming task patterns for different time slots. Additionally, we present\nheuristics for real-time adaptation to varying solar energy and task patterns\nfrom predicted values for different time slots. Our comparative analysis shows\nthat our methods outperform state-of-the-art approaches upto 40\\% for real-life\ndatasets.\n","authors":["Suvarthi Sarkar","Abinash Kumar Ray","Aryabartta Sahu"],"pdf_url":"https://arxiv.org/pdf/2410.16724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16731v4","updated":"2024-10-24T00:44:49Z","published":"2024-02-26T16:52:35Z","title":"PyGim: An Efficient Graph Neural Network Library for Real\n  Processing-In-Memory Architectures","summary":"  Graph Neural Networks (GNNs) are emerging ML models to analyze\ngraph-structure data. Graph Neural Network (GNN) execution involves both\ncompute-intensive and memory-intensive kernels, the latter dominates the total\ntime, being significantly bottlenecked by data movement between memory and\nprocessors. Processing-In-Memory (PIM) systems can alleviate this data movement\nbottleneck by placing simple processors near or inside to memory arrays. In\nthis work, we introduce PyGim, an efficient ML library that accelerates GNNs on\nreal PIM systems. We propose intelligent parallelization techniques for\nmemory-intensive kernels of GNNs tailored for real PIM systems, and develop\nhandy Python API for them. We provide hybrid GNN execution, in which the\ncompute-intensive and memory-intensive kernels are executed in\nprocessor-centric and memory-centric computing systems, respectively. We\nextensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using\nemerging GNN models, and demonstrate that it outperforms its state-of-the-art\nCPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource\nutilization than CPU and GPU systems. Our work provides useful recommendations\nfor software, system and hardware designers. PyGim is publicly available at\nhttps://github.com/CMU-SAFARI/PyGim.\n","authors":["Christina Giannoula","Peiming Yang","Ivan Fernandez Vega","Jiacheng Yang","Sankeerth Durvasula","Yu Xin Li","Mohammad Sadrosadati","Juan Gomez Luna","Onur Mutlu","Gennady Pekhimenko"],"pdf_url":"https://arxiv.org/pdf/2402.16731v4.pdf","comment":null}],"Databases":[{"id":"http://arxiv.org/abs/2410.14066v3","updated":"2024-10-24T13:28:18Z","published":"2024-10-17T22:28:07Z","title":"Lightweight Correlation-Aware Table Compression","summary":"  The growing adoption of data lakes for managing relational data necessitates\nefficient, open storage formats that provide high scan performance and\ncompetitive compression ratios. While existing formats achieve fast scans\nthrough lightweight encoding techniques, they have reached a plateau in terms\nof minimizing storage footprint. Recently, correlation-aware compression\nschemes have been shown to reduce file sizes further. Yet, current approaches\neither incur significant scan overheads or require manual specification of\ncorrelations, limiting their practicability. We present $\\texttt{Virtual}$, a\nframework that integrates seamlessly with existing open formats to\nautomatically leverage data correlations, achieving substantial compression\ngains while having minimal scan performance overhead. Experiments on data-gov\ndatasets show that $\\texttt{Virtual}$ reduces file sizes by up to 40% compared\nto Apache Parquet.\n","authors":["Mihail Stoian","Alexander van Renen","Jan Kobiolka","Ping-Lin Kuo","Josif Grabocka","Andreas Kipf"],"pdf_url":"https://arxiv.org/pdf/2410.14066v3.pdf","comment":"Third Table Representation Learning Workshop (TRL @ NeurIPS 2024)"}],"Performance":[{"id":"http://arxiv.org/abs/2402.16731v4","updated":"2024-10-24T00:44:49Z","published":"2024-02-26T16:52:35Z","title":"PyGim: An Efficient Graph Neural Network Library for Real\n  Processing-In-Memory Architectures","summary":"  Graph Neural Networks (GNNs) are emerging ML models to analyze\ngraph-structure data. Graph Neural Network (GNN) execution involves both\ncompute-intensive and memory-intensive kernels, the latter dominates the total\ntime, being significantly bottlenecked by data movement between memory and\nprocessors. Processing-In-Memory (PIM) systems can alleviate this data movement\nbottleneck by placing simple processors near or inside to memory arrays. In\nthis work, we introduce PyGim, an efficient ML library that accelerates GNNs on\nreal PIM systems. We propose intelligent parallelization techniques for\nmemory-intensive kernels of GNNs tailored for real PIM systems, and develop\nhandy Python API for them. We provide hybrid GNN execution, in which the\ncompute-intensive and memory-intensive kernels are executed in\nprocessor-centric and memory-centric computing systems, respectively. We\nextensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using\nemerging GNN models, and demonstrate that it outperforms its state-of-the-art\nCPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource\nutilization than CPU and GPU systems. Our work provides useful recommendations\nfor software, system and hardware designers. PyGim is publicly available at\nhttps://github.com/CMU-SAFARI/PyGim.\n","authors":["Christina Giannoula","Peiming Yang","Ivan Fernandez Vega","Jiacheng Yang","Sankeerth Durvasula","Yu Xin Li","Mohammad Sadrosadati","Juan Gomez Luna","Onur Mutlu","Gennady Pekhimenko"],"pdf_url":"https://arxiv.org/pdf/2402.16731v4.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2404.08401v4","updated":"2024-10-24T14:41:42Z","published":"2024-04-12T11:15:15Z","title":"PnLCalib: Sports Field Registration via Points and Lines Optimization","summary":"  Camera calibration in broadcast sports videos presents numerous challenges\nfor accurate sports field registration due to multiple camera angles, varying\ncamera parameters, and frequent occlusions of the field. Traditional\nsearch-based methods depend on initial camera pose estimates, which can\nstruggle in non-standard positions and dynamic environments. In response, we\npropose an optimization-based calibration pipeline that leverages a 3D soccer\nfield model and a predefined set of keypoints to overcome these limitations.\nOur method also introduces a novel refinement module that improves initial\ncalibration by using detected field lines in a non-linear optimization process.\nThis approach outperforms existing techniques in both multi-view and\nsingle-view 3D camera calibration tasks, while maintaining competitive\nperformance in homography estimation. Extensive experimentation on real-world\nsoccer datasets, including SoccerNet-Calibration, WorldCup 2014, and\nTS-WorldCup, highlights the robustness and accuracy of our method across\ndiverse broadcast scenarios. Our approach offers significant improvements in\ncamera calibration precision and reliability.\n","authors":["Marc Gutiérrez-Pérez","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2404.08401v4.pdf","comment":"Extended version of \"No Bells, Just Whistles: Sports Field\n  Registration Leveraging Geometric Properties\""},{"id":"http://arxiv.org/abs/2406.05804v5","updated":"2024-10-24T07:07:43Z","published":"2024-06-09T14:42:55Z","title":"A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning","summary":"  Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.\n","authors":["Xinzhe Li"],"pdf_url":"https://arxiv.org/pdf/2406.05804v5.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.17579v2","updated":"2024-10-24T05:24:53Z","published":"2024-10-23T06:08:45Z","title":"Bonsai: Gradient-free Graph Distillation for Node Classification","summary":"  Graph distillation has emerged as a promising avenue to enable scalable\ntraining of GNNs by compressing the training dataset while preserving essential\ngraph characteristics. Our study uncovers significant shortcomings in current\ngraph distillation techniques. First, the majority of the algorithms\nparadoxically require training on the full dataset to perform distillation.\nSecond, due to their gradient-emulating approach, these methods require fresh\ndistillation for any change in hyperparameters or GNN architecture, limiting\ntheir flexibility and reusability. Finally, they fail to achieve substantial\nsize reduction due to synthesizing fully-connected, edge-weighted graphs. To\naddress these challenges, we present Bonsai, a novel graph distillation method\nempowered by the observation that \\textit{computation trees} form the\nfundamental processing units of message-passing GNNs. Bonsai distills datasets\nby encoding a careful selection of \\textit{exemplar} trees that maximize the\nrepresentation of all computation trees in the training set. This unique\napproach imparts Bonsai as the first linear-time, model-agnostic graph\ndistillation algorithm for node classification that outperforms existing\nbaselines across $6$ real-world datasets on accuracy, while being $22$ times\nfaster on average. Bonsai is grounded in rigorous mathematical guarantees on\nthe adopted approximation strategies making it robust to GNN architectures,\ndatasets, and parameters.\n","authors":["Mridul Gupta","Samyak Jain","Vansh Ramani","Hariprasad Kodamana","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2410.17579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19599v5","updated":"2024-10-24T03:14:22Z","published":"2023-05-31T06:59:21Z","title":"RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine\n  Semantic Re-alignment","summary":"  Recent advances in text-to-image diffusion models have achieved remarkable\nsuccess in generating high-quality, realistic images from textual descriptions.\nHowever, these approaches have faced challenges in precisely aligning the\ngenerated visual content with the textual concepts described in the prompts. In\nthis paper, we propose a two-stage coarse-to-fine semantic re-alignment method,\nnamed RealignDiff, aimed at improving the alignment between text and images in\ntext-to-image diffusion models. In the coarse semantic re-alignment phase, a\nnovel caption reward, leveraging the BLIP-2 model, is proposed to evaluate the\nsemantic discrepancy between the generated image caption and the given text\nprompt. Subsequently, the fine semantic re-alignment stage employs a local\ndense caption generation module and a re-weighting attention modulation module\nto refine the previously generated images from a local semantic view.\nExperimental results on the MS-COCO and ViLG-300 datasets demonstrate that the\nproposed two-stage coarse-to-fine semantic re-alignment method outperforms\nother baseline re-alignment techniques by a substantial margin in both visual\nquality and semantic similarity with the input prompt.\n","authors":["Zutao Jiang","Guian Fang","Jianhua Han","Guansong Lu","Hang Xu","Shengcai Liao","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2305.19599v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17546v2","updated":"2024-10-24T04:21:54Z","published":"2024-10-23T03:53:46Z","title":"Advancing Interpretability in Text Classification through Prototype\n  Learning","summary":"  Deep neural networks have achieved remarkable performance in various\ntext-based tasks but often lack interpretability, making them less suitable for\napplications where transparency is critical. To address this, we propose\nProtoLens, a novel prototype-based model that provides fine-grained,\nsub-sentence level interpretability for text classification. ProtoLens uses a\nPrototype-aware Span Extraction module to identify relevant text spans\nassociated with learned prototypes and a Prototype Alignment mechanism to\nensure prototypes are semantically meaningful throughout training. By aligning\nthe prototype embeddings with human-understandable examples, ProtoLens provides\ninterpretable predictions while maintaining competitive accuracy. Extensive\nexperiments demonstrate that ProtoLens outperforms both prototype-based and\nnon-interpretable baselines on multiple text classification benchmarks. Code\nand data are available at\n\\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}.\n","authors":["Bowen Wei","Ziwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.17546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07840v4","updated":"2024-10-24T05:11:13Z","published":"2023-07-15T16:16:22Z","title":"RegExplainer: Generating Explanations for Graph Neural Networks in\n  Regression Tasks","summary":"  Graph regression is a fundamental task that has gained significant attention\nin various graph learning tasks. However, the inference process is often not\neasily interpretable. Current explanation techniques are limited to\nunderstanding Graph Neural Network (GNN) behaviors in classification tasks,\nleaving an explanation gap for graph regression models. In this work, we\npropose a novel explanation method to interpret the graph regression models\n(XAIG-R). Our method addresses the distribution shifting problem and\ncontinuously ordered decision boundary issues that hinder existing methods away\nfrom being applied in regression tasks. We introduce a novel objective based on\nthe graph information bottleneck theory (GIB) and a new mix-up framework, which\ncan support various GNNs and explainers in a model-agnostic manner.\nAdditionally, we present a self-supervised learning strategy to tackle the\ncontinuously ordered labels in regression tasks. We evaluate our proposed\nmethod on three benchmark datasets and a real-life dataset introduced by us,\nand extensive experiments demonstrate its effectiveness in interpreting GNN\nmodels in regression tasks.\n","authors":["Jiaxing Zhang","Zhuomin Chen","Hao Mei","Longchao Da","Dongsheng Luo","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2307.07840v4.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17439v2","updated":"2024-10-24T13:34:47Z","published":"2024-10-22T21:30:58Z","title":"Evaluating AI-Generated Essays with GRE Analytical Writing Assessment","summary":"  The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs.\n","authors":["Yang Zhong","Jiangang Hao","Michael Fauss","Chen Li","Yuan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17439v2.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17248v2","updated":"2024-10-24T15:06:36Z","published":"2024-10-22T17:59:55Z","title":"HyperspectralViTs: General Hyperspectral Models for On-board Remote\n  Sensing","summary":"  On-board processing of hyperspectral data with machine learning models would\nenable unprecedented amount of autonomy for a wide range of tasks, for example\nmethane detection or mineral identification. This can enable early warning\nsystem and could allow new capabilities such as automated scheduling across\nconstellations of satellites. Classical methods suffer from high false positive\nrates and previous deep learning models exhibit prohibitive computational\nrequirements. We propose fast and accurate machine learning architectures which\nsupport end-to-end training with data of high spectral dimension without\nrelying on hand-crafted products or spectral band compression preprocessing. We\nevaluate our models on two tasks related to hyperspectral data processing. With\nour proposed general architectures, we improve the F1 score of the previous\nmethane detection state-of-the-art models by 27% on a newly created synthetic\ndataset and by 13% on the previously released large benchmark dataset. We also\ndemonstrate that training models on the synthetic dataset improves performance\nof models finetuned on the dataset of real events by 6.9% in F1 score in\ncontrast with training from scratch. On a newly created dataset for mineral\nidentification, our models provide 3.5% improvement in the F1 score in contrast\nto the default versions of the models. With our proposed models we improve the\ninference speed by 85% in contrast to previous classical and deep learning\napproaches by removing the dependency on classically computed features. With\nour architecture, one capture from the EMIT sensor can be processed within 30\nseconds on realistic proxy of the ION-SCV 004 satellite.\n","authors":["Vít Růžička","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2410.17248v2.pdf","comment":"13 pages, This work has been submitted to the IEEE for possible\n  publication"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.17998v2","updated":"2024-10-24T17:47:20Z","published":"2024-10-23T16:12:59Z","title":"Estimating the Spectral Moments of the Kernel Integral Operator from\n  Finite Sample Matrices","summary":"  Analyzing the structure of sampled features from an input data distribution\nis challenging when constrained by limited measurements in both the number of\ninputs and features. Traditional approaches often rely on the eigenvalue\nspectrum of the sample covariance matrix derived from finite measurement\nmatrices; however, these spectra are sensitive to the size of the measurement\nmatrix, leading to biased insights. In this paper, we introduce a novel\nalgorithm that provides unbiased estimates of the spectral moments of the\nkernel integral operator in the limit of infinite inputs and features from\nfinitely sampled measurement matrices. Our method, based on dynamic\nprogramming, is efficient and capable of estimating the moments of the operator\nspectrum. We demonstrate the accuracy of our estimator on radial basis function\n(RBF) kernels, highlighting its consistency with the theoretical spectra.\nFurthermore, we showcase the practical utility and robustness of our method in\nunderstanding the geometry of learned representations in neural networks.\n","authors":["Chanwoo Chun","SueYeon Chung","Daniel D. Lee"],"pdf_url":"https://arxiv.org/pdf/2410.17998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17579v2","updated":"2024-10-24T05:24:53Z","published":"2024-10-23T06:08:45Z","title":"Bonsai: Gradient-free Graph Distillation for Node Classification","summary":"  Graph distillation has emerged as a promising avenue to enable scalable\ntraining of GNNs by compressing the training dataset while preserving essential\ngraph characteristics. Our study uncovers significant shortcomings in current\ngraph distillation techniques. First, the majority of the algorithms\nparadoxically require training on the full dataset to perform distillation.\nSecond, due to their gradient-emulating approach, these methods require fresh\ndistillation for any change in hyperparameters or GNN architecture, limiting\ntheir flexibility and reusability. Finally, they fail to achieve substantial\nsize reduction due to synthesizing fully-connected, edge-weighted graphs. To\naddress these challenges, we present Bonsai, a novel graph distillation method\nempowered by the observation that \\textit{computation trees} form the\nfundamental processing units of message-passing GNNs. Bonsai distills datasets\nby encoding a careful selection of \\textit{exemplar} trees that maximize the\nrepresentation of all computation trees in the training set. This unique\napproach imparts Bonsai as the first linear-time, model-agnostic graph\ndistillation algorithm for node classification that outperforms existing\nbaselines across $6$ real-world datasets on accuracy, while being $22$ times\nfaster on average. Bonsai is grounded in rigorous mathematical guarantees on\nthe adopted approximation strategies making it robust to GNN architectures,\ndatasets, and parameters.\n","authors":["Mridul Gupta","Samyak Jain","Vansh Ramani","Hariprasad Kodamana","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2410.17579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07840v4","updated":"2024-10-24T05:11:13Z","published":"2023-07-15T16:16:22Z","title":"RegExplainer: Generating Explanations for Graph Neural Networks in\n  Regression Tasks","summary":"  Graph regression is a fundamental task that has gained significant attention\nin various graph learning tasks. However, the inference process is often not\neasily interpretable. Current explanation techniques are limited to\nunderstanding Graph Neural Network (GNN) behaviors in classification tasks,\nleaving an explanation gap for graph regression models. In this work, we\npropose a novel explanation method to interpret the graph regression models\n(XAIG-R). Our method addresses the distribution shifting problem and\ncontinuously ordered decision boundary issues that hinder existing methods away\nfrom being applied in regression tasks. We introduce a novel objective based on\nthe graph information bottleneck theory (GIB) and a new mix-up framework, which\ncan support various GNNs and explainers in a model-agnostic manner.\nAdditionally, we present a self-supervised learning strategy to tackle the\ncontinuously ordered labels in regression tasks. We evaluate our proposed\nmethod on three benchmark datasets and a real-life dataset introduced by us,\nand extensive experiments demonstrate its effectiveness in interpreting GNN\nmodels in regression tasks.\n","authors":["Jiaxing Zhang","Zhuomin Chen","Hao Mei","Longchao Da","Dongsheng Luo","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2307.07840v4.pdf","comment":"Accepted by NeurIPS 2024"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2410.17820v2","updated":"2024-10-24T12:01:31Z","published":"2024-10-23T12:26:10Z","title":"Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination","summary":"  Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. Scaling the generator leads to notable improvements in ToT\nperformance, even when using a smaller model as the discriminator, whereas\nscaling the discriminator with a fixed generator yields only marginal gains.\nOur results show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT.\n","authors":["Qiqi Chen","Xinpeng Wang","Philipp Mondorf","Michael A. Hedderich","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.17820v2.pdf","comment":"Code: github.com/mainlp/tot-eval"},{"id":"http://arxiv.org/abs/2406.05804v5","updated":"2024-10-24T07:07:43Z","published":"2024-06-09T14:42:55Z","title":"A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning","summary":"  Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.\n","authors":["Xinzhe Li"],"pdf_url":"https://arxiv.org/pdf/2406.05804v5.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.17552v2","updated":"2024-10-24T02:35:09Z","published":"2024-10-23T04:34:49Z","title":"ESpeW: Robust Copyright Protection for LLM-based EaaS via\n  Embedding-Specific Watermark","summary":"  Embeddings as a Service (EaaS) is emerging as a crucial role in AI\napplications. Unfortunately, EaaS is vulnerable to model extraction attacks,\nhighlighting the urgent need for copyright protection. Although some\npreliminary works propose applying embedding watermarks to protect EaaS, recent\nresearch reveals that these watermarks can be easily removed. Hence, it is\ncrucial to inject robust watermarks resistant to watermark removal attacks.\nExisting watermarking methods typically inject a target embedding into\nembeddings through linear interpolation when the text contains triggers.\nHowever, this mechanism results in each watermarked embedding having the same\ncomponent, which makes the watermark easy to identify and eliminate. Motivated\nby this, in this paper, we propose a novel embedding-specific watermarking\n(ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach\ninvolves injecting unique, yet readily identifiable watermarks into each\nembedding. Watermarks inserted by ESpeW are designed to maintain a significant\ndistance from one another and to avoid sharing common components, thus making\nit significantly more challenging to remove the watermarks. Extensive\nexperiments on four popular datasets demonstrate that ESpeW can even watermark\nsuccessfully against a highly aggressive removal strategy without sacrificing\nthe quality of embeddings. Code is available at\nhttps://github.com/liudan193/ESpeW.\n","authors":["Zongqi Wang","Baoyuan Wu","Jingyuan Deng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2410.17552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17546v2","updated":"2024-10-24T04:21:54Z","published":"2024-10-23T03:53:46Z","title":"Advancing Interpretability in Text Classification through Prototype\n  Learning","summary":"  Deep neural networks have achieved remarkable performance in various\ntext-based tasks but often lack interpretability, making them less suitable for\napplications where transparency is critical. To address this, we propose\nProtoLens, a novel prototype-based model that provides fine-grained,\nsub-sentence level interpretability for text classification. ProtoLens uses a\nPrototype-aware Span Extraction module to identify relevant text spans\nassociated with learned prototypes and a Prototype Alignment mechanism to\nensure prototypes are semantically meaningful throughout training. By aligning\nthe prototype embeddings with human-understandable examples, ProtoLens provides\ninterpretable predictions while maintaining competitive accuracy. Extensive\nexperiments demonstrate that ProtoLens outperforms both prototype-based and\nnon-interpretable baselines on multiple text classification benchmarks. Code\nand data are available at\n\\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}.\n","authors":["Bowen Wei","Ziwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.17546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17450v2","updated":"2024-10-24T17:38:44Z","published":"2024-10-22T21:55:54Z","title":"Interação entre robôs humanoides: desenvolvendo a\n  colaboração e comunicação autônoma","summary":"  This study investigates the interaction between humanoid robots NAO and\nPepper, emphasizing their potential applications in educational settings. NAO,\nwidely used in education, and Pepper, designed for social interactions, of er\nnew opportunities for autonomous communication and collaboration. Through a\nseries of programmed interactions, the robots demonstrated their ability to\ncommunicate and coordinate actions autonomously, highlighting their potential\nas tools for enhancing learning environments. The research also explores the\nintegration of emerging technologies, such as artificial intelligence, into\nthese systems, allowing robots to learn from each other and adapt their\nbehavior. The findings suggest that NAO and Pepper can significantly contribute\nto both technical learning and the development of social and emotional skills\nin students, of ering innovative pedagogical approaches through the use of\nhumanoid robotics.\n","authors":["Moraes Pablo","Rodríguez Mónica","Peters Christopher","Sodre Hiago","Mazondo Ahilen","Sandin Vincent","Barcelona Sebastian","Moraes William","Fernández Santiago","Assunção Nathalie","de Vargas Bruna","Dörnbach Tobias","Kelbouscas André","Grando Ricardo"],"pdf_url":"https://arxiv.org/pdf/2410.17450v2.pdf","comment":"in Portuguese language"},{"id":"http://arxiv.org/abs/2410.17439v2","updated":"2024-10-24T13:34:47Z","published":"2024-10-22T21:30:58Z","title":"Evaluating AI-Generated Essays with GRE Analytical Writing Assessment","summary":"  The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs.\n","authors":["Yang Zhong","Jiangang Hao","Michael Fauss","Chen Li","Yuan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17439v2.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.12883v3","updated":"2024-10-24T04:51:21Z","published":"2024-07-16T17:58:27Z","title":"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive\n  Retrieval","summary":"  Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. Our dataset consists of 1,384\nreal-world queries spanning diverse domains, such as economics, psychology,\nmathematics, and coding. These queries are drawn from naturally occurring and\ncarefully curated human data. Extensive evaluation reveals that even\nstate-of-the-art retrieval models perform poorly on BRIGHT. The leading model\non the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of\n59.0 nDCG@10, produces a score of nDCG@10 of 18.3 on BRIGHT. We show that\nincorporating explicit reasoning about the query improves retrieval performance\nby up to 12.2 points. Moreover, incorporating retrieved documents from the\ntop-performing retriever boosts question-answering performance by over 6.6\npoints. We believe that BRIGHT paves the way for future research on retrieval\nsystems in more realistic and challenging settings.\n","authors":["Hongjin Su","Howard Yen","Mengzhou Xia","Weijia Shi","Niklas Muennighoff","Han-yu Wang","Haisu Liu","Quan Shi","Zachary S. Siegel","Michael Tang","Ruoxi Sun","Jinsung Yoon","Sercan O. Arik","Danqi Chen","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2407.12883v3.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2410.16973v2","updated":"2024-10-24T15:49:03Z","published":"2024-10-22T12:51:51Z","title":"Learning Mathematical Rules with Large Language Models","summary":"  In this paper, we study the ability of large language models to learn\nspecific mathematical rules such as distributivity or simplifying equations. We\npresent an empirical analysis of their ability to generalize these rules, as\nwell as to reuse them in the context of word problems. For this purpose, we\nprovide a rigorous methodology to build synthetic data incorporating such\nrules, and perform fine-tuning of large language models on such data. Our\nexperiments show that our model can learn and generalize these rules to some\nextent, as well as suitably reuse them in the context of word problems.\n","authors":["Nelson Vadori","Antoine Gorceix","Bastien Le Chenadec","Ahmad Rammal","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2410.16973v2.pdf","comment":"NeurIPS'24 MATH-AI, the 4th Workshop on Mathematical Reasoning and AI"},{"id":"http://arxiv.org/abs/2410.07114v4","updated":"2024-10-24T12:39:19Z","published":"2024-09-19T19:48:31Z","title":"System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam","summary":"  The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch students' average of 40.63 points. Neither model had\naccess to the exam figures. Since there was a risk of model contami-nation\n(i.e., the knowledge cutoff for o1-preview and GPT-4o was after the exam was\npublished online), we repeated the procedure with a new Mathematics B exam that\nwas published after the cutoff date. The results again indicated that\no1-preview performed strongly (97.8th percentile), which suggests that\ncontamination was not a factor. We also show that there is some variability in\nthe output of o1-preview, which means that sometimes there is 'luck' (the\nanswer is correct) or 'bad luck' (the output has diverged into something that\nis incorrect). We demonstrate that the self-consistency approach, where\nrepeated prompts are given and the most common answer is selected, is a useful\nstrategy for identifying the correct answer. It is concluded that while\nOpenAI's new model series holds great potential, certain risks must be\nconsidered.\n","authors":["Joost de Winter","Dimitra Dodou","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2410.07114v4.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2404.08401v4","updated":"2024-10-24T14:41:42Z","published":"2024-04-12T11:15:15Z","title":"PnLCalib: Sports Field Registration via Points and Lines Optimization","summary":"  Camera calibration in broadcast sports videos presents numerous challenges\nfor accurate sports field registration due to multiple camera angles, varying\ncamera parameters, and frequent occlusions of the field. Traditional\nsearch-based methods depend on initial camera pose estimates, which can\nstruggle in non-standard positions and dynamic environments. In response, we\npropose an optimization-based calibration pipeline that leverages a 3D soccer\nfield model and a predefined set of keypoints to overcome these limitations.\nOur method also introduces a novel refinement module that improves initial\ncalibration by using detected field lines in a non-linear optimization process.\nThis approach outperforms existing techniques in both multi-view and\nsingle-view 3D camera calibration tasks, while maintaining competitive\nperformance in homography estimation. Extensive experimentation on real-world\nsoccer datasets, including SoccerNet-Calibration, WorldCup 2014, and\nTS-WorldCup, highlights the robustness and accuracy of our method across\ndiverse broadcast scenarios. Our approach offers significant improvements in\ncamera calibration precision and reliability.\n","authors":["Marc Gutiérrez-Pérez","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2404.08401v4.pdf","comment":"Extended version of \"No Bells, Just Whistles: Sports Field\n  Registration Leveraging Geometric Properties\""},{"id":"http://arxiv.org/abs/2408.14947v3","updated":"2024-10-24T04:57:51Z","published":"2024-08-27T10:44:34Z","title":"ERX: A Fast Real-Time Anomaly Detection Algorithm for Hyperspectral Line\n  Scanning","summary":"  Detecting unexpected objects (anomalies) in real time has great potential for\nmonitoring, managing, and protecting the environment. Hyperspectral line-scan\ncameras are a low-cost solution that enhance confidence in anomaly detection\nover RGB and multispectral imagery. However, existing line-scan algorithms are\ntoo slow when using small computers (e.g. those onboard a drone or small\nsatellite), do not adapt to changing scenery, or lack robustness against\ngeometric distortions. This paper introduces the Exponentially moving RX\nalgorithm (ERX) to address these issues, and compares it with existing RX-based\nanomaly detection methods for hyperspectral line scanning. Three large and more\ncomplex datasets are also introduced to better assess the practical challenges\nwhen using line-scan cameras (two hyperspectral and one multispectral). ERX is\nevaluated using a Jetson Xavier NX compute module, achieving the best\ncombination of speed and detection performance. This research paves the way for\nfuture studies in grouping and locating anomalous objects, adaptive and\nautomatic threshold selection, and real-time field tests. The datasets and the\nPython code are available at: https://github.com/WiseGamgee/HyperAD.\n","authors":["Samuel Garske","Bradley Evans","Christopher Artlett","KC Wong"],"pdf_url":"https://arxiv.org/pdf/2408.14947v3.pdf","comment":"17 pages, 13 figures, 4 tables, code and datasets accessible at\n  https://github.com/WiseGamgee/HyperAD"},{"id":"http://arxiv.org/abs/2305.19599v5","updated":"2024-10-24T03:14:22Z","published":"2023-05-31T06:59:21Z","title":"RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine\n  Semantic Re-alignment","summary":"  Recent advances in text-to-image diffusion models have achieved remarkable\nsuccess in generating high-quality, realistic images from textual descriptions.\nHowever, these approaches have faced challenges in precisely aligning the\ngenerated visual content with the textual concepts described in the prompts. In\nthis paper, we propose a two-stage coarse-to-fine semantic re-alignment method,\nnamed RealignDiff, aimed at improving the alignment between text and images in\ntext-to-image diffusion models. In the coarse semantic re-alignment phase, a\nnovel caption reward, leveraging the BLIP-2 model, is proposed to evaluate the\nsemantic discrepancy between the generated image caption and the given text\nprompt. Subsequently, the fine semantic re-alignment stage employs a local\ndense caption generation module and a re-weighting attention modulation module\nto refine the previously generated images from a local semantic view.\nExperimental results on the MS-COCO and ViLG-300 datasets demonstrate that the\nproposed two-stage coarse-to-fine semantic re-alignment method outperforms\nother baseline re-alignment techniques by a substantial margin in both visual\nquality and semantic similarity with the input prompt.\n","authors":["Zutao Jiang","Guian Fang","Jianhua Han","Guansong Lu","Hang Xu","Shengcai Liao","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2305.19599v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01486v5","updated":"2024-10-24T09:32:17Z","published":"2023-05-02T15:10:01Z","title":"ARBEx: Attentive Feature Extraction with Reliability Balancing for\n  Robust Facial Expression Learning","summary":"  In this paper, we introduce a framework ARBEx, a novel attentive feature\nextraction framework driven by Vision Transformer with reliability balancing to\ncope against poor class distributions, bias, and uncertainty in the facial\nexpression learning (FEL) task. We reinforce several data pre-processing and\nrefinement methods along with a window-based cross-attention ViT to squeeze the\nbest of the data. We also employ learnable anchor points in the embedding space\nwith label distributions and multi-head self-attention mechanism to optimize\nperformance against weak predictions with reliability balancing, which is a\nstrategy that leverages anchor points, attention scores, and confidence values\nto enhance the resilience of label predictions. To ensure correct label\nclassification and improve the models' discriminative power, we introduce\nanchor loss, which encourages large margins between anchor points.\nAdditionally, the multi-head self-attention mechanism, which is also trainable,\nplays an integral role in identifying accurate labels. This approach provides\ncritical elements for improving the reliability of predictions and has a\nsubstantial positive effect on final prediction capabilities. Our adaptive\nmodel can be integrated with any deep neural network to forestall challenges in\nvarious recognition tasks. Our strategy outperforms current state-of-the-art\nmethodologies, according to extensive experiments conducted in a variety of\ncontexts.\n","authors":["Azmine Toushik Wasi","Karlo Šerbetar","Raima Islam","Taki Hasan Rafi","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2305.01486v5.pdf","comment":"Extended version is accepted in ACCV 2024 as GReFEL"},{"id":"http://arxiv.org/abs/2407.05180v3","updated":"2024-10-24T11:18:24Z","published":"2024-04-22T10:33:06Z","title":"ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in\n  Robotic Surgical Skill Assessment","summary":"  In surgical skill assessment, the Objective Structured Assessments of\nTechnical Skills (OSATS) and Global Rating Scale (GRS) are well-established\ntools for evaluating surgeons during training. These metrics, along with\nperformance feedback, help surgeons improve and reach practice standards.\nRecent research on the open-source JIGSAWS dataset, which includes both GRS and\nOSATS labels, has focused on regressing GRS scores from kinematic data, video,\nor their combination. However, we argue that regressing GRS alone is limiting,\nas it aggregates OSATS scores and overlooks clinically meaningful variations\nduring a surgical trial. To address this, we developed a recurrent transformer\nmodel that tracks a surgeon's performance throughout a session by mapping\nhidden states to six OSATS, derived from kinematic data, using a clinically\nmotivated objective function. These OSATS scores are averaged to predict GRS,\nallowing us to compare our model's performance against state-of-the-art (SOTA)\nmethods. We report Spearman's Correlation Coefficients (SCC) demonstrating that\nour model outperforms SOTA using kinematic data (SCC 0.83-0.88), and matches\nperformance with video-based models. Our model also surpasses SOTA in most\ntasks for average OSATS predictions (SCC 0.46-0.70) and specific OSATS (SCC\n0.56-0.95). The generation of pseudo-labels at the segment level translates\nquantitative predictions into qualitative feedback, vital for automated\nsurgical skill assessment pipelines. A senior surgeon validated our model's\noutputs, agreeing with 77% of the weakly-supervised predictions (p=0.006).\n","authors":["Julien Quarez","Marc Modat","Sebastien Ourselin","Jonathan Shapey","Alejandro Granados"],"pdf_url":"https://arxiv.org/pdf/2407.05180v3.pdf","comment":null}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2410.17351v2","updated":"2024-10-24T15:57:45Z","published":"2024-10-22T18:35:05Z","title":"Hierarchical Multi-agent Reinforcement Learning for Cyber Network\n  Defense","summary":"  Recent advances in multi-agent reinforcement learning (MARL) have created\nopportunities to solve complex real-world tasks. Cybersecurity is a notable\napplication area, where defending networks against sophisticated adversaries\nremains a challenging task typically performed by teams of security operators.\nIn this work, we explore novel MARL strategies for building autonomous cyber\nnetwork defenses that address challenges such as large policy spaces, partial\nobservability, and stealthy, deceptive adversarial strategies. To facilitate\nefficient and generalized learning, we propose a hierarchical Proximal Policy\nOptimization (PPO) architecture that decomposes the cyber defense task into\nspecific sub-tasks like network investigation and host recovery. Our approach\ninvolves training sub-policies for each sub-task using PPO enhanced with domain\nexpertise. These sub-policies are then leveraged by a master defense policy\nthat coordinates their selection to solve complex network defense tasks.\nFurthermore, the sub-policies can be fine-tuned and transferred with minimal\ncost to defend against shifts in adversarial behavior or changes in network\nsettings. We conduct extensive experiments using CybORG Cage 4, the\nstate-of-the-art MARL environment for cyber defense. Comparisons with multiple\nbaselines across different adversaries show that our hierarchical learning\napproach achieves top performance in terms of convergence speed, episodic\nreturn, and several interpretable metrics relevant to cybersecurity, including\nthe fraction of clean machines on the network, precision, and false positives\non recoveries.\n","authors":["Aditya Vikram Singh","Ethan Rathbun","Emma Graham","Lisa Oakley","Simona Boboila","Alina Oprea","Peter Chin"],"pdf_url":"https://arxiv.org/pdf/2410.17351v2.pdf","comment":"9 pages, 7 figures, AAMAS preprint"},{"id":"http://arxiv.org/abs/2408.16387v3","updated":"2024-10-24T14:15:40Z","published":"2024-08-29T09:50:21Z","title":"Enhancing MOTION2NX for Efficient, Scalable and Secure Image Inference\n  using Convolutional Neural Networks","summary":"  This work contributes towards the development of an efficient and scalable\nopen-source Secure Multi-Party Computation (SMPC) protocol on machines with\nmoderate computational resources. We use the ABY2.0 SMPC protocol implemented\non the C++ based MOTION2NX framework for secure convolutional neural network\n(CNN) inference application with semi-honest security. Our list of\ncontributions are as follows. Firstly, we enhance MOTION2NX by providing a\ntensorized version of several primitive functions including the Hadamard\nproduct, indicator function and argmax function. Secondly, we adapt an existing\nHelper node algorithm, working in tandem with the ABY2.0 protocol, for\nefficient convolution computation to reduce execution time and RAM usage.\nThirdly, we also present a novel splitting algorithm that divides the\ncomputations at each CNN layer into multiple configurable chunks. This novel\nsplitting algorithm, providing significant reduction in RAM usage, is of\nindependent interest and is applicable to general SMPC protocols.\n","authors":["Haritha K","Ramya Burra","Srishti Mittal","Sarthak Sharma","Abhilash Venkatesh","Anshoo Tandon"],"pdf_url":"https://arxiv.org/pdf/2408.16387v3.pdf","comment":"20 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:2310.10133"},{"id":"http://arxiv.org/abs/2410.14792v2","updated":"2024-10-24T14:23:39Z","published":"2024-10-18T18:04:27Z","title":"CountCrypt: Quantum Cryptography between QCMA and PP","summary":"  We construct a quantum oracle relative to which BQP = QCMA but\nquantum-computation-classical-communication (QCCC) key exchange, QCCC\ncommitments, and two-round quantum key distribution exist. We also construct an\noracle relative to which BQP = QMA, but quantum lightning (a stronger variant\nof quantum money) exists. This extends previous work by Kretschmer [Kretschmer,\nTQC22], which showed that there is a quantum oracle relative to which BQP = QMA\nbut pseudorandom state generators (a quantum variant of pseudorandom\ngenerators) exist. We also show that QCCC key exchange, QCCC commitments, and\ntwo-round quantum key distribution can all be used to build one-way puzzles.\nOne-way puzzles are a version of \"quantum samplable\" one-wayness and are an\nintermediate primitive between pseudorandom state generators and EFI pairs, the\nminimal quantum primitive. In particular, one-way puzzles cannot exist if BQP =\nPP. Our results together imply that aside from pseudorandom state generators,\nthere is a large class of quantum cryptographic primitives which can exist even\nif BQP = QCMA, but are broken if BQP = PP. Furthermore, one-way puzzles are a\nminimal primitive for this class. We denote this class \"CountCrypt\".\n","authors":["Eli Goldin","Tomoyuki Morimae","Saachi Mutreja","Takashi Yamakawa"],"pdf_url":"https://arxiv.org/pdf/2410.14792v2.pdf","comment":"50 pages, 1 figure"}],"Networking and Internet Architecture":[{"id":"http://arxiv.org/abs/2410.17078v2","updated":"2024-10-24T17:12:19Z","published":"2024-10-22T14:56:50Z","title":"FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters","summary":"  The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce.\n","authors":["Hasibul Jamil","Abdul Alim","Laurent Schares","Pavlos Maniotis","Liran Schour","Ali Sydney","Abdullah Kayi","Tevfik Kosar","Bengi Karacali"],"pdf_url":"https://arxiv.org/pdf/2410.17078v2.pdf","comment":"Submitted for peer reviewing in IEEE ICC 2025"},{"id":"http://arxiv.org/abs/2410.04168v3","updated":"2024-10-24T09:36:26Z","published":"2024-10-05T14:14:08Z","title":"R-ACP: Real-Time Adaptive Collaborative Perception Leveraging Robust\n  Task-Oriented Communications","summary":"  Collaborative perception enhances sensing in multi-robot and vehicular\nnetworks by fusing information from multiple agents, improving perception\naccuracy and sensing range. However, mobility and non-rigid sensor mounts\nintroduce extrinsic calibration errors, necessitating online calibration,\nfurther complicated by limited overlap in sensing regions. Moreover,\nmaintaining fresh information is crucial for timely and accurate sensing. To\naddress calibration errors and ensure timely and accurate perception, we\npropose a robust task-oriented communication strategy to optimize online\nself-calibration and efficient feature sharing for Real-time Adaptive\nCollaborative Perception (R-ACP). Specifically, we first formulate an Age of\nPerceived Targets (AoPT) minimization problem to capture data timeliness of\nmulti-view streaming. Then, in the calibration phase, we introduce a\nchannel-aware self-calibration technique based on re-identification (Re-ID),\nwhich adaptively compresses key features according to channel capacities,\neffectively addressing calibration issues via spatial and temporal cross-camera\ncorrelations. In the streaming phase, we tackle the trade-off between bandwidth\nand inference accuracy by leveraging an Information Bottleneck (IB) based\nencoding method to adjust video compression rates based on task relevance,\nthereby reducing communication overhead and latency. Finally, we design a\npriority-aware network to filter corrupted features to mitigate performance\ndegradation from packet corruption. Extensive studies demonstrate that our\nframework outperforms five baselines, improving multiple object detection\naccuracy (MODA) by 25.49% and reducing communication costs by 51.36% under\nseverely poor channel conditions.\n","authors":["Zhengru Fang","Jingjing Wang","Yanan Ma","Yihang Tao","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2410.04168v3.pdf","comment":null}],"Neural and Evolutionary Computing":[{"id":"http://arxiv.org/abs/2410.16677v2","updated":"2024-10-24T01:09:59Z","published":"2024-10-22T04:20:51Z","title":"The Neuromorphic Analog Electronic Nose","summary":"  Rapid detection of gas concentration is important in different domains like\ngas leakage monitoring, pollution control, and so on, for the prevention of\nhealth hazards. Out of different types of gas sensors, Metal oxide (MOx)\nsensors are extensively used in such applications because of their portability,\nlow cost, and high sensitivity for specific gases. However, how to effectively\nsample the MOx data for the real-time detection of gas and its concentration\nlevel remains an open question. Here we introduce a simple analog front-end for\none MOx sensor that encodes the gas concentration in the time difference\nbetween pulses of two separate pathways. This front-end design is inspired by\nthe spiking output of a mammalian olfactory bulb. We show that for a gas pulse\ninjected in a constant airflow, the time difference between pulses decreases\nwith increasing gas concentration, similar to the spike time difference between\nthe two principal output neurons in the olfactory bulb. The circuit design is\nfurther extended to a MOx sensor array and this sensor array front-end was\ntested in the same environment for gas identification and concentration\nestimation. Encoding of gas stimulus features in analog spikes at the sensor\nlevel itself may result in data and power-efficient real-time gas sensing\nsystems in the future that can ultimately be used in uncontrolled and turbulent\nenvironments for longer periods without data explosion.\n","authors":["Shavika Rastogi","Nik Dennler","Michael Schmuker","André van Schaik"],"pdf_url":"https://arxiv.org/pdf/2410.16677v2.pdf","comment":null}]}}